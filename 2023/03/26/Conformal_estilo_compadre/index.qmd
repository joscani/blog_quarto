---
title: Conformal prediction. Estilo compadre
date: '2023-03-26'
categories: 
  - Estadística
  - R
  - 2023
execute: 
  message: false
  warning: false
  echo: true
format: 
  html: 
    fig-height: 5
    fig-dpi: 300
    fig-width: 8.88
    fig-align: center
    code-fold: show
    code-summary: "Mostrar / ocultar código"
knitr:
  opts_chunk:
    out.width: 80%
    fig.showtext: TRUE
    collapse: true
    comment: "#>"
image: ""
---

## Intro

El jueves pasado asistí al más que recomendable meetup de [PyData
Madrid](https://www.meetup.com/es-ES/pydata-madrid/events/292228851/),
que cuenta entre sus organizadores con el gran [Juan Luis Cano
Rodríguez](https://www.meetup.com/es-ES/pydata-madrid/?_locale=es-ES),
antiguo compañero mío de curro y tocayo de iniciales.

El caso es que en una de las charlas, [Ignacio
Peletier](https://www.linkedin.com/in/ignacio-peletier/), mencionó de
pasada lo del "Conformal Prediction". Y siendo que Ignacio es un gran
científico de datos, y que hacía unos meses que había tenido varias
charlas con [Carlos](https://www.datanalytics.com/) sobre el
particular, pues he decidido ver un poco más en detalle de qué iba el
asunto .

## Recursos

Un excelente sitio para empezar a bichear con este tema es el Readme de
este [repo](https://github.com/valeman/awesome-conformal-prediction),
dónde han ido recopilando enlaces a libros, posts, papers y código
relativo a lo de conformal prediction.

En particular, uno de los recursos que me ha gustado es este
[minicurso](https://mindfulmodeler.substack.com/p/e-mail-course-on-conformal-prediction)
de Christoph Molnar.

Otro recurso útil es este
[post](https://www.datanalytics.com/2023/03/02/prediccion-conforme/) de
Carlos, dónde se esboza un poco en qué consiste esto de la predicción
conforme y en por qué no es algo tan novedoso como se cree.

## Experimentando

La predicción conforme se puede aplicar tanto a modelos de regresión de
clasificación. Su objetivo es *simplemente* medir la incertidumbre de
una predicción dada.

En el caso de regresión no tiene mucho misterio:

-   Se entrena un modelo usando un conjunto de datos de *entrenamiento*.

-   Se mide el error en un conjunto de datos de *validación*,
    *calibración*, utilizando la norma L1, es decir
    $\mid y - \hat{y}\mid$

-   Se elige una medida de dispersión del error, por ejemplo el cuantil
    $(1- \alpha) = 0.95$ de los errores anteriores.

-   Para una nueva predicción se da su intervalo como
    $(\hat{y} - q_{1-\alpha}, \hat{y} + q_{1-\alpha})$

En el caso de clasificación la cosa es más divertida. Puesto que lo que
se quiere obtener es un conjunto de etiquetas probables. Tipo {A} {A, B}
{B, C}

En este caso según he leído
[aquí](https://mindfulmodeler.substack.com/p/week-1-getting-started-with-conformal)
el algoritmo sería

-   Se entrena un modelo usando un conjunto de datos de *entrenamiento*.

-   Se mide el error en un conjunto de datos de *validación*,
    *calibración*, viendo para cada observación el valor que el modelo
    le ha dado para la predicción de la clase verdadera. Es un conjunto
    de validación , sabemos cuál es la verdad. Y se calcula el error
    como $1- p_{i}$ siendo $p_i$, la probabilidad predicha para la clase
    verdadera

-   Se calcula el cuantil de orden $1-\alpha$ de esos *errores* y se
    guarda. Se entiende que el modelo está bien calibrado y que el
    conjunto de validación y que los *scores* que da el modelo se pueden
    asumir como probabilidades

-   Para una nueva predicción se tendrá una $p_i$ para cada clase. Se
    calcula $1-p_i$ para cada clase y se considera que esa clase forma
    parte del *prediction set* si ese valor es menor o igual que el
    valor del cuantil anterior.

Pues vamos a ver como se haría con R en estilo compadre, y puede que con
alguna pequeña modificación por mi parte.

### Ejemplo

```{r}
library(tidyverse)
library(MASS)
```

Vamos a usar el conjunto de datos `housing` 
![Help housing](housing_help.png)

```{r}

skimr::skim(housing)

```


Y vamos a justar un modelito tonto usando regresión logística ordinal, sobre los 40 primeros datos


```{r}

house.plr <- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing[1:40,])

head(predict(house.plr, type = "probs"))

```

Guardamos las predicciones para el conjunto de validación , que va a ser las filas de la 41 a la 55, junto con el valor de `Sat` verdadero


```{r}

predictions <- as.data.frame(predict(house.plr, type = "probs", newdata = housing[41:55,]))

tt <- cbind(predictions, True_class=housing$Sat[41:55])

tt

```

Ahora, para la primera fila sería hacer (1-0.2524), puesto que la clase real es "Medium" y para la segunda sería (1-0.44), puesto que la clase real es "High". No estoy muy inspirado hoy y no he conseguido una forma elegante de hacerlo en R, y ChatGpt no me ha servido de mucha ayuda, seguramente porque aún no soy muy ducho preguntándole. 

Así que he tirado iterando para cada fila con un map y quedándode con el valor predicho de la columna cuyo nombre coincida con el valor en `True_class`

```{r}

tt$prob_true_class <- map_dbl(1:nrow(tt), .f = function(i) 
    tt[i, colnames(tt) == tt$True_class[i]])

tt$resid <- 1-tt$prob_true_class

head(tt)
```

Definimos un $\alpha = 0.3$ y calculamos el cuantil 70 . 

```{r}
(qhat = quantile(tt$resid, 0.7))
```


Y ya estamos listos para hacer la *predicción conforme* para nuevos datos. 


```{r}
# predecimos de la fila 51 a la 70 
predicciones <- predict(house.plr, newdata = housing[51:70,], type  = "probs")

head(predicciones)

```

Nos creamos un data.frame que indique si el valor de 1 - predicciones es menor o igual que el cuantil elegido

```{r}
set <- as.data.frame(1 - predicciones <= qhat)

head(set)

```


Al igual que antes, utilizo un map para obtener el conjunto de etiquetas, para la primera fila serían todas, para la segunda sería {"Medium", "High"}


```{r}

set$conformal <-  map_chr(1:nrow(set), .f= function(i) {
     set_list = colnames(set)[unlist(set[i,])]
     paste0(set_list, collapse = ",")
     })

head(set)

```


Se lo pego al dataset original de test (filas 51  a 70), junto con las predicciones y la clase verdadera.


```{r}

set_fin <-  cbind( True_class = housing$Sat[51:70], as.data.frame(predicciones),
                  set_conformal =set$conformal)

head(set_fin)

```

Y ya estaría. 

Una cosa que se suele calcular es la cobertura de cada clase, es decir, la proporción de veces que cada clase  está dentro del conjunto. 


```{r}
set_fin <- set_fin |> 
    mutate(
        class_in_set = map2_lgl(.x = True_class,
                               .y = set_conformal , 
                               ~ .x %in%  unlist(str_split(.y,",")))
    )

head(set_fin)
```


```{r}
set_fin |> 
    group_by(True_class) |> 
    summarise(cov = mean(class_in_set))

```



### Modificación 1. 

No me convence lo de tener un sólo cuantil, común a todas las clases, ¿no sería mejor tener una medida de cómo se distribuyen los errores para cada una de las clases?


Usamos el conjunto de *validación* dónde tenemos el $1-p_i$ que nos dice en cuánto se ha equivocado el modelo en predecir la clase real

```{r}
head(tt)
```

Calculamos el quantil 70 para cada clase, y así vemos que varía por clase

```{r}
(qhat_by_class <- tt |> 
    group_by(True_class) |> 
    summarise(qhat = quantile(resid, 0.7)) |> 
        pivot_wider(names_from = True_class, values_from = qhat))



```


```{r}
predicciones <- predict(house.plr, newdata = housing[51:70,], type  = "probs")
complementarios <- 1-predicciones
head(complementarios)
```

Y vemos si cada $1-p_i$ es menor o igual que el cuantil correspondiente de cada clase

```{r}
set_adjust <- data.frame(Low = complementarios[,1] <= qhat_by_class$Low,
                        Medium = complementarios[,2] <= qhat_by_class$Medium,
                         High = complementarios[,3] <= qhat_by_class$High )


head(set_adjust)

```


```{r}
set_adjust$conformal <-  map_chr(1:nrow(set_adjust), .f= function(i) {
    set_list = colnames(set_adjust)[unlist(set_adjust[i,])]
    paste0(set_list, collapse = ",")
})

head(set_adjust)
```

Como antes, nos quedamos con la clase de verdad, la predicción en probabilidad de cada clase y la predicción conforme

```{r}
set_adjust_fin <-  cbind( True_class = housing$Sat[51:70], as.data.frame(predict(house.plr, newdata = housing[51:70,],type="probs")),
                   set_conformal =set_adjust$conformal)

head(set_adjust_fin)
```


```{r}
set_adjust_fin <- set_adjust_fin |> 
    mutate(
        class_in_set = map2_lgl(.x = True_class,
                                .y = set_conformal , 
                                ~ .x %in%  unlist(str_split(.y,",")))
    )

set_adjust_fin
```

Y aquí ya vemos que la cobertura es distinta y que la clase "High" ya no está en el 100% de los *prediction sets*

```{r}
set_adjust_fin |> 
    group_by(True_class) |> 
    summarise(cov = mean(class_in_set))

```

De hecho si tabulamos ambas predicciones conformes , vemos que de las 10 predicciones  que el primer método ponía como {Low, Medium, High} , el segundo  pone 7 como {Low, Medium }  y 3 como {Low, Medium, High}

```{r}

table(set_fin$set_conformal, set_adjust_fin$set_conformal)
```




### Modificación 2. 

Vale, todo esto está muy bien, pero ¿y si simplemente para cada observación ordeno de forma decreciente su probabilidad predicha y me quedo con las clases que lleguen al 60% de probabilidad, por ejemplo? 


```{r}

(predicciones_df <-  as.data.frame(predicciones ))

```


```{r}
modificacion_2 <- predicciones_df |> 
     rownames_to_column(var = "individuo") |> 
    pivot_longer(cols = Low:High) |> 
    group_by(individuo) |> 
    arrange( desc(value)) |> 
    mutate(suma_acumulada = cumsum(value)) |> 
    arrange(individuo)

head(modificacion_2, 10)

```

Uhmm, pero no me acaba de convencer ordenar de forma descendente por la probabilidad predicha de cada clase. Por ejemplo para el individuo 51, si tomo Low +High llegaría a 0.74, pero si tomo Low + Medium llego al 67% . Si quisiera el menor conjunto de etiquetas que lleguen como mínimo al 60% la opción buena sería Low + Medium para ese individuo. 

No me veo con ganas de implementar todas las posibles sumas de probabilidades estimadas y elegir el conjunto que cumpla la restricción de llegar al menos al 60% y si hay varios para mismo individuos que se quede con el conjunto más pequeño.



## Conclusión. 

* Lo de la predicción conforme para el caso de regresión me parece bastante sencillo, no es más que sumar y restar una medida de dispersión de los residuos a la predicción para nuevos datos.  

 * Para clasificación es un poco más interesante, sobre todo para casos en los que el usuario quiere una etiqueta o etiquetas y no se conforma con las probabilidades predichas de cada clase. 
 
 
* Subyace la hipótesis de que los scores del modelo están bien calibrados y reflejan la verdadera probabilidad. 


Pues nada más, tengan un feliz día. 

[
  {
    "objectID": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "href": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "title": "EPA, muestreo y partial pooling",
    "section": "",
    "text": "Sale la EPA a a finales de Abril, con datos de Enero a Marzo. Es proverbial el retraso en la publicación de resultados por parte de las administraciones públicas. En intercambio de tweets con Carlos Gil, comentaba la posibilidad de ir actualizando datos poco a poco, en plan como las elecciones: - Al 20% del escrutinio de la EPA, el número de parados y ocupados en España es de X y cosas así.\nGracias a que la EPA tiene un buen diseño muestral no sería tan difícil hacerlo, e incluso realizar buensa estimaciones con poco escrutado, al fin y al cabo la epa es un panel,(los sujetos permanecen en la EPA varias oleadas) y es de suponer que hay cierta relación entre la variable latente “estar en paro” y que se trate del mismo individuo, y relación de esa variable con los de determinado grupo de edad al que pertenece, y que la estimación en otros grupos de edad ayude a estimar la tasa de paro en otro grupo, etc…. En fin, que me lío.\nPues de toda esa estructura e información compartida es de lo que van, grosso modo, los modelos mixtos y el partial pooling. En este mismo blog los he comentado alguna vez y he puesto algún ejemplo. Con la EPA hice hace unos años un ejercicio para ver precisamente cómo, con poca muestra, se pueden tener buenas estimaciones. Aquí os lo dejo, al final hay algunas referencias, justo las que usé (no me gusta poner referencias de cosas que no he leído solo por rellenar).\nPues nada, buen confinamiento, yo voy a ver si instalo la nueva versión de R en mi linux, dudo entre arriesgarme y hacerlo a pelo o usar un docker."
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "",
    "text": "Y seguimos dando vueltas a los datos de post anteriores. Siempre hay quien dice que el bayesiano no sirve para big data y qué se acaba el universo antes de que termine de ajustar tu modelo (esto último creo que se lo he dicho yo alguna vez a Carlos).\nPero ya hemos visto en los dos post anteriores que podemos condensar los datos en menos filas sin perder información, así que , ¿por qué no utilizar un modelo bayesiano?\nDel post anterior\nY tenemos nuestros conjuntos de train y de test en local"
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "Modelo bayesiano.",
    "text": "Modelo bayesiano.\nPues ahora vamos a probar a hacer un modelo bayesiano jerárquico, podríamos hacer el equivalente a glmer usando la librería rstanarm y ajustar varias regresiones logísticas independientes, pero en vez de eso vamos a ver como ajustar directamente la distribución multinomial usando brms.\nLos modelos serían algo así como\n\\[\n\\begin{equation} ans \\sim Multinomial(\\boldsymbol{\\theta}) \\end{equation}\n\\]\nDónde\n\\[\n\\begin{equation}\n\\boldsymbol{\\theta} = \\{\\theta_{Rec}, \\theta_{Best}, \\theta_{Neut}, \\theta_{\\text{No_way}}\\}\n\\end{equation}\n\\]\nLo bueno de stan y de brms es que se puede modelar directamente la Multinomial, es decir, el número de “éxitos” en cada categoría dado un número de intentos. En brms podemos usar trials para especificarlo. Sería el equivalente al weights en glmer. De esta forma podemos trabajar con los datos agregados en vez de tenerlos individuales. Si tengo, 1000 clientes con edad < 21 y valor_cliente = 8, en vez de poner 1000 filas, pongo una columna de frecuencias, que es lo que hemos hecho.\n\nLibrerías\nYo uso cmdstan como backend para brms en vez de rstan, está más actualizado y tarda menos en muestrear.\n\n# Core libraries\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(cmdstanr)\n\n# For beauty plots\nlibrary(ggridges)\n\n## Using all cores. 12 in my machine\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n\n\n\nAdecuando los datos\nPara poder ajustar el modelo de regresión multinomial se necesita tener los datos de una determinada forma, básicamente tener una columna de tipo matriz. Para eso vamos a pivotar los datos y usar cbind\nPivotamos\n\ntrain_wider <-   train_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)), \n    total = Rec + Neut + Best + No_way\n  ) \n\ntest_wider <- test_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)),\n    total = Rec + Neut + Best + No_way\n  )\n\n\nDT::datatable(train_wider)\n\n\n\n\n\n\nY ahora unimos las columnas que indican el conteo en cada perfil de Rec, Best, Neut y NoWay en un columna que es una matriz\n\n# lo hacemos solo para el train, para el test no hace falta\n\ntrain_wider$cell_counts <- with(train_wider, cbind(Rec, Best, Neut, No_way))\nclass(train_wider$cell_counts)\n#> [1] \"matrix\" \"array\"\n\n\nDT::datatable( train_wider %>% \n                 select(tipo, valor_cliente,\n                        cell_counts, everything()\n))\n\n\n\n\n\n\nPues ya podemos ajustar el modelo. Brms tiene una función get_prior para poner las priors por defecto.\nVoy a usar un modelo con efectos aleatorios que tarda unos pocos minutos, pero si usamos cell_counts | trials(total) ~ edad_cat + valor_cliente el modelo se ajusta en menos de 60 segundos. Bueno, vamos a verlo\n\n\nAjuste de los modelos\nModelo efectos fijos\n\nformula_efectos_fijos <- brmsformula(\n  cell_counts | trials(total) ~ edad_cat + valor_cliente\n)\n\n# get priors\npriors <- get_prior(formula_efectos_fijos, train_wider, family = multinomial())\n\ntictoc::tic(\"Modelo efectos fijos\")\nmodel_multinomial1 <- brm(formula_efectos_fijos, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\",\n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 2 finished in 47.9 seconds.\n#> Chain 3 finished in 48.8 seconds.\n#> Chain 1 finished in 49.5 seconds.\n#> Chain 4 finished in 53.0 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 49.8 seconds.\n#> Total execution time: 53.1 seconds.\ntictoc::toc()\n#> Modelo efectos fijos: 70.554 sec elapsed\n\nModelo con efectos aleatorios\nY tarda unos 9 minutos o así\n\nformula <- brmsformula(\n  cell_counts | trials(total) ~ (1|edad_cat) + (1|valor_cliente\n))\n\n# get priors\npriors <- get_prior(formula, train_wider, family = multinomial())\n\nPodemos ver las priors que ha considerado por defecto. Y vemos las priors que ha tomado para modelar la distribución de las \\(\\sigma\\) asociadas a edad_cat y valor_cliente\n\npriors\n#>                 prior     class      coef         group resp    dpar nlpar lb\n#>                (flat) Intercept                                              \n#>  student_t(3, 0, 2.5) Intercept                               muBest         \n#>  student_t(3, 0, 2.5)        sd                               muBest        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5) Intercept                               muNeut         \n#>  student_t(3, 0, 2.5)        sd                               muNeut        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5) Intercept                              muNoway         \n#>  student_t(3, 0, 2.5)        sd                              muNoway        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente      muNoway        0\n#>  ub       source\n#>          default\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n\n\ntictoc::tic(\"modelo mixto\")\nmodel_multinomial2 <- brm(formula, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\", \n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 715.6 seconds.\n#> Chain 4 finished in 728.4 seconds.\n#> Chain 2 finished in 728.8 seconds.\n#> Chain 3 finished in 732.7 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 726.4 seconds.\n#> Total execution time: 732.8 seconds.\ntictoc::toc()\n#> modelo mixto: 755.055 sec elapsed\n\nPodemos ver el modelo con\n\nsummary(model_multinomial2)\n#>  Family: multinomial \n#>   Links: muBest = logit; muNeut = logit; muNoway = logit \n#> Formula: cell_counts | trials(total) ~ (1 | edad_cat) + (1 | valor_cliente) \n#>    Data: train_wider (Number of observations: 184) \n#>   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 12000\n#> \n#> Group-Level Effects: \n#> ~edad_cat (Number of levels: 5) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.95      0.45     0.43     2.14 1.00     2460\n#> sd(muNeut_Intercept)      0.55      0.31     0.23     1.40 1.00     2566\n#> sd(muNoway_Intercept)     0.55      0.29     0.24     1.38 1.00     2237\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      4370\n#> sd(muNeut_Intercept)      4061\n#> sd(muNoway_Intercept)     3810\n#> \n#> ~valor_cliente (Number of levels: 10) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.98      0.30     0.58     1.74 1.00     1792\n#> sd(muNeut_Intercept)      0.53      0.16     0.31     0.94 1.00     1841\n#> sd(muNoway_Intercept)     1.71      0.44     1.07     2.78 1.00     1681\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      3351\n#> sd(muNeut_Intercept)      3528\n#> sd(muNoway_Intercept)     2940\n#> \n#> Population-Level Effects: \n#>                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> muBest_Intercept     -0.05      0.55    -1.15     1.01 1.00     1209     2238\n#> muNeut_Intercept      1.02      0.33     0.35     1.67 1.01     1036     2136\n#> muNoway_Intercept     0.68      0.60    -0.55     1.85 1.00      907     1612\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPintarlo\n\nplot(model_multinomial2, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE incluso ver el modelo en stan\n\nmodel_multinomial2$model\n#> // generated with brms 2.18.0\n#> functions {\n#>   /* multinomial-logit log-PMF\n#>    * Args:\n#>    *   y: array of integer response values\n#>    *   mu: vector of category logit probabilities\n#>    * Returns:\n#>    *   a scalar to be added to the log posterior\n#>    */\n#>   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#>     return multinomial_lpmf(y | softmax(mu));\n#>   }\n#> }\n#> data {\n#>   int<lower=1> N; // total number of observations\n#>   int<lower=2> ncat; // number of categories\n#>   array[N, ncat] int Y; // response array\n#>   array[N] int trials; // number of trials\n#>   // data for group-level effects of ID 1\n#>   int<lower=1> N_1; // number of grouping levels\n#>   int<lower=1> M_1; // number of coefficients per level\n#>   array[N] int<lower=1> J_1; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_1_muBest_1;\n#>   // data for group-level effects of ID 2\n#>   int<lower=1> N_2; // number of grouping levels\n#>   int<lower=1> M_2; // number of coefficients per level\n#>   array[N] int<lower=1> J_2; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_2_muBest_1;\n#>   // data for group-level effects of ID 3\n#>   int<lower=1> N_3; // number of grouping levels\n#>   int<lower=1> M_3; // number of coefficients per level\n#>   array[N] int<lower=1> J_3; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_3_muNeut_1;\n#>   // data for group-level effects of ID 4\n#>   int<lower=1> N_4; // number of grouping levels\n#>   int<lower=1> M_4; // number of coefficients per level\n#>   array[N] int<lower=1> J_4; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_4_muNeut_1;\n#>   // data for group-level effects of ID 5\n#>   int<lower=1> N_5; // number of grouping levels\n#>   int<lower=1> M_5; // number of coefficients per level\n#>   array[N] int<lower=1> J_5; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_5_muNoway_1;\n#>   // data for group-level effects of ID 6\n#>   int<lower=1> N_6; // number of grouping levels\n#>   int<lower=1> M_6; // number of coefficients per level\n#>   array[N] int<lower=1> J_6; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_6_muNoway_1;\n#>   int prior_only; // should the likelihood be ignored?\n#> }\n#> transformed data {\n#>   \n#> }\n#> parameters {\n#>   real Intercept_muBest; // temporary intercept for centered predictors\n#>   real Intercept_muNeut; // temporary intercept for centered predictors\n#>   real Intercept_muNoway; // temporary intercept for centered predictors\n#>   vector<lower=0>[M_1] sd_1; // group-level standard deviations\n#>   array[M_1] vector[N_1] z_1; // standardized group-level effects\n#>   vector<lower=0>[M_2] sd_2; // group-level standard deviations\n#>   array[M_2] vector[N_2] z_2; // standardized group-level effects\n#>   vector<lower=0>[M_3] sd_3; // group-level standard deviations\n#>   array[M_3] vector[N_3] z_3; // standardized group-level effects\n#>   vector<lower=0>[M_4] sd_4; // group-level standard deviations\n#>   array[M_4] vector[N_4] z_4; // standardized group-level effects\n#>   vector<lower=0>[M_5] sd_5; // group-level standard deviations\n#>   array[M_5] vector[N_5] z_5; // standardized group-level effects\n#>   vector<lower=0>[M_6] sd_6; // group-level standard deviations\n#>   array[M_6] vector[N_6] z_6; // standardized group-level effects\n#> }\n#> transformed parameters {\n#>   vector[N_1] r_1_muBest_1; // actual group-level effects\n#>   vector[N_2] r_2_muBest_1; // actual group-level effects\n#>   vector[N_3] r_3_muNeut_1; // actual group-level effects\n#>   vector[N_4] r_4_muNeut_1; // actual group-level effects\n#>   vector[N_5] r_5_muNoway_1; // actual group-level effects\n#>   vector[N_6] r_6_muNoway_1; // actual group-level effects\n#>   real lprior = 0; // prior contributions to the log posterior\n#>   r_1_muBest_1 = sd_1[1] * z_1[1];\n#>   r_2_muBest_1 = sd_2[1] * z_2[1];\n#>   r_3_muNeut_1 = sd_3[1] * z_3[1];\n#>   r_4_muNeut_1 = sd_4[1] * z_4[1];\n#>   r_5_muNoway_1 = sd_5[1] * z_5[1];\n#>   r_6_muNoway_1 = sd_6[1] * z_6[1];\n#>   lprior += student_t_lpdf(Intercept_muBest | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNeut | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNoway | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_5 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_6 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] muBest = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNeut = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNoway = rep_vector(0.0, N);\n#>     // linear predictor matrix\n#>     array[N] vector[ncat] mu;\n#>     muBest += Intercept_muBest;\n#>     muNeut += Intercept_muNeut;\n#>     muNoway += Intercept_muNoway;\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n]\n#>                    + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNeut[n] += r_3_muNeut_1[J_3[n]] * Z_3_muNeut_1[n]\n#>                    + r_4_muNeut_1[J_4[n]] * Z_4_muNeut_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNoway[n] += r_5_muNoway_1[J_5[n]] * Z_5_muNoway_1[n]\n#>                     + r_6_muNoway_1[J_6[n]] * Z_6_muNoway_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       mu[n] = transpose([0, muBest[n], muNeut[n], muNoway[n]]);\n#>     }\n#>     for (n in 1 : N) {\n#>       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#>     }\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#>   target += std_normal_lpdf(z_1[1]);\n#>   target += std_normal_lpdf(z_2[1]);\n#>   target += std_normal_lpdf(z_3[1]);\n#>   target += std_normal_lpdf(z_4[1]);\n#>   target += std_normal_lpdf(z_5[1]);\n#>   target += std_normal_lpdf(z_6[1]);\n#> }\n#> generated quantities {\n#>   // actual population-level intercept\n#>   real b_muBest_Intercept = Intercept_muBest;\n#>   // actual population-level intercept\n#>   real b_muNeut_Intercept = Intercept_muNeut;\n#>   // actual population-level intercept\n#>   real b_muNoway_Intercept = Intercept_muNoway;\n#> }\n\nViendo el código en stan que genera brms utiliza parametrización con multinomial_lpmf que toma el log de la probabilidad de la multinomial y usa softmax sobre el predictor lineal. multivariate_discrete_stan\nEn la parte de functions tiene\nreal multinomial_logit2_lpmf(int[] y, vector mu) {\n      return multinomial_lpmf(y | softmax(mu));\n  }\nY en la de model\n   for (n in 1:N) {\n      target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n     }\nY en la parte del predictor lineal mu[n] es dónde ha ido añadiendo los group levels effects.\nPor ejemplo la parte de la edad_cat para la categoría Best está en la parte de transformed parameters dónde z_1[1] se modela como normal y sd_1 como una t de student\nr_1_muBest_1 = (sd_1[1] * (z_1[1]));\nY en la parte de model va añadiendo términos al muBest que es al final el que entra en la parte de la verosimilitud.\nmuBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n] + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\nAquí añade el efecto de la edad r_1_muBest_1[J_1[n]] lo multiplica por Z_1_mubest_1[n] que es el indicador en los datos de la matriz Z para los efectos aleatorios (todo igual a 1) y luego añade el efecto de la variable valor_cliente.\nLa verdad es que eel bloque model que genera brms es un poco complicado. Imagino que genera código optimizado. Para los que quieran verlo todo con stan directamente este libro tiene un ejemplo básico\nEn brms tenemos la función make_standata que nos genera los datos tal y como se los pasa a Stan.\n\ndatos_stan <- make_standata(formula, data = train_wider, \n              family = multinomial(),\n              prior =  priors)\n\n\nnames(datos_stan)\n#>  [1] \"N\"             \"Y\"             \"trials\"        \"ncat\"         \n#>  [5] \"K_muBest\"      \"X_muBest\"      \"Z_1_muBest_1\"  \"Z_2_muBest_1\" \n#>  [9] \"K_muNeut\"      \"X_muNeut\"      \"Z_3_muNeut_1\"  \"Z_4_muNeut_1\" \n#> [13] \"K_muNoway\"     \"X_muNoway\"     \"Z_5_muNoway_1\" \"Z_6_muNoway_1\"\n#> [17] \"J_1\"           \"J_2\"           \"J_3\"           \"J_4\"          \n#> [21] \"J_5\"           \"J_6\"           \"N_1\"           \"M_1\"          \n#> [25] \"NC_1\"          \"N_2\"           \"M_2\"           \"NC_2\"         \n#> [29] \"N_3\"           \"M_3\"           \"NC_3\"          \"N_4\"          \n#> [33] \"M_4\"           \"NC_4\"          \"N_5\"           \"M_5\"          \n#> [37] \"NC_5\"          \"N_6\"           \"M_6\"           \"NC_6\"         \n#> [41] \"prior_only\"\n\n\n# datos\ndatos_stan$N\n#> [1] 184\n\n# numero de niveles edad\ndatos_stan$N_1\n#> [1] 5\n\n# numero niveles valor_cliente\ndatos_stan$N_2\n#> [1] 10\n\nEn los J_1, J_2, está codificado a que nivel de edad y valor_cliente perteneces esa fila. J_3 y J_4 es igual a J_1 y J_2. Lo repite para cada categoría de respuesta.\n\ndatos_stan$J_1\n#>   [1] 1 2 2 3 4 2 4 5 1 5 3 3 4 2 2 2 3 4 1 3 4 4 5 1 2 2 3 4 5 1 1 2 3 3 4 1 3\n#>  [38] 4 4 1 1 1 2 5 1 2 4 4 5 1 1 2 4 4 5 5 2 3 4 5 2 2 4 4 4 5 5 3 4 1 4 1 3 4\n#>  [75] 1 1 2 2 3 4 5 5 5 4 5 1 3 3 4 5 1 1 3 4 5 1 1 3 5 1 2 3 3 1 4 5 3 1 1 3 1\n#> [112] 1 2 3 3 1 2 3 1 2 2 5 3 3 2 3 5 1 4 5 3 5 5 2 4 5 1 2 2 5 1 3 3 2 1 2 4 2\n#> [149] 3 1 3 4 4 1 3 4 5 5 3 4 5 2 4 5 3 4 2 5 1 4 1 2 2 1 2 3 5 2 2 4 3 2 5 3\n\n\ndatos_stan$J_2\n#>   [1]  1  1  1  1  1  2  2  3  3  3  3  3  3  4  5  5  5  5  6  6  6  6  7  7  7\n#>  [26]  7  7  7  8  8  8  8  8  8  8  9  9  9  9 10 10  1  1  2  2  2  2  2  4  3\n#>  [51]  3  3  3  3  4  4  4  4  4  5  5  5  5  5  5  6  6  6  6  7  7  8  8  8  9\n#>  [76]  9  9  9  9  9  1  1  1  1  2  2  2  2  3  4  4  4  4  4  5  5  5  5  6  6\n#> [101]  6  6  6  7  7  8 10  1  1  1  2  2  2  2  3  3  3  3  5  6  6  7  7  7  8\n#> [126]  8  9  9  9  1  1  2  2  2  2  3  4  4  4  5  5  5  5  6  8  8  8  9  9 10\n#> [151]  1  1  1  4  4  4  6  7  7  7  8  8  8  9  2  4  3  5  6  6  7  7  1  6  9\n#> [176]  9  3  3  7 10  4 10  8 10\n\nPero yo estoy interesado en ver 2 cosas, como de bien predice sobre test y cuál es la probabilidad de cada clase condicionada a cada perfil\nPredicción\nPodemos obtener o bien todas las estimaciones o resumirlas\n\npredicciones_test <-  posterior_predict(model_multinomial2, newdata = test_wider)\n\nAquí lo que tenemos es un array de dimensiones 12000, 180, 4 . Que se corresponde a tener las 12000 estimaciones ( 4 cadenas x 3000 muestras efectivas) , para las 180 filas del conjunto de test\n\ndim(predicciones_test)\n#> [1] 12000   181     4\n\nPor ejemplo para la fila 35 de test que sería\n\ntest_wider[1,]\n#> # A tibble: 1 × 8\n#>   tipo  valor_cliente edad_cat   Rec  Best  Neut No_way total\n#>   <fct>         <dbl> <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl>\n#> 1 C                 0 21- 40     141   110   336     80   667\n\nY las predicciones (de la 1 a la 20) de las 1200\n\npredicciones_test[1:20, 1, ]\n#>       Rec Best Neut No_way\n#>  [1,]  75  153  255    184\n#>  [2,]  74  151  259    183\n#>  [3,]  78  126  239    224\n#>  [4,]  88  157  224    198\n#>  [5,]  79  148  260    180\n#>  [6,]  82  134  257    194\n#>  [7,]  61  145  250    211\n#>  [8,]  60  152  257    198\n#>  [9,]  80  132  242    213\n#> [10,]  62  136  263    206\n#> [11,]  72  151  272    172\n#> [12,]  75  133  259    200\n#> [13,]  78  150  240    199\n#> [14,]  78  129  244    216\n#> [15,]  78  137  249    203\n#> [16,]  73  136  265    193\n#> [17,]  68  142  251    206\n#> [18,]  68  157  234    208\n#> [19,]  67  143  248    209\n#> [20,]  62  180  227    198\n\nComo ahora todo es tidy voy a usar tidybayespara tener esa predicción.\n\npredicciones_tidy <- test_wider %>% \n  add_epred_draws(model_multinomial2) \n\nY se nos ha quedado un dataset muy muy grande\n\ndim(predicciones_tidy)\n#> [1] 8688000      14\n\n\nDT::datatable(predicciones_tidy %>% \n                ungroup() %>% \n                sample_n(30) %>% \n                select(edad_cat, valor_cliente,.category, .epred))\n\n\n\n\n\n\nPero si quisiéramos pintar las probabilidades estimadas tendríamos que dividir el valor predicho de cada categoría por el total de clientes en cada fila del conjunto de datos de test. Hay una forma más sencilla construyendo un conjunto de datos que tenga todas las combinaciones de edad_cat y valor_cliente y añadiendo columna totalcon valor 1.\n\n\nfake_data <- test_wider %>% \n  tidyr::expand(edad_cat, valor_cliente) %>% \n  mutate(total = 1)\n\n\ndf_pintar <-  fake_data %>% \n  add_epred_draws(model_multinomial2) %>% \n  mutate(valor_cliente = as_factor(valor_cliente))\n\nDe esta forma, al tener total = 1, el modelo devuelve la probabilidad de cada clase, si total = 13, hubiera devuelto “el reparto” de esos 13 individuos en los 4 grupos\n\nDT::datatable(df_pintar %>% \n  sample_n(30) %>% \n  select(edad_cat, valor_cliente, .category, .epred))\n\n\n\n\n\n\nAñadir las 12000 predicciones por fila ya “sólo” nos deja unos 2 millones de filas\n\ndim(df_pintar)\n#> [1] 2400000       9\n\nPintemos\nPor ejemplo si queremos ver las estimaciones que le da según la edad podemos ver la distribución posteriori de la probabilidad de cada segmento condicionada a cada grupo de edad. Salen distribuciones con varias modas debido a la variable valor_cliente que no estamos representando\n\ndf_pintar %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n             ) +\n  geom_density_ridges(scale = 0.8, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() \n\n\n\n\n\n\n\n\nSi vemos la posteriori para los clientes de mayor valor. Se ve claramente que a menor edad mayor probabilidad de pertenecer al segmento “Best” , mientras que a mayor edad es mucho más probabilidad del segmento “No_way”.\n\ndf_pintar %>%  \n  filter(valor_cliente == 0) %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Cliente valor: 0\")\n\n\n\n\n\n\n\n\nTeniendo toda la distribución podemos ver los resultados desde otro punto de vista. Por ejemplo, ver las probabilidades para los menores de 21.\n\ndf_pintar %>%  \n  filter(edad_cat %in% c(\"<21\")) %>% \n  ggplot(aes(x=.epred, y = valor_cliente, fill = .category) \n  ) +\n  geom_density_ridges(scale = 3, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Clientes menores de 21\\n Probabilidades estimadas\")\n\n\n\n\n\n\n\n\nEn fin, que se puede hacer estadística bayesiana aún con grandes volúmenes de datos, si te conviertes en lo que mi amigo Antonio llama un “artesano del dato”.\nFeliz semana"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "Series\n\nCachitos nochevieja\nPost relacionados con extracción de imágenes y análisis de subtítulos de “cachitos nochevieja”\n\n\nJulia\nPost relacionados Julia\n\n\n\nTodos los posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMétricas modelo con quarto y h2o\n\n\n\n\n\n\n2024\n\n\nquarto\n\n\nh2o\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nYa vendrán tiempos mejores\n\n\n\n\n\n\n2024\n\n\namistad\n\n\nrock\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLujuria e intervención\n\n\n\n\n\n\n2024\n\n\nestadística\n\n\nfull luxury bayes\n\n\nanálisis bayesiano\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nJornadas Hispalenses\n\n\n\n\n\n\n2024\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nNomenclatura\n\n\n\n\n\n\n2024\n\n\nestadística\n\n\ntiempos modernos\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nGuarnición de guerra\n\n\n\n\n\n\n2024\n\n\ncurro\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nNeovim\n\n\n\n\n\n\n2024\n\n\nlinux\n\n\nNeovim\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2024. Cuarta parte. The python way\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2024\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\npython\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2023. Tercera parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2024\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2023. Segunda parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2024\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2023. Primera parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2024\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nWeb-R\n\n\n\n\n\n\nr\n\n\n2023\n\n\nwebr\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nShinylive-r\n\n\n\n\n\n\nr\n\n\n2023\n\n\nshiny\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nVuelta a la facultad. SVD\n\n\n\n\n\n\n2023\n\n\nR\n\n\nÁlgebra\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nXIII Jornadas de R. Barcelona\n\n\n\n\n\n\n2023\n\n\nR\n\n\nComunidad\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nShinylive-python\n\n\n\n\n\n\npython\n\n\n2023\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte III\n\n\n\n\n\n\n2023\n\n\nestadística\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nPyrotecnia. Full luxury bayes con numpyro\n\n\n\n\n\n\npython\n\n\n2023\n\n\nnumpyro\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-análisis. Agregando encuestas\n\n\n\n\n\n\nmuestreo\n\n\n2023\n\n\nencuestas electorales\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\nJul 22, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nCategóricas a lo catboost. Pensamientos\n\n\n\n\n\n\nEstadística\n\n\ncategóricas\n\n\nR\n\n\ncatboost\n\n\n2023\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMapeando\n\n\n\n\n\n\nEstadística\n\n\ngis\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\n\n\nEstadística\n\n\nmachine learning\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nConformal prediction. Estilo compadre\n\n\n\n\n\n\nEstadística\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nArrow y S3\n\n\n\n\n\n\nbig data\n\n\nR\n\n\nC++\n\n\nS3\n\n\nAWS\n\n\n2023\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nExplicatividad no usual\n\n\n\n\n\n\nestadística\n\n\nranger\n\n\nExplicatividad\n\n\n2023\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\n\n\nestadística\n\n\nbrms\n\n\nanálisis bayesiano\n\n\n2023\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2022. Tercera parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2022. Segunda parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nConsejos para dejar spss\n\n\n\n\n\n\nestadística\n\n\nsociología\n\n\n2022\n\n\nspss\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\nJulia\n\n\nproduccion\n\n\nlinux\n\n\n2022\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\ndocker\n\n\nR\n\n\napi\n\n\n2022\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n37 min\n\n\n\n\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n2022\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\nestadística\n\n\nInvestigación operativa\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n48 min\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\nJulia\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nIO Parte 1\n\n\n\n\n\n\nestadística\n\n\npython\n\n\nR\n\n\nInvestigación operativa\n\n\nJulia\n\n\n2022\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n2022\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\nanálisis bayesiano\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n2022\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nPluralista\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\ntextmining\n\n\nocr\n\n\n2022\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2021\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCocinando\n\n\n\n\n\n\nmuestreo\n\n\n2022\n\n\nencuestas electorales\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\n\n\nestadística\n\n\nspark\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecturas para el finde\n\n\n\n\n\n\nestadística\n\n\nanálisis bayesiano\n\n\n2021\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿A dónde va Vicente?\n\n\n\n\n\n\nárboles\n\n\nciencia de datos\n\n\nh2o\n\n\nestadísticca\n\n\n2021\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\ncorrespondencias\n\n\nfactorización\n\n\nfactorial\n\n\n2021\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿A/B qué?\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\n\n\nR\n\n\npython\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\nAug 28, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\n2021\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\nsoftware\n\n\n2021\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nImputando datos. La estructura importa\n\n\n\n\n\n\nestadística\n\n\nimputación\n\n\n2021\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\n\n\nestadística\n\n\nbig data\n\n\nanálisis bayesiano\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\n\n\n2021\n\n\nR\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\n\n\nciencia de datos\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\nMar 13, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\nMar 8, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nUna colina\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\nFeb 14, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\nestadística\n\n\nlinux\n\n\npolémica\n\n\nocr\n\n\n2021\n\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nTendencias\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte II\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\nDec 30, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte I\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplillo con NMF\n\n\n\n\n\n\nestadística\n\n\ncorrespondencias\n\n\nfactorización\n\n\nnmf\n\n\n2020\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\n\n\nestadística\n\n\nempresas\n\n\nbig data\n\n\n2020\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nR 4.0.2 en amazon linux\n\n\n\n\n\n\nR\n\n\nlinux\n\n\naws\n\n\n2020\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nPredicción, Estimación y Atribución\n\n\n\n\n\n\nestadística\n\n\nciencia de datos\n\n\n2020\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\nJun 4, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFactoriales….\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nEPA, muestreo y partial pooling\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nEncuesta\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmuestreo\n\n\nencuestas\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nEstimación muy burda del número de contagios.\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 29, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nEl virus\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecciones aprendidas instalando paquetes de R\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2020\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nCosas de pandas\n\n\n\n\n\n\nR python\n\n\npython\n\n\n2020\n\n\n\n\n\n\n\n\n\nFeb 17, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFinde de cacharreo\n\n\n\n\n\n\nsoftware\n\n\nh2o\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLa fatal arrogancia\n\n\n\n\n\n\nciencia de datos\n\n\n2019\n\n\n\n\n\n\n\n\n\nNov 30, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCosas que deben cambiar\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCodificación parcial y python\n\n\n\n\n\n\n2019\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 15, 2019\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nMalditas proporciones pequeñas III\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nEl randomforest no nos deja ver el árbol\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\nciencia de datos\n\n\ntiempos modernos\n\n\n\n\n\n\n\n\n\nJul 2, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nMalditas proporciones pequeñas II\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 25, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nMalditas proporciones pequeñas I\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 24, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n¿Agile?\n\n\n\n\n\n\n2019\n\n\nciencia de datos\n\n\nagile\n\n\n\n\n\n\n\n\n\nJun 16, 2019\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAgua con gas\n\n\n\n\n\n\nh2o\n\n\n2019\n\n\nbigdata\n\n\n\n\n\n\n\n\n\nJun 7, 2019\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nJornadas de usuarios de R (y ya van 11)\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nmachine learning\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 4, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nBurbuja o no burbuja, esa es la cuestión\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\ntiempos modernos\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 1, 2019\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 20, 2024\n\n\nMétricas modelo con quarto y h2o\n\n\n\n\nApr 12, 2024\n\n\nYa vendrán tiempos mejores\n\n\n\n\nMar 31, 2024\n\n\nLujuria e intervención\n\n\n\n\nMar 18, 2024\n\n\nJornadas Hispalenses\n\n\n\n\nFeb 24, 2024\n\n\nNomenclatura\n\n\n\n\nFeb 16, 2024\n\n\nGuarnición de guerra\n\n\n\n\nJan 13, 2024\n\n\nNeovim\n\n\n\n\nJan 9, 2024\n\n\nCachitos 2024. Cuarta parte. The python way\n\n\n\n\nJan 8, 2024\n\n\nCachitos 2023. Tercera parte\n\n\n\n\nJan 4, 2024\n\n\nCachitos 2023. Segunda parte\n\n\n\n\nJan 2, 2024\n\n\nCachitos 2023. Primera parte\n\n\n\n\nDec 17, 2023\n\n\nWeb-R\n\n\n\n\nDec 16, 2023\n\n\nShinylive-r\n\n\n\n\nDec 3, 2023\n\n\nVuelta a la facultad. SVD\n\n\n\n\nNov 19, 2023\n\n\nXIII Jornadas de R. Barcelona\n\n\n\n\nSep 29, 2023\n\n\nShinylive-python\n\n\n\n\nSep 9, 2023\n\n\n¿Y si … ? Parte III\n\n\n\n\nJul 27, 2023\n\n\nPyrotecnia. Full luxury bayes con numpyro\n\n\n\n\nJul 22, 2023\n\n\nMeta-análisis. Agregando encuestas\n\n\n\n\nJun 9, 2023\n\n\nCategóricas a lo catboost. Pensamientos\n\n\n\n\nMay 6, 2023\n\n\nMapeando\n\n\n\n\nApr 23, 2023\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\nMar 26, 2023\n\n\nConformal prediction. Estilo compadre\n\n\n\n\nFeb 19, 2023\n\n\nArrow y S3\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\nDec 4, 2022\n\n\nConsejos para dejar spss\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 29, 2022\n\n\nLeaflet example\n\n\n\n\nOct 27, 2022\n\n\nAquí estoy de nuevo\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\nDec 30, 2020\n\n\n¿Y si … ? Parte II\n\n\n\n\nNov 15, 2020\n\n\n¿Y si … ? Parte I\n\n\n\n\nOct 21, 2020\n\n\nEjemplillo con NMF\n\n\n\n\nOct 18, 2020\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\nOct 15, 2020\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\nAug 20, 2020\n\n\nR 4.0.2 en amazon linux\n\n\n\n\nJun 11, 2020\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\nJun 7, 2020\n\n\nPredicción, Estimación y Atribución\n\n\n\n\nJun 4, 2020\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\nJun 2, 2020\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\nMay 24, 2020\n\n\nFactoriales….\n\n\n\n\nApr 28, 2020\n\n\nEPA, muestreo y partial pooling\n\n\n\n\nApr 8, 2020\n\n\nEncuesta\n\n\n\n\nMar 29, 2020\n\n\nEstimación muy burda del número de contagios.\n\n\n\n\nMar 10, 2020\n\n\nEl virus\n\n\n\n\nMar 1, 2020\n\n\nLecciones aprendidas instalando paquetes de R\n\n\n\n\nFeb 17, 2020\n\n\nCosas de pandas\n\n\n\n\nFeb 8, 2020\n\n\nFinde de cacharreo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2021.html",
    "href": "2021.html",
    "title": "2021",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2023.html",
    "href": "2023.html",
    "title": "2023",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 17, 2023\n\n\nWeb-R\n\n\n\n\nDec 16, 2023\n\n\nShinylive-r\n\n\n\n\nDec 3, 2023\n\n\nVuelta a la facultad. SVD\n\n\n\n\nNov 19, 2023\n\n\nXIII Jornadas de R. Barcelona\n\n\n\n\nSep 29, 2023\n\n\nShinylive-python\n\n\n\n\nSep 9, 2023\n\n\n¿Y si … ? Parte III\n\n\n\n\nJul 27, 2023\n\n\nPyrotecnia. Full luxury bayes con numpyro\n\n\n\n\nJul 22, 2023\n\n\nMeta-análisis. Agregando encuestas\n\n\n\n\nJun 9, 2023\n\n\nCategóricas a lo catboost. Pensamientos\n\n\n\n\nMay 6, 2023\n\n\nMapeando\n\n\n\n\nApr 23, 2023\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\nMar 26, 2023\n\n\nConformal prediction. Estilo compadre\n\n\n\n\nFeb 19, 2023\n\n\nArrow y S3\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2020/04/08/encuesta/index.html",
    "href": "2020/04/08/encuesta/index.html",
    "title": "Encuesta",
    "section": "",
    "text": "Estudio serológico covid19\nEl muestreo, ese gran olvidado. Se trata de una herramienta muy útil cuando se quiere saber una o varias características de una población pero, por lo que sea, no es factible abordar a toda la población o porque se necesita tener una idea aproximada de dicha característica de forma rápida.\nBueno, pues si queremos saber el porcentaje y el número de personas que han tenido el coronavirus por provincia la herramienta adecuada es el muestreo. Se trata de elegir una muestra representativa a nivel provincial y hacerles test serológicos a todos los incluidos en esa muestra y así poder extrapolar los resultados al conjunto de la provincia. Por fin, el Instituto de Salud Carlos III junto con el INE van a realizar dicho estudio.\nEn todo muestreo hay una fase crucial, que es la del diseño muestral, tengo que decir que después de leer el documento técnico que me parece un muy buen diseño. Se trata de un diseño bietápico estratificado con un tamaño mínimo de 600 personas por provincia y dónde por ejemplo se van a hacer 5000 encuestas en Madrid.\nEl tamaño muestral total elegido, 30 mil hogares (unos 62 mil personas), y la implicación del INE garantizan la rigurosidad y la representatividad de la encuesta. Yo por mi parte, solo comentar que una vez se tengan los microdatos, existen formas de mejorar algo las estimaciones en áreas pequeñas, entendiendo áreas pequeñas a combinaciones de variables con poca representación en la muestra, por ejemplo, si quisieramos saber la proporción de mujeres contagiadas en Cádiz cuya edad esté entre 20 y 25 años. En ese caso, es probable que una estimación directa\n\\[ \\hat{Prop} = \\dfrac{\\text{Positivos en ese grupo}}{\\text{Total personas encuestadas en ese grupo}} \\] sea poco precisa debido a que haya caído poca muestra en ese grupo.\nPara estos casos puede ser útil la utilización de estimaciones con partial pooling, entrada blog. A colación de esto, hice un estudio hace unos años sobre como incluso con poca muestra las estimaciones de este tipo suelen arrojar mejores estimaciones, aquí"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html",
    "href": "2023/02/19/Arrow y S3/index.html",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#ejemplo-1",
    "href": "2023/02/19/Arrow y S3/index.html#ejemplo-1",
    "title": "Arrow y S3",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\nMostrar / ocultar código\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket <- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#>  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#>  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#>  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#> [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#> [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n\n\n\n\nMostrar / ocultar código\ntic()\nbucket <- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi <- open_dataset(bucket) \nremote_taxi\n#> FileSystemDataset with 158 Parquet files\n#> vendor_name: string\n#> pickup_datetime: timestamp[ms]\n#> dropoff_datetime: timestamp[ms]\n#> passenger_count: int64\n#> trip_distance: double\n#> pickup_longitude: double\n#> pickup_latitude: double\n#> rate_code: string\n#> store_and_fwd: string\n#> dropoff_longitude: double\n#> dropoff_latitude: double\n#> payment_type: string\n#> fare_amount: double\n#> extra: double\n#> mta_tax: double\n#> tip_amount: double\n#> tolls_amount: double\n#> total_amount: double\n#> improvement_surcharge: double\n#> congestion_surcharge: double\n#> pickup_location_id: int64\n#> dropoff_location_id: int64\n#> year: int32\n#> month: int32\ntoc()\n#> 6.287 sec elapsed\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero. (Viendo el htop de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\nMostrar / ocultar código\ntic()\nresult <- remote_taxi |> \n    filter(year == 2019, month == 1) |>\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n    ) |>\n    mutate(pct_shared = shared_trips / all_trips * 100) |>\n    collect()\n\nresult |> \n    print(n = 200)\n#> # A tibble: 1 × 3\n#>   all_trips shared_trips pct_shared\n#>       <int>        <int>      <dbl>\n#> 1   7667255      2094091       27.3\ntoc()\n#> 12.59 sec elapsed\n\n\nNo está mal , ¿verdad?"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#ejemplo-2",
    "href": "2023/02/19/Arrow y S3/index.html#ejemplo-2",
    "title": "Arrow y S3",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\n\nMostrar / ocultar código\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun <- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds <- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n\n\nCuántos datos\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <int>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 24.383 sec elapsed\n\n\nY usando duckdb como engine de consulta .\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    to_duckdb() %>%\n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <dbl>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 33.624 sec elapsed\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente."
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#nota",
    "href": "2023/02/19/Arrow y S3/index.html#nota",
    "title": "Arrow y S3",
    "section": "Nota",
    "text": "Nota\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta documentación\nFeliz domingo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hola a todos",
    "section": "",
    "text": "Hola a todos\nSoy José Luis Cañadas Reche, científico de datos (o estadístico que sabe algo de programación )\nMe puedes encontrar en Twitter o GitHub y en Fosstodon.\n\n\n\nBlogs: Últimos post y enlaces a blog antiguo y nuevo\n\nJornadas Hispalenses\n\n\nNomenclatura\n\n\nGuarnición de guerra\n\n\nNvim\n\n\nCachitos 2023 Cuarta parte\n\n\nCachitos 2023 Tercera parte\n\n\nCachitos 2023 Segunda parte\n\n\nCachitos 2023 Primera parte\n\n\nWeb-R\n\n\nShinylive R\n\n\nSVD. Vuelta a la facultad\n\n\nXIII Jornadas de R. Barcelona\n\n\nShinylive intento fallido \n\n\n¿Y si … ? Parte III \n\n\nPyrotecnia-1\n\n\nMeta-análisis. Agregando encuestas\n\n\nCategóricas a lo catboost\n\n\nMapeando\n\n\nRegresión cuantil con catboost\n\n\nConformal prediction. Estilo compadre\n\n\nArrow y S3\n\n\nExplicatividad no usual\n\n\nBlog nuevo\n\n\nBlog antiguo\n\n\n\nInvestigación operativa\n\nEjemplo simple\nEjemplo simple de como usar Julia, R y python para investigación operativa\n\n\nLocalización: Indios y jefes\nAsignar localizaciones según demanda y diferentes restricciones. Un problema usual"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html",
    "href": "2023/02/19/Arrow_y_s3/index.html",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#ejemplo-1",
    "href": "2023/02/19/Arrow_y_s3/index.html#ejemplo-1",
    "title": "Arrow y S3",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\nMostrar / ocultar código\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket &lt;- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#&gt;  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#&gt;  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#&gt;  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#&gt; [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#&gt; [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n\n\n\n\nMostrar / ocultar código\ntic()\nbucket &lt;- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi &lt;- open_dataset(bucket) \nremote_taxi\n#&gt; FileSystemDataset with 158 Parquet files\n#&gt; vendor_name: string\n#&gt; pickup_datetime: timestamp[ms]\n#&gt; dropoff_datetime: timestamp[ms]\n#&gt; passenger_count: int64\n#&gt; trip_distance: double\n#&gt; pickup_longitude: double\n#&gt; pickup_latitude: double\n#&gt; rate_code: string\n#&gt; store_and_fwd: string\n#&gt; dropoff_longitude: double\n#&gt; dropoff_latitude: double\n#&gt; payment_type: string\n#&gt; fare_amount: double\n#&gt; extra: double\n#&gt; mta_tax: double\n#&gt; tip_amount: double\n#&gt; tolls_amount: double\n#&gt; total_amount: double\n#&gt; improvement_surcharge: double\n#&gt; congestion_surcharge: double\n#&gt; pickup_location_id: int64\n#&gt; dropoff_location_id: int64\n#&gt; year: int32\n#&gt; month: int32\ntoc()\n#&gt; 8.916 sec elapsed\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero. (Viendo el htop de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\nMostrar / ocultar código\ntic()\nresult &lt;- remote_taxi |&gt; \n    filter(year == 2019, month == 1) |&gt;\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count &gt; 1, na.rm = TRUE)\n    ) |&gt;\n    mutate(pct_shared = shared_trips / all_trips * 100) |&gt;\n    collect()\n\nresult |&gt; \n    print(n = 200)\n#&gt; # A tibble: 1 × 3\n#&gt;   all_trips shared_trips pct_shared\n#&gt;       &lt;int&gt;        &lt;int&gt;      &lt;dbl&gt;\n#&gt; 1   7667255      2094091       27.3\ntoc()\n#&gt; 12.982 sec elapsed\n\n\nNo está mal , ¿verdad?"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#ejemplo-2",
    "href": "2023/02/19/Arrow_y_s3/index.html#ejemplo-2",
    "title": "Arrow y S3",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\n\nMostrar / ocultar código\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun &lt;- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds &lt;- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n\n\nCuántos datos\n\n\nMostrar / ocultar código\n\ntic()\nres &lt;- ds %&gt;% \n    filter(year == 2021 & month == 3)  |&gt; \n    select(year, month, day ) |&gt; \n    group_by(day) |&gt; \n    summarise(\n        n_filas = n()\n    ) |&gt; \n    collect()\n\nres |&gt; \n    arrange(day) |&gt; \n    print(n  = 10)\n#&gt; # A tibble: 30 × 2\n#&gt;      day n_filas\n#&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1     1 6260454\n#&gt;  2     2 6245312\n#&gt;  3     3 6243455\n#&gt;  4     4 6242304\n#&gt;  5     5 6241816\n#&gt;  6     6 6241089\n#&gt;  7     7 6241633\n#&gt;  8     8 6241651\n#&gt;  9    10 6240299\n#&gt; 10    11 6239817\n#&gt; # … with 20 more rows\ntoc()\n#&gt; 33.974 sec elapsed\n\n\nY usando duckdb como engine de consulta .\n\n\nMostrar / ocultar código\n\ntic()\nres &lt;- ds %&gt;% \n    filter(year == 2021 & month == 3)  |&gt; \n    select(year, month, day ) |&gt; \n    to_duckdb() %&gt;%\n    group_by(day) |&gt; \n    summarise(\n        n_filas = n()\n    ) |&gt; \n    collect()\n\nres |&gt; \n    arrange(day) |&gt; \n    print(n  = 10)\n#&gt; # A tibble: 30 × 2\n#&gt;      day n_filas\n#&gt;    &lt;int&gt;   &lt;dbl&gt;\n#&gt;  1     1 6260454\n#&gt;  2     2 6245312\n#&gt;  3     3 6243455\n#&gt;  4     4 6242304\n#&gt;  5     5 6241816\n#&gt;  6     6 6241089\n#&gt;  7     7 6241633\n#&gt;  8     8 6241651\n#&gt;  9    10 6240299\n#&gt; 10    11 6239817\n#&gt; # … with 20 more rows\ntoc()\n#&gt; 38.924 sec elapsed\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente."
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#nota",
    "href": "2023/02/19/Arrow_y_s3/index.html#nota",
    "title": "Arrow y S3",
    "section": "Nota",
    "text": "Nota\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta documentación\nFeliz domingo"
  },
  {
    "objectID": "2020/03/29/estimación-muy-burda-del-número-de-contagios/index.html",
    "href": "2020/03/29/estimación-muy-burda-del-número-de-contagios/index.html",
    "title": "Estimación muy burda del número de contagios.",
    "section": "",
    "text": "Leo por ahí estimaciones de que hay en España más de 1 millón de contagiados y la verdad es que no tengo ni idea. Pero no se me ocurre ir poniendo ese dato por ahí como verdad absoluta, como hacen algunos .\n\n\nHagamos un ejercicio simple y muy burdo, lo reconozco. Supongamos que el número de fallecidos por coronavirus está bien recogido, lo miro en mi dashboard que para eso lo hice y me dice que hoy 29 de Marzo hay un total acumulado de 6528 fallecidos.\n\n\nLos de las estimaciones de más de un millón de contagiados me dicen que usan una tasa de letalidad global del 2% ergo, le saldrían que esos 6528 se corresponderían con una población de contagiados de 326400, bastante lejos del más de un millón..\n\n\nQue si, que ya sé que desde que te contagias hasta que te recuperas pueden pasar de 6 a 22 días, pero aún así me parece que no se puede dar esa cifra tan alegremente.\n\n\nOtras estimaciones algo mas serias, también burdas, pero al menos se reconoce y se intenta medir la incertidumbre dan una cifra (si se ejecuta el modelo) entre 150 mil y 300 mil contagiados, aquí y el github con el código\n\n\nEn fin, yo voy a hacer el ejercicio de aplicar unas tasas de letalidad por edad, por ver qué sale. Ya aviso, que esto tiene la validez que tiene , aunque no creo que mucha menos que la de la empresa del millón.\n\n\nLeemos fallecidos oficiales por edad y sexo del repo de datadista (muchas gracias )\n\nlibrary(tidyverse)\n## ── Attaching packages ───────────────────────────────────────────────────────────── tidyverse 1.3.0.9000 ──\n## ✓ ggplot2 3.3.0     ✓ purrr   0.3.3\n## ✓ tibble  2.1.3     ✓ dplyr   0.8.5\n## ✓ tidyr   1.0.2     ✓ stringr 1.4.0\n## ✓ readr   1.3.1     ✓ forcats 0.5.0\n## ── Conflicts ───────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\ndat1 <- read_csv(\"https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/nacional_covid19_rango_edad.csv\")\n## Parsed with column specification:\n## cols(\n##   fecha = col_date(format = \"\"),\n##   rango_edad = col_character(),\n##   sexo = col_character(),\n##   casos_confirmados = col_double(),\n##   hospitalizados = col_double(),\n##   ingresos_uci = col_double(),\n##   fallecidos = col_double()\n## )\n\nLas tasas de letalidad que vamos a utilizar vienen de un estudio en China, estudio\n\n\n\n\n\n\nAge\n\n\n(deaths/cases)\n\n\nCFR (95% CI)\n\n\n\n\n\n\n≤ 9 years\n\n\n(0/416)\n\n\n0%\n\n\n\n\n10 to 19 years\n\n\n(1/549)\n\n\n0.18% (0.03 to 1.02%)\n\n\n\n\n20 to 49 years\n\n\n(63/19790)\n\n\n0.32% (0.25% to 0.41%)\n\n\n\n\n50 to 59 years\n\n\n(130/10,008)\n\n\n1.3% (1.1% to 1.5%)\n\n\n\n\n60 to 69. years\n\n\n(309/8583)\n\n\n3.6% (3.2% to 4.0%)\n\n\n\n\n70 to 79 years\n\n\n(312/3918)\n\n\n8.0% (7.2% to 8.9%)\n\n\n\n\n≥80 years\n\n\n(208/1408)\n\n\n14.8% (13.0% to 16.7%)\n\n\n\n\n\ntmp <- dat1 %>%\n    filter(rango_edad != \"Total\" & sexo == \"ambos\")  %>%\n    mutate(rango_edad_2 =\n               fct_collapse(rango_edad,\n                   \"20-49\" = c(\"20-29\",\"30-39\",\"40-49\"),\n                   \">= 80\"  = c(\"80 y +\",\"80-89\",\"90 y +\")\n               )\n           ) %>% \n    group_by(rango_edad_2) %>% \n    summarise(fallecidos = sum(fallecidos))\n\ntmp$tasa_letalidad <-  c(0, 0.18, 0.32, 1.3, 3.6,8, 14.8)/100\n\nY nos saldría nuestra estimación estilo compadre de esta forma\n\n(tmp <-  tmp %>% \n    mutate(contagiados_estim = fallecidos / tasa_letalidad) )\n## # A tibble: 7 x 4\n##   rango_edad_2 fallecidos tasa_letalidad contagiados_estim\n##   <fct>             <dbl>          <dbl>             <dbl>\n## 1 0-9                   0         0                   NaN \n## 2 10-19                 7         0.0018             3889.\n## 3 20-49               194         0.0032            60625 \n## 4 50-59               245         0.013             18846.\n## 5 60-69               797         0.036             22139.\n## 6 70-79              2253         0.08              28162.\n## 7 >= 80              5966         0.148             40311.\n\nY un total de contagiados de\n\nsum(tmp$contagiados_estim, na.rm=T)\n## [1] 173972.2\n\nEn fin, y oigan, si con el número de fallecidos el número de contagiados es el que dice esa empresa, no sería tan mala noticia, significaría que la letalidad es más baja de lo que dicen ciertos estudios."
  },
  {
    "objectID": "2020/03/10/el-virus/index.html",
    "href": "2020/03/10/el-virus/index.html",
    "title": "El virus",
    "section": "",
    "text": "En estos tiempos tan asépticos ya no estamos acostumbrados (en algunos países), a tratar con agentes patógenos altamente contagiosos como el que llena los titulares de periódicos y televisiones estos días.\nSin más, vamos a comparar los datos de España e Italia, plagiando con total descaro a mi amigo Carlos Gil que puso este post de ayer y en este de hoy.\nYo me acabo de enterar de que estoy en cuarentena preventiva por un posible contagio de la mujer de un compañero, así que en casita a teletrabajar unos días.\nEl código\nA pintar.\nSi quitamos 9 días a la fecha de España vemos que estamso alineados a como estaba Itala en ese día.\nY bueno, sigo plagiando a Carlos."
  },
  {
    "objectID": "2020/03/10/el-virus/index.html#mapa-en-leaflet",
    "href": "2020/03/10/el-virus/index.html#mapa-en-leaflet",
    "title": "El virus",
    "section": "\nMapa en leaflet\n",
    "text": "Mapa en leaflet\n\n\nY ahora un mapita con los datos del último día, del 9 de Marzo en el momento de escribir estas líneas\n\ncvirus_map_data <- cvirus_longer %>%\n  filter(fecha == max(fecha))\n\n\npal <- colorNumeric(\n  palette = \"Reds\",\n  domain = c(-1, log(max(cvirus_map_data$casos + 1)))\n)\n\n\nleaflet(cvirus_map_data) %>%\n  # addProviderTiles('CartoDB.Positron') %>%\n  addProviderTiles(\"Stamen.Toner\") %>%\n  addCircleMarkers(\n    lng = ~Long,\n    lat = ~Lat,\n    label = ~ paste0( país, \": \", casos ),\n    radius = ~ 3 * log( casos + 1 ) ,\n    color = ~ pal(log( casos + 1 ) )\n  ) \n\n\n\n\n\nY me he quedado con ganas de hacer un mapa con mapview del estilo de estos, mapview, pero la verdad es que entre unas cosas y otras hoy ando bastante cansado. Mañana lo hago."
  },
  {
    "objectID": "2020/03/01/lecciones-aprendidas-instalando-paquetes-de-r/index.html",
    "href": "2020/03/01/lecciones-aprendidas-instalando-paquetes-de-r/index.html",
    "title": "Lecciones aprendidas instalando paquetes de R",
    "section": "",
    "text": "Ay, la nube.. que bien suena ¿verdad? Si, hasta que te toca pelearte con amazonlinux y versiones viejunas de R. Total, que me ha tocado lidiar un poco con la versión de R 3.4.1 de hace ya 3 años y tener que compilar en mi máquina un montón de librerías para amazon linux (que viene siendo un centos 7 modificado por aws)\nAsí que lo primero es montarse un Dockerfile dónde id diciendo qué librerías de sistemas hay que añadir, y alguna ñapa por problemas con el compilador de C.\nFROM amazonlinux:2018.03-with-sources\nMAINTAINER canadasreche@gmail.com \n\n# Update yum\nRUN yum -y update \n\n# set locales\nRUN echo \"LANG=en_US.utf8\" >> /etc/locale.conf\n#RUN localedef -c -f UTF-8 -i en_US en_US.UTF-8\nRUN export LC_ALL=en_US.UTF-8\n\n\n# Install system libraries\n# El make -j 8 es para que al compilar en c use 9 jobs\nRUN export MAKE='make -j 8'\nRUN yum install -y xorg-x11-xauth.x86_64 xorg-x11-server-utils.x86_64 xterm libXt libX11-devel \\\nlibXt-devel libcurl-devel git compat-gmp4 compat-libffi5 libxml2-devel libjpeg-devel openssl-devel \\\nboost boost-devel autoconf flex bison libssh2-devel java-1.8.0-openjdk java-1.8.0-openjdk-devel \\\nfontconfig-devel cairo-devel\n\n# Development tools \nRUN yum groupinstall 'Development Tools' -y\n\n# Install and update R\nRUN yum install -y R-core R-base R-core-devel R-devel\nRUN yum update -y R-core R-base R-core-devel R-devel\n\n\n# ENV JAVA_HOME /usr/java/latest\n\n# Fix problem with c compiler\nRUN mkdir ~/.R\nRUN echo \"CC=gcc64\" >> ~/.R/Makevars\n\nCMD [\"bash\"] \nY ahora una vez que nos ponemos en el directorio dónde tenemos el dockerfile, lo construimos con\ndocker build -t amazon-linux-r .\nSi todo ha ido bien, ya tenemos nuestra imagen de docker de amazon linux con R 3.4.1 instalado.\nCreamos y entramos en un container de esa imagen dónde adjuntamos un volumen (carpeta que se va a compartir entre mi máquina y el docker)\n docker run --rm -it -v ~/Descargas/libcentosR-3.4.1:/libR amazon-linux-r /bin/bash\nY listo ya estamos preparados para instalar paquetes\nEntramos en R y lo primero que hacemos es cambiar el .libPaths , para que todo lo que instalemos se quede en la carpeta que compartimos\n\n\nMostrar / ocultar código\n.libPaths(\"/libR\")\n\n\nComo me acabo de comprar un portátil con 6 cores, establezco la variable de entorno MAKE para que el código de C se compile usando 6 jobs. Esto hará que la instalación de la mayoría de librerías vaya mucho más rápida.\n\n\nMostrar / ocultar código\nSys.setenv(MAKE = \"make -j 6\")\n\n\nComo la versión de R que hay en amazon linux es viejuna (junio de 2017) y como hubo un cambio drástico en la versión 3.5 necesitamos hacer una vuelta al pasado para tener los repos de CRAN que habia en ese momento. Para eso, en primer lugar instalamos la librería checkpoint que nos va a facilitar el trabajo. Con esta librería podemos apuntar a los repos de CRAN que había en una fecha determinada. En realidad apuntamos a un repo de microsoft que hace mirror diarios del CRAN.\n\n\nMostrar / ocultar código\ninstall.packages(\"checkpoint\")\nlibrary(checkpoint)\n\n# apuntamos justo al repo que había antes de la verión  de R 3.5\nsetSnapshot(\"2018-03-31\")\n\n\nY ahora ya podemos instalar las librerías, por ejemplo estas.\n\n\nMostrar / ocultar código\n\nlist.of.packages <- c(\n  \"BayesFactor\", \"C50\", \"car\", \"caret\", \"catboost\",\n  \"coin\", \"cowplot\", \"DALEX\", \"DALEXtra\", \"DataExplorer\", \"dqrng\",\n  \"drifter\", \"EIX\", \"emmeans\", \"factoextra\", \"FactoMineR\", \"FFTrees\",\n  \"flextable\", \"forecast\", \"gdtools\", \"ggforce\", \"ggiraph\", \"ggiraphExtra\",\n  \"ggpubr\", \"glmnet\", \"highcharter\", \"iBreakDown\", \"igraph\", \"imbalance\",\n  \"iml\", \"ingredients\", \"inum\", \"KernelKnn\", \"libcoin\", \"lime\",\n  \"lme4\", \"minqa\", \"ModelMetrics\", \"multcomp\", \"mvtnorm\", \"networkD3\",\n  \"party\", \"partykit\", \"pbkrtest\", \"plotrix\", \"prediction\", \"randomForestExplainer\",\n  \"ranger\", \"RcppArmadillo\", \"RcppEigen\", \"RMySQL\", \"RSpectra\",\n  \"sitmo\", \"sjPlot\", \"sjstats\", \"smotefamily\",\n  \"survey\", \"systemfonts\", \"threejs\", \"uwot\", \"xgb2sql\",\n  \"xgboost\", \"yarrr\", \"ztable\", \"tcltk\"\n)\n\nnew.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, \"Package\"])]\nif (length(new.packages)) install.packages(new.packages)\n\n\nY una vez que estén instadas podemos hacer una vuelta al futuro y actualizar las que podamos.\n\n\nMostrar / ocultar código\n\nsetSnapshot(\"2018-03-31\")\nupdate.packages(ask=FALSE)\n\n\nY esto es todo, parece sencillo, pero me ha quitado bastante tiempo todas estas pequeñas vicisitudes.."
  },
  {
    "objectID": "2020/02/17/cosas-de-pandas/index.html",
    "href": "2020/02/17/cosas-de-pandas/index.html",
    "title": "Cosas de pandas",
    "section": "",
    "text": "Estoy usando Rmarkdown así que primero defino que versión o entorno de python quiero\n\nSys.setenv(RETICULATE_PYTHON = \"/usr/bin/python3\")\n\n\nimport pandas as pd\ndf = pd.DataFrame({'A' : [1, 2, 3, 4], 'B' : [4, 3, 2, 1]})\ndf\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n\n\nEl index es autonumérico\n\ndf.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nPues ahora viene lo raro, al menos para mí. Si hacemos iloc O:3 me devuelve las 3 primeras filas (como debe ser)\n\ndf.iloc[0:3,]\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n\n\nPero si hacemos loc poniendo 0:3 me devuelve 4 filas !!!\n\ndf.loc[0:3, ]\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n\n\n¿Algún pythoniso que me pueda aclarar el porqué de este comportamiento?"
  },
  {
    "objectID": "2020/02/08/finde-de-cacharreo/index.html",
    "href": "2020/02/08/finde-de-cacharreo/index.html",
    "title": "Finde de cacharreo",
    "section": "",
    "text": "Bueno, pues he cambiado de portátil. Me he dado un capricho y me he pillado un slimbook prox 15 con 6 cores (12 hilos) , 32 Gb de RAM y una tarjeta gráfica nvidia de las normalitas.\nPues después de algunos (muchos) problemas con los drivers de la tarjeta nvidia en linux, más específicamente en linux mint xfce, he conseguido que todo vaya bien, y hasta he podido probar computación con gpu en R.\nHe probado h2o4gpu y también keras tirando de tensorflow, tirando de la docu de rstudio y de este pequeño tutorial para ver que al menos funcionaba.\nPues visto que ya va todo suave me dispongo a ver El gran carnaval y disfruta del sábado con el gran Kirk Douglas.\nSalud y no os flipéis mucho con el Deep Lenin, que tampoco es para tanto."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html",
    "href": "2023/01/29/explicatividad_no_usual/index.html",
    "title": "Explicatividad no usual",
    "section": "",
    "text": "Buscando en el portátil sobre otras cosas me he encontrado un pequeño ejercicio para implementar la idea que se comenta aquí\nLa idea es muy sencilla, tal y como comenta Carlos. Si tengo un modelo que sea tipo randomForest\n\nDe cada observación a predecir se anota en qué nodo terminal ha caído en cada árbol\nPara cada nodo terminal en cada árbol se recorre el camino hacia “arriba” para saber qué variables están implicadas en ese nodo\nSe cuenta para cada nodo terminal el número de veces que aparece una variable en cada árbol (o se da más importancia a las qeu estén más altos en el árbol)\nSe agrega de alguna manera para cada observación qué variables y cómo de importantes están en los caminos de los nodos terminales en los que han caído.\nEsa info sería la importancia de las variables a nivel individual\nSe podría clusterizar observaciones con similares variables importantes\n\nAntes de nada, sí, ya sé que existen cosas como los shap values y que a partir de ellos se puede hacer algo parecido. Pero no está de más de vez en cuando buscarse uno las habichueleas de forma más artesanal.."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-1",
    "href": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-1",
    "title": "Explicatividad no usual",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nPues ale, vamos a hacerlo con iris, tan denostado hoy en día. Pobre Fisher o Anderson si levantaran la cabeza.\n\n\nMostrar / ocultar código\nlibrary(tidyverse)\nlibrary(ranger)\n\n# ya veremos para que usamos furrr y FactoMineR\nlibrary(furrr) \nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\n\n\nMostrar / ocultar código\n\n# 5 arbolitos tiene mi.. \nset.seed(47)\nrg_iris &lt;-  ranger(Species ~ . , data = iris, num.trees = 5)\n\n\nInfo del árbol 3\n\n\nMostrar / ocultar código\n(arbol3 &lt;- treeInfo(rg_iris, tree = 3))\n#&gt;    nodeID leftChild rightChild splitvarID splitvarName splitval terminal\n#&gt; 1       0         1          2          0 Sepal.Length     5.55    FALSE\n#&gt; 2       1         3          4          2 Petal.Length     2.45    FALSE\n#&gt; 3       2         5          6          3  Petal.Width     1.70    FALSE\n#&gt; 4       3        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 5       4         7          8          2 Petal.Length     4.25    FALSE\n#&gt; 6       5         9         10          1  Sepal.Width     3.60    FALSE\n#&gt; 7       6        11         12          0 Sepal.Length     6.00    FALSE\n#&gt; 8       7        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 9       8        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 10      9        13         14          2 Petal.Length     4.95    FALSE\n#&gt; 11     10        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 12     11        15         16          2 Petal.Length     4.85    FALSE\n#&gt; 13     12        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 14     13        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 15     14        17         18          3  Petal.Width     1.55    FALSE\n#&gt; 16     15        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 17     16        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 18     17        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 19     18        19         20          2 Petal.Length     5.45    FALSE\n#&gt; 20     19        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 21     20        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt;    prediction\n#&gt; 1        &lt;NA&gt;\n#&gt; 2        &lt;NA&gt;\n#&gt; 3        &lt;NA&gt;\n#&gt; 4      setosa\n#&gt; 5        &lt;NA&gt;\n#&gt; 6        &lt;NA&gt;\n#&gt; 7        &lt;NA&gt;\n#&gt; 8  versicolor\n#&gt; 9   virginica\n#&gt; 10       &lt;NA&gt;\n#&gt; 11     setosa\n#&gt; 12       &lt;NA&gt;\n#&gt; 13  virginica\n#&gt; 14 versicolor\n#&gt; 15       &lt;NA&gt;\n#&gt; 16 versicolor\n#&gt; 17  virginica\n#&gt; 18  virginica\n#&gt; 19       &lt;NA&gt;\n#&gt; 20 versicolor\n#&gt; 21  virginica\n\n\nAnalizando un poco, vemos que el nodo raíz (0) se parte por la variable Sepal.Length. Luego el nodo 1 se bifurca a la izquierda hacia el 3 y a la derecha hacia el 4, siendo Petal.Length la variable que decide esa partición.\nLa idea sería recorrer el árbol partiendo de un nodo terminal y ver qué camino ha seguido. Para eso hacemos el siguiente código\nHacemos un ejemplo, partiendo del nodo terminal 8\nCamino del Nodo 8\n\n\nMostrar / ocultar código\nnodo_terminal &lt;-  8\n\nnodos &lt;- nodo_terminal # vamos a ir sobreescribiendo la variable nodos hasta llegar al nodo raíz 0 \nvariables &lt;- vector() # guardamos el nombre de las variables de split\n\n  while (!0 %in% nodos) {\n    tmp &lt;- arbol3 %&gt;%\n      filter(leftChild %in% nodos |\n        rightChild %in% nodos)\n    \n    print(str_glue(\"Nodo hijo: {nodos}\"))\n    \n    nodos &lt;- unique(tmp$nodeID)\n    print(str_glue(\"Nodo padre: {nodos}\"))\n    \n    \n    print(str_glue(\"variable de split en nodo padre: {tmp$splitvarName}\"))\n    \n    variables &lt;- c(variables, unique(tmp$splitvarName)) # la última variable de este vector es la que está más arriba en el árbol\n\n  }\n#&gt; Nodo hijo: 8\n#&gt; Nodo padre: 4\n#&gt; variable de split en nodo padre: Petal.Length\n#&gt; Nodo hijo: 4\n#&gt; Nodo padre: 1\n#&gt; variable de split en nodo padre: Petal.Length\n#&gt; Nodo hijo: 1\n#&gt; Nodo padre: 0\n#&gt; variable de split en nodo padre: Sepal.Length\n\n\nY vemos que para llegar al nodo terminal 8 ha utilizado dos veces la variable Petal.Length y una la variable Sepal.Length\nNos creamos una funcioncita para esto, donde al final construyo un data.frame donde guardo eel nodo terminal que estamos investigando, las variables que se han usado para llegar a ese nodo y una variable peso que se calcula asignando un peso igual 1 a la variable que está más alta en el árbol y menos a las demás. Si hay 4 variables se crea un vector c(4,3,2,1) en orden de más alta en el árbol a más baja y se divide por el número de variables. así se tendrían estos pesos 1, 0.75, 0.5, 0.25\n\n\nMostrar / ocultar código\n\nextraerVariables_nodos &lt;- function(nodo_terminal, info_arbol) {\n  nodos &lt;- nodo_terminal\n  variables &lt;- vector()\n\n  while (!0 %in% nodos) {\n    tmp &lt;- info_arbol %&gt;%\n      filter(leftChild %in% nodos |\n        rightChild %in% nodos)\n\n    variables &lt;- c(variables, unique(tmp$splitvarName))\n\n    nodos &lt;- unique(tmp$nodeID)\n  }\n\n  return(\n      data.frame(\n          nodo_terminal = nodo_terminal,\n          variables = variables,\n          peso = seq_along(variables) / sum(length(variables))\n      )\n  )\n}\n\n\nComprobamos\n\n\nMostrar / ocultar código\n\nextraerVariables_nodos(nodo_terminal = 8, info_arbol = arbol3)\n#&gt;   nodo_terminal    variables      peso\n#&gt; 1             8 Petal.Length 0.3333333\n#&gt; 2             8 Petal.Length 0.6666667\n#&gt; 3             8 Sepal.Length 1.0000000\n\n\nOk. Lo suyo sería extraer la misma info pero para cada nodo terminal del árbol que estamos considerando. Pues nos creamos la funcioncita, que dado un modelo y un número de árbol, saque la info anterior para todos los nodos terminales\n\n\nMostrar / ocultar código\n\nextraerVariablePorArbol &lt;- function(modelo, arbol, verbose = FALSE) {\n  \n  info_arbol &lt;- treeInfo(modelo, arbol)\n  nodos_terminales &lt;- treeInfo(modelo, arbol) %&gt;%\n    filter(terminal == TRUE) %&gt;%\n    pull(nodeID) %&gt;%\n    unique()\n if(verbose) print(nodos_terminales)\n  \n  variables_por_arbol &lt;- map_df(\n    nodos_terminales,\n    function(nodos) {\n      extraerVariables_nodos(nodos, info_arbol)\n    }\n  )\n\n  variables_por_arbol$arbol &lt;- arbol\n  variables_por_arbol\n}\n\n\nComprobemos\n\n\nMostrar / ocultar código\n# arbol 3 \n(importancia_individual_arbol3 &lt;- extraerVariablePorArbol(rg_iris, 3))\n#&gt;    nodo_terminal    variables      peso arbol\n#&gt; 1              3 Petal.Length 0.5000000     3\n#&gt; 2              3 Sepal.Length 1.0000000     3\n#&gt; 3              7 Petal.Length 0.3333333     3\n#&gt; 4              7 Petal.Length 0.6666667     3\n#&gt; 5              7 Sepal.Length 1.0000000     3\n#&gt; 6              8 Petal.Length 0.3333333     3\n#&gt; 7              8 Petal.Length 0.6666667     3\n#&gt; 8              8 Sepal.Length 1.0000000     3\n#&gt; 9             10  Sepal.Width 0.3333333     3\n#&gt; 10            10  Petal.Width 0.6666667     3\n#&gt; 11            10 Sepal.Length 1.0000000     3\n#&gt; 12            12 Sepal.Length 0.3333333     3\n#&gt; 13            12  Petal.Width 0.6666667     3\n#&gt; 14            12 Sepal.Length 1.0000000     3\n#&gt; 15            13 Petal.Length 0.2500000     3\n#&gt; 16            13  Sepal.Width 0.5000000     3\n#&gt; 17            13  Petal.Width 0.7500000     3\n#&gt; 18            13 Sepal.Length 1.0000000     3\n#&gt; 19            15 Petal.Length 0.2500000     3\n#&gt; 20            15 Sepal.Length 0.5000000     3\n#&gt; 21            15  Petal.Width 0.7500000     3\n#&gt; 22            15 Sepal.Length 1.0000000     3\n#&gt; 23            16 Petal.Length 0.2500000     3\n#&gt; 24            16 Sepal.Length 0.5000000     3\n#&gt; 25            16  Petal.Width 0.7500000     3\n#&gt; 26            16 Sepal.Length 1.0000000     3\n#&gt; 27            17  Petal.Width 0.2000000     3\n#&gt; 28            17 Petal.Length 0.4000000     3\n#&gt; 29            17  Sepal.Width 0.6000000     3\n#&gt; 30            17  Petal.Width 0.8000000     3\n#&gt; 31            17 Sepal.Length 1.0000000     3\n#&gt; 32            19 Petal.Length 0.1666667     3\n#&gt; 33            19  Petal.Width 0.3333333     3\n#&gt; 34            19 Petal.Length 0.5000000     3\n#&gt; 35            19  Sepal.Width 0.6666667     3\n#&gt; 36            19  Petal.Width 0.8333333     3\n#&gt; 37            19 Sepal.Length 1.0000000     3\n#&gt; 38            20 Petal.Length 0.1666667     3\n#&gt; 39            20  Petal.Width 0.3333333     3\n#&gt; 40            20 Petal.Length 0.5000000     3\n#&gt; 41            20  Sepal.Width 0.6666667     3\n#&gt; 42            20  Petal.Width 0.8333333     3\n#&gt; 43            20 Sepal.Length 1.0000000     3\n\n\nSolo queda extraer lo mismo pero para cada arbolito\n\n\nMostrar / ocultar código\nextraerVariablesPorModelo &lt;- function(modelo, parallel = TRUE) {\n  \n  \n  arboles &lt;- modelo$num.trees\n\n  if (parallel) {\n    # Si hay muchos árboles usamos procesamiento en paralelo\n    future::plan(multisession)\n    \n    furrr::future_map_dfr(\n      seq_len(arboles),\n      function(arbol) {\n        extraerVariablePorArbol(modelo, arbol = arbol)\n      }\n    )\n  } else{\n      map_df(\n          seq_len(arboles),\n          function(arbol) {\n              extraerVariablePorArbol(modelo, arbol = arbol)\n          }\n      )  \n  }\n}\n\n\n\n\nMostrar / ocultar código\n(importancia_individual_todos_arboles &lt;-  extraerVariablesPorModelo(rg_iris, parallel = FALSE))\n#&gt;     nodo_terminal    variables      peso arbol\n#&gt; 1               1  Petal.Width 1.0000000     1\n#&gt; 2               5 Petal.Length 0.3333333     1\n#&gt; 3               5  Petal.Width 0.6666667     1\n#&gt; 4               5  Petal.Width 1.0000000     1\n#&gt; 5               8  Petal.Width 0.3333333     1\n#&gt; 6               8  Petal.Width 0.6666667     1\n#&gt; 7               8  Petal.Width 1.0000000     1\n#&gt; 8               9  Petal.Width 0.2500000     1\n#&gt; 9               9 Petal.Length 0.5000000     1\n#&gt; 10              9  Petal.Width 0.7500000     1\n#&gt; 11              9  Petal.Width 1.0000000     1\n#&gt; 12             10  Petal.Width 0.2500000     1\n#&gt; 13             10 Petal.Length 0.5000000     1\n#&gt; 14             10  Petal.Width 0.7500000     1\n#&gt; 15             10  Petal.Width 1.0000000     1\n#&gt; 16             11 Petal.Length 0.2500000     1\n#&gt; 17             11  Petal.Width 0.5000000     1\n#&gt; 18             11  Petal.Width 0.7500000     1\n#&gt; 19             11  Petal.Width 1.0000000     1\n#&gt; 20             12 Petal.Length 0.2500000     1\n#&gt; 21             12  Petal.Width 0.5000000     1\n#&gt; 22             12  Petal.Width 0.7500000     1\n#&gt; 23             12  Petal.Width 1.0000000     1\n#&gt; 24              3  Petal.Width 0.5000000     2\n#&gt; 25              3 Sepal.Length 1.0000000     2\n#&gt; 26              4  Petal.Width 0.5000000     2\n#&gt; 27              4 Sepal.Length 1.0000000     2\n#&gt; 28              8  Sepal.Width 0.3333333     2\n#&gt; 29              8  Petal.Width 0.6666667     2\n#&gt; 30              8 Sepal.Length 1.0000000     2\n#&gt; 31             10 Sepal.Length 0.3333333     2\n#&gt; 32             10  Petal.Width 0.6666667     2\n#&gt; 33             10 Sepal.Length 1.0000000     2\n#&gt; 34             11 Petal.Length 0.2500000     2\n#&gt; 35             11  Sepal.Width 0.5000000     2\n#&gt; 36             11  Petal.Width 0.7500000     2\n#&gt; 37             11 Sepal.Length 1.0000000     2\n#&gt; 38             13  Sepal.Width 0.2500000     2\n#&gt; 39             13 Sepal.Length 0.5000000     2\n#&gt; 40             13  Petal.Width 0.7500000     2\n#&gt; 41             13 Sepal.Length 1.0000000     2\n#&gt; 42             14  Sepal.Width 0.2500000     2\n#&gt; 43             14 Sepal.Length 0.5000000     2\n#&gt; 44             14  Petal.Width 0.7500000     2\n#&gt; 45             14 Sepal.Length 1.0000000     2\n#&gt; 46             15  Sepal.Width 0.2000000     2\n#&gt; 47             15 Petal.Length 0.4000000     2\n#&gt; 48             15  Sepal.Width 0.6000000     2\n#&gt; 49             15  Petal.Width 0.8000000     2\n#&gt; 50             15 Sepal.Length 1.0000000     2\n#&gt; 51             17  Sepal.Width 0.1666667     2\n#&gt; 52             17  Sepal.Width 0.3333333     2\n#&gt; 53             17 Petal.Length 0.5000000     2\n#&gt; 54             17  Sepal.Width 0.6666667     2\n#&gt; 55             17  Petal.Width 0.8333333     2\n#&gt; 56             17 Sepal.Length 1.0000000     2\n#&gt; 57             18  Sepal.Width 0.1666667     2\n#&gt; 58             18  Sepal.Width 0.3333333     2\n#&gt; 59             18 Petal.Length 0.5000000     2\n#&gt; 60             18  Sepal.Width 0.6666667     2\n#&gt; 61             18  Petal.Width 0.8333333     2\n#&gt; 62             18 Sepal.Length 1.0000000     2\n#&gt; 63              3 Petal.Length 0.5000000     3\n#&gt; 64              3 Sepal.Length 1.0000000     3\n#&gt; 65              7 Petal.Length 0.3333333     3\n#&gt; 66              7 Petal.Length 0.6666667     3\n#&gt; 67              7 Sepal.Length 1.0000000     3\n#&gt; 68              8 Petal.Length 0.3333333     3\n#&gt; 69              8 Petal.Length 0.6666667     3\n#&gt; 70              8 Sepal.Length 1.0000000     3\n#&gt; 71             10  Sepal.Width 0.3333333     3\n#&gt; 72             10  Petal.Width 0.6666667     3\n#&gt; 73             10 Sepal.Length 1.0000000     3\n#&gt; 74             12 Sepal.Length 0.3333333     3\n#&gt; 75             12  Petal.Width 0.6666667     3\n#&gt; 76             12 Sepal.Length 1.0000000     3\n#&gt; 77             13 Petal.Length 0.2500000     3\n#&gt; 78             13  Sepal.Width 0.5000000     3\n#&gt; 79             13  Petal.Width 0.7500000     3\n#&gt; 80             13 Sepal.Length 1.0000000     3\n#&gt; 81             15 Petal.Length 0.2500000     3\n#&gt; 82             15 Sepal.Length 0.5000000     3\n#&gt; 83             15  Petal.Width 0.7500000     3\n#&gt; 84             15 Sepal.Length 1.0000000     3\n#&gt; 85             16 Petal.Length 0.2500000     3\n#&gt; 86             16 Sepal.Length 0.5000000     3\n#&gt; 87             16  Petal.Width 0.7500000     3\n#&gt; 88             16 Sepal.Length 1.0000000     3\n#&gt; 89             17  Petal.Width 0.2000000     3\n#&gt; 90             17 Petal.Length 0.4000000     3\n#&gt; 91             17  Sepal.Width 0.6000000     3\n#&gt; 92             17  Petal.Width 0.8000000     3\n#&gt; 93             17 Sepal.Length 1.0000000     3\n#&gt; 94             19 Petal.Length 0.1666667     3\n#&gt; 95             19  Petal.Width 0.3333333     3\n#&gt; 96             19 Petal.Length 0.5000000     3\n#&gt; 97             19  Sepal.Width 0.6666667     3\n#&gt; 98             19  Petal.Width 0.8333333     3\n#&gt; 99             19 Sepal.Length 1.0000000     3\n#&gt; 100            20 Petal.Length 0.1666667     3\n#&gt; 101            20  Petal.Width 0.3333333     3\n#&gt; 102            20 Petal.Length 0.5000000     3\n#&gt; 103            20  Sepal.Width 0.6666667     3\n#&gt; 104            20  Petal.Width 0.8333333     3\n#&gt; 105            20 Sepal.Length 1.0000000     3\n#&gt; 106             1  Petal.Width 1.0000000     4\n#&gt; 107             5  Petal.Width 0.3333333     4\n#&gt; 108             5 Petal.Length 0.6666667     4\n#&gt; 109             5  Petal.Width 1.0000000     4\n#&gt; 110             6  Petal.Width 0.3333333     4\n#&gt; 111             6 Petal.Length 0.6666667     4\n#&gt; 112             6  Petal.Width 1.0000000     4\n#&gt; 113             9 Sepal.Length 0.2500000     4\n#&gt; 114             9 Petal.Length 0.5000000     4\n#&gt; 115             9 Petal.Length 0.7500000     4\n#&gt; 116             9  Petal.Width 1.0000000     4\n#&gt; 117            10 Sepal.Length 0.2500000     4\n#&gt; 118            10 Petal.Length 0.5000000     4\n#&gt; 119            10 Petal.Length 0.7500000     4\n#&gt; 120            10  Petal.Width 1.0000000     4\n#&gt; 121            12  Petal.Width 0.2500000     4\n#&gt; 122            12 Petal.Length 0.5000000     4\n#&gt; 123            12 Petal.Length 0.7500000     4\n#&gt; 124            12  Petal.Width 1.0000000     4\n#&gt; 125            13 Sepal.Length 0.2000000     4\n#&gt; 126            13  Petal.Width 0.4000000     4\n#&gt; 127            13 Petal.Length 0.6000000     4\n#&gt; 128            13 Petal.Length 0.8000000     4\n#&gt; 129            13  Petal.Width 1.0000000     4\n#&gt; 130            14 Sepal.Length 0.2000000     4\n#&gt; 131            14  Petal.Width 0.4000000     4\n#&gt; 132            14 Petal.Length 0.6000000     4\n#&gt; 133            14 Petal.Length 0.8000000     4\n#&gt; 134            14  Petal.Width 1.0000000     4\n#&gt; 135             1 Petal.Length 1.0000000     5\n#&gt; 136             6  Sepal.Width 0.3333333     5\n#&gt; 137             6  Petal.Width 0.6666667     5\n#&gt; 138             6 Petal.Length 1.0000000     5\n#&gt; 139             8 Petal.Length 0.3333333     5\n#&gt; 140             8  Petal.Width 0.6666667     5\n#&gt; 141             8 Petal.Length 1.0000000     5\n#&gt; 142            10 Petal.Length 0.2500000     5\n#&gt; 143            10  Sepal.Width 0.5000000     5\n#&gt; 144            10  Petal.Width 0.7500000     5\n#&gt; 145            10 Petal.Length 1.0000000     5\n#&gt; 146            11 Sepal.Length 0.2500000     5\n#&gt; 147            11 Petal.Length 0.5000000     5\n#&gt; 148            11  Petal.Width 0.7500000     5\n#&gt; 149            11 Petal.Length 1.0000000     5\n#&gt; 150            12 Sepal.Length 0.2500000     5\n#&gt; 151            12 Petal.Length 0.5000000     5\n#&gt; 152            12  Petal.Width 0.7500000     5\n#&gt; 153            12 Petal.Length 1.0000000     5\n#&gt; 154            14 Sepal.Length 0.2000000     5\n#&gt; 155            14 Petal.Length 0.4000000     5\n#&gt; 156            14  Sepal.Width 0.6000000     5\n#&gt; 157            14  Petal.Width 0.8000000     5\n#&gt; 158            14 Petal.Length 1.0000000     5\n#&gt; 159            15 Petal.Length 0.1666667     5\n#&gt; 160            15 Sepal.Length 0.3333333     5\n#&gt; 161            15 Petal.Length 0.5000000     5\n#&gt; 162            15  Sepal.Width 0.6666667     5\n#&gt; 163            15  Petal.Width 0.8333333     5\n#&gt; 164            15 Petal.Length 1.0000000     5\n#&gt; 165            16 Petal.Length 0.1666667     5\n#&gt; 166            16 Sepal.Length 0.3333333     5\n#&gt; 167            16 Petal.Length 0.5000000     5\n#&gt; 168            16  Sepal.Width 0.6666667     5\n#&gt; 169            16  Petal.Width 0.8333333     5\n#&gt; 170            16 Petal.Length 1.0000000     5\n\n\nAhora ya tenemos qué variables llevan a cada nodo terminal en cada árbol e incluso un peso que vale 1 si la variable es la primera en el “camino” hacia el nodo\nPero lo que nosotros queremos es para cada observación que predecimos, ver su nodo terminal en cada árbol y pegarle las variables importantes en cada nodo.\nSería algo así.\n\n\nMostrar / ocultar código\n# lo hacems de momento con todo iris, en la realidad serían los datos de test o el conjunto de datos a predecir. \n\n\nnodos_terminales &lt;- predict(rg_iris, iris, type = \"terminalNodes\")$predictions\n\n# cada fila corresponde a una observación y cada columna al nodo terminal en cada árbol\nhead(nodos_terminales, 10 )\n#&gt;       [,1] [,2] [,3] [,4] [,5]\n#&gt;  [1,]    1    3    3    1    1\n#&gt;  [2,]    1    3    3    1    1\n#&gt;  [3,]    1    3    3    1    1\n#&gt;  [4,]    1    3    3    1    1\n#&gt;  [5,]    1    3    3    1    1\n#&gt;  [6,]    1    3    3    1    1\n#&gt;  [7,]    1    3    3    1    1\n#&gt;  [8,]    1    3    3    1    1\n#&gt;  [9,]    1    3    3    1    1\n#&gt; [10,]    1    3    3    1    1\n\n\nLo ponemos de otra forma.\n\n\nMostrar / ocultar código\n# añadimos el id de la fila\nnodos_terminales_df &lt;- nodos_terminales %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(var = \"id\")\n  \n  \ncolnames(nodos_terminales_df)[-1] &lt;- 1:(ncol(nodos_terminales_df)-1)\n\nhead(nodos_terminales_df)\n#&gt;   id 1 2 3 4 5\n#&gt; 1  1 1 3 3 1 1\n#&gt; 2  2 1 3 3 1 1\n#&gt; 3  3 1 3 3 1 1\n#&gt; 4  4 1 3 3 1 1\n#&gt; 5  5 1 3 3 1 1\n#&gt; 6  6 1 3 3 1 1\n\n\nPivotamos para facilitar luego las agregaciones por observaciones\n\n\nMostrar / ocultar código\n nodos_terminales_df &lt;- nodos_terminales_df %&gt;% \n    tidyr::pivot_longer( colnames(nodos_terminales_df)[-1], names_to = \"arbol\", values_to = \"nodo_terminal\")\n\nhead(nodos_terminales_df)\n#&gt; # A tibble: 6 × 3\n#&gt;   id    arbol nodo_terminal\n#&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 1     1                 1\n#&gt; 2 1     2                 3\n#&gt; 3 1     3                 3\n#&gt; 4 1     4                 1\n#&gt; 5 1     5                 1\n#&gt; 6 2     1                 1\n\n\na la importancia en todos los árboles lo llamo info_modelo\n\n\nMostrar / ocultar código\n  \ninfo_modelo &lt;-  importancia_individual_todos_arboles\ninfo_modelo$arbol &lt;- as.character(info_modelo$arbol)\n\nhead(info_modelo)\n#&gt;   nodo_terminal    variables      peso arbol\n#&gt; 1             1  Petal.Width 1.0000000     1\n#&gt; 2             5 Petal.Length 0.3333333     1\n#&gt; 3             5  Petal.Width 0.6666667     1\n#&gt; 4             5  Petal.Width 1.0000000     1\n#&gt; 5             8  Petal.Width 0.3333333     1\n#&gt; 6             8  Petal.Width 0.6666667     1\n\n\nHacemos el join con la info de cada nodo terminal para cada observación con las variables que llevan a cada nodo terminal (en cada árbol)\n\n\nMostrar / ocultar código\nfinal &lt;- nodos_terminales_df %&gt;% \n    left_join(info_modelo, by = c(\"nodo_terminal\", \"arbol\"))\n\n# para el individuo 30\n\nfinal %&gt;% \n    filter(id == 30)\n#&gt; # A tibble: 7 × 5\n#&gt;   id    arbol nodo_terminal variables     peso\n#&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 30    1                 1 Petal.Width    1  \n#&gt; 2 30    2                 3 Petal.Width    0.5\n#&gt; 3 30    2                 3 Sepal.Length   1  \n#&gt; 4 30    3                 3 Petal.Length   0.5\n#&gt; 5 30    3                 3 Sepal.Length   1  \n#&gt; 6 30    4                 1 Petal.Width    1  \n#&gt; 7 30    5                 1 Petal.Length   1\n\n\nAgregamos la info para cada individuo, de forma que contemos cuántas veces aparece cada variable, sumamos los pesos y ordenamos\n\n\nMostrar / ocultar código\nres &lt;- final %&gt;%\n    group_by(id, variables) %&gt;%\n    summarise(\n      total = n(),\n      ponderado = sum(peso)) %&gt;%\n    group_by(id) %&gt;%\n    mutate(\n      importancia_caso = total / sum(total),\n      importancia_ponderada = ponderado / sum(ponderado)\n    ) %&gt;% \n    top_n(10, importancia_ponderada) %&gt;% \n    ungroup() %&gt;% \n    arrange(as.numeric(id), desc(importancia_ponderada))\n\n\n\n\nMostrar / ocultar código\nres %&gt;% \n    filter(id == 30)\n#&gt; # A tibble: 3 × 6\n#&gt;   id    variables    total ponderado importancia_caso importancia_ponderada\n#&gt;   &lt;chr&gt; &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n#&gt; 1 30    Petal.Width      3       2.5            0.429                 0.417\n#&gt; 2 30    Sepal.Length     2       2              0.286                 0.333\n#&gt; 3 30    Petal.Length     2       1.5            0.286                 0.25\n\n\nY esa sería la importancia de las variables específica para la observación 30"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#todo-junto.",
    "href": "2023/01/29/explicatividad_no_usual/index.html#todo-junto.",
    "title": "Explicatividad no usual",
    "section": "Todo junto.",
    "text": "Todo junto.\nNos podemos crear una funcioncita que lo haga todo.\n\n\nMostrar / ocultar código\ngetIndividualImportance &lt;-  function(modelo, data, top = modelo$num.independent.variables, ...){\n    \n params_ellipsis &lt;- list(...)\n  \n  # get terminalNodes\n  nodos_terminales &lt;- predict(modelo, data, type = \"terminalNodes\")$predictions\n  \n  nodos_terminales_df &lt;- nodos_terminales %&gt;% \n    as.data.frame() \n  nodos_terminales_df$id &lt;- rownames(data)\n  nodos_terminales_df &lt;-  nodos_terminales_df %&gt;%\n      dplyr::select(id, everything())\n      \n  \n  \n  colnames(nodos_terminales_df)[-1] &lt;- 1:(ncol(nodos_terminales_df)-1)\n  \n  nodos_terminales_df &lt;- nodos_terminales_df %&gt;% \n    tidyr::pivot_longer( colnames(nodos_terminales_df)[-1], names_to = \"arbol\", values_to = \"nodo_terminal\")\n  \n  # get variables_path for each tree and terminal node\n  info_modelo &lt;-  extraerVariablesPorModelo(modelo, parallel = params_ellipsis$parallel)\n  info_modelo$arbol &lt;- as.character(info_modelo$arbol)\n  \n  # join both\n  \n  final &lt;- nodos_terminales_df %&gt;% \n    left_join(info_modelo, by = c(\"nodo_terminal\", \"arbol\"))\n  \n  res &lt;- final %&gt;%\n    group_by(id, variables) %&gt;%\n    summarise(\n      total = n(),\n      ponderado = sum(peso)) %&gt;%\n    group_by(id) %&gt;%\n    # para poder comparar luego observaciones, para cadda individuo, divido las veces qeu \n     # aparece una variable por el total de veces que han aparecido todas sus variables\n    mutate(\n      importancia_caso = total / sum(total),\n      importancia_ponderada = ponderado / sum(ponderado)\n    ) %&gt;% \n    top_n(top, importancia_ponderada) %&gt;% \n    ungroup() %&gt;% \n    arrange(as.numeric(id), desc(importancia_ponderada))\n  \n}\n\n\nY comprobamos\n\n\nMostrar / ocultar código\nexplicatividad_iris &lt;-  getIndividualImportance(rg_iris, iris, parallel = TRUE)\n\n\n\n\nMostrar / ocultar código\nDT::datatable(explicatividad_iris)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#agrupando-observaciones-con-similar-importancia-de-variables",
    "href": "2023/01/29/explicatividad_no_usual/index.html#agrupando-observaciones-con-similar-importancia-de-variables",
    "title": "Explicatividad no usual",
    "section": "Agrupando observaciones con similar importancia de variables",
    "text": "Agrupando observaciones con similar importancia de variables\nPodríamos hacer ahora un PCA pero yo voy a utilizar un CA usando la importancia_ponderada\n\n\nMostrar / ocultar código\n\n  tabla_para_diagonalizar &lt;- xtabs(ponderado ~ id+ variables, data= explicatividad_iris)\n  tabla_para_diagonalizar\n#&gt;      variables\n#&gt; id    Petal.Length Petal.Width Sepal.Length Sepal.Width\n#&gt;   1      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   10     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   100    2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   101    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   102    2.8333333   5.4166667    3.0000000   0.2500000\n#&gt;   103    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   104    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   105    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   106    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   107    3.6666667   4.3333333    2.3333333   0.6666667\n#&gt;   108    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   109    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   11     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   110    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   111    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   112    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   113    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   114    2.8333333   5.1666667    3.2500000   0.2500000\n#&gt;   115    2.8333333   5.4166667    3.0000000   0.2500000\n#&gt;   116    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   117    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   118    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   119    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   12     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   120    3.8000000   5.5500000    2.2500000   1.9000000\n#&gt;   121    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   122    2.2500000   5.5000000    3.0000000   0.2500000\n#&gt;   123    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   124    2.2500000   5.5833333    2.6666667   0.0000000\n#&gt;   125    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   126    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   127    2.4166667   5.6666667    2.9166667   0.0000000\n#&gt;   128    2.2500000   5.5833333    2.6666667   0.0000000\n#&gt;   129    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   13     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   130    4.0666667   6.0666667    2.2000000   2.1666667\n#&gt;   131    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   132    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   133    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   134    3.8000000   5.9000000    2.2000000   2.1000000\n#&gt;   135    3.9500000   5.9500000    2.2000000   1.9000000\n#&gt;   136    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   137    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   138    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   139    2.6666667   5.7500000    3.0833333   0.0000000\n#&gt;   14     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   140    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   141    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   142    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   143    2.8333333   5.4166667    3.0000000   0.2500000\n#&gt;   144    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   145    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   146    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   147    2.5833333   5.0000000    2.9166667   0.0000000\n#&gt;   148    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   149    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   15     1.0000000   3.3333333    2.0000000   0.6666667\n#&gt;   150    3.0833333   5.6666667    3.0000000   0.2500000\n#&gt;   16     1.0000000   3.3333333    2.0000000   0.6666667\n#&gt;   17     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   18     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   19     1.0000000   3.3333333    2.0000000   0.6666667\n#&gt;   2      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   20     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   21     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   22     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   23     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   24     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   25     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   26     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   27     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   28     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   29     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   3      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   30     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   31     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   32     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   33     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   34     1.5000000   2.6666667    2.0000000   0.3333333\n#&gt;   35     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   36     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   37     1.5000000   2.6666667    2.0000000   0.3333333\n#&gt;   38     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   39     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   4      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   40     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   41     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   42     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   43     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   44     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   45     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   46     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   47     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   48     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   49     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   5      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   50     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   51     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   52     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   53     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   54     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   55     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   56     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   57     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   58     3.6666667   4.3333333    2.3333333   0.6666667\n#&gt;   59     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   6      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   60     3.0000000   4.1666667    2.0000000   0.3333333\n#&gt;   61     3.4000000   4.3000000    2.2000000   0.6000000\n#&gt;   62     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   63     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   64     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   65     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   66     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   67     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   68     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   69     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   7      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   70     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   71     2.6666667   5.8333333    3.2500000   0.2500000\n#&gt;   72     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   73     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   74     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   75     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   76     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   77     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   78     3.9166667   5.6666667    2.2500000   2.1666667\n#&gt;   79     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   8      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   80     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   81     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   82     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   83     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   84     4.0666667   6.0666667    2.2000000   2.1666667\n#&gt;   85     3.0000000   4.1666667    2.0000000   0.3333333\n#&gt;   86     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   87     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   88     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   89     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   9      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   90     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   91     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   92     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   93     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   94     3.4000000   4.3000000    2.2000000   0.6000000\n#&gt;   95     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   96     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   97     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   98     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   99     3.4000000   4.3000000    2.2000000   0.6000000\n\n\nY al hacer un CA podemos ver qué individuos están asociados con las variables pero por la importancia ponderada.\n\n\nMostrar / ocultar código\n\nres_ca &lt;- FactoMineR::CA(tabla_para_diagonalizar, graph = FALSE)\n\nfviz_ca(res_ca)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-2",
    "href": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-2",
    "title": "Explicatividad no usual",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\nUtilicemos esto para los datos de Boston Housing\n\n\nMostrar / ocultar código\nboston_df &lt;-  MASS::Boston\n\n\n\nHousing Values in Suburbs of Boston Description The Boston data frame has 506 rows and 14 columns.\n\n\nUsage Boston Format This data frame contains the following columns:\n\n\ncrim per capita crime rate by town.\n\n\nzn proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus proportion of non-retail business acres per town.\n\n\nchas Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox nitrogen oxides concentration (parts per 10 million).\n\n\nrm average number of rooms per dwelling.\n\n\nage proportion of owner-occupied units built prior to 1940.\n\n\ndis weighted mean of distances to five Boston employment centres.\n\n\nrad index of accessibility to radial highways.\n\n\ntax full-value property-tax rate per $10,000.\n\n\nptratio pupil-teacher ratio by town.\n\n\nblack 1000(Bk−0.63)^2 where BkBk is the proportion of blacks by town.\n\n\nlstat lower status of the population (percent).\n\n\nmedv median value of owner-occupied homes in $1000s.\n\n\n\nMostrar / ocultar código\nset.seed(47)\n\nidx &lt;-  sample(1:nrow(boston_df),300)\ntrain_df &lt;- boston_df[idx,]\n\ntest_df &lt;- boston_df[-idx, ]\n\n\n\nModelo con ranger\n\n\nMostrar / ocultar código\nrg_boston &lt;-  ranger(medv ~ ., data = train_df, num.trees = 50)\n\n\nVariables importantes a nivel individual\nPor simplificar, voy a seleccionar solo las 5 variables más importantes para cada observación\n\n\nMostrar / ocultar código\n\nimportancia_individual &lt;- getIndividualImportance(rg_boston, test_df,top = 5, parallel = TRUE)\n\n\n\n\nMostrar / ocultar código\ndim(importancia_individual)\n#&gt; [1] 1030    6\n\n\n\n\nMostrar / ocultar código\nDT::datatable(importancia_individual)\n\n\n\n\n\n\n\n\nAgrupando\n\n\nMostrar / ocultar código\ntabla_diag_boston &lt;- xtabs(ponderado ~ id+ variables, data= importancia_individual)\nhead(tabla_diag_boston)\n#&gt;      variables\n#&gt; id         age     crim      dis    indus    lstat      nox  ptratio       rm\n#&gt;   100 18.34266  0.00000  0.00000 21.95556 37.81429 17.20397  0.00000 38.40952\n#&gt;   108  0.00000  0.00000 28.51039 27.88223 50.73328 34.23095  0.00000 41.07002\n#&gt;   110 27.78868  0.00000 29.29312  0.00000 51.24393 33.02176  0.00000 42.08247\n#&gt;   111  0.00000  0.00000 27.27855 33.53332 49.47596 26.90144  0.00000 43.21389\n#&gt;   112  0.00000  0.00000 24.53920 25.53335 54.25059 30.61195  0.00000 39.80125\n#&gt;   119  0.00000  0.00000 26.33785 28.28968 53.85364 35.92384  0.00000 39.11696\n#&gt;      variables\n#&gt; id         tax\n#&gt;   100  0.00000\n#&gt;   108  0.00000\n#&gt;   110  0.00000\n#&gt;   111  0.00000\n#&gt;   112  0.00000\n#&gt;   119  0.00000\n\n\n\n\nMostrar / ocultar código\nres_ca &lt;- FactoMineR::CA(tabla_diag_boston, graph = FALSE)\n\nfviz_ca(res_ca)\n\n\n\n\n\n\n\n\n\nPodemos hacer un HCPC usando las dimensiones obtenidas. Lo que hace es un cluster jerárquico usando las dimensiones obtenidas en la estructura factorial.\n\n\nMostrar / ocultar código\nres_hcpc &lt;- HCPC(res_ca, graph = FALSE)\n\nfviz_cluster(res_hcpc,\n             repel = TRUE,            # Avoid label overlapping\n             show.clust.cent = TRUE, # Show cluster centers\n             palette = \"jco\",         # Color palette see ?ggpubr::ggpar\n             ggtheme = theme_minimal(),\n             main = \"Factor map\"\n             )\n\n\n\n\n\n\n\n\n\n\n\nMostrar / ocultar código\nplot(res_hcpc, choice = \"3D.map\")\n\n\n\n\n\n\n\n\n\nUna utilidad interesante es la descripción de las variables de los clusters. Dónde nos dice cuales son la variables más importantes para cada uno.\nCuando en un cluster su Intern % para una variable se desvíe mucho de glob % quiere decir que en esa variable la distribución es distinta de en la población general y por tanto es una variables que caracteriza al cluster.\nEn este caso estaremos encontrando grupos de individuos con mismas variables importantes en el randomForest.\nClaramente se ven grupos dónde es muy importante la variable de criminalidad o la edad\n\n\nMostrar / ocultar código\nres_hcpc$desc.var\n#&gt; $`1`\n#&gt;          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; crim    15.592962  2.127929    642.8840   657.2566  0.000000e+00        Inf\n#&gt; nox     20.157813 12.416539    831.0888  3835.1142  2.807733e-52  15.215104\n#&gt; dis     11.939900  9.329212    492.2715  2881.5271  2.572108e-09   5.956809\n#&gt; rm      20.191263 25.487484    832.4679  7872.3559  1.117021e-17  -8.561196\n#&gt; age      4.411035  7.568723    181.8631  2337.7625  1.230524e-18  -8.811891\n#&gt; tax      0.000000  1.001897      0.0000   309.4575  9.327733e-20  -9.096514\n#&gt; ptratio  0.000000  3.118072      0.0000   963.0835  2.250504e-61 -16.529487\n#&gt; indus    0.000000 10.860935      0.0000  3354.6328 2.069270e-222 -31.835808\n#&gt; \n#&gt; $`2`\n#&gt;         Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; age     14.32559  7.568723    803.0250  2337.7625  7.150050e-85  19.521920\n#&gt; nox     19.56112 12.416539   1096.5037  3835.1142  9.886451e-65  16.989118\n#&gt; dis     15.61069  9.329212    875.0616  2881.5271  3.449026e-63  16.779470\n#&gt; rm      21.85203 25.487484   1224.9213  7872.3559  2.382720e-12  -7.010027\n#&gt; tax      0.00000  1.001897      0.0000   309.4575  1.880511e-27 -10.855366\n#&gt; crim     0.00000  2.127929      0.0000   657.2566  2.961603e-58 -16.090764\n#&gt; ptratio  0.00000  3.118072      0.0000   963.0835  1.146579e-85 -19.615212\n#&gt; indus    0.00000 10.860935      0.0000  3354.6328 2.480702e-311 -37.718398\n#&gt; \n#&gt; $`3`\n#&gt;         Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; dis     14.41601  9.329212    885.4575  2881.5271  7.965757e-48  14.528753\n#&gt; indus   15.72645 10.860935    965.9468  3354.6328  4.377749e-39  13.078362\n#&gt; nox     17.05504 12.416539   1047.5513  3835.1142  1.891207e-32  11.860817\n#&gt; rm      24.28797 25.487484   1491.8110  7872.3559  1.531797e-02  -2.424773\n#&gt; tax      0.00000  1.001897      0.0000   309.4575  2.402280e-30 -11.448146\n#&gt; crim     0.00000  2.127929      0.0000   657.2566  1.885153e-64 -16.951216\n#&gt; ptratio  0.00000  3.118072      0.0000   963.0835  8.318594e-95 -20.657729\n#&gt; age      0.00000  7.568723      0.0000  2337.7625 1.165375e-235 -32.779199\n#&gt; \n#&gt; $`4`\n#&gt;          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; age     13.713854  7.568723    994.4487  2337.7625 7.010364e-100  21.214544\n#&gt; indus   15.980898 10.860935   1158.8415  3354.6328  3.363370e-53  15.353354\n#&gt; rm      27.297454 25.487484   1979.4522  7872.3559  6.332137e-05   4.000079\n#&gt; nox     11.031380 12.416539    799.9313  3835.1142  3.337066e-05  -4.149154\n#&gt; tax      0.000000  1.001897      0.0000   309.4575  1.541120e-36 -12.624810\n#&gt; crim     0.000000  2.127929      0.0000   657.2566  1.026729e-77 -18.661061\n#&gt; dis      3.479402  9.329212    252.3059  2881.5271 1.613296e-102 -21.498348\n#&gt; ptratio  0.000000  3.118072      0.0000   963.0835 2.246449e-114 -22.730329\n#&gt; \n#&gt; $`5`\n#&gt;           Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; ptratio 13.4631336  3.118072   731.46909   963.0835  0.000000e+00        Inf\n#&gt; indus   16.0021844 10.860935   869.41894  3354.6328  2.134777e-37  12.779485\n#&gt; rm      30.0450942 25.487484  1632.38800  7872.3559  6.922077e-17   8.348354\n#&gt; age      5.9723379  7.568723   324.48468  2337.7625  4.791243e-07  -5.034489\n#&gt; dis      5.8922388  9.329212   320.13279  2881.5271  6.783780e-24 -10.079838\n#&gt; tax      0.0000000  1.001897     0.00000   309.4575  1.567007e-26 -10.659939\n#&gt; crim     0.2645368  2.127929    14.37262   657.2566  4.931237e-37 -12.714201\n#&gt; nox      1.1050567 12.416539    60.03913  3835.1142 3.406088e-250 -33.783844\n#&gt; \n#&gt; $`6`\n#&gt;          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; tax     13.270152  1.001897   309.45748   309.4575  0.000000e+00        Inf\n#&gt; ptratio  9.932087  3.118072   231.61443   963.0835  3.328525e-58  16.083531\n#&gt; indus   15.455762 10.860935   360.42551  3354.6328  2.442145e-12   7.006580\n#&gt; rm      30.502626 25.487484   711.31557  7872.3559  1.684450e-08   5.641633\n#&gt; crim     0.000000  2.127929     0.00000   657.2566  4.470886e-23  -9.892856\n#&gt; dis      2.414161  9.329212    56.29780  2881.5271  1.144880e-43 -13.857568\n#&gt; age      1.455459  7.568723    33.94103  2337.7625  5.050710e-44 -13.916197\n#&gt; nox      0.000000 12.416539     0.00000  3835.1142 2.015379e-140 -25.227201\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"descfreq\" \"list\"\n\n\nY por supuesto tenemos los datos con el cluster asignado y los valores de cada variable (no son los valores originales de las variables , sino la importancia ponderada que tenían con el procedimiento descrito para cada observación )\n\n\nMostrar / ocultar código\nres_hcpc$data.clust %&gt;% \n         dplyr::select(clust, everything()) %&gt;% \n    slice_sample(prop = 0.3) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nSi unimos con el dataset original\n\n\nMostrar / ocultar código\ntest_df_with_cluster &lt;-  res_hcpc$data.clust %&gt;% \n    rownames_to_column(var = \"id\") %&gt;% \n    dplyr::select(id, clust)\n\nunido &lt;- test_df %&gt;% \n    rownames_to_column(var = \"id\") %&gt;% \n    inner_join(test_df_with_cluster, by = \"id\")\n\n\nY efectivamente vemos que el cluster 1 tiene mucho más ratio de criminalidad, y además es la variable más importante para ese grupo en relación con la variable dependiente medv. No causa sorpresa ver que es justo en ese cluster dónde el precio de la propiedad es más bajo\n\n\nMostrar / ocultar código\nunido %&gt;% \n    group_by(clust) %&gt;% \n    summarise(across(c(lstat,crim, age, black, medv), list(mean = mean, median = median), .names = \"{.col}_{.fn}\" )) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nCarlos en el post que inspira este, comenta que este tipo de procedimientos sería útil para aquellas de las observaciones con un mayor score predicho. En este ejemplo se podría aplicar para clusterizar las observaciones con un mayor valor predicho del valor de la propiedad."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#nota",
    "href": "2023/01/29/explicatividad_no_usual/index.html#nota",
    "title": "Explicatividad no usual",
    "section": "Nota",
    "text": "Nota\n\nHice el código deprisa y corriendo, es claramente mejorable y podría ir mucho más rápido. El objetivo era mostrar como se puede obtener variables importantes a nivel de observación en este tipo de modelos, simplemente recorriendo por qué camino ha ido cada observación en cada árbol\n\n\nEstaría chulo representar espacialmente la distribución de los clusters obtenidos"
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html",
    "title": "Conformal prediction. Estilo compadre",
    "section": "",
    "text": "El jueves pasado asistí al más que recomendable meetup de PyData Madrid, que cuenta entre sus organizadores con el gran Juan Luis Cano Rodríguez, antiguo compañero mío de curro y tocayo de iniciales.\nEl caso es que en una de las charlas, Ignacio Peletier, mencionó de pasada lo del “Conformal Prediction”. Y siendo que Ignacio es un gran científico de datos, y que hacía unos meses que había tenido varias charlas con Carlos sobre el particular, pues he decidido ver un poco más en detalle de qué iba el asunto ."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#recursos",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#recursos",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Recursos",
    "text": "Recursos\nUn excelente sitio para empezar a bichear con este tema es el Readme de este repo, dónde han ido recopilando enlaces a libros, posts, papers y código relativo a lo de conformal prediction.\nEn particular, uno de los recursos que me ha gustado es este minicurso de Christoph Molnar.\nOtro recurso útil es este post de Carlos, dónde se esboza un poco en qué consiste esto de la predicción conforme y en por qué no es algo tan novedoso como se cree."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#experimentando",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#experimentando",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Experimentando",
    "text": "Experimentando\nLa predicción conforme se puede aplicar tanto a modelos de regresión de clasificación. Su objetivo es simplemente medir la incertidumbre de una predicción dada.\nEn el caso de regresión no tiene mucho misterio:\n\nSe entrena un modelo usando un conjunto de datos de entrenamiento.\nSe mide el error en un conjunto de datos de validación, calibración, utilizando la norma L1, es decir \\(\\mid y - \\hat{y}\\mid\\)\nSe elige una medida de dispersión del error, por ejemplo el cuantil \\((1- \\alpha) = 0.95\\) de los errores anteriores.\nPara una nueva predicción se da su intervalo como \\((\\hat{y} - q_{1-\\alpha}, \\hat{y} + q_{1-\\alpha})\\)\n\nEn el caso de clasificación la cosa es más divertida. Puesto que lo que se quiere obtener es un conjunto de etiquetas probables. Tipo {A} {A, B} {B, C}\nEn este caso según he leído aquí el algoritmo sería\n\nSe entrena un modelo usando un conjunto de datos de entrenamiento.\nSe mide el error en un conjunto de datos de validación, calibración, viendo para cada observación el valor que el modelo le ha dado para la predicción de la clase verdadera. Es un conjunto de validación , sabemos cuál es la verdad. Y se calcula el error como \\(1- p_{i}\\) siendo \\(p_i\\), la probabilidad predicha para la clase verdadera\nSe calcula el cuantil de orden \\(1-\\alpha\\) de esos errores y se guarda. Se entiende que el modelo está bien calibrado y que el conjunto de validación y que los scores que da el modelo se pueden asumir como probabilidades\nPara una nueva predicción se tendrá una \\(p_i\\) para cada clase. Se calcula \\(1-p_i\\) para cada clase y se considera que esa clase forma parte del prediction set si ese valor es menor o igual que el valor del cuantil anterior.\n\nPues vamos a ver como se haría con R en estilo compadre, y puede que con alguna pequeña modificación por mi parte.\n\nEjemplo\n\n\nMostrar / ocultar código\nlibrary(tidyverse)\nlibrary(MASS)\n\n\nVamos a usar el conjunto de datos housing \n\n\nMostrar / ocultar código\n\nskimr::skim(housing)\n\n\n\nData summary\n\n\nName\nhousing\n\n\nNumber of rows\n72\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSat\n0\n1\nTRUE\n3\nLow: 24, Med: 24, Hig: 24\n\n\nInfl\n0\n1\nFALSE\n3\nLow: 24, Med: 24, Hig: 24\n\n\nType\n0\n1\nFALSE\n4\nTow: 18, Apa: 18, Atr: 18, Ter: 18\n\n\nCont\n0\n1\nFALSE\n2\nLow: 36, Hig: 36\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nFreq\n0\n1\n23.35\n17.67\n3\n10\n19.5\n31.75\n86\n▇▅▂▁▁\n\n\n\n\n\nY vamos a justar un modelito tonto usando regresión logística ordinal, sobre los 40 primeros datos\n\n\nMostrar / ocultar código\n\nhouse.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing[1:40,])\n\nhead(predict(house.plr, type = \"probs\"))\n#&gt;         Low    Medium      High\n#&gt; 1 0.3639420 0.2575531 0.3785049\n#&gt; 2 0.3639420 0.2575531 0.3785049\n#&gt; 3 0.3639420 0.2575531 0.3785049\n#&gt; 4 0.3263764 0.2552793 0.4183443\n#&gt; 5 0.3263764 0.2552793 0.4183443\n#&gt; 6 0.3263764 0.2552793 0.4183443\n\n\nGuardamos las predicciones para el conjunto de validación , que va a ser las filas de la 41 a la 55, junto con el valor de Sat verdadero\n\n\nMostrar / ocultar código\n\npredictions &lt;- as.data.frame(predict(house.plr, type = \"probs\", newdata = housing[41:55,]))\n\ntt &lt;- cbind(predictions, True_class=housing$Sat[41:55])\n\ntt\n#&gt;          Low    Medium      High True_class\n#&gt; 41 0.3055662 0.2524868 0.4419471     Medium\n#&gt; 42 0.3055662 0.2524868 0.4419471       High\n#&gt; 43 0.1595073 0.1930758 0.6474169        Low\n#&gt; 44 0.1595073 0.1930758 0.6474169     Medium\n#&gt; 45 0.1595073 0.1930758 0.6474169       High\n#&gt; 46 0.4671694 0.2484190 0.2844116        Low\n#&gt; 47 0.4671694 0.2484190 0.2844116     Medium\n#&gt; 48 0.4671694 0.2484190 0.2844116       High\n#&gt; 49 0.4260865 0.2544760 0.3194375        Low\n#&gt; 50 0.4260865 0.2544760 0.3194375     Medium\n#&gt; 51 0.4260865 0.2544760 0.3194375       High\n#&gt; 52 0.2425400 0.2363201 0.5211399        Low\n#&gt; 53 0.2425400 0.2363201 0.5211399     Medium\n#&gt; 54 0.2425400 0.2363201 0.5211399       High\n#&gt; 55 0.4014928 0.2566312 0.3418760        Low\n\n\nAhora, para la primera fila sería hacer (1-0.2524), puesto que la clase real es “Medium” y para la segunda sería (1-0.44), puesto que la clase real es “High”. No estoy muy inspirado hoy y no he conseguido una forma elegante de hacerlo en R, y ChatGpt no me ha servido de mucha ayuda, seguramente porque aún no soy muy ducho preguntándole.\nAsí que he tirado iterando para cada fila con un map y quedándode con el valor predicho de la columna cuyo nombre coincida con el valor en True_class\n\n\nMostrar / ocultar código\n\ntt$prob_true_class &lt;- map_dbl(1:nrow(tt), .f = function(i) \n    tt[i, colnames(tt) == tt$True_class[i]])\n\ntt$resid &lt;- 1-tt$prob_true_class\n\nhead(tt)\n#&gt;          Low    Medium      High True_class prob_true_class     resid\n#&gt; 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#&gt; 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#&gt; 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#&gt; 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#&gt; 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#&gt; 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n\n\nDefinimos un \\(\\alpha = 0.3\\) y calculamos el cuantil 70 .\n\n\nMostrar / ocultar código\n(qhat = quantile(tt$resid, 0.7))\n#&gt;       70% \n#&gt; 0.7507675\n\n\nY ya estamos listos para hacer la predicción conforme para nuevos datos.\n\n\nMostrar / ocultar código\n# predecimos de la fila 51 a la 70 \npredicciones &lt;- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\n\nhead(predicciones)\n#&gt;          Low    Medium      High\n#&gt; 51 0.4260865 0.2544760 0.3194375\n#&gt; 52 0.2425400 0.2363201 0.5211399\n#&gt; 53 0.2425400 0.2363201 0.5211399\n#&gt; 54 0.2425400 0.2363201 0.5211399\n#&gt; 55 0.4014928 0.2566312 0.3418760\n#&gt; 56 0.4014928 0.2566312 0.3418760\n\n\nNos creamos un data.frame que indique si el valor de 1 - predicciones es menor o igual que el cuantil elegido\n\n\nMostrar / ocultar código\nset &lt;- as.data.frame(1 - predicciones &lt;= qhat)\n\nhead(set)\n#&gt;      Low Medium High\n#&gt; 51  TRUE   TRUE TRUE\n#&gt; 52 FALSE  FALSE TRUE\n#&gt; 53 FALSE  FALSE TRUE\n#&gt; 54 FALSE  FALSE TRUE\n#&gt; 55  TRUE   TRUE TRUE\n#&gt; 56  TRUE   TRUE TRUE\n\n\nAl igual que antes, utilizo un map para obtener el conjunto de etiquetas, para la primera fila serían todas, para la segunda sería {“Medium”, “High”}\n\n\nMostrar / ocultar código\n\nset$conformal &lt;-  map_chr(1:nrow(set), .f= function(i) {\n     set_list = colnames(set)[unlist(set[i,])]\n     paste0(set_list, collapse = \",\")\n     })\n\nhead(set)\n#&gt;      Low Medium High       conformal\n#&gt; 51  TRUE   TRUE TRUE Low,Medium,High\n#&gt; 52 FALSE  FALSE TRUE            High\n#&gt; 53 FALSE  FALSE TRUE            High\n#&gt; 54 FALSE  FALSE TRUE            High\n#&gt; 55  TRUE   TRUE TRUE Low,Medium,High\n#&gt; 56  TRUE   TRUE TRUE Low,Medium,High\n\n\nSe lo pego al dataset original de test (filas 51 a 70), junto con las predicciones y la clase verdadera.\n\n\nMostrar / ocultar código\n\nset_fin &lt;-  cbind( True_class = housing$Sat[51:70], as.data.frame(predicciones),\n                  set_conformal =set$conformal)\n\nhead(set_fin)\n#&gt;    True_class       Low    Medium      High   set_conformal\n#&gt; 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399            High\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399            High\n#&gt; 54       High 0.2425400 0.2363201 0.5211399            High\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High\n\n\nY ya estaría.\nUna cosa que se suele calcular es la cobertura de cada clase, es decir, la proporción de veces que cada clase está dentro del conjunto.\n\n\nMostrar / ocultar código\nset_fin &lt;- set_fin |&gt; \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                               .y = set_conformal , \n                               ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nhead(set_fin)\n#&gt;    True_class       Low    Medium      High   set_conformal class_in_set\n#&gt; 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High         TRUE\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n\n\n\n\nMostrar / ocultar código\nset_fin |&gt; \n    group_by(True_class) |&gt; \n    summarise(cov = mean(class_in_set))\n#&gt; # A tibble: 3 × 2\n#&gt;   True_class   cov\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;\n#&gt; 1 Low        0.571\n#&gt; 2 Medium     0.5  \n#&gt; 3 High       1\n\n\n\n\nModificación 1.\nNo me convence lo de tener un sólo cuantil, común a todas las clases, ¿no sería mejor tener una medida de cómo se distribuyen los errores para cada una de las clases?\nUsamos el conjunto de validación dónde tenemos el \\(1-p_i\\) que nos dice en cuánto se ha equivocado el modelo en predecir la clase real\n\n\nMostrar / ocultar código\nhead(tt)\n#&gt;          Low    Medium      High True_class prob_true_class     resid\n#&gt; 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#&gt; 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#&gt; 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#&gt; 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#&gt; 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#&gt; 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n\n\nCalculamos el quantil 70 para cada clase, y así vemos que varía por clase\n\n\nMostrar / ocultar código\n(qhat_by_class &lt;- tt |&gt; \n    group_by(True_class) |&gt; \n    summarise(qhat = quantile(resid, 0.7)) |&gt; \n        pivot_wider(names_from = True_class, values_from = qhat))\n#&gt; # A tibble: 1 × 3\n#&gt;     Low Medium  High\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.726  0.761 0.656\n\n\n\n\nMostrar / ocultar código\npredicciones &lt;- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\ncomplementarios &lt;- 1-predicciones\nhead(complementarios)\n#&gt;          Low    Medium      High\n#&gt; 51 0.5739135 0.7455240 0.6805625\n#&gt; 52 0.7574600 0.7636799 0.4788601\n#&gt; 53 0.7574600 0.7636799 0.4788601\n#&gt; 54 0.7574600 0.7636799 0.4788601\n#&gt; 55 0.5985072 0.7433688 0.6581240\n#&gt; 56 0.5985072 0.7433688 0.6581240\n\n\nY vemos si cada \\(1-p_i\\) es menor o igual que el cuantil correspondiente de cada clase\n\n\nMostrar / ocultar código\nset_adjust &lt;- data.frame(Low = complementarios[,1] &lt;= qhat_by_class$Low,\n                        Medium = complementarios[,2] &lt;= qhat_by_class$Medium,\n                         High = complementarios[,3] &lt;= qhat_by_class$High )\n\n\nhead(set_adjust)\n#&gt;      Low Medium  High\n#&gt; 51  TRUE   TRUE FALSE\n#&gt; 52 FALSE  FALSE  TRUE\n#&gt; 53 FALSE  FALSE  TRUE\n#&gt; 54 FALSE  FALSE  TRUE\n#&gt; 55  TRUE   TRUE FALSE\n#&gt; 56  TRUE   TRUE FALSE\n\n\n\n\nMostrar / ocultar código\nset_adjust$conformal &lt;-  map_chr(1:nrow(set_adjust), .f= function(i) {\n    set_list = colnames(set_adjust)[unlist(set_adjust[i,])]\n    paste0(set_list, collapse = \",\")\n})\n\nhead(set_adjust)\n#&gt;      Low Medium  High  conformal\n#&gt; 51  TRUE   TRUE FALSE Low,Medium\n#&gt; 52 FALSE  FALSE  TRUE       High\n#&gt; 53 FALSE  FALSE  TRUE       High\n#&gt; 54 FALSE  FALSE  TRUE       High\n#&gt; 55  TRUE   TRUE FALSE Low,Medium\n#&gt; 56  TRUE   TRUE FALSE Low,Medium\n\n\nComo antes, nos quedamos con la clase de verdad, la predicción en probabilidad de cada clase y la predicción conforme\n\n\nMostrar / ocultar código\nset_adjust_fin &lt;-  cbind( True_class = housing$Sat[51:70], as.data.frame(predict(house.plr, newdata = housing[51:70,],type=\"probs\")),\n                   set_conformal =set_adjust$conformal)\n\nhead(set_adjust_fin)\n#&gt;    True_class       Low    Medium      High set_conformal\n#&gt; 51       High 0.4260865 0.2544760 0.3194375    Low,Medium\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399          High\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399          High\n#&gt; 54       High 0.2425400 0.2363201 0.5211399          High\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760    Low,Medium\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760    Low,Medium\n\n\n\n\nMostrar / ocultar código\nset_adjust_fin &lt;- set_adjust_fin |&gt; \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                                .y = set_conformal , \n                                ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nset_adjust_fin\n#&gt;    True_class       Low    Medium      High   set_conformal class_in_set\n#&gt; 51       High 0.4260865 0.2544760 0.3194375      Low,Medium        FALSE\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#&gt; 57       High 0.4014928 0.2566312 0.3418760      Low,Medium        FALSE\n#&gt; 58        Low 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#&gt; 59     Medium 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#&gt; 60       High 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#&gt; 61        Low 0.1967801 0.2160332 0.5871868            High        FALSE\n#&gt; 62     Medium 0.1967801 0.2160332 0.5871868            High        FALSE\n#&gt; 63       High 0.1967801 0.2160332 0.5871868            High         TRUE\n#&gt; 64        Low 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#&gt; 65     Medium 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#&gt; 66       High 0.4732214 0.2472855 0.2794931      Low,Medium        FALSE\n#&gt; 67        Low 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#&gt; 68     Medium 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#&gt; 69       High 0.4320378 0.2537829 0.3141793      Low,Medium        FALSE\n#&gt; 70        Low 0.2470311 0.2378946 0.5150743            High        FALSE\n\n\nY aquí ya vemos que la cobertura es distinta y que la clase “High” ya no está en el 100% de los prediction sets\n\n\nMostrar / ocultar código\nset_adjust_fin |&gt; \n    group_by(True_class) |&gt; \n    summarise(cov = mean(class_in_set))\n#&gt; # A tibble: 3 × 2\n#&gt;   True_class   cov\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;\n#&gt; 1 Low        0.571\n#&gt; 2 Medium     0.667\n#&gt; 3 High       0.429\n\n\nDe hecho si tabulamos ambas predicciones conformes , vemos que de las 10 predicciones que el primer método ponía como {Low, Medium, High} , el segundo pone 7 como {Low, Medium } y 3 como {Low, Medium, High}\n\n\nMostrar / ocultar código\n\ntable(set_fin$set_conformal, set_adjust_fin$set_conformal)\n#&gt;                  \n#&gt;                   High Low,Medium Low,Medium,High\n#&gt;   High               7          0               0\n#&gt;   Low,High           0          3               0\n#&gt;   Low,Medium,High    0          7               3\n\n\n\n\nModificación 2.\nVale, todo esto está muy bien, pero ¿y si simplemente para cada observación ordeno de forma decreciente su probabilidad predicha y me quedo con las clases que lleguen al 60% de probabilidad, por ejemplo?\n\n\nMostrar / ocultar código\n\n(predicciones_df &lt;-  as.data.frame(predicciones ))\n#&gt;          Low    Medium      High\n#&gt; 51 0.4260865 0.2544760 0.3194375\n#&gt; 52 0.2425400 0.2363201 0.5211399\n#&gt; 53 0.2425400 0.2363201 0.5211399\n#&gt; 54 0.2425400 0.2363201 0.5211399\n#&gt; 55 0.4014928 0.2566312 0.3418760\n#&gt; 56 0.4014928 0.2566312 0.3418760\n#&gt; 57 0.4014928 0.2566312 0.3418760\n#&gt; 58 0.3622588 0.2575226 0.3802186\n#&gt; 59 0.3622588 0.2575226 0.3802186\n#&gt; 60 0.3622588 0.2575226 0.3802186\n#&gt; 61 0.1967801 0.2160332 0.5871868\n#&gt; 62 0.1967801 0.2160332 0.5871868\n#&gt; 63 0.1967801 0.2160332 0.5871868\n#&gt; 64 0.4732214 0.2472855 0.2794931\n#&gt; 65 0.4732214 0.2472855 0.2794931\n#&gt; 66 0.4732214 0.2472855 0.2794931\n#&gt; 67 0.4320378 0.2537829 0.3141793\n#&gt; 68 0.4320378 0.2537829 0.3141793\n#&gt; 69 0.4320378 0.2537829 0.3141793\n#&gt; 70 0.2470311 0.2378946 0.5150743\n\n\n\n\nMostrar / ocultar código\nmodificacion_2 &lt;- predicciones_df |&gt; \n     rownames_to_column(var = \"individuo\") |&gt; \n    pivot_longer(cols = Low:High) |&gt; \n    group_by(individuo) |&gt; \n    arrange( desc(value)) |&gt; \n    mutate(suma_acumulada = cumsum(value)) |&gt; \n    arrange(individuo)\n\nhead(modificacion_2, 10)\n#&gt; # A tibble: 10 × 4\n#&gt; # Groups:   individuo [4]\n#&gt;    individuo name   value suma_acumulada\n#&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1 51        Low    0.426          0.426\n#&gt;  2 51        High   0.319          0.746\n#&gt;  3 51        Medium 0.254          1    \n#&gt;  4 52        High   0.521          0.521\n#&gt;  5 52        Low    0.243          0.764\n#&gt;  6 52        Medium 0.236          1    \n#&gt;  7 53        High   0.521          0.521\n#&gt;  8 53        Low    0.243          0.764\n#&gt;  9 53        Medium 0.236          1    \n#&gt; 10 54        High   0.521          0.521\n\n\nUhmm, pero no me acaba de convencer ordenar de forma descendente por la probabilidad predicha de cada clase. Por ejemplo para el individuo 51, si tomo Low +High llegaría a 0.74, pero si tomo Low + Medium llego al 67% . Si quisiera el menor conjunto de etiquetas que lleguen como mínimo al 60% la opción buena sería Low + Medium para ese individuo.\nNo me veo con ganas de implementar todas las posibles sumas de probabilidades estimadas y elegir el conjunto que cumpla la restricción de llegar al menos al 60% y si hay varios para mismo individuos que se quede con el conjunto más pequeño."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#conclusión.",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#conclusión.",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Conclusión.",
    "text": "Conclusión.\n\nLo de la predicción conforme para el caso de regresión me parece bastante sencillo, no es más que sumar y restar una medida de dispersión de los residuos a la predicción para nuevos datos.\nPara clasificación es un poco más interesante, sobre todo para casos en los que el usuario quiere una etiqueta o etiquetas y no se conforma con las probabilidades predichas de cada clase.\nSubyace la hipótesis de que los scores del modelo están bien calibrados y reflejan la verdadera probabilidad.\n\nPues nada más, tengan un feliz día."
  },
  {
    "objectID": "2019/11/30/la-fatal-arrogancia/index.html",
    "href": "2019/11/30/la-fatal-arrogancia/index.html",
    "title": "La fatal arrogancia",
    "section": "",
    "text": "Important\n\n\n\nNo, no voy a hablar de liberalismo ni de Hayek. Solo quería hacer una pequeña reflexión sobre las nuevas generaciones de científicos de datos o como se les quiera llamar.\n\n\nVengo observando hace cosa de 3 años, que las nuevas generaciones creen que es fácil utilizar modelos estadísticos (o de Machín Lenin como dice algún amigo mío) para predecir cosas como la bolsa, o acertar ,cual demiurgo, si se va a sufrir un cáncer y cosas por el estilo.\nLa fatal arrogancia a la que me refiero es aquella que hace que hasta los que trabajamos en estos temas, nos dejemos llevar por el “hype” (no sé bien como traducir esto) que se vive actualmente y nos creamos la cantidad de mentiras que se dicen. Es fácil entusiasmarse por cosas como el deep learning, reinforcement learning o el último algoritmo de boosting, pero ninguna de estas cosas puede hacer magia y separar totalmente la señal del ruido.\nTambién noto lo de la fatal arrogancia entre aquellos que dicen que para qué necesitan saber estadística, ¿a cuántos conocéis que den intervalos de confianza o de credibilidad en vez de solo estimaciones puntuales? Seguramente a muy pocos.\nEn fin, de lo que estoy seguro es que si hay alguno que ha conseguido a predecir con éxito notable los movimientos de la bolsa, lo que menos hace es alardear de su éxito dando charlas en meetups.\nNos vemos en los bares."
  },
  {
    "objectID": "2023/04/23/quantile-catboost/index.html",
    "href": "2023/04/23/quantile-catboost/index.html",
    "title": "Regresión cuantil a lo machín lenin con catboost",
    "section": "",
    "text": "Hay veces, más de las que se cree, en que nos interesa estimar un cuantil en vez de la media. Si tenemos una variable dependinte \\(y\\) y una o varias independientes \\(X\\), lo que se suele hacer es una regresión cuantil.\nSi visitamos uno de los papers originales de dicha técnica Computing Regression Quantiles vemos que trata de minimizar la siguiente expresión.\n\\[\n\\arg\\min_b R_{\\theta}(b)  = \\sum_{i = 1}^{n}\\rho_\\theta \\left( y_i - x_ib\\right)\n\\] Con \\(\\theta \\in (0,1)\\) y\n\\[\n\\begin{equation}\n    \\rho_\\theta(u) =\n        \\begin{cases}\n        \\theta u  &  u \\geq 0\\\\\n        (\\theta -1) & u  &lt; 0 \\\\\n        \\end{cases}\n\\end{equation}\n\\]\nLo cual es simplemente “penalizar” por \\(\\theta\\) cuando el residuo sea mayor o igual que 0, es decir, cuando nos equivocamos por arriba y por \\((\\theta -1)\\) si nos equivocamos por abajo.\nEjemplo, si \\(y_i = 40\\) y \\(f(x) = 50\\) y queremos estimar el cuantil 0.95. Entonces como el residuo es menor que 0, se pondera por 0.05\n\\[\\rho_(40 - 50) = (0.95 -1) (40 - 50) = 0.5 \\] Si en cambio \\(f(x) = 30\\), es decir, nos equivocamos por abajo, pero a la misma distancia del valor real entonces\n\\[\\rho(40-30) = 0.95 (40-30) = 9.5 \\]\nY por tanto la función a minimizar \\(\\arg\\min_b R_{\\theta}(b)\\) cuando \\(\\theta &gt; 0.5\\) va a tener un valor mucho mayor cuando nos “equivocamos” por abajo que por arriba. Y debido a cómo está definido \\(\\rho_\\theta(u)\\) se consigue la regresión cuantil con cuantil igual a \\(\\theta\\). En el paper (de 1987) viene mejor explicado y el algoritmo para resolverlo en el caso de que \\(f(x)\\) sea lineal.\nFuera coñas, el caso es que la gente de yandex en su librería catboost han utilizado esto para hacer la regresión cuantil, simplemente utilizando la expresión anterior como función de pérdida. Aquí se puede ver las diferentes funciones de pérdida que usan según el caso.\nPara la regresión cuantil usan\n\\[L(t, a, \\alpha) = \\dfrac{\\sum_{i}^{N} \\omega_i(\\alpha - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}\\] Dónde\nComo vemos, es lo mismo que se cuenta en el paper de 1987. Pero al meterlo como función de pérdida en el algoritmo sirve para el algoritmo de boosting que se utiliza en la librería.\nLa gente de catboost, atinadamente ha dicho, y ¿por qué no construimos un función de pérdida que minimice globalmente varios cuantiles? Lo cual es algo así como “encuéntrame la distribución de los parámetros que mejor se ajusta a estos datos en vez de un sólo parámetro”.\nPero esto son arbolitos y boosting, no hay lo que se dice un parámetro de la función propiamente dicho, por lo que al final lo que se “aprende” debe ser la configuración de árboles que minimiza globalmente los cuantiles indicados.\nBueno, la función de pérdida “multi-quantile” es una modificación simple de la anterior.\n\\[L(t, a, \\alpha_q) = \\dfrac{\\sum_{i}^{N} \\omega_i \\sum_{q=1}^{Q}(\\alpha_q - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}\\]"
  },
  {
    "objectID": "2023/04/23/quantile-catboost/index.html#ejemplo",
    "href": "2023/04/23/quantile-catboost/index.html#ejemplo",
    "title": "Regresión cuantil a lo machín lenin con catboost",
    "section": "Ejemplo",
    "text": "Ejemplo\nEl ejemplo no es mío, lo he visto por algún sitio que no me acuerdo.\n\n\n\n\n\n\nTip\n\n\n\ncatboost se puede utilizar en R y python.\n\n\n\n\nMostrar / ocultar código\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\nsns.set()\n\nn = 800\n\n# X aleatorias\nx_train = np.random.rand(n)\nx_test = np.random.rand(n)\n\n# un poquito de ruido gaussiano\n\nnoise_train = np.random.normal(0, 0.3, n)\nnoise_test = np.random.normal(0, 0.3, n)\n\n# Simulamos y_train e y _x como y = 2 + 3 * x + ruido\na, b = 2, 3\n\n# al lio\ny_train = a * x_train + b + noise_train\ny_test = a * x_test + b + noise_test\n\n\nPintamos\n\n\nMostrar / ocultar código\nsns.scatterplot(x = x_train, y = y_train).set(title = \"Ejemplillo\")\n\n\n\n\n\n\n\n\n\nVaos a predecir 10 cuantiles\n\n\nMostrar / ocultar código\nquantiles = [q/10 for q in range(1, 10)]\n\n# se ponen en string separados por commas\nquantile_str = str(quantiles).replace('[','').replace(']','')\n\nprint(quantile_str)\n#&gt; 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n\n\nModelito\n\n\nMostrar / ocultar código\nmodel = CatBoostRegressor(iterations=100,\n                          loss_function=f'MultiQuantile:alpha={quantile_str}')\n\nmodel.fit(x_train.reshape(-1,1), y_train)\n#&gt; 0:   learn: 0.1977907    total: 49.8ms   remaining: 4.93s\n#&gt; 1:   learn: 0.1934878    total: 54.5ms   remaining: 2.67s\n#&gt; 2:   learn: 0.1891389    total: 61.2ms   remaining: 1.98s\n#&gt; 3:   learn: 0.1849242    total: 64.9ms   remaining: 1.56s\n#&gt; 4:   learn: 0.1808818    total: 68.4ms   remaining: 1.3s\n#&gt; 5:   learn: 0.1769520    total: 71.7ms   remaining: 1.12s\n#&gt; 6:   learn: 0.1732251    total: 74.7ms   remaining: 993ms\n#&gt; 7:   learn: 0.1696774    total: 77.9ms   remaining: 896ms\n#&gt; 8:   learn: 0.1661482    total: 80.9ms   remaining: 818ms\n#&gt; 9:   learn: 0.1628541    total: 84.5ms   remaining: 760ms\n#&gt; 10:  learn: 0.1596378    total: 87.6ms   remaining: 709ms\n#&gt; 11:  learn: 0.1565479    total: 90.7ms   remaining: 665ms\n#&gt; 12:  learn: 0.1536016    total: 93.8ms   remaining: 628ms\n#&gt; 13:  learn: 0.1507692    total: 96.9ms   remaining: 595ms\n#&gt; 14:  learn: 0.1480845    total: 100ms    remaining: 567ms\n#&gt; 15:  learn: 0.1454840    total: 103ms    remaining: 541ms\n#&gt; 16:  learn: 0.1429863    total: 106ms    remaining: 518ms\n#&gt; 17:  learn: 0.1407059    total: 109ms    remaining: 497ms\n#&gt; 18:  learn: 0.1383697    total: 112ms    remaining: 477ms\n#&gt; 19:  learn: 0.1361779    total: 115ms    remaining: 460ms\n#&gt; 20:  learn: 0.1340707    total: 118ms    remaining: 444ms\n#&gt; 21:  learn: 0.1320342    total: 122ms    remaining: 432ms\n#&gt; 22:  learn: 0.1300775    total: 125ms    remaining: 418ms\n#&gt; 23:  learn: 0.1282447    total: 129ms    remaining: 408ms\n#&gt; 24:  learn: 0.1264588    total: 133ms    remaining: 399ms\n#&gt; 25:  learn: 0.1247594    total: 137ms    remaining: 390ms\n#&gt; 26:  learn: 0.1232137    total: 141ms    remaining: 381ms\n#&gt; 27:  learn: 0.1216563    total: 145ms    remaining: 373ms\n#&gt; 28:  learn: 0.1202034    total: 149ms    remaining: 365ms\n#&gt; 29:  learn: 0.1187735    total: 153ms    remaining: 357ms\n#&gt; 30:  learn: 0.1174263    total: 157ms    remaining: 350ms\n#&gt; 31:  learn: 0.1161250    total: 162ms    remaining: 344ms\n#&gt; 32:  learn: 0.1148583    total: 172ms    remaining: 350ms\n#&gt; 33:  learn: 0.1136688    total: 176ms    remaining: 342ms\n#&gt; 34:  learn: 0.1125292    total: 180ms    remaining: 335ms\n#&gt; 35:  learn: 0.1114357    total: 184ms    remaining: 327ms\n#&gt; 36:  learn: 0.1104088    total: 187ms    remaining: 318ms\n#&gt; 37:  learn: 0.1094203    total: 190ms    remaining: 309ms\n#&gt; 38:  learn: 0.1085302    total: 193ms    remaining: 301ms\n#&gt; 39:  learn: 0.1076702    total: 196ms    remaining: 294ms\n#&gt; 40:  learn: 0.1068348    total: 200ms    remaining: 288ms\n#&gt; 41:  learn: 0.1060263    total: 204ms    remaining: 281ms\n#&gt; 42:  learn: 0.1052530    total: 207ms    remaining: 274ms\n#&gt; 43:  learn: 0.1045115    total: 212ms    remaining: 269ms\n#&gt; 44:  learn: 0.1037861    total: 216ms    remaining: 264ms\n#&gt; 45:  learn: 0.1031053    total: 220ms    remaining: 259ms\n#&gt; 46:  learn: 0.1024645    total: 225ms    remaining: 254ms\n#&gt; 47:  learn: 0.1018457    total: 229ms    remaining: 248ms\n#&gt; 48:  learn: 0.1012497    total: 232ms    remaining: 242ms\n#&gt; 49:  learn: 0.1006996    total: 235ms    remaining: 235ms\n#&gt; 50:  learn: 0.1001835    total: 239ms    remaining: 229ms\n#&gt; 51:  learn: 0.0996695    total: 242ms    remaining: 223ms\n#&gt; 52:  learn: 0.0991990    total: 246ms    remaining: 218ms\n#&gt; 53:  learn: 0.0987716    total: 249ms    remaining: 212ms\n#&gt; 54:  learn: 0.0983443    total: 252ms    remaining: 206ms\n#&gt; 55:  learn: 0.0979411    total: 255ms    remaining: 200ms\n#&gt; 56:  learn: 0.0975621    total: 258ms    remaining: 195ms\n#&gt; 57:  learn: 0.0972083    total: 261ms    remaining: 189ms\n#&gt; 58:  learn: 0.0968639    total: 265ms    remaining: 184ms\n#&gt; 59:  learn: 0.0965243    total: 269ms    remaining: 179ms\n#&gt; 60:  learn: 0.0961923    total: 272ms    remaining: 174ms\n#&gt; 61:  learn: 0.0958829    total: 275ms    remaining: 168ms\n#&gt; 62:  learn: 0.0956101    total: 278ms    remaining: 163ms\n#&gt; 63:  learn: 0.0953473    total: 281ms    remaining: 158ms\n#&gt; 64:  learn: 0.0950908    total: 284ms    remaining: 153ms\n#&gt; 65:  learn: 0.0948327    total: 287ms    remaining: 148ms\n#&gt; 66:  learn: 0.0946033    total: 290ms    remaining: 143ms\n#&gt; 67:  learn: 0.0943820    total: 293ms    remaining: 138ms\n#&gt; 68:  learn: 0.0941723    total: 297ms    remaining: 133ms\n#&gt; 69:  learn: 0.0939663    total: 300ms    remaining: 129ms\n#&gt; 70:  learn: 0.0937677    total: 304ms    remaining: 124ms\n#&gt; 71:  learn: 0.0935819    total: 307ms    remaining: 119ms\n#&gt; 72:  learn: 0.0933969    total: 310ms    remaining: 115ms\n#&gt; 73:  learn: 0.0932181    total: 313ms    remaining: 110ms\n#&gt; 74:  learn: 0.0930514    total: 316ms    remaining: 105ms\n#&gt; 75:  learn: 0.0928960    total: 320ms    remaining: 101ms\n#&gt; 76:  learn: 0.0927433    total: 323ms    remaining: 96.4ms\n#&gt; 77:  learn: 0.0925871    total: 326ms    remaining: 91.9ms\n#&gt; 78:  learn: 0.0924635    total: 329ms    remaining: 87.6ms\n#&gt; 79:  learn: 0.0923467    total: 333ms    remaining: 83.1ms\n#&gt; 80:  learn: 0.0922292    total: 336ms    remaining: 78.7ms\n#&gt; 81:  learn: 0.0921089    total: 339ms    remaining: 74.3ms\n#&gt; 82:  learn: 0.0919860    total: 342ms    remaining: 70.1ms\n#&gt; 83:  learn: 0.0918862    total: 346ms    remaining: 65.9ms\n#&gt; 84:  learn: 0.0917925    total: 349ms    remaining: 61.6ms\n#&gt; 85:  learn: 0.0916927    total: 353ms    remaining: 57.4ms\n#&gt; 86:  learn: 0.0916089    total: 356ms    remaining: 53.1ms\n#&gt; 87:  learn: 0.0915482    total: 356ms    remaining: 48.6ms\n#&gt; 88:  learn: 0.0914624    total: 362ms    remaining: 44.7ms\n#&gt; 89:  learn: 0.0913841    total: 365ms    remaining: 40.5ms\n#&gt; 90:  learn: 0.0912981    total: 368ms    remaining: 36.4ms\n#&gt; 91:  learn: 0.0912323    total: 371ms    remaining: 32.3ms\n#&gt; 92:  learn: 0.0911620    total: 374ms    remaining: 28.2ms\n#&gt; 93:  learn: 0.0910968    total: 378ms    remaining: 24.1ms\n#&gt; 94:  learn: 0.0910390    total: 381ms    remaining: 20ms\n#&gt; 95:  learn: 0.0909742    total: 384ms    remaining: 16ms\n#&gt; 96:  learn: 0.0909243    total: 387ms    remaining: 12ms\n#&gt; 97:  learn: 0.0908512    total: 390ms    remaining: 7.95ms\n#&gt; 98:  learn: 0.0907883    total: 393ms    remaining: 3.97ms\n#&gt; 99:  learn: 0.0907341    total: 396ms    remaining: 0us\n#&gt; &lt;catboost.core.CatBoostRegressor object at 0x7f75bdded3c0&gt;\n\n\nPredecimos\n\n\nMostrar / ocultar código\n\n# Make predictions on the test set\npreds = model.predict(x_test.reshape(-1, 1))\npreds = pd.DataFrame(preds, columns=[f'pred_{q}' for q in quantiles])\n\npreds.head(6)\n#&gt;    pred_0.1  pred_0.2  pred_0.3  ...  pred_0.7  pred_0.8  pred_0.9\n#&gt; 0  3.401601  3.542894  3.619259  ...  3.938859  4.041568  4.165809\n#&gt; 1  4.150397  4.269908  4.355643  ...  4.701792  4.816983  4.958197\n#&gt; 2  4.350918  4.476999  4.595515  ...  4.933372  5.017778  5.147716\n#&gt; 3  2.829240  2.936415  3.034001  ...  3.365471  3.446338  3.577815\n#&gt; 4  3.456021  3.574613  3.658350  ...  3.966363  4.047775  4.166815\n#&gt; 5  3.356988  3.505757  3.580486  ...  3.858433  3.961821  4.084200\n#&gt; \n#&gt; [6 rows x 9 columns]\n\n\nPintamos\n\n\nMostrar / ocultar código\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x_test, y_test)\n\nfor col in ['pred_0.1', 'pred_0.5', 'pred_0.9']:\n    ax.scatter(x_test.reshape(-1,1), preds[col], alpha=0.50, label=col)\n\nax.legend()\n\n\n\n\n\n\n\n\n\nY ya estaría, no parece mala alternativa si uno tiene que hacer este tipo de cosas.\n\n\n\n\n\n\nTip\n\n\n\nOjalá le sirva a mi amigo Kenet para una cosa que estaba bicheando.\n\n\nPues poco más. Feliz domingo\nCon R también se puede, como no.\n\n\nMostrar / ocultar código\nlibrary(reticulate) # para comunicar R y python y poder convertir datos y funciones de uno a otro bidireccionalmente\nlibrary(catboost)\n\nX_train &lt;- as.matrix(py$x_train) # catboost en R espera  una matriz\nY_train &lt;-  as.matrix(py$y_train)\n\n\nX_test &lt;- as.matrix(py$x_test) \nY_test &lt;-  as.matrix(py$y_test)\n\nhead(X_train) ; head(Y_train)\n#&gt;           [,1]\n#&gt; [1,] 0.6499924\n#&gt; [2,] 0.9125034\n#&gt; [3,] 0.4820761\n#&gt; [4,] 0.2060426\n#&gt; [5,] 0.5411845\n#&gt; [6,] 0.1779657\n#&gt;          [,1]\n#&gt; [1,] 4.165367\n#&gt; [2,] 5.018479\n#&gt; [3,] 4.146041\n#&gt; [4,] 2.942043\n#&gt; [5,] 4.287877\n#&gt; [6,] 3.589826\n\n(quantiles_str &lt;-  py$quantile_str)\n#&gt; [1] \"0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\"\n\n\n\n\nMostrar / ocultar código\ntrain_pool &lt;- catboost.load_pool(data = X_train, label = Y_train)\ntest_pool &lt;- catboost.load_pool(data = X_test)\nloss_function &lt;-  paste0(\"MultiQuantile:alpha=\", quantiles_str)\n\nfit_params &lt;-  list(\n    iterations = 100,\n    loss_function= loss_function\n    )\n\n\n\n\nMostrar / ocultar código\n\nmodel &lt;- catboost.train(train_pool, params=fit_params)\n#&gt; 0:   learn: 0.1977907    total: 50.6ms   remaining: 5s\n#&gt; 1:   learn: 0.1934878    total: 56.9ms   remaining: 2.79s\n#&gt; 2:   learn: 0.1891389    total: 61.3ms   remaining: 1.98s\n#&gt; 3:   learn: 0.1849242    total: 66.1ms   remaining: 1.58s\n#&gt; 4:   learn: 0.1808818    total: 71ms remaining: 1.35s\n#&gt; 5:   learn: 0.1769520    total: 74.5ms   remaining: 1.17s\n#&gt; 6:   learn: 0.1732251    total: 78.2ms   remaining: 1.04s\n#&gt; 7:   learn: 0.1696774    total: 83ms remaining: 955ms\n#&gt; 8:   learn: 0.1661482    total: 86.6ms   remaining: 876ms\n#&gt; 9:   learn: 0.1628541    total: 89.7ms   remaining: 807ms\n#&gt; 10:  learn: 0.1596378    total: 93.1ms   remaining: 753ms\n#&gt; 11:  learn: 0.1565479    total: 97.3ms   remaining: 714ms\n#&gt; 12:  learn: 0.1536016    total: 102ms    remaining: 683ms\n#&gt; 13:  learn: 0.1507692    total: 105ms    remaining: 647ms\n#&gt; 14:  learn: 0.1480845    total: 109ms    remaining: 615ms\n#&gt; 15:  learn: 0.1454840    total: 113ms    remaining: 592ms\n#&gt; 16:  learn: 0.1429863    total: 118ms    remaining: 576ms\n#&gt; 17:  learn: 0.1407059    total: 122ms    remaining: 557ms\n#&gt; 18:  learn: 0.1383697    total: 126ms    remaining: 536ms\n#&gt; 19:  learn: 0.1361779    total: 129ms    remaining: 518ms\n#&gt; 20:  learn: 0.1340707    total: 133ms    remaining: 500ms\n#&gt; 21:  learn: 0.1320342    total: 137ms    remaining: 485ms\n#&gt; 22:  learn: 0.1300775    total: 140ms    remaining: 468ms\n#&gt; 23:  learn: 0.1282447    total: 144ms    remaining: 455ms\n#&gt; 24:  learn: 0.1264588    total: 147ms    remaining: 441ms\n#&gt; 25:  learn: 0.1247594    total: 151ms    remaining: 429ms\n#&gt; 26:  learn: 0.1232137    total: 154ms    remaining: 416ms\n#&gt; 27:  learn: 0.1216563    total: 157ms    remaining: 404ms\n#&gt; 28:  learn: 0.1202034    total: 161ms    remaining: 394ms\n#&gt; 29:  learn: 0.1187735    total: 164ms    remaining: 383ms\n#&gt; 30:  learn: 0.1174263    total: 169ms    remaining: 375ms\n#&gt; 31:  learn: 0.1161250    total: 172ms    remaining: 365ms\n#&gt; 32:  learn: 0.1148583    total: 175ms    remaining: 356ms\n#&gt; 33:  learn: 0.1136688    total: 180ms    remaining: 349ms\n#&gt; 34:  learn: 0.1125292    total: 185ms    remaining: 343ms\n#&gt; 35:  learn: 0.1114357    total: 188ms    remaining: 334ms\n#&gt; 36:  learn: 0.1104088    total: 195ms    remaining: 332ms\n#&gt; 37:  learn: 0.1094203    total: 199ms    remaining: 325ms\n#&gt; 38:  learn: 0.1085302    total: 202ms    remaining: 316ms\n#&gt; 39:  learn: 0.1076702    total: 205ms    remaining: 308ms\n#&gt; 40:  learn: 0.1068348    total: 212ms    remaining: 305ms\n#&gt; 41:  learn: 0.1060263    total: 216ms    remaining: 298ms\n#&gt; 42:  learn: 0.1052530    total: 219ms    remaining: 290ms\n#&gt; 43:  learn: 0.1045115    total: 223ms    remaining: 284ms\n#&gt; 44:  learn: 0.1037861    total: 228ms    remaining: 278ms\n#&gt; 45:  learn: 0.1031053    total: 232ms    remaining: 272ms\n#&gt; 46:  learn: 0.1024645    total: 235ms    remaining: 265ms\n#&gt; 47:  learn: 0.1018457    total: 242ms    remaining: 262ms\n#&gt; 48:  learn: 0.1012497    total: 246ms    remaining: 256ms\n#&gt; 49:  learn: 0.1006996    total: 250ms    remaining: 250ms\n#&gt; 50:  learn: 0.1001835    total: 253ms    remaining: 243ms\n#&gt; 51:  learn: 0.0996695    total: 257ms    remaining: 237ms\n#&gt; 52:  learn: 0.0991990    total: 261ms    remaining: 232ms\n#&gt; 53:  learn: 0.0987716    total: 265ms    remaining: 225ms\n#&gt; 54:  learn: 0.0983443    total: 268ms    remaining: 219ms\n#&gt; 55:  learn: 0.0979411    total: 272ms    remaining: 214ms\n#&gt; 56:  learn: 0.0975621    total: 275ms    remaining: 208ms\n#&gt; 57:  learn: 0.0972083    total: 279ms    remaining: 202ms\n#&gt; 58:  learn: 0.0968639    total: 283ms    remaining: 197ms\n#&gt; 59:  learn: 0.0965243    total: 287ms    remaining: 191ms\n#&gt; 60:  learn: 0.0961923    total: 292ms    remaining: 187ms\n#&gt; 61:  learn: 0.0958829    total: 298ms    remaining: 182ms\n#&gt; 62:  learn: 0.0956101    total: 304ms    remaining: 178ms\n#&gt; 63:  learn: 0.0953473    total: 308ms    remaining: 173ms\n#&gt; 64:  learn: 0.0950908    total: 313ms    remaining: 168ms\n#&gt; 65:  learn: 0.0948327    total: 317ms    remaining: 163ms\n#&gt; 66:  learn: 0.0946033    total: 324ms    remaining: 160ms\n#&gt; 67:  learn: 0.0943820    total: 329ms    remaining: 155ms\n#&gt; 68:  learn: 0.0941723    total: 332ms    remaining: 149ms\n#&gt; 69:  learn: 0.0939663    total: 338ms    remaining: 145ms\n#&gt; 70:  learn: 0.0937677    total: 342ms    remaining: 140ms\n#&gt; 71:  learn: 0.0935819    total: 346ms    remaining: 134ms\n#&gt; 72:  learn: 0.0933969    total: 349ms    remaining: 129ms\n#&gt; 73:  learn: 0.0932181    total: 353ms    remaining: 124ms\n#&gt; 74:  learn: 0.0930514    total: 358ms    remaining: 119ms\n#&gt; 75:  learn: 0.0928960    total: 361ms    remaining: 114ms\n#&gt; 76:  learn: 0.0927433    total: 364ms    remaining: 109ms\n#&gt; 77:  learn: 0.0925871    total: 369ms    remaining: 104ms\n#&gt; 78:  learn: 0.0924635    total: 374ms    remaining: 99.3ms\n#&gt; 79:  learn: 0.0923467    total: 377ms    remaining: 94.3ms\n#&gt; 80:  learn: 0.0922292    total: 380ms    remaining: 89.2ms\n#&gt; 81:  learn: 0.0921089    total: 386ms    remaining: 84.8ms\n#&gt; 82:  learn: 0.0919860    total: 390ms    remaining: 79.9ms\n#&gt; 83:  learn: 0.0918862    total: 394ms    remaining: 75ms\n#&gt; 84:  learn: 0.0917925    total: 398ms    remaining: 70.2ms\n#&gt; 85:  learn: 0.0916927    total: 404ms    remaining: 65.8ms\n#&gt; 86:  learn: 0.0916089    total: 408ms    remaining: 61ms\n#&gt; 87:  learn: 0.0915482    total: 409ms    remaining: 55.8ms\n#&gt; 88:  learn: 0.0914624    total: 412ms    remaining: 50.9ms\n#&gt; 89:  learn: 0.0913841    total: 416ms    remaining: 46.2ms\n#&gt; 90:  learn: 0.0912981    total: 420ms    remaining: 41.6ms\n#&gt; 91:  learn: 0.0912323    total: 424ms    remaining: 36.9ms\n#&gt; 92:  learn: 0.0911620    total: 427ms    remaining: 32.2ms\n#&gt; 93:  learn: 0.0910968    total: 431ms    remaining: 27.5ms\n#&gt; 94:  learn: 0.0910390    total: 437ms    remaining: 23ms\n#&gt; 95:  learn: 0.0909742    total: 441ms    remaining: 18.4ms\n#&gt; 96:  learn: 0.0909243    total: 444ms    remaining: 13.7ms\n#&gt; 97:  learn: 0.0908512    total: 449ms    remaining: 9.17ms\n#&gt; 98:  learn: 0.0907883    total: 454ms    remaining: 4.58ms\n#&gt; 99:  learn: 0.0907341    total: 457ms    remaining: 0us\n\n\n\n\nMostrar / ocultar código\npredicciones &lt;- catboost.predict(model, pool = test_pool)\n\n\n\n\nMostrar / ocultar código\ncolnames(predicciones) &lt;- paste0(\"quantile_\", 1:9) \n\nhead(predicciones)\n#&gt;      quantile_1 quantile_2 quantile_3 quantile_4 quantile_5 quantile_6\n#&gt; [1,]   3.401601   3.542894   3.619259   3.716297   3.794767   3.868639\n#&gt; [2,]   4.150397   4.269908   4.355643   4.431774   4.502443   4.586961\n#&gt; [3,]   4.350918   4.476999   4.595515   4.651673   4.713752   4.815297\n#&gt; [4,]   2.829240   2.936415   3.034001   3.123039   3.208654   3.292293\n#&gt; [5,]   3.456021   3.574613   3.658350   3.741498   3.824226   3.897069\n#&gt; [6,]   3.356988   3.505757   3.580486   3.651319   3.720015   3.791630\n#&gt;      quantile_7 quantile_8 quantile_9\n#&gt; [1,]   3.938859   4.041568   4.165809\n#&gt; [2,]   4.701792   4.816983   4.958197\n#&gt; [3,]   4.933372   5.017778   5.147716\n#&gt; [4,]   3.365471   3.446338   3.577815\n#&gt; [5,]   3.966363   4.047775   4.166815\n#&gt; [6,]   3.858433   3.961821   4.084200"
  },
  {
    "objectID": "test_quarto_things.html",
    "href": "test_quarto_things.html",
    "title": "to_test",
    "section": "",
    "text": "Note\n\n\n\ndfdf\n\n\n\ntab1tab2tab3\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nplot(mtcars$mpg, mtcars$cyl)\n\n\n\n\n\n\n\n\n\n\nparece que mola\n\n\na ver\n\n\n\n\n😨\n\n\n\n\n\n\nTip\n\n\n\nmi tip"
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "",
    "text": "El título de la entrada, sobre todo lo de la parte de “estilo compadre” viene de mis tiempos en consultoría, y tiene que ver con la necesidad de dar soluciones subóptimas a problemas acuciantes. Otra de mis frases, de la que puede que se acuerden Boris, Laura y Lourdes fue la de “si me das madera te hago un troncomóvil, no un ferrari”, lo cual es el equivalente a GIGO de toda la vida, pero a mi estilo.\nVamos al lío, últimamente ando estudianddo estadística bayesiana con el excelente material que pone a disposición de todo el mundo, y gratis, Aki Vehtari en este sitio Curso BDA3. Aki Vehtari es uno de los autores junto con Gelman y otros del libro Bayesian Data Analysis.\nEn la página 48 y siguientes tienen un ejemplo de como realizar inferencia bayesiana para el ratio muertes por cáncer usando un modelo básico Poisson-Gamma. Pero lo interesante es que comentan como construir una priori a partir de los datos, y que la forma en que lo hacen en este ejemplo puede considerarse una aproximación a como se construye en los modelos jerárquicos.\nTotal, que dado que en mi pueblo han aumentado, por desgracia, los casos y nos han confinado perimetralmente, voy a hacer el ejercicio de utilizar los datos del área sanitaria granada nordeste y adaptar el ejemplo.\nAviso que mi conocimiento de estadística bayesiana es limitado y muy probablemente puede que haga algo mal. Estoy aprendiendo, jejej."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#datos",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#datos",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "Datos",
    "text": "Datos\nEn primer lugar los datos por municipios vienen aqui. Y bueno, estaría bien que estuvieran un poco mejor organizados, puesto que solo puedes bajarte los últimos datos actualizados, no hay serie histórica por municipio, o al menos yo no la he encontrado.\nDespués de bajarme el excel al final me quedo solo con los datos de los municipios del área sanitaria Granada Nordeste.\n\nlibrary(tidyverse)\ng_nordeste <- read_csv(here::here(\"data/g_nordeste_20210326.csv\"))\n\ng_nordeste\n#> # A tibble: 46 × 3\n#>    lugar_de_residencia poblacion_miles confirmados_pdia_14_dias\n#>    <chr>                         <dbl>                    <dbl>\n#>  1 Alamedilla                    0.569                        0\n#>  2 Albuñán                       0.409                        0\n#>  3 Aldeire                       0.63                         0\n#>  4 Alicún de Ortega              0.471                        0\n#>  5 Alquife                       0.58                         0\n#>  6 Baza                         20.4                         27\n#>  7 Beas de Guadix                0.329                        0\n#>  8 Benalúa                       3.31                        11\n#>  9 Benamaurel                    2.26                         4\n#> 10 Calahorra (La)                0.668                        0\n#> # ℹ 36 more rows\n\nY podríamos contruir las tasas brutas por cada mil habitantes.\n\ng_nordeste <- g_nordeste %>% \n  mutate(tasa_bruta = confirmados_pdia_14_dias / poblacion_miles)\n\ng_nordeste %>% \n  arrange(-tasa_bruta)\n#> # A tibble: 46 × 4\n#>    lugar_de_residencia poblacion_miles confirmados_pdia_14_dias tasa_bruta\n#>    <chr>                         <dbl>                    <dbl>      <dbl>\n#>  1 Cogollos de Guadix            0.642                       17      26.5 \n#>  2 Purullena                     2.31                        29      12.6 \n#>  3 Cortes de Baza                1.84                        22      11.9 \n#>  4 Peza (La)                     1.17                         8       6.86\n#>  5 Dólar                         0.628                        4       6.37\n#>  6 Zújar                         2.54                        15       5.90\n#>  7 Cúllar                        4.09                        23       5.62\n#>  8 Cuevas del Campo              1.74                         6       3.44\n#>  9 Benalúa                       3.31                        11       3.32\n#> 10 Huéneja                       1.17                         3       2.56\n#> # ℹ 36 more rows\n\nBueno, y vemos que mi pueblo, está el tercero con mayor tasa con 12.58 por 1000 habitantes o 1258 por cada 100 mil (si revisan la situación el próximo martes posiblemente cierren la activad esencial)."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#inferencia",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#inferencia",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "Inferencia",
    "text": "Inferencia\nBueno, pues podríamos considerar que los casos en un municipio \\(y_i\\) la verosimilitud sería de la forma\n\\[y_i \\sim Poisson(X_i\\cdot\\theta_i)\\] dónde \\(X_i\\) sería la población en miles y \\(\\theta_i\\) la tasa por cada 1000 habitantes.\nAhora el tema para hacer inferencia bayesiana es especificar la prior, y como comentan en el libro podríamos construir la prior utilizando los datos. El número de casos sigue una “predictive distribution” binomial negativa y con un poco de álgebra llegan a igualar la media y varianza de las tasas brutas y obtener los parámetros de esa binomial negativa. Aquí es cuándo llega lo de “estilo compadre”, en vez de considerar la binomial negativa, yo voy a ajustar una Gamma a las tasas brutas y calculo el \\(\\alpha\\) y \\(\\beta\\) de la gamma por el método de los momentos.\nElegir una Gamma como Prior es en parte porque es distribución conjugada de la Poisson y la posterior se obtiene directamente.\nSin más, sería resolver estas ecuaciones\n\\[E[\\text{tasas brutas}] = \\dfrac{\\alpha_{prior}}{\\beta_{prior}}\\] \\[Var[\\text{tasas brutas}] = \\dfrac{\\alpha_{prior}}{\\beta_{prior}^{2}}\\]\nDespejando\n\nmedia = mean(g_nordeste$tasa_bruta)\ncuasivarianza = var(g_nordeste$tasa_bruta)\n(beta = media/cuasivarianza)\n#> [1] 0.1028402\n(alpha = media* beta)\n#> [1] 0.2312364\n\nPor lo que usaremos como prior una \\(Gamma(0.10, 0.23)\\), que parece un poco débil, seguramente porque las tasas brutas son muy diferentes entre los municipios.\nComparando la densidad de las tasas brutas con la densidad de la priori no parece mala elección\n\n# repito 10 veces cada tasa para tner suficients puntos para ver la densidad  estimada \ndf <- data.frame(raw_thetas = rep(g_nordeste$tasa_bruta,10),   \n                 simulados = rgamma(nrow(g_nordeste)*10, alpha, beta) )\n\ndf %>% \n    ggplot(aes(x=raw_thetas)) + \n    geom_density(size = 1) +\n    geom_density(aes(x=simulados), col = \"darkred\", linetype=2, size = 1)\n\n\n\n\n\n\n\n\nY ya sólo quedaría calcular la posterior para cada municipio. Que sería de esta forma\n\\[P\\left( \\theta_i\\mid data \\right) \\sim \\text{Gamma}(\\alpha_{prior} + y_i, \\beta_{prior} + x_i)\\] Dónde \\(y_i\\) es el número de casos en los últimos 14 días en cada municipio y \\(x_i\\) los expuestos, es decir, la población (en miles) en cada municipio. Con esto ya podemos calcular, y añadimos también los intervalos de credibilidad\n\ng_nordeste <- g_nordeste %>% \n    mutate(\n        posterior_a = alpha + confirmados_pdia_14_dias,\n        posterior_b = beta + poblacion_miles,\n        posterior_mean = posterior_a/posterior_b, \n        lb = qgamma(.025, posterior_a, posterior_b), \n        ub = qgamma(.025, posterior_a, posterior_b, lower.tail = FALSE)\n    ) \n\nY si vemos los datos de mi pueblo y de alguno más.\n\ng_nordeste %>%\n    filter(lugar_de_residencia %in% c(\"Cortes de Baza\",\"Castilléjar\", \"Baza\",\"Castril\", \"Benamaurel\", \"Zújar\")) %>%\n    select(lugar_de_residencia, poblacion_miles,\n           confirmados_pdia_14_dias,\n           tasa_bruta,\n           posterior_mean) %>% \n  arrange(-posterior_mean)\n#> # A tibble: 6 × 5\n#>   lugar_de_residencia poblacion_miles confirmados_pdia_14_dias tasa_bruta\n#>   <chr>                         <dbl>                    <dbl>      <dbl>\n#> 1 Cortes de Baza                 1.84                       22     11.9  \n#> 2 Zújar                          2.54                       15      5.90 \n#> 3 Benamaurel                     2.26                        4      1.77 \n#> 4 Baza                          20.4                        27      1.32 \n#> 5 Castilléjar                    1.32                        1      0.757\n#> 6 Castril                        2.02                        0      0    \n#> # ℹ 1 more variable: posterior_mean <dbl>\n\nPues no varía mucho la posterior con respecto a a la bruta. Puede deberse a dos motivos, uno, que al tener gran variabilidad las tasas brutas en los municipios considerados la información que comparten es poca comparada con la información específica que aporta cada municipio y la verosimilitud se impone a la prior y por otro lado, al no haber hecho full bayesian para estimar la prior , hemos utilizado los datos de los muncipios dos veces, una para obtener los parámetros de la prior y otra para la posterior, lo que puede llevar a sobreajuste. En los modelos jerárquicos bien estimados (y no al estilo compadre), si se estima bien, pero esto es una aproximación para entender un poco la lógica que hay debajo.\nY ya solo falta pintar .\n\ng_nordeste %>% \n    ggplot(aes(x=reorder(lugar_de_residencia, posterior_mean), y = posterior_mean)) +\n    geom_point(color = \"darkred\", size = rel(2)) +\n    geom_errorbar(aes(ymin = lb , ymax = ub)) +\n    coord_flip() +\n    labs(x = \"municipio\", y = \"Tasa x 1000 habitantes\")\n\n\n\n\n\n\n\n\nLos intervalos de credibilidad más pequeños se corresponden con los municipios con mayor población. A la vista de estos datos, se deberían usar este tipo de estimadores (bien hechos) sobre todo para estimar en municipios con una población menor, y no tomar decisiones basadas en una estimación puntual.\nCoda. Utilizando un glmer con family poisson (o con binomial si se modela la tasa directamente) con efecto aleatorio el lugar de residencia se obtienen prácticamente los mismos resultados"
  },
  {
    "objectID": "2019/09/10/index.html",
    "href": "2019/09/10/index.html",
    "title": "Cosas que deben cambiar",
    "section": "",
    "text": "La semana pasada estuve en la bella ciudad de Alcoy en el congreso de Estadística e Investigación Operativa gracias a que nos invitaron a dar una sesión invitada presentando la Comunidad R-hispano.\nComo estoy en el mundo de la empresa mi percepción fue que, salvo en las charlas de investigación operativa, la distancia entre lo que se hace y se enseña en la universidad y lo que se utiliza en la empresa es bestial.\nLo peor de todo es que después de hablar con varios profesores allí presentes me invadió el pesimismo, los planes de estudios están obsoletos, se enseña mucha teoría y apenas hay algo que se parezca a un ejercicio que tenga el más mínimo parecido con lo que luego se van a encontrar esos alumnos en el mundo real.\nEn esta era de palabras grandilocuentes como big data, machine learning, reinforcement learning o real time analytics los informáticos están ganando la batalla a los matemáticos y estadísticos, son ellos los que están adaptando sus planes de estudio, son ellos los que están creando los masters de ciencia de datos, y los pocos profesores de matemáticas o estadística que quieren colaborar con los informáticos en crear grados y másters adecuados se encuentran a menudo con las reticencias cuando no negativa de sus propios departamentos.\nAfortunadamente en otros lugares del planeta hay gente que está poniendo las bases matemáticas formales para todas estas técnicas, entre los que cabe destacar a Friedman, Tibshirani, Hastie o Efron.\nOs dejo las referencias a un par de libros, que todo analista de datos debería tener en su colección.\n\nElements of Statistical Learning\nComputer Age Statistical Inference: Algorithms, Evidence and Data Science\n\nAsí que, aún queda esperanza.."
  },
  {
    "objectID": "2019/09/10/cosas-que-deben-cambiar/index.html",
    "href": "2019/09/10/cosas-que-deben-cambiar/index.html",
    "title": "Cosas que deben cambiar",
    "section": "",
    "text": "La semana pasada estuve en la bella ciudad de Alcoy en el congreso de Estadística e Investigación Operativa gracias a que nos invitaron a dar una sesión invitada presentando la Comunidad R-hispano.\nComo estoy en el mundo de la empresa mi percepción fue que, salvo en las charlas de investigación operativa, la distancia entre lo que se hace y se enseña en la universidad y lo que se utiliza en la empresa es bestial.\nLo peor de todo es que después de hablar con varios profesores allí presentes me invadió el pesimismo, los planes de estudios están obsoletos, se enseña mucha teoría y apenas hay algo que se parezca a un ejercicio que tenga el más mínimo parecido con lo que luego se van a encontrar esos alumnos en el mundo real.\nEn esta era de palabras grandilocuentes como big data, machine learning, reinforcement learning o real time analytics los informáticos están ganando la batalla a los matemáticos y estadísticos, son ellos los que están adaptando sus planes de estudio, son ellos los que están creando los masters de ciencia de datos, y los pocos profesores de matemáticas o estadística que quieren colaborar con los informáticos en crear grados y másters adecuados se encuentran a menudo con las reticencias cuando no negativa de sus propios departamentos.\nAfortunadamente en otros lugares del planeta hay gente que está poniendo las bases matemáticas formales para todas estas técnicas, entre los que cabe destacar a Friedman, Tibshirani, Hastie o Efron.\nOs dejo las referencias a un par de libros, que todo analista de datos debería tener en su colección.\n\nElements of Statistical Learning\nComputer Age Statistical Inference: Algorithms, Evidence and Data Science\n\nAsí que, aún queda esperanza.."
  },
  {
    "objectID": "2019/07/15/codificación-parcial/index.html",
    "href": "2019/07/15/codificación-parcial/index.html",
    "title": "Codificación parcial y python",
    "section": "",
    "text": "O como se conoce en estos tiempos modernos one hot encoding. En realidad se trata simplemente de cómo codificar una variable categórica en un conjunto de números que un algoritmo pueda utilizar.\nYa hablé de esto mismo en el post codificación de variables categóricas I\nBásicamente, la codificación parcial lo que hace es crearse tantas variables indicadoras como niveles tengo en mi variable menos 1.\nEjemplo. Construimos un conjunto de datos simple, con 3 variables\n\nset.seed(155)\n\nx1 <- rnorm(n = 100, mean = 4, sd = 1.5 )\nx2_cat <- factor(rep(c(\"a\",\"b\",\"c\",\"d\"),  25 ))\ny <- numeric(length = 100)\n\n# Construimos artificialmente relación entre x e y\ny <- 2 +  4 * x1 + rnorm(25, 0, 1)\n\n# cambiamos \"el intercept\" según la variable categórica\n\ny[x2_cat == \"a\"] <- y[x2_cat == \"a\"] + 8\ny[x2_cat == \"b\"] <- y[x2_cat == \"b\"] - 5\ny[x2_cat == \"c\"] <- y[x2_cat == \"c\"] + 3\ny[x2_cat == \"d\"] <- y[x2_cat == \"d\"] - 3\n\ndat <- data.frame(y, x1, x2_cat)\nhead(dat)\n#>          y       x1 x2_cat\n#> 1 31.00555 5.200100      a\n#> 2 19.13571 5.061407      b\n#> 3 20.49049 3.888438      c\n#> 4 17.93157 4.978832      d\n#> 5 25.23501 3.989047      a\n#> 6 21.86234 6.272138      b\n\nEn R al definir x2_cat como un factor él ya sabe que para ciertos métodos (por ejemplo una regresión) hay que codificar esa variable y por defecto utiliza la codificación parcial. Con la función contrasts vemos como lo hace.\n\ncontrasts(dat$x2_cat)\n#>   b c d\n#> a 0 0 0\n#> b 1 0 0\n#> c 0 1 0\n#> d 0 0 1\n\nY en las columnas tenemos las 3 variables indicadoras que ha construido. ¿Por qué 3? pues muy fácil, puesto que la “a” se puede codificar con el valor 0 en las tres variables indicadoras. Esto que parece una obviedad evita problemas de colinealidad en los algoritmos de regresión por ejemplo.\n\nfit1 <-  lm(y ~ x1 + x2_cat, data = dat)\nsummary(fit1)\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2_cat, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.55549 -0.67355 -0.00943  0.59814  1.99480 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.12924    0.28393   35.67   <2e-16 ***\n#> x1            3.94536    0.06034   65.39   <2e-16 ***\n#> x2_catb     -12.95740    0.26148  -49.55   <2e-16 ***\n#> x2_catc      -4.97712    0.25845  -19.26   <2e-16 ***\n#> x2_catd     -10.98359    0.25785  -42.60   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9094 on 95 degrees of freedom\n#> Multiple R-squared:  0.9855, Adjusted R-squared:  0.9849 \n#> F-statistic:  1616 on 4 and 95 DF,  p-value: < 2.2e-16\n\n¿Qué hubiera pasado si hubiéramos tratado con 4 variables indicadoras?\n\ndat2 <- dat\ndat2$ind1 <- ifelse(dat$x2_cat == \"a\", 1, 0)\ndat2$ind2 <- ifelse(dat$x2_cat == \"b\", 1, 0)\ndat2$ind3 <- ifelse(dat$x2_cat == \"c\", 1, 0)\ndat2$ind4 <- ifelse(dat$x2_cat == \"d\", 1, 0)\n\n\nhead(dat2[dat2$x2_cat==\"d\", ],3)\n#>           y       x1 x2_cat ind1 ind2 ind3 ind4\n#> 4  17.93157 4.978832      d    0    0    0    1\n#> 8   9.30838 2.736958      d    0    0    0    1\n#> 12 12.31765 2.943479      d    0    0    0    1\n\nSi metemos ahora esas variables en el modelo\n\nfit2 <-  lm(y ~ x1 +  ind2 + ind3 + ind4 + ind1, data = dat2)\nsummary(fit2)\n#> \n#> Call:\n#> lm(formula = y ~ x1 + ind2 + ind3 + ind4 + ind1, data = dat2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.55549 -0.67355 -0.00943  0.59814  1.99480 \n#> \n#> Coefficients: (1 not defined because of singularities)\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.12924    0.28393   35.67   <2e-16 ***\n#> x1            3.94536    0.06034   65.39   <2e-16 ***\n#> ind2        -12.95740    0.26148  -49.55   <2e-16 ***\n#> ind3         -4.97712    0.25845  -19.26   <2e-16 ***\n#> ind4        -10.98359    0.25785  -42.60   <2e-16 ***\n#> ind1               NA         NA      NA       NA    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9094 on 95 degrees of freedom\n#> Multiple R-squared:  0.9855, Adjusted R-squared:  0.9849 \n#> F-statistic:  1616 on 4 and 95 DF,  p-value: < 2.2e-16\n\nY vemos que como hay colinealidad R no estima el coeficiente de una de las variables indicadoras y hasta nos avisa con el mensaje Coefficients: (1 not defined because of singularities)\nPues la verdad es que mola que R sepa como tratar las categóricas si las has definido como factor pero también hace que la gente se olvide de que lo que en realidad hace es la codificación parcial.\nHablando de esto con un colega salió a colación que en python hay que explicitar la codificación y que quizá eso sea bueno porque así se sabe lo que se está haciendo y no hay lugar a dudas. Hasta aquí todo correcto, salvo que leyendo la documentación de pandas get_dummies resulta que por defecto construye tantas variables indicadoras como categorías y sólo tiene como opcional lo de quitar la primera con el parámetro drop_first, total me dije, no pasa nada, veamos como lo hace scikit learn y nada, resulta que por defecto también deja todas OneHotEncoder.\nReflexionando me dije, bueno, pues entonces cuando haga una regresión lineal con sklearn si uso las opciones por defecto de codificar las categóricas pues me debe saltar lo mismo que en R, es decir que hay un coeficiente que no puede estimar, pero resulta que sklearn hace un pelín de trampa y no salta el error, y no salta porque en sklearn la regresión lineal no ajusta una regresión lineal clásica, sino que por defecto y sin que tú lo pidas te hace una regresión regularizada y entonces no salta ese problema.\nPues la verdad , ¿qué puedo decir? no me hace gracia que por defecto no me quite la variable indicadora que sobra ni que haga regresión con regularización sin yo decirle nada.\nEn fin, veamos el ejemplo con python, aprovecho que escribo en un rmarkdown y puedo pasar objetos de R a python entre chunks sin muchos problemas.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndat_py = r.dat\ndat_py.describe()\n#>                 y          x1\n#> count  100.000000  100.000000\n#> mean    18.633855    3.988010\n#> std      7.402175    1.540500\n#> min     -1.163152    0.315987\n#> 25%     14.283915    2.815231\n#> 50%     19.325282    3.885684\n#> 75%     22.438970    5.062777\n#> max     43.075931    8.304381\ndat_py.x2_cat.value_counts()\n#> a    25\n#> b    25\n#> c    25\n#> d    25\n#> Name: x2_cat, dtype: int64\n\nconvertimos a dummies con pandas por ejemplo\n\ndat_py = pd.get_dummies(data=dat_py)\nprint(dat_py.head())\n#>            y        x1  x2_cat_a  x2_cat_b  x2_cat_c  x2_cat_d\n#> 0  31.005546  5.200100         1         0         0         0\n#> 1  19.135715  5.061407         0         1         0         0\n#> 2  20.490494  3.888438         0         0         1         0\n#> 3  17.931571  4.978832         0         0         0         1\n#> 4  25.235006  3.989047         1         0         0         0\n\n\nx_variables = ['x1', 'x2_cat_a', 'x2_cat_b','x2_cat_c','x2_cat_d']\n# Selecciono y convierto a numpy array\nX = dat_py[x_variables].values  \ny = dat_py['y'].values\nX[0:3]\n#> array([[5.20010037, 1.        , 0.        , 0.        , 0.        ],\n#>        [5.06140731, 0.        , 1.        , 0.        , 0.        ],\n#>        [3.88843797, 0.        , 0.        , 1.        , 0.        ]])\ny[0:3]\n#> array([31.00554596, 19.1357146 , 20.49049385])\n\n\n\nlm = LinearRegression()\nfit_python = lm.fit(X,y)\nprint('Intercept: ',fit_python.intercept_)\n#> Intercept:  2.899716060541241\nprint('Coef: ',fit_python.coef_)\n#> Coef:  [ 3.94536095  7.22952708 -5.72787734  2.25241099 -3.75406074]\n\nY vemos que si estima todos los coeficientes cuando no debería haber podido, esto tiene que ver como he dicho antes con que LinearRegression de sklearn no es la regresión lineal al uso sino que mete regularización.\nOtro día veremos la librería statmodels de python cuya salida nos da una información más rica de los modelos y bastante parecida a lo que estamos acostumbrados con R.\nNota: Leyendo la docu de LinearRegression en ningún sitio dice que use regularización así que no alcanzo a entender por qué ha podido estimar todos los coeficientes. A ver si alguno de mis amigos pythonisos me lo aclara."
  },
  {
    "objectID": "2019/07/03/malditas-proporciones-pequeñas-iii/index.html",
    "href": "2019/07/03/malditas-proporciones-pequeñas-iii/index.html",
    "title": "Malditas proporciones pequeñas III",
    "section": "",
    "text": "Volviendo al ejemplo de lo de las proporciones pequeñas, se trataba básicamente de que se tenía una población con una prevalencia de cierto evento del 4 x 1000 más o menos y en post anteriores veíamos cómo calcular tamaños de muestra y tal para ver cómo detectar un incremento de un 15% en esa proporción.\nAhora vamos a suponer que tenemos una población de 1.5 millones, pero que hay 5 grupos diferenciados, con prevalencias del 6, 5, 4, 3 y 2 por mil respectivamente y todos del mismo tamaño. Simulemos dicha población\n\nset.seed(155)\n\ngr1 <- rbinom(n = 3E5, size = 1, prob = 0.006)\ngr2 <- rbinom(n = 3E5, size = 1, prob = 0.005)\ngr3 <- rbinom(n = 3E5, size = 1, prob = 0.004)\ngr4 <- rbinom(n = 3E5, size = 1, prob = 0.003)\ngr5 <- rbinom(n = 3E5, size = 1, prob = 0.002)\n\npop <-  data.frame(grupo = rep(letters[1:5],each= 3E5),\n                   evento = c(gr1,gr2,gr3,gr4,gr5))\n\nVeamos 30 casos al azar.\n\npop[sample(1:nrow(pop), 30),]\n#>         grupo evento\n#> 92075       a      0\n#> 424873      b      0\n#> 1201923     e      0\n#> 1159523     d      0\n#> 830570      c      0\n#> 546477      b      0\n#> 1120381     d      0\n#> 613315      c      0\n#> 485130      b      0\n#> 52029       a      0\n#> 619858      c      0\n#> 590223      b      0\n#> 1034676     d      0\n#> 1153071     d      0\n#> 1266210     e      0\n#> 502866      b      0\n#> 99782       a      0\n#> 1388671     e      0\n#> 26049       a      0\n#> 971047      d      0\n#> 709908      c      0\n#> 376850      b      0\n#> 487569      b      0\n#> 365383      b      0\n#> 376533      b      0\n#> 1094390     d      0\n#> 873846      c      0\n#> 514258      b      0\n#> 1423814     e      0\n#> 730321      c      0\n\nComprobamos la prevalencia en la población total y por grupos\n\nmean(pop$evento)\n#> [1] 0.003958\n\n\nwith(pop,tapply(evento, grupo, mean))\n#>           a           b           c           d           e \n#> 0.005836667 0.004963333 0.004213333 0.002846667 0.001930000\n\nSupongamos ahora que encuentro un grupo por ahí del mismo tamaño (300k) con una prevalencia igual al mejor grupo que tengo, es decir, del 6 x 1000 y que la gente de marketing me cree y me deja que quite 300K con peor prevalencia y que los sustituya con mi grupo. ¿Cuánto sería la mejora de la prevalencia en esa nueva población?\nSimulemos\n\npop_new <- pop\npop_new$evento[pop_new$grupo==\"e\"] <- rbinom(n = 3E5, size = 1, prob = 0.006)\n\nmean(pop_new$evento)\n#> [1] 0.004748667\nmean(pop_new$evento)/mean(pop$evento)\n#> [1] 1.199764\n\nLa mejora sería del 19.9764191, % pero, ¿es porque he tenido suerte?. Si todos los meses encontrara un grupo así de majo, ¿en qué valores de mejora me estaría moviendo?\nSimulemos 100 realizaciones de este ejercicio\n\nres <- replicate(100, {\n  pop$evento[pop$grupo==\"e\"] <- rbinom(n = 3E5, size = 1, prob = 0.006)\n  return(100 * (mean(pop$evento) / 0.004 - 1))\n})\n\nY si dibujamos la función de densidad tenemos\n\nplot(density(res), main = \"% de mejora\", lwd = 2, col = \"darkblue\")\n\n\n\n\n\n\n\n\nVaya, pues parece que aún encontrando ese grupo tan molón y quitando el grupo malo mi mejora se va a quedar en torno al 19%.. Uhmm, ¿qué le digo a mi jefe cuándo me pida una mejora del 30%?"
  },
  {
    "objectID": "2023/05/06/mapeando/index.html",
    "href": "2023/05/06/mapeando/index.html",
    "title": "Mapeando",
    "section": "",
    "text": "Siempre me ha gustado el tema de los Sistemas de información geográfica y derivados. Ya cuando trabajaba en el IESA fui a un curso en Vigo sobre gvSIG y luego aprendí cosas con QGIS , el cual me sigue pareciendo un software más que excelente.\nHoy en día se pueden hacer muchísimas cosas con Python y con R, incluso tienen conectores con librerías de javascript para hacer cosas resultonas y con interactividad, veáse esto por ejemplo. En realidad tanto python como R tiran de las mismas librerías de bajo nivel, a saber, gdal, libproj, libgeos y similares, las cuales, para ser honestos, pueden meterte en un infierno de dependencias en según qué sistemas unix. Eso sí, una vez sales de ahí es una gozada.\nTotal, el caso es que el pasado jueves me llega una duda del gran Leonardo Hansa, que reproduzco a continuación.\nY bueno, como no tenía mucho más que hacer esa tarde y soy un picado de la vida, pues me dije.\nY me puse manos a la obra a investigar a ver cómo se podría hacer eso de encontrar polígonos adyacentes a uno dado usando Rstats. El caso es que llegué a varios hilos dónde se comentaban diferentes formas, una era usando la función poly2nb de la librería spdep y también cómo hacerlo usando la función st_intersects de la librería sf . En este issue incluso comentaban los grandes de estas cosas, Edzer Pebesma , Roger Bivand y nuestro conocico Virgilio Gómez Rubio.\nBueno, vamos al código, que de eso se trata.\nMostrar / ocultar código\nlibrary(tidyverse)\nlibrary(sf)\n\nmapa &lt;- st_read(here::here(\"data/shapefiles/cod_postales/cp_19_dis.shp\"))\n#&gt; Reading layer `cp_19_dis' from data source \n#&gt;   `/media/hd1/canadasreche@gmail.com/blog_quarto/data/shapefiles/cod_postales/cp_19_dis.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 10809 features and 4 fields (with 1 geometry empty)\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2021660 ymin: 3203377 xmax: 481778.5 ymax: 5433002\n#&gt; Projected CRS: WGS 84 / Pseudo-Mercator\n\nhead(mapa)\n#&gt; Simple feature collection with 6 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1536953 ymin: 3373964 xmax: -41802.13 ymax: 5247186\n#&gt; Projected CRS: WGS 84 / Pseudo-Mercator\n#&gt;      cp cp_num cp_2 cp_2_num                       geometry\n#&gt; 1 35560  35560   35       35 MULTIPOLYGON (((-1518970 33...\n#&gt; 2 27330  27330   27       27 MULTIPOLYGON (((-821864.3 5...\n#&gt; 3 46680  46680   46       46 MULTIPOLYGON (((-51610.46 4...\n#&gt; 4 49706  49706   49       49 MULTIPOLYGON (((-641488.4 5...\n#&gt; 5 21120  21120   21       21 MULTIPOLYGON (((-776955.2 4...\n#&gt; 6 16623  16623   16       16 MULTIPOLYGON (((-256256.7 4...\nPinto códigos postales de la provincia de Madrid por ejemplo, coloreando por área de cada polígono.\nMostrar / ocultar código\nmapa |&gt;\n    \n    # calculo area usando st_area\n    mutate(area_m2 = st_area(mapa) |&gt; \n               as.numeric()) |&gt;\n    filter(cp_2 == 28) |&gt;\n    ggplot() +\n    geom_sf(aes(fill = area_m2)) +\n    scale_fill_viridis_c()\nPara encontrar los polígonos adyacentes uso st_intersects que permite por ejemplo saber qué puntos están dentro de un polígono y cosas así. Al aplicarlo sobre una geometría de tipo polígono lo que encuentra son los polígonos adyacentes.\nMostrar / ocultar código\n# mido a ver cuánto tarda. \ntictoc::tic()\nlista_adyacentes &lt;- st_intersects(mapa)\ntictoc::toc()\n#&gt; 2.531 sec elapsed\nAhora si quisiera saber qué polígonos son adyacentes a uno dado, es simplemente seleccionar en lista adyacentes, por ejemplo\nMostrar / ocultar código\nlista_adyacentes[1]\n#&gt; [[1]]\n#&gt; [1]    1 3684 4621 6345 7017 7996\nPara ver los adyacentes a mi código postal en Madrid.\nMostrar / ocultar código\n(fila_mi_cp &lt;-  mapa |&gt; \n    rownames_to_column() |&gt; \n    filter(cp == \"28043\") |&gt; \n    pull(rowname))\n#&gt; [1] \"9138\"\n\n\n(mis_vecinos_fila &lt;- lista_adyacentes[as.numeric(fila_mi_cp)])\n#&gt; [[1]]\n#&gt; [1]  493 4831 4998 5127 6941 7662 9138 9427\nPero me devuelve el número de fila, para ver el cp sería\nMostrar / ocultar código\nmi_cp &lt;-  \"28043\"\n(mis_vecinos &lt;- mapa$cp[mis_vecinos_fila[[1]]])\n#&gt; [1] \"28028\" \"28042\" \"28002\" \"28022\" \"28033\" \"28016\" \"28043\" \"28027\"\n\n(adyacentes &lt;-  setdiff(mis_vecinos, mi_cp))\n#&gt; [1] \"28028\" \"28042\" \"28002\" \"28022\" \"28033\" \"28016\" \"28027\"\nPintamos\nMostrar / ocultar código\n  mapa |&gt; \n    filter(cp_2 == 28) |&gt;\n        mutate(\n            tipo = case_when(\n                cp == mi_cp ~ \"mi_cp\", \n                cp %in% adyacentes ~ \"mis_copostales_vecinos\", \n                TRUE ~ \"resto de codpostales\"\n            )\n        ) |&gt; \n        ggplot() +\n        geom_sf(aes(fill = tipo)) +\n        scale_fill_viridis_d()\ny listo.. Lo ponemos en unas funcioncitas todo\nMostrar / ocultar código\nget_adyacentes &lt;- function(mapa = mapa, id_col = \"cp_num\") {\n  \n  # quiza sacar el st_intersects de la función sea mejor\n  nb &lt;-  st_intersects(mapa)\n  \n  get_nb &lt;-  function(x){\n      res &lt;- mapa[[id_col]][x]\n      res\n  }\n  \n  adjacency_list &lt;-  lapply(nb, get_nb)\n  adjacency_list_names &lt;-  mapa[[id_col]][1:length(adjacency_list)]\n  names(adjacency_list) &lt;- adjacency_list_names\n  adjacency_list\n}\n\n\n\n\nget_mis_vecinos &lt;-  function(mi_cp, cps_adyacentes){\n    cp_simbol &lt;-  as.symbol(mi_cp) # a simbolo para poder llamara cps_adyacentes[[`18814`]]\n    mis_vecinos &lt;-  cps_adyacentes[[cp_simbol]]\n}\n\n\n\n# mapa seleccionando solo tu provincia \n\n\nplot_cp_vecinos &lt;-  function(mi_cp, cps_adyacentes, mapa){\n    cp_simbol &lt;-  as.symbol(mi_cp)\n    mis_vecinos &lt;-  cps_adyacentes[[cp_simbol]]\n    mi_prop &lt;-  stringr::str_sub(mi_cp, 1, 2) |&gt; as.numeric()\n    adyacentes &lt;-  setdiff(mis_vecinos, mi_cp)\n    mapa |&gt; \n        filter(cp_2_num == mi_prop) |&gt; \n        mutate(\n            tipo = case_when(\n                cp_num == mi_cp ~ \"mi_cp\", \n                cp_num %in% adyacentes ~ \"mis_copostales_vecinos\", \n                TRUE ~ \"resto de codpostales\"\n            )\n        ) |&gt; \n        ggplot() +\n        geom_sf(aes(fill = tipo)) +\n        scale_fill_viridis_d()\n}\nY ya podemos usarlo\nMostrar / ocultar código\ncps_adyacentes &lt;-  get_adyacentes(mapa, id_col = \"cp_num\")\n\n(mis_cps_vecinos_pueblo &lt;- get_mis_vecinos(18814, cps_adyacentes))\n#&gt; [1] 18815 18800 18817 18811 18814 18818 18816\nPintamos\nMostrar / ocultar código\n\nplot_cp_vecinos(18814, cps_adyacentes, mapa) +\n    labs(title = \"CP núcleo principal Cortes de Baza\")\nMostrar / ocultar código\n# plaza cascorro\n\nplot_cp_vecinos(28005, cps_adyacentes, mapa) +\n    labs(title = \"cp Plaza Cascorro\")\nMostrar / ocultar código\n# Pongo Carmona por ser el de mayor área \n\n(get_mis_vecinos(41410, cps_adyacentes))\n#&gt;  [1] 41620 41300 41449 41339 41420 41359 41610 41600 41320 41309 41520 41429\n#&gt; [13] 41410 41510 41310 41016 41500 41440\n\nplot_cp_vecinos(41410, cps_adyacentes, mapa) +\n    labs(title= \"Carmona\")"
  },
  {
    "objectID": "2023/05/06/mapeando/index.html#notas",
    "href": "2023/05/06/mapeando/index.html#notas",
    "title": "Mapeando",
    "section": "Notas",
    "text": "Notas\n\nLos códigos postales que uso están desactualizados. La capa de shapefile es un producto que vende Correos a precio no barato. Antes se podían descargar de CartoCiudad, pero ya no. Algún enlace interesante:\n\npost de Raúl Vaquerizo\npost de nosolosig\n\nLibros online sobre GIS en R:\n\nr-spatial\ngeocomputation with R\n\nApplied Spatial Data Analysis with R . Un poco antiguo ya, no sé si Virgilio, Roger y Edzer están trabajando sobre nueva edición o no"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "",
    "text": "La gente de Yandex es gente lista y son los que están detrás de catboost. Ya el pasado mes de Abril conté como hacían la regresión cuantil y obtenían estimación de varios cuantiles a la vez aquí"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#codificación-de-las-categóricas.",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#codificación-de-las-categóricas.",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "Codificación de las categóricas.",
    "text": "Codificación de las categóricas.\nCatboost por defecto usa one-hot-encoding pero si por algo es conocido es por tener otro método de codificación, el cual viene descrito en la docu. Otro sitio dónde viene relativamente bien explicado es en este post\nVamos a ver el detalle, para cuándo hay variables categóricas y la variable a predecir es binaria.\nLa idea en la que se basan tiene que ver con los test de permutación. Lo que hacen son varias iteraciones desordenando los datos y en cada iteración\n\nDesordenan las filas del data frame de forma aleatoria de forma que se crea un nuevo orden\nLa codificación del nivel de la variable categórica se calcula para cada fila como: avg_target=countInClass+priortotalCount+1\\text{avg_target} = \\dfrac{countInClass + prior}{totalCount +1} pero usando sólo los datos previos a esa fila\n\nY luego para cada observación toman como codificación la media de las codificaciones obtenidas en las diferentes permutaciones.\nEn avg_target\\text{avg_target} de la fila i se tiene que\n\ncountInClass: Cuenta las veces que en todos los datos previos a la fila i, se tiene un target = 1 para cuando el nivel de la variable categórica es igual al de la fila i.\nprior: Constante que se define al principio del algoritmo. Puede ser la proporción de 1’s en los datos por ejemplo.\ntotalCount: El número de observaciones con el mismo nivel en la variable categórica que tiene la fila i, en los datos previos.\n\nEn el segundo post podemos ver la siguiente figura.\n\n\n\nFigura\n\n\nEn este caso, si queremos calcular el valor de avg_target\\text{avg_target} para la quinta observación es tan sencillo como\n\nprior : La fijamos a 0.05, por ejemplo\ncountInClass: En los datos previos sólo había un caso en que el target fuera 1 y la marca Toyota, por lo tanto 1.\ntotalCount: En los datos previos hay 2 observaciones con marca Toyota.\n\nAsí que avg_target=countInClass+priortotalCount+1=1+0.052+1=0.35\\text{avg_target} = \\dfrac{countInClass + prior}{totalCount +1} = \\dfrac{1+0.05}{2+1} = 0.35\nLos autores de catboost reconocen que de esta forma si sólo haces una permutación de los datos para los primeros valores no se tiene info suficiente para obtener una buena codificación, así que proponen hacer varias permutaciones y tomar como codificación la media de las codificaciones anteriores."
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/delete.html",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/delete.html",
    "title": "Math",
    "section": "",
    "text": "FMVGAPimplied=100*(Implied_Interest_RateFMV_Interest_Rate−1)\n\\mbox{FMV}_{GAP}^{implied}  = 100* (\\frac{\\mbox{Implied_Interest_Rate}}{\\mbox{FMV_Interest_Rate}} - 1)"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#pensamientos",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#pensamientos",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "Pensamientos",
    "text": "Pensamientos\nPero, pero…\n\n¿No os recuerda un poco a como se hace un aprendizaje bayesiano? . Es decir parto de una priori (puede que poco informativa) y conforme voy obteniendo datos voy actualizando la distribución de mi parámetro, y de esa forma puedo obtener la posterior predictive distribution, que es la que aplicaría por ejemplo a un dato no visto.\nDe hecho al hacer varias permutaciones ¿ no está convergiendo de alguna manera la solución de catboost hacia la aproximación bayesiana?\n¿No os parece un poco de sobreingeniería, para algo que quizá con una aproximación estilo compadre bayesiana se podría obtener algo muy similar y con menos esfuerzo?\n\n\nPruebecilla\nEl ejemplo que viene en la docu, dónde se ejemplifica con un pequeño dataset de 7 filas y muestran una de las permutaciones generadas.\n\n\n\npermutation1\n\n\nQue tras aplicar el algoritmo quedaría para esta permutación queda como \nReplicamos en código\n\n\nMostrar / ocultar código\nlibrary(tidyverse) # pa 4 tontás de hacer sample y de groups bys que hago luego\nlibrary(parallel) # para usar varios cores con mclapply\nlibrary(patchwork) # pa juntar ggplots\n\n\n\n\nMostrar / ocultar código\n\nmydf &lt;- tribble(\n    ~id,~f2, ~cat, ~label, \n    1,53,\"rock\",  0,\n    2,55,\"indie\", 0, \n    3,40,\"rock\",  1, \n    4,42,\"rock\",  1,\n    5,34,\"pop\",   1,\n    6,48,\"indie\", 1,\n    7,45, \"rock\",  0\n)\n\nmydf\n#&gt; # A tibble: 7 × 4\n#&gt;      id    f2 cat   label\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     1    53 rock      0\n#&gt; 2     2    55 indie     0\n#&gt; 3     3    40 rock      1\n#&gt; 4     4    42 rock      1\n#&gt; 5     5    34 pop       1\n#&gt; 6     6    48 indie     1\n#&gt; 7     7    45 rock      0\n\n\nFuncioncita para obtener la codificación a lo catboost\n\n\nMostrar / ocultar código\navg_target &lt;- function(prev_df, nivel, prior = 0.05){\n    countInClass &lt;- sum(prev_df[['label']][prev_df[['cat']]== nivel])\n    totalCount &lt;- sum(prev_df[['cat']]==nivel)\n    res &lt;-  (countInClass + prior) /(totalCount + 1)\n    return(res)\n}\n\n\nA la primer fila se le asigna siempre la prior\n\n\nMostrar / ocultar código\n# No estaba fino para ver como podría hacerlo sin iterar sobre todas las filas. \n\nfoo1 &lt;-  function(df, prior = 0.05) {\n    \n    df$cat_code[1] &lt;-  prior\n    \n    for (fila in 2:nrow(df)) {\n        prev_df &lt;- df[1:(fila - 1),]\n        df$cat_code[fila] &lt;-\n            avg_target(prev_df = prev_df, nivel = df$cat[fila], prior = prior)\n    }\n    return(df)\n}\n\n\n\n\nMostrar / ocultar código\nres1 &lt;- foo1(mydf, prior = 0.05)\nres1\n#&gt; # A tibble: 7 × 5\n#&gt;      id    f2 cat   label cat_code\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      0    0.05 \n#&gt; 2     2    55 indie     0    0.05 \n#&gt; 3     3    40 rock      1    0.025\n#&gt; 4     4    42 rock      1    0.35 \n#&gt; 5     5    34 pop       1    0.05 \n#&gt; 6     6    48 indie     1    0.025\n#&gt; 7     7    45 rock      0    0.512\n\n\nAhora lo repetimos varias veces. Dónde en cada iteración hacemos una permutación de las filas\n\n\nMostrar / ocultar código\nfoo2 &lt;-  function(df, prior = 0.05) {\n    require(tidyverse)\n    mynew_df &lt;-  df |&gt; slice_sample(prop = 1, replace = FALSE)\n    mynew_df$cat_code[1] &lt;-  prior\n    \n    for (fila in 2:nrow(mynew_df)) {\n        prev_df &lt;- mynew_df[1:(fila - 1),]\n        mynew_df$cat_code[fila] &lt;-\n            avg_target(prev_df = prev_df, nivel = mynew_df$cat[fila], prior = prior)\n    }\n    return(mynew_df)\n}\n\n\n\n\nMostrar / ocultar código\n\niteraciones &lt;-  1000\n\nres2 &lt;-  bind_rows(mclapply(1:iteraciones, FUN = function(x) foo2(mydf,prior = 0.05), mc.cores = 10))\n\ndim(res2)\n#&gt; [1] 7000    5\n\n(res2 &lt;- res2 |&gt;\n    group_by(id) |&gt;\n    mutate(cat_code = mean(cat_code)) |&gt;\n    distinct() |&gt; \n    arrange(id) )\n#&gt; # A tibble: 7 × 5\n#&gt; # Groups:   id [7]\n#&gt;      id    f2 cat   label cat_code\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      0   0.340 \n#&gt; 2     2    55 indie     0   0.284 \n#&gt; 3     3    40 rock      1   0.175 \n#&gt; 4     4    42 rock      1   0.194 \n#&gt; 5     5    34 pop       1   0.05  \n#&gt; 6     6    48 indie     1   0.0373\n#&gt; 7     7    45 rock      0   0.342\n\n\n\n\nAproximación compadre bayesiana\n¿Y si tomamos como priori una ℬ(2,2)\\mathcal{B}(2,2) y para cada categoría {rock, indie, pop} tomamos como su distribución a posteriori\nℬ(2+exitos en datos,2+fracasos en los datos)\\mathcal{B}(2 + \\text{exitos en datos},2 + \\text{fracasos en los datos}) y para obtener un valor de la codificación para cada observación simplemente extraemos un valor aleatorio de esa distribución a posteriori?\n\n\nMostrar / ocultar código\n\nres3 &lt;-  mydf |&gt; \n    group_by(cat) |&gt; \n    mutate(\n        n = n(), \n        exitos = sum(label)\n    )  |&gt; \n    ungroup() |&gt; \n    mutate(\n     cat_code = map2_dbl(exitos, n, function(exitos, n) rbeta(1, exitos + 2 , n - exitos + 2))\n    ) \n\nres3\n#&gt; # A tibble: 7 × 7\n#&gt;      id    f2 cat   label     n exitos cat_code\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      0     4      2    0.407\n#&gt; 2     2    55 indie     0     2      1    0.467\n#&gt; 3     3    40 rock      1     4      2    0.530\n#&gt; 4     4    42 rock      1     4      2    0.532\n#&gt; 5     5    34 pop       1     1      1    0.287\n#&gt; 6     6    48 indie     1     2      1    0.290\n#&gt; 7     7    45 rock      0     4      2    0.801\n\n\nPues tiene pinta de que esta aproximación podría ser tan válida como la que describen los de catboost y en principio es más sencilla.\n\n\nMás datos\nCreemos un dataset artificial partiendo de estos mismos datos.\n\n\nMostrar / ocultar código\nn= 200\nmydf_big= mydf[rep(seq_len(nrow(mydf)), n), ]\nmydf_big$id &lt;- 1:nrow(mydf_big)\n\nnrow(mydf_big)\n#&gt; [1] 1400\n\nhead(mydf_big, 10)\n#&gt; # A tibble: 10 × 4\n#&gt;       id    f2 cat   label\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1     1    53 rock      0\n#&gt;  2     2    55 indie     0\n#&gt;  3     3    40 rock      1\n#&gt;  4     4    42 rock      1\n#&gt;  5     5    34 pop       1\n#&gt;  6     6    48 indie     1\n#&gt;  7     7    45 rock      0\n#&gt;  8     8    53 rock      0\n#&gt;  9     9    55 indie     0\n#&gt; 10    10    40 rock      1\n\n\n\n\nMostrar / ocultar código\n## Cambiamos de forma aleatoria el valor de label para un % de las observaciones, para que no sea 200 veces exactamente el original\n\ntable(mydf_big$label)\n#&gt; \n#&gt;   0   1 \n#&gt; 600 800\n\nmydf_big$label &lt;- rbinom(n = nrow(mydf_big), size =1,  prob = ifelse(mydf_big$label==0, 0.3, 0.9))\ntable(mydf_big$label)\n#&gt; \n#&gt;   0   1 \n#&gt; 508 892\n\n# vemos que hemos cambiado algunos valores en label\nhead(mydf_big, 10)\n#&gt; # A tibble: 10 × 4\n#&gt;       id    f2 cat   label\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;\n#&gt;  1     1    53 rock      1\n#&gt;  2     2    55 indie     1\n#&gt;  3     3    40 rock      1\n#&gt;  4     4    42 rock      1\n#&gt;  5     5    34 pop       1\n#&gt;  6     6    48 indie     1\n#&gt;  7     7    45 rock      0\n#&gt;  8     8    53 rock      0\n#&gt;  9     9    55 indie     0\n#&gt; 10    10    40 rock      1\n\n\n\n\nComparamos\nPara elegir las priori poco informativa para ambos métodos vemos una muestra de los datos de tamaño 10\n\n\nMostrar / ocultar código\n(muestra &lt;- mydf_big |&gt; \n    slice_sample(n = 10) |&gt; \n    group_by(label) |&gt;\n    count())\n#&gt; # A tibble: 2 × 2\n#&gt; # Groups:   label [2]\n#&gt;   label     n\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     0     7\n#&gt; 2     1     3\n\n\n(prior_shape1 &lt;-  muestra$n[muestra$label==1])\n#&gt; [1] 3\n(prior_shape2 &lt;-  muestra$n[muestra$label==0])\n#&gt; [1] 7\n\n\n(prior_catboost &lt;- prior_shape1 /(prior_shape1 + prior_shape2))\n#&gt; [1] 0.3\n\n\n\nCatboost codificación\n\n\nMostrar / ocultar código\niteraciones = 50\ntictoc::tic(\"catbost_code\")\n\ncod_catboost &lt;- bind_rows(mclapply(1:iteraciones, FUN = function(x) foo2(mydf_big, prior = prior_catboost), mc.cores = 10))\n\n\ntictoc::toc(log=TRUE)\n#&gt; catbost_code: 3.293 sec elapsed\n\ndim(cod_catboost)\n#&gt; [1] 70000     5\n\n\n\n\nMostrar / ocultar código\ncod_catboost &lt;- cod_catboost |&gt;\n    group_by(id) |&gt;\n    mutate(cat_code = mean(cat_code)) |&gt;\n    distinct() |&gt; \n    arrange(id)\n\n\ndim(cod_catboost)\n#&gt; [1] 1400    5\n\nhead(cod_catboost)\n#&gt; # A tibble: 6 × 5\n#&gt; # Groups:   id [6]\n#&gt;      id    f2 cat   label cat_code\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      1    0.587\n#&gt; 2     2    55 indie     1    0.567\n#&gt; 3     3    40 rock      1    0.577\n#&gt; 4     4    42 rock      1    0.586\n#&gt; 5     5    34 pop       1    0.910\n#&gt; 6     6    48 indie     1    0.582\n\n\n\n\nEstilo compadre codificación\n\n\nMostrar / ocultar código\ntictoc::tic(\"estilo compadre\")\nestilo_compadre &lt;-  mydf_big |&gt; \n    group_by(cat) |&gt; \n    mutate(\n        n = n(), \n        exitos = sum(label)\n    )  |&gt; \n    ungroup() |&gt; \n    mutate(\n     cat_code = map2_dbl(exitos, n, function(exitos, n) rbeta(1, exitos + prior_shape1 , n - exitos + prior_shape2))\n    ) \n\ntictoc::toc(log = TRUE)\n#&gt; estilo compadre: 0.023 sec elapsed\n\nhead(estilo_compadre)\n#&gt; # A tibble: 6 × 7\n#&gt;      id    f2 cat   label     n exitos cat_code\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      1   800    474    0.604\n#&gt; 2     2    55 indie     1   400    233    0.539\n#&gt; 3     3    40 rock      1   800    474    0.572\n#&gt; 4     4    42 rock      1   800    474    0.563\n#&gt; 5     5    34 pop       1   200    185    0.854\n#&gt; 6     6    48 indie     1   400    233    0.558\n\n\n¿cómo de parecidas son las dos codificaciones?\nPor el momento parece que bastante\n\n\nMostrar / ocultar código\ncor(cod_catboost$cat_code, estilo_compadre$cat_code)\n#&gt; [1] 0.9819561\n\n\nParece que la codificación estilo compadre es un poco más dispersa, lo cual no tiene por qué ser necesariamente malo.\n\n\nMostrar / ocultar código\n\ncod_catboost |&gt; \n    group_by(cat) |&gt; \n    summarise(media = mean(cat_code), \n              low = quantile(cat_code, 0.05), \n              high = quantile(cat_code, 0.95), \n              sd_value = sd(cat_code))\n#&gt; # A tibble: 3 × 5\n#&gt;   cat   media   low  high sd_value\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 indie 0.577 0.567 0.587  0.00635\n#&gt; 2 pop   0.906 0.888 0.916  0.00871\n#&gt; 3 rock  0.585 0.575 0.592  0.00520\n\n\nestilo_compadre |&gt; \n    group_by(cat) |&gt; \n    summarise(media = mean(cat_code), \n              low = quantile(cat_code, 0.05), \n              high = quantile(cat_code, 0.95), \n              sd_value = sd(cat_code))\n#&gt; # A tibble: 3 × 5\n#&gt;   cat   media   low  high sd_value\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 indie 0.578 0.541 0.616   0.0240\n#&gt; 2 pop   0.896 0.857 0.933   0.0209\n#&gt; 3 rock  0.589 0.561 0.617   0.0173\n\n\n\n\nMostrar / ocultar código\np1 &lt;- cod_catboost |&gt; \n    ggplot(aes(x=cat_code, fill=cat))  +\n    geom_density(alpha = 0.4) +\n    labs(title = \"Gráfico densidad de las codificanes\", \n         subtitle = \"catboost\")\n\np2 &lt;- estilo_compadre |&gt; \n    ggplot(aes(x=cat_code, fill=cat))  +\n    geom_density(alpha = 0.4) +\n        labs(title = \"Gráfico densidad de las codificanes\", \n         subtitle = \"Estilo bayesian compadre\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nY si hacemos un modelito tonto usando estas codificaciones. Si, ya sé que son datos fakes y que no tiene mucho sentido y tal, y que lo suyo sería con unos datos reales (mandadme algunos !! )\n\n\nMostrar / ocultar código\n# set.seed(47)\n\nid_train &lt;-  sample(1:nrow(mydf_big), size = 700)\n\n\ntrain_predict_simple &lt;- function(df){\n    train &lt;-  df[id_train, ]\n    test &lt;- df[-id_train, ]\n    \n    fit_base &lt;- glm(label ~ f2+ cat, data = train, family = binomial)\n    fit &lt;-  glm(label ~  f2 +  cat_code, data = train, family = binomial)\n    \n    auc_base &lt;- pROC::auc(test$label, predict(fit_base, test, type = \"response\"))\n    auc &lt;- pROC::auc(test$label, predict(fit, test, type = \"response\"))\n    \n    return(list(auc_base = auc_base, auc = auc))\n    \n}\n\n\nmclapply(list(cod_catboost, estilo_compadre), train_predict_simple, mc.cores = 2)\n#&gt; [[1]]\n#&gt; [[1]]$auc_base\n#&gt; Area under the curve: 0.8231\n#&gt; \n#&gt; [[1]]$auc\n#&gt; Area under the curve: 0.7693\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; [[2]]$auc_base\n#&gt; Area under the curve: 0.8231\n#&gt; \n#&gt; [[2]]$auc\n#&gt; Area under the curve: 0.7658"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#más-pensamientos",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#más-pensamientos",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "Más pensamientos",
    "text": "Más pensamientos\n\nEl ejemplo que he hecho no es del todo válido puesto que tanto para la codificación con catboost como la de estilo compadre han intervenido todos los datos.\nLa variable categórica que codifico sólo tiene 3 niveles, de hecho no haría falta hacer este tipo de codificación. Tengo pendiente probar con algo como códigos postales o similar.\nLa forma en que catboost hace esta codificación me parece que está en mitad entre la aproximación bayesiana y hacer un target_encoding al uso. De hecho si hay un nivel con muy pocos valores el valor de la codificación de catboost para ese nivel va a parecerse más a la prior que elijas que a la proporción de éxitos en esa categoría, lo cual es muy parecido a la estimación bayesiana compadre.\nSe podrían utilizar codificaciones basadas en modelos mixtos o algún tipo de combinación convexa entre la información particular que tiene una categoría y la general aportada por el conjunto de los datos."
  },
  {
    "objectID": "2019/07/02/el-randomforest-no-nos-deja-ver-el-árbol/index.html",
    "href": "2019/07/02/el-randomforest-no-nos-deja-ver-el-árbol/index.html",
    "title": "El randomforest no nos deja ver el árbol",
    "section": "",
    "text": "En primer lugar, el título de este post se lo debo a Mario Passani y no va sobre estadística.\nEn realidad va sobre que últimamente me da la sensación de que la profesión del científico de datos se está centrado más en las formas que en el fondo. Os cuento un par de ejemplos.\nHace un tiempo en una empresa en la que trabajé se estaban implementando temas de análisis exploratorios usando pyspark de forma que se generaban unos bonitos htmls con info sobre las variables de un sparkdataframe. Hasta aquí todo bien, el git del proyecto estaba muy bonito, había gente que programaba de forma más que aceptable, pero oye, de repente cuando me pongo a utilizar la librería y pido que me saque un boxplot resulta que me salta a la vista la ausencia de los puntitos típicos de los outliers (y sabía que había varios), total, que cuando me pongo a bichear el código resulta que como bigotes del boxplot me habían puesto el mínimo y el máximo. Hay que decir que el gráfico era muy bonito y todo eso, pero era inútil, no habían implementado un boxplot, se le parecía pero no era.\nOtro caso me lo contó un compañero de una telco. Resulta que un compañero suyo había implementado toda una etl que juntaba varias tablas para al final obtener datos sobre la fuga de clientes, mi compañero al ver el resultado le dijo que algo estaba mal porque salían unas tasas de “churn” bastante mayores que las de meses anteriores. El hacedor de la etl le dijo que se peinara y que eso estaba bien porque, ojo ¡¡había pasado los test unitarios!! y por lo que se ve acabó la cosa un pelín disputada.\nA lo que me refiero, noto que últimamente nos estamos centrado más en el cómo implementamos las cosas que en saber lo que estamos haciendo, y no quiero decir con esto que las formas no sean importantes, sino simplemente que tengamos claro qué narices estamos haciendo."
  },
  {
    "objectID": "2019/06/25/malditas-proporciones-pequeñas-ii/index.html",
    "href": "2019/06/25/malditas-proporciones-pequeñas-ii/index.html",
    "title": "Malditas proporciones pequeñas II",
    "section": "",
    "text": "¿Cuál sería el tamaño muestral mínimo para estimar un incremento del 15% en una proporción de digamos 0.004?\nEn realidad me gustaría hacer como cuenta mi amigo Carlos en este post, pero no puedo ir examinando unidades y actualizar los intervalos de credibilidad hasta que la anchura me convenza, porque ni siquiera conozco al tío de la furgoneta que ha de ir “examinando” cada unidad experimental, amén de que para conseguir 4 tiene que examinar cerca de 1000. Así que veamos como se ha hecho toda la vida.\nNos interesa es minimizar los errore tipo I y tipo II. Recordemos lo que eran.\n\nError tipo I : Error de falso positivo, decir que hay diferencias cuando en realidad no las hay (H0 es cierta pero digo que no)\nError tipo II: Error falso negativo, error que cometo al decir que no hay diferencias cuando en realidad si las hay. (H0 es falsa pero decimos que es verdadera) 8 \n\nSi utilizamos el paquete de R pwr podemos calcularlo para un error tipo I, (el alpha de siempre) de 0.05 y una potencia (1 - error tipo II) de 0.9\n\np1 <- 0.004\np2 <- p1 * 1.15\nlibrary(pwr)\n\npotencia_0.9 <- pwr.p.test(ES.h(p1=p1, p2= p2),\n                           sig.level= 0.05, power = 0.9) \nplot(potencia_0.9)\n\n\n\n\n\n\n\n\nY nos sale que el tamaño de muestra mínimo está en torno a 125 mil .\nHay otras librerías para calcular dicho tamaño muestral, por ejemplo SampleSizeProportions que según pone en la documentación lo hace teniendo en cuenta el intervalo de credibilidad deseado. Tengo que mirar mejor esta librería.\n\nlibrary(SampleSizeProportions)\n\nlen <-  (p1 * 1.15 - p1/1.15)\nc1 <- 40\nc2 <- 46\nd1 <- 9960\nd2 <- 9954\n\npropdiff.modwoc(len = len, c1 = c1, d1 = d1, c2 = c2, d2 = d2)\n#> [1] 120910 120910\n\nY sale unos 120 mil para cada grupo.\nO también podemos ver cosas como está que comentan los amigos de WinVector, aquí o aquí\n\nestimateT <- function(lowProb,difference,errorProb) {\n  -log(errorProb/2)*lowProb/(difference^2)\n}\n\n# detectar diferencias de 15%\nlowProb <- 0.004\nincremento <- 0.15\nestimateT(lowProb, lowProb*incremento,  0.0006 )\n#> [1] 90130.31\n\nY nos sale un tamaño aproximado de 90 mil. Sea como fuere el tener que determinar tamaños de muestra para poder medir variaciones de un 15% en proporciones pequeñas implica tener muestras de tamaño 100000, así que como dice el título, ¡malditas proporciones pequeñas!"
  },
  {
    "objectID": "2019/06/24/malditas-proporciones-pequeñas-i/index.html",
    "href": "2019/06/24/malditas-proporciones-pequeñas-i/index.html",
    "title": "Malditas proporciones pequeñas I",
    "section": "",
    "text": "Cuando uno está en esto de ganarse la vida mediante la ciencia de datos, se da cuenta de que la vida no es tan maravillosa como lo cuentan los libros de texto ni los cursos de los másters y ni siquiera los concursos de kaggle.\nRecientemente en un proyecto nos piden detectar un efecto de un incremento del 15% en una proporción entre dos grupos, digamos en forma canónica, grupo de control y tratamiento. Hasta aquí todo normal y uno podría hacer calcular intervalos de confianza (o de credibilidad si nos vamos al mundo bayesiano) de manera más o menos fácil. Veamos como sería utilizando simulación.\nSupongamos una p1 = 0.5 y una p2 = p1 * 1.16 ( un pelín superior al 15%) Supongamos también que nuestros grupos tienen tamaño n1 = n2 = 100000. Así que vamos a calcular mediante simulación la probabilidad que p2/p1 &gt;= 1.15. ¿Fácil, verdad?\np1 y p2 los simulamos suponiendo una distribución beta. Supongamos que p2 es un 16% mayor que p1\n\nset.seed(-1) # prueba a poner una semilla negativa en python a ver que pasa\np1 &lt;- 0.5\np2 &lt;- p1 * 1.16\nn &lt;- 1E5\nsim &lt;- 1E6\n\nsim_beta1 &lt;- rbeta(sim, p1 * n, n - p1 * n)\nsim_beta2 &lt;- rbeta(sim, p2*n, n - p2 * n)\n\nmedian(sim_beta1)\n#&gt; [1] 0.5000039\nmedian(sim_beta2)\n#&gt; [1] 0.5800018\n\nSi dibujamos sus funciones de densidad con R base sin muchas zirigoncias.\n\nplot(density(sim_beta1), main = \"prop\", col = \"darkblue\", lty = 2, xlim = c(0.45, 0.6))\n\nlines(density(sim_beta2), col = \"darkred\")\n\n\n\n\n\n\n\n\nY claramente si que parece que están separadas. Podemos estimar la probabilidad de que p2 sea un 15% mayor como la proporción de veces que p2/p1 &gt;= 1.15 en el millón de simulaciones\n\nmean(sim_beta2/sim_beta1 &gt;= 1.15)\n#&gt; [1] 0.98152\n\nY podríamos estar bastante seguros de que p2 es al menos un 15% mayor que p1.\nPero ¿qué pasa si las proporciones son pequeñas, digamos de un 4 por mil?\n\nset.seed(-1)\np1 &lt;- 0.004\np2 &lt;- p1 * 1.16\nn &lt;- 1E5\nsim &lt;- 1E6\n\nsim_beta1 &lt;- rbeta(sim, p1 * n, n - p1 * n)\nsim_beta2 &lt;- rbeta(sim, p2*n, n - p2 * n)\n\nmedian(sim_beta1)\n#&gt; [1] 0.003997118\nmedian(sim_beta2)\n#&gt; [1] 0.004636583\n\n\nplot(density(sim_beta1), main = \"prop\", col = \"darkblue\", lty=2, xlim = c(0.003, 0.006)  )\n\nlines(density(sim_beta2), col = \"darkred\")\n\n\n\n\n\n\n\n\n\nmean(sim_beta2/sim_beta1 &gt;= 1.15)\n#&gt; [1] 0.550476\n\nPues ya no podemos estar tan seguros de que el incremento haya sido de alrededor un 15%.\nEn próximas entradas veremos como calcular de forma clásica el tamaño muestral necesario para detectar ese efecto."
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html",
    "href": "2023/01/21/regresion-poisson-brms/index.html",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "",
    "text": "Me llamó la atención ayer el excelente post de Carlos sobre regresión de poisson casi trivival con numpyro y le dije que iba a ver si podía replicarlo usando brms u otra cosa que se comunique con stan. También estuve bicheando su código que tiene colgado aquí\nLo primero, es que se trata de un ejercicio relativamente común, que es la de estimar el punto en que una serie de datos cambia de tendencia. Existen cosas como prophet (que por debajo es Stan) o librerías como mcp cmp o changepoint en R específicas para este tipo de cosas. Pero a Carlos le gusta , con buen criterio, especificar directamente el modelo.\nHe de reconocer que me ha gustado la sintaxis de numpyro\nBueno vamos al lío."
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#datos-y-librerías",
    "href": "2023/01/21/regresion-poisson-brms/index.html#datos-y-librerías",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Datos y librerías",
    "text": "Datos y librerías\nVoy implementar los dos primeros modelos que hay en el notebook de Carlos usando la función ulam de la librería rethinking de Richard McElreath. Dicha librería es un interfaz, incompleto aunque didáctico, de hacer modelos bayesianos con Stan.\nEl último modelo, el que detecta el punto de cambio no he sido capaz de hacerlo con esta librería, pero se podría hacer usando stan directamente, como aquí.\nLos datos, aunque él ha puesto la variable del tiempo de 1 a 23, yo la voy a poner de 1999 a 2021, porque intuía a qué datos se refería y que los he buscado.\n\nlibrary(rethinking) \nlibrary(cmdstanr)\nlibrary(brms)\n\n# por si uso splines\nlibrary(mgcv)\n\n\n# uso cmdstan como backend para stan desde R en vez de rstan\nset_cmdstan_path(\"/home/jose/cmdstan\")\nset_ulam_cmdstan(TRUE)\n\n\n\nd  <- list(\n      y = c(54, 63, 50, 54, 71, 72, 57, 69, 71, \n    76, 57, 73, 62, 51, 54, 55, 60, 49, \n    50, 53, 56, 49, 48),\n\n    t = 1999:2021\n\n)"
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#modelo-lambda-constante",
    "href": "2023/01/21/regresion-poisson-brms/index.html#modelo-lambda-constante",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Modelo lambda constante",
    "text": "Modelo lambda constante\n\nm0 <- ulam(\n    alist(y ~ poisson(lambda),\n          lambda <- a    ,\n          a ~ normal(60, 5)\n          ) ,\n    data = d,\n    chains = 2,\n    cores = 2 ,\n    sample = TRUE,\n    iter = 3000\n    \n)\n#> Running MCMC with 2 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 1 finished in 0.0 seconds.\n#> Chain 2 finished in 0.0 seconds.\n#> \n#> Both chains finished successfully.\n#> Mean chain execution time: 0.0 seconds.\n#> Total execution time: 0.4 seconds.\n\n\nprecis(m0)\n#>            result\n#> mean    58.976852\n#> sd       1.542638\n#> 5.5%    56.450528\n#> 94.5%   61.456426\n#> n_eff 1004.327368\n#> Rhat     1.000203"
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#modelo-dónde-lambda-es-una-recta",
    "href": "2023/01/21/regresion-poisson-brms/index.html#modelo-dónde-lambda-es-una-recta",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Modelo dónde lambda es una recta",
    "text": "Modelo dónde lambda es una recta\n\nm1 <- ulam(\n    alist(\n        y ~ poisson(lambda),\n        lambda <- a + b * t   ,\n        a ~ normal(60, 5),\n        b ~ normal(0, 1)\n        \n    ) ,\n    data = d,\n    chains = 2,\n    cores = 2 ,\n    sample = TRUE,\n    iter = 3000\n    \n)\n#> Running MCMC with 2 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 1 finished in 0.2 seconds.\n#> Chain 2 finished in 0.2 seconds.\n#> \n#> Both chains finished successfully.\n#> Mean chain execution time: 0.2 seconds.\n#> Total execution time: 0.3 seconds.\n\nprecis(m1)\n#>            mean          sd         5.5%        94.5%    n_eff    Rhat4\n#> a 60.2808693333 5.071538760 52.390828500 68.641990000 454.4379 1.005453\n#> b -0.0007081383 0.002636486 -0.005088067  0.003417291 452.2575 1.005254"
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#modelo-para-detectar-el-punto-de-cambio.",
    "href": "2023/01/21/regresion-poisson-brms/index.html#modelo-para-detectar-el-punto-de-cambio.",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Modelo para detectar el punto de cambio.",
    "text": "Modelo para detectar el punto de cambio.\nLa verdad que me habría gustado seguir usando ulam para el modelo pero he sido incapaz de replicar este cacho de código de numpyro\ndef model02(t, datos):\n\n  knot = numpyro.sample(\"knot\", dist.Normal(len(t)/2, len(t)/4))\n\n  a0 = numpyro.sample(\"a0\", dist.Normal(60, 5))\n  b0 = numpyro.sample(\"b0\", dist.Normal( 0, 1))\n\n  b1 = numpyro.sample(\"b1\", dist.Normal(0, 1))  \n\n  λ = a0 + t * b0 + jnp.where(t > knot, (t - knot) * b1, 0)\n\n  with numpyro.plate(\"data\", len(t)):\n    numpyro.sample(\"obs\", dist.Poisson(λ), obs=datos)\n\nEl problema reside en jnp.where(t > knot, (t - knot) * b1, 0) , para resolverlo habría que programar directamente en stan o que ulam pudiera usar la función step de Stan, que si x es menor que 0 vale 0 y 1 en otro caso. El código en ulam si funcionara sería así\n\nm2 <- ulam(\n        alist(\n            y ~ poisson(lambda),\n            lambda <- b0 +  b1 * t + b2 * (t - knot) * step(t - knot) ,\n            knot ~ normal(23/2, 23/4),\n            b0 ~ normal(60, 5),\n            b1 ~ normal(0, 1),\n            b2 ~ normal(0, 1),\n\n        ) ,\n        data=d, chains=2, cores=1 , sample=TRUE, \n        iter = 3000)\n\npero no funciona , así que vamos a ver como sería usando brms. brms tiene una sintaxis digamos que peculiar, y su objetivo es parecerse a la especificación que se hace en R de los modelos lineales con lm y la que se hace con lme4. Es decir sería algo similar a esto\n\n\n brm( y ~ x1 + x2 + (1 | var_efecto_aleatorio) , \n                 family = poisson(\"identity\"), \n                 data = df)\n\ndónde no se especifican los \\(\\beta_i\\) de forma explícita. Pero también tiene sintaxis para hacerlo explícito. Veamos.\n\n# Datos en dataframe\ndf  <- data.frame(y = d$y, t = d$t)\n\n\nbform <- bf(\n  y ~ b0 + b1 * t + \n  \n  # brms si acepta la función step de Stan \n  # cuando t-change >= 0  entonces step(t-change) = 1, es decir, cuanto t > change y 0 en otro caso     \n  b2 * (t-change) * step( t - change),\n  \n  # Hay que poner que estime el \"intercept\" de los parámetros y el nl = TRUE para que brms sepa que son parámetros\n  # y no variables en los datos \n  b0 ~ 1,\n  b1 ~ 1, \n  b2 ~ 1,\n  change ~ 1, \n  nl = TRUE\n)\n\nEspecificamos las priors. En brms se escriben las priors y se “concatenan” mediante +\n\nbprior <- prior(normal(60, 5), nlpar = \"b0\") +\n          prior(normal(0, 1), nlpar = \"b1\") +\n          prior(normal(0, 1), nlpar = \"b2\") +\n         # para el cambio ponemos como media la mitad del intervalo y unos 5 años de desviación típica\n          prior(normal( 2010, 23/4), nlpar = \"change\")\n\nY ya podemos escribir el modelo en brms que lo compilara a stan y hace el mcmc\n\nmbrm <-\n    brm(\n        bform,\n        family = poisson(\"identity\"), # pongo de link la identidad en vez del log por hacer como en el post original\n        prior = bprior,\n        data = df,\n        backend = \"cmdstanr\",\n        cores = 6,\n        chains = 6,\n        iter = 4000,\n        warmup = 500\n    )\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 5 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 6 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 6 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 6 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 5 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 2 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 2 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 2 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 5 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 5 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 5 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 5 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 1 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 5 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 1 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 6 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 5 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 5 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 5 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 5 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 6 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 3 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 5 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 5 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 6 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 5 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 finished in 2.5 seconds.\n#> Chain 5 finished in 2.3 seconds.\n#> Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 4 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 4 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 6 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 6 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 6 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 6 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 1 finished in 2.9 seconds.\n#> Chain 3 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 6 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 6 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 6 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 6 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 6 finished in 3.0 seconds.\n#> Chain 4 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 3 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 finished in 4.4 seconds.\n#> Chain 3 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 3 finished in 18.0 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 5.5 seconds.\n#> Total execution time: 18.1 seconds.\n\nY bueno, no ha tardado mucho, unos 3 segundos por cadena.\n\nsummary(mbrm)\n#>  Family: poisson \n#>   Links: mu = identity \n#> Formula: y ~ b0 + b1 * t + b2 * (t - change) * step(t - change) \n#>          b0 ~ 1\n#>          b1 ~ 1\n#>          b2 ~ 1\n#>          change ~ 1\n#>    Data: df (Number of observations: 23) \n#>   Draws: 6 chains, each with iter = 4000; warmup = 500; thin = 1;\n#>          total post-warmup draws = 21000\n#> \n#> Population-Level Effects: \n#>                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> b0_Intercept        50.33     22.21     1.59    69.57 1.40       13       10\n#> b1_Intercept         0.01      0.01    -0.00     0.03 1.37       13       23\n#> b2_Intercept        -1.07      0.48    -1.93    -0.32 1.00     8488     8579\n#> change_Intercept  2008.67      3.31  2002.91  2014.20 1.00     9493     7851\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPodemos pintar la curva media estimada y su intervalo de credibilidad\n\nplot(conditional_effects(mbrm),\n     points = TRUE)\n\n\n\n\n\n\n\n\nTambién la posterior predict function de los diferentes puntos. Evidentemente esto tiene más variabilidad que la posterior predict de la media condicionada que era el gráfico anterior.\n\nplot(conditional_effects(mbrm, method = \"posterior_predict\"),\n     points = TRUE)\n\n\n\n\n\n\n\n\nY básicamente, obtenermos los mismo resultados que Carlos con numpyro.\nPodemos obtener un histograma de la posterior del punto de cambio\n\npunto_cambio_posterior <- as_draws_df(mbrm, variable = \"b_change_Intercept\")\n\nggplot2::ggplot(punto_cambio_posterior, aes(x =b_change_Intercept ))   +\n    ggplot2::geom_histogram()\n\n\n\n\n\n\n\n\n\nposterior <- as_draws_df(mbrm)\n\nhead(posterior)\n#> # A draws_df: 6 iterations, 1 chains, and 6 variables\n#>   b_b0_Intercept b_b1_Intercept b_b2_Intercept b_change_Intercept lprior lp__\n#> 1             52        0.00607          -1.33               2010   -9.0  -87\n#> 2             69       -0.00512          -0.56               2006   -9.1  -89\n#> 3             61        0.00087          -0.40               2006   -7.3  -87\n#> 4             63       -0.00099          -0.48               2006   -7.5  -86\n#> 5             59        0.00275          -1.17               2006   -7.9  -85\n#> 6             58        0.00339          -1.18               2006   -8.1  -86\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\ndim(posterior)\n#> [1] 21000     9\n\nPintando las posterior predict directamente\n\n\n# Muy old R base, pero es que es la hora de comer y no tengo azúcar en el cerebro\n\nn_samples <- 10000\nidx_sample <-  sample(1:nrow(posterior), size = n_samples, replace = TRUE)\n\nposterior_df <-  as.data.frame(posterior)\n\n\nmake_curve_functions <-  function(fila) {\n    \n    b0     <-  posterior_df[fila, \"b_b0_Intercept\"]\n    b1     <-  posterior_df[fila, \"b_b1_Intercept\"]\n    b2     <-  posterior_df[fila, \"b_b2_Intercept\"]\n    change <-  posterior_df[fila, \"b_change_Intercept\"]\n    lambda <- b0 + b1 * t + ifelse(t > change, (t-change) * b2, 0) \n    return(lambda)\n    \n}\n\n\nt <-  df$t\n\nres <- sapply(idx_sample, make_curve_functions)\n\n\nplot(t, res[,1], pch = 19, col = scales::alpha(\"lightblue\", 0.7), ylim = c(40, 80), type = \"l\")\n\nfor (id in 2:n_samples) {\n    points(t, res[,id], pch = 19, col = scales::alpha(\"lightblue\", 0.7), type = \"l\")\n    \n}\n\npoints(t, df$y, pch = 19)\n\n\n\n\n\n\n\n\nUna cosa interesante de brms es que nos construye código en Stan que luego nosotros podemos modificar\n\nmake_stancode(bform, data = df,prior= bprior, family = poisson(\"identity\"))\n#> // generated with brms 2.19.0\n#> functions {\n#> }\n#> data {\n#>   int<lower=1> N;  // total number of observations\n#>   int Y[N];  // response variable\n#>   int<lower=1> K_b0;  // number of population-level effects\n#>   matrix[N, K_b0] X_b0;  // population-level design matrix\n#>   int<lower=1> K_b1;  // number of population-level effects\n#>   matrix[N, K_b1] X_b1;  // population-level design matrix\n#>   int<lower=1> K_b2;  // number of population-level effects\n#>   matrix[N, K_b2] X_b2;  // population-level design matrix\n#>   int<lower=1> K_change;  // number of population-level effects\n#>   matrix[N, K_change] X_change;  // population-level design matrix\n#>   // covariate vectors for non-linear functions\n#>   int C_1[N];\n#>   int prior_only;  // should the likelihood be ignored?\n#> }\n#> transformed data {\n#> }\n#> parameters {\n#>   vector[K_b0] b_b0;  // population-level effects\n#>   vector[K_b1] b_b1;  // population-level effects\n#>   vector[K_b2] b_b2;  // population-level effects\n#>   vector[K_change] b_change;  // population-level effects\n#> }\n#> transformed parameters {\n#>   real lprior = 0;  // prior contributions to the log posterior\n#>   lprior += normal_lpdf(b_b0 | 60, 5);\n#>   lprior += normal_lpdf(b_b1 | 0, 1);\n#>   lprior += normal_lpdf(b_b2 | 0, 1);\n#>   lprior += normal_lpdf(b_change | 2010, 23/4);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] nlp_b0 = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] nlp_b1 = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] nlp_b2 = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] nlp_change = rep_vector(0.0, N);\n#>     // initialize non-linear predictor term\n#>     vector[N] mu;\n#>     nlp_b0 += X_b0 * b_b0;\n#>     nlp_b1 += X_b1 * b_b1;\n#>     nlp_b2 += X_b2 * b_b2;\n#>     nlp_change += X_change * b_change;\n#>     for (n in 1:N) {\n#>       // compute non-linear predictor values\n#>       mu[n] = (nlp_b0[n] + nlp_b1[n] * C_1[n] + nlp_b2[n] * (C_1[n] - nlp_change[n]) * step(C_1[n] - nlp_change[n]));\n#>     }\n#>     target += poisson_lpmf(Y | mu);\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#> }\n#> generated quantities {\n#> }\n\n\nNotas\nUna forma fácil de ajustar estos datos es usando splines.\n\nm_spline  <- brm(\n                 y ~ s(t), \n                 family = poisson(\"identity\"), \n                 data = df, \n                 backend= \"cmdstanr\"\n)\n#> Running MCMC with 4 sequential chains...\n#> \n#> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 1 finished in 0.3 seconds.\n#> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 2 finished in 0.4 seconds.\n#> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 finished in 0.3 seconds.\n#> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 4 finished in 0.3 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 0.3 seconds.\n#> Total execution time: 1.9 seconds.\n\nsummary(m_spline)\n#>  Family: poisson \n#>   Links: mu = identity \n#> Formula: y ~ s(t) \n#>    Data: df (Number of observations: 23) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Smooth Terms: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sds(st_1)    12.78      7.77     1.41    30.74 1.00     1376     1528\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    58.83      1.57    55.82    61.89 1.00     3590     2668\n#> st_1         -3.76     31.71   -56.38    67.67 1.00      906      572\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(conditional_smooths(m_spline, method = \"posterior_predict\") \n    ) \n\n\n\n\n\n\n\n\nY tendríamos la posterior del spline , pero la verdad, ahora no tengo ganas de buscar como interpretar ese spline para traducir a punto de cambio de tendencia.\nPor otro lado, leyendo por ahí he visto una implementación un pelín diferente\n\nbform_alt <- bf(\n    y ~ b0 +\n        # aqui es donde cambia un poco\n        b1 * (t - change) * step(change - t) +\n        # post cambio\n        b2 * (t - change) * step(t - change),\n    b0 + b1 + b2 + change ~ 1,\n    nl = TRUE\n)\n\nbprior <- prior(normal(60, 5), nlpar = \"b0\") +\n    prior(normal(0, 1), nlpar = \"b1\") +\n    prior(normal(0, 1), nlpar = \"b2\") +\n    prior(normal(2010, 23 / 4), nlpar = \"change\")\n\n\nmbrm_alt <-\n    brm(\n        bform_alt, family = poisson(\"identity\"),\n        prior = bprior,\n        data = df,\n        backend = \"cmdstanr\",\n        cores = 6,\n        chains = 6,\n        iter = 4000,\n        warmup = 500\n    )\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 5 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 6 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 5 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 5 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 6 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 6 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 5 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 1 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 1 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 6 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 3 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 3 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 5 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 5 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 5 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 3 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 5 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 6 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 4 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 4 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 4 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 1 finished in 1.4 seconds.\n#> Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 3 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 5 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 6 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 2 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 2 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 2 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 5 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 6 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 6 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 5 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 5 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 5 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 5 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 6 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 6 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 finished in 1.7 seconds.\n#> Chain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 5 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 6 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 5 finished in 1.7 seconds.\n#> Chain 6 finished in 1.6 seconds.\n#> Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 finished in 1.8 seconds.\n#> Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 2 finished in 2.0 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 1.7 seconds.\n#> Total execution time: 2.2 seconds.\n\nY sale prácticamente lo mismo\n\nsummary(mbrm_alt)\n#>  Family: poisson \n#>   Links: mu = identity \n#> Formula: y ~ b0 + b1 * (t - change) * step(change - t) + b2 * (t - change) * step(t - change) \n#>          b0 ~ 1\n#>          b1 ~ 1\n#>          b2 ~ 1\n#>          change ~ 1\n#>    Data: df (Number of observations: 23) \n#>   Draws: 6 chains, each with iter = 4000; warmup = 500; thin = 1;\n#>          total post-warmup draws = 21000\n#> \n#> Population-Level Effects: \n#>                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> b0_Intercept        63.38      2.92    57.00    68.66 1.00     7444     7067\n#> b1_Intercept         0.50      0.68    -0.71     1.92 1.00     6217     7902\n#> b2_Intercept        -1.05      0.42    -1.90    -0.27 1.00     8058     8853\n#> change_Intercept  2007.95      2.88  2002.84  2014.69 1.00     7429     6453\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nMe queda pendiente hacerlo con Turing.jl Saludos"
  },
  {
    "objectID": "2023/metanalisis.html",
    "href": "2023/metanalisis.html",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "",
    "text": "Ya en 2022 os mostraba uno de los ingredientes principales de la cocina electoral, al menos de la tradicional, no de la postmoderna Alaminos-Tezanos.\nHoy os quiero contar como haría yo la agregación de encuestas, cuando no se tienen los datos brutos. En primer lugar aviso de lo que sigue a continuación sólo lo he hecho por diversión y faltaría mucho más trabajo para considerarlo un intento serio.\nLa diversión viene por este tweet de Anabel Forte que puso como contestación a un hilo dónde Kiko Llaneras explicaba su modelo de predicción agregando encuestas y haciendo simulaciones. Aquí el hilo de kiko y en la imagen la respuesta de Ana.\n\n\n\nTweet de Ana\n\n\nTotal, que dado que conozco a Ana y a Virgilio y son bayesianos y yo sólo un aprendiz de la cosa, pues he intentado un metaanálisis bayesiano sencillo juntando varias encuestas."
  },
  {
    "objectID": "2023/metanalisis.html#introducción",
    "href": "2023/metanalisis.html#introducción",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "",
    "text": "Ya en 2022 os mostraba uno de los ingredientes principales de la cocina electoral, al menos de la tradicional, no de la postmoderna Alaminos-Tezanos.\nHoy os quiero contar como haría yo la agregación de encuestas, cuando no se tienen los datos brutos. En primer lugar aviso de lo que sigue a continuación sólo lo he hecho por diversión y faltaría mucho más trabajo para considerarlo un intento serio.\nLa diversión viene por este tweet de Anabel Forte que puso como contestación a un hilo dónde Kiko Llaneras explicaba su modelo de predicción agregando encuestas y haciendo simulaciones. Aquí el hilo de kiko y en la imagen la respuesta de Ana.\n\n\n\nTweet de Ana\n\n\nTotal, que dado que conozco a Ana y a Virgilio y son bayesianos y yo sólo un aprendiz de la cosa, pues he intentado un metaanálisis bayesiano sencillo juntando varias encuestas."
  },
  {
    "objectID": "2023/metanalisis.html#datos",
    "href": "2023/metanalisis.html#datos",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Datos",
    "text": "Datos\nLo primero era intentar encontrar datos de las encuestas que se han hecho, importante que tengan tanto la estimación como el tamaño muestral. Si, ya sé que cada empresa tiene su cocina y sus cosas, que unas son telefónicas, que otras son tracking o paneles y tal, pero ya he dicho que lo estoy haciendo por diversión..\nBueno, pues aquí he encontrado la info que buscaba. El tema es que la tabla está en una tabla de datawrapper enlace_table y no he sido capaz de escrapear de forma programática, que se le va a hacer, no vale uno pa to.\nComo eran muchas encuestas pues he ido seleccionando algunas del mes de julio y al final me he quedado con unas 23. Para cada encuesta he puesto su tamaño muestral, la diferencia entre la fecha de las elecciones y la fecha de la realización de la encuesta, variable time , también he convertido a votos la estimación que dan para pp, psoe, sumar, vox y resto, simplemente multiplicando la estimación que dan por su tamaño muestral.\nMejor vemos la tabla\n\nlibrary(tidyverse)\nlibrary(DT)\ndf &lt;-  read_csv(here::here(\"data/encuestas_agregadas.csv\")) |&gt; \n    select(empresa, time, partido, everything())\n\ndatatable(df)\n\n\n\n\n\n\nPintamos\n\n\ncolores &lt;-  c(\n    \"pp\" = \"#005999\",\n    \"psoe\" = \"#FF0126\", \n    \"sumar\" = \"#A00B85\", \n    \"vox\" = \"#51962A\", \n    \"resto\" = \"grey\"\n    )\n\ndf |&gt; \n    ggplot(aes(x = time, y = estim,color = partido )) +\n    geom_point() +\n    scale_color_manual(values = colores) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nLa selección de encuestas la he hecho sin mucho orden, son todas del mes de julio, algunas empresas repiten como sigma2 , gad3, simple_logica o sociométrica, otras veces he puesto como nombre el medio (okdiario o prisa).\nBueno, pues vamos a ver como hago el metaanálisis.\n\nPreparación datos\nVoy a poner los datos en un formato que me conviene más para lo que quiero hacer.\n\nn es tamaño de muestra\ntime : es días hasta elecciones, -7 quiere decir qeu la encuesta se publicó (o se hizo, no lo sé) 7 días antes del 23 de julio\nColumnas resultantes de multiplicar la estimación en la encuesta para cada partido por el tamaño muestral\n\nComo vemos voy a considerar 23 encuestas.\n\ndf_wider &lt;- df |&gt; \n    select(-estim) |&gt; \n    pivot_wider( id_cols = c(empresa, n, time),\n                 names_from = partido, \n                 values_from = votos) |&gt; \n    arrange(empresa)\n\nDT::datatable(df_wider)\n\n\n\n\n\n\nPues con esto ya puedo hacer mi intento de meta-análisis, que es probable que esté mal, que no soy un experto en estas cosas."
  },
  {
    "objectID": "2023/metanalisis.html#meta-análisis",
    "href": "2023/metanalisis.html#meta-análisis",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Meta-análisis",
    "text": "Meta-análisis\nPues lo voy a hacer de forma bayesiana. Los datos los tenemos a nivel de encuesta, por lo que puedo considerar que los votos estimados a cada partido en cada encuesta siguen una distribución multinomial , dónde n (tamaño muestral) es el número de intentos y tengo el vector de votos a cada partido que se obtendría. La suma de pp+psoe+sumar+vox+resto es igual a n para cada fila de los datos.\nTambién puedo considerar que las estimaciones de varias encuestas realizadas por la misma empresa no son independientes, no es descabellado ¿verdad?. Y también podría considerar que las estimaciones varían conforme se acerca la fecha de las elecciones y que esta variación podría ser diferente para cada empresa encuestadora. Pues con estos ingredientes ya puedo hacer el “meta-análisis”\nUtilizo la librería brms que me va a permitir hacerlo con una interfaz sencilla. Y en algún momento del futuro miraré como hacerlo con numpyro que me está picando con eso Carlos\n\n\nlibrary(cmdstanr)\nlibrary(brms)\nlibrary(tidybayes)\n\noptions(brms.backend=\"cmdstanr\")\n\nCreamos una columna que una las columnas de los votos a partidos para que sea nuestro vector de respuesta multinomial\n\n\ndf_wider$cell_counts &lt;- with(df_wider, cbind(pp, psoe,sumar, vox, resto))\n\nDT::datatable(head(df_wider))\n\n\n\n\n\n\nY pasamos a ajustar el modelo, dónde vamos a considerar como efecto aleatorio la empresa y como efecto fijo el tiempo, aunque diferente para cada empresa.\nEn la fórmula de brms añadimos informacióna la variable respuesta, en este caso añadimos la info del tamaño muestral. Mirando cosas sobre meta-análisis con brms se puede añadir cosas como desviación estándar de la estimación del efecto y cosas así.\n\n\nformula &lt;- brmsformula(\n    cell_counts | trials(n) ~  (time |empresa))\n\n# vemos las priors por defecto qeu h\n(priors &lt;- get_prior(formula, df_wider, family = multinomial()))\n#&gt;                 prior     class      coef   group resp    dpar nlpar lb ub\n#&gt;                lkj(1)       cor                                           \n#&gt;                lkj(1)       cor           empresa                         \n#&gt;                (flat) Intercept                                           \n#&gt;  student_t(3, 0, 2.5) Intercept                         mupsoe            \n#&gt;  student_t(3, 0, 2.5)        sd                         mupsoe        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa       mupsoe        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa       mupsoe        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa       mupsoe        0   \n#&gt;  student_t(3, 0, 2.5) Intercept                        muresto            \n#&gt;  student_t(3, 0, 2.5)        sd                        muresto        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa      muresto        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa      muresto        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa      muresto        0   \n#&gt;  student_t(3, 0, 2.5) Intercept                        musumar            \n#&gt;  student_t(3, 0, 2.5)        sd                        musumar        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa      musumar        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa      musumar        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa      musumar        0   \n#&gt;  student_t(3, 0, 2.5) Intercept                          muvox            \n#&gt;  student_t(3, 0, 2.5)        sd                          muvox        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa        muvox        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa        muvox        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa        muvox        0   \n#&gt;        source\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n\nAjustamos el modelo\n\nmodel_multinomial &lt;-\n    brm(\n        formula,\n        df_wider,\n        multinomial(),\n        prior = priors,\n        iter = 4000,\n        warmup = 1000,\n        cores = 4,\n        chains = 4,\n        seed = 47,\n        backend = \"cmdstanr\",\n        control = list(adapt_delta = 0.95), \n        refresh = 0\n    )\n#&gt; Running MCMC with 4 parallel chains...\n#&gt; \n#&gt; Chain 1 finished in 8.5 seconds.\n#&gt; Chain 3 finished in 8.6 seconds.\n#&gt; Chain 4 finished in 8.7 seconds.\n#&gt; Chain 2 finished in 9.7 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 8.9 seconds.\n#&gt; Total execution time: 9.8 seconds.\n\nsummary(model_multinomial)\n#&gt;  Family: multinomial \n#&gt;   Links: mupsoe = logit; musumar = logit; muvox = logit; muresto = logit \n#&gt; Formula: cell_counts | trials(n) ~ (time | empresa) \n#&gt;    Data: df_wider (Number of observations: 23) \n#&gt;   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 12000\n#&gt; \n#&gt; Group-Level Effects: \n#&gt; ~empresa (Number of levels: 10) \n#&gt;                                     Estimate Est.Error l-95% CI u-95% CI Rhat\n#&gt; sd(mupsoe_Intercept)                    0.05      0.03     0.00     0.13 1.00\n#&gt; sd(mupsoe_time)                         0.00      0.00     0.00     0.01 1.00\n#&gt; sd(musumar_Intercept)                   0.14      0.06     0.05     0.28 1.00\n#&gt; sd(musumar_time)                        0.00      0.00     0.00     0.02 1.00\n#&gt; sd(muvox_Intercept)                     0.11      0.06     0.01     0.25 1.00\n#&gt; sd(muvox_time)                          0.01      0.01     0.00     0.02 1.00\n#&gt; sd(muresto_Intercept)                   0.05      0.04     0.00     0.16 1.00\n#&gt; sd(muresto_time)                        0.01      0.00     0.00     0.01 1.00\n#&gt; cor(mupsoe_Intercept,mupsoe_time)       0.06      0.58    -0.94     0.95 1.00\n#&gt; cor(musumar_Intercept,musumar_time)     0.28      0.56    -0.88     0.98 1.00\n#&gt; cor(muvox_Intercept,muvox_time)        -0.03      0.56    -0.95     0.93 1.00\n#&gt; cor(muresto_Intercept,muresto_time)     0.21      0.58    -0.91     0.98 1.00\n#&gt;                                     Bulk_ESS Tail_ESS\n#&gt; sd(mupsoe_Intercept)                    3451     3599\n#&gt; sd(mupsoe_time)                         2876     5041\n#&gt; sd(musumar_Intercept)                   3697     3162\n#&gt; sd(musumar_time)                        3236     4666\n#&gt; sd(muvox_Intercept)                     2951     2225\n#&gt; sd(muvox_time)                          1842     4433\n#&gt; sd(muresto_Intercept)                   5233     4971\n#&gt; sd(muresto_time)                        3078     3653\n#&gt; cor(mupsoe_Intercept,mupsoe_time)       5869     6716\n#&gt; cor(musumar_Intercept,musumar_time)     7015     7025\n#&gt; cor(muvox_Intercept,muvox_time)         6490     6740\n#&gt; cor(muresto_Intercept,muresto_time)     4284     6808\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; mupsoe_Intercept     -0.20      0.03    -0.25    -0.15 1.00     6469     6941\n#&gt; musumar_Intercept    -0.99      0.05    -1.09    -0.90 1.00     4232     5876\n#&gt; muvox_Intercept      -0.97      0.05    -1.06    -0.87 1.00     3835     6313\n#&gt; muresto_Intercept    -1.10      0.03    -1.15    -1.04 1.00     7182     7017\n#&gt; \n#&gt; Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).\n\nY ya tendríamos el modelo."
  },
  {
    "objectID": "2023/metanalisis.html#predicción-estimación",
    "href": "2023/metanalisis.html#predicción-estimación",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "¿Predicción / estimación?",
    "text": "¿Predicción / estimación?\nVuelvo a decir que esto es sólo por diversión, para hacer algo serio tendría que haber usado mayor número de encuestas y realizadas en diferentes momentos del tiempo, tener las estimaciones que daban en cada provincia y realizar la estimación de escaños. Todo eso y más ya lo hace, y muy bien, Kiko Llaneras para El País\n¿Cómo podríamos estimar lo que va a pasar el día de las elecciones con este modelo?\nPues podríamos considerar que las elecciones fueran una encuesta realizada por una empresa que no tengo en los datos (un nuevo nivel de la variable empresa) , en este caso el gobierno, y ponemos la variable time = 0\n\n# pongo n = 1 para que me devuelva las probabilidades , si ponenemos n = 15000000 nos devolvería una # estimación de cuántos votos van a cada partido\n\nnewdata &lt;- tibble(\n    empresa = \"votaciones_dia_23\", \n    time = 0,\n    n= 1)\n\nnewdata\n#&gt; # A tibble: 1 × 3\n#&gt;   empresa            time     n\n#&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 votaciones_dia_23     0     1\n\nAhora utilizando una función de la librería tidybayes tenemos una forma fácil de añadir las posterior predict\n\nestimaciones &lt;-  newdata %&gt;% \n    add_epred_draws(model_multinomial, allow_new_levels = TRUE) %&gt;% \n    mutate(partido = as_factor(.category)) |&gt; \n    select(-.category)\n\ndim(estimaciones )\n#&gt; [1] 60000    10\n\nDT::datatable(head(estimaciones, 100))\n\n\n\n\n\n\nY tenemos 12000 estimaciones de la posteriori para cada partido. Esto se podría decir que es equivalente a las 15000 simulaciones que hace Kiko con su modelo. Bueno, salvo que él en cada simulación calcula más cosas, como los escaños obtenidos etc..\nPodemos hacer un summary de las posteriores y ver intervalo de credibilidad al 80% por ejemplo\n\nestimaciones |&gt; \n    group_by(partido) |&gt; \n    summarise(\n        media = mean(.epred), \n        mediana= median(.epred), \n        low = quantile(.epred, 0.05), \n        high= quantile(.epred, 0.95)\n    )\n#&gt; # A tibble: 5 × 5\n#&gt;   partido media mediana   low  high\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 pp      0.344   0.344 0.324 0.364\n#&gt; 2 psoe    0.282   0.282 0.265 0.298\n#&gt; 3 sumar   0.128   0.127 0.110 0.148\n#&gt; 4 vox     0.131   0.131 0.116 0.148\n#&gt; 5 resto   0.115   0.115 0.105 0.125\n\nO pintar las distribuciones. .\n\n\nestimaciones %&gt;% \n    ggplot(aes(x=.epred, fill = partido)) +\n    geom_density(alpha = 0.5 ) +\n    scale_x_continuous(labels = scales::percent) +\n    scale_fill_manual(values = colores) +\n    labs(title=\"Agregando encuestas por diversión. Resultado\",\n        x = \"Porcentaje estimado\", \n        y = \"Density\")\n\n\n\n\n\n\n\n\n¿Qué más cosas podemos hacer? Ya que tengo las posterioris puedo usarlas y calcular las posterioris del bloque PP+ Vox o de cualquier otra cosa.\nSupongamos que hubiera 15 millones de votos válidos.\n\nvotantes &lt;- 15e6\nposterioris &lt;- estimaciones  |&gt; \n    ungroup() |&gt; \n    mutate(votos = votantes * .epred) |&gt; \n    select(partido, votos) |&gt; \n    pivot_wider(names_from = partido, values_from = votos) \n\n# tenemos columnas que son listas.  hay que hacer un unnest\nposterioris\n#&gt; # A tibble: 1 × 5\n#&gt;   pp             psoe           sumar          vox            resto         \n#&gt;   &lt;list&gt;         &lt;list&gt;         &lt;list&gt;         &lt;list&gt;         &lt;list&gt;        \n#&gt; 1 &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt;\n\n\nposterioris &lt;- posterioris  |&gt; \n    unnest(c(pp, psoe, sumar, vox, resto)) \n\nhead(posterioris)\n#&gt; # A tibble: 6 × 5\n#&gt;         pp     psoe    sumar      vox    resto\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 5023147. 4400631. 1873502. 2108276. 1594443.\n#&gt; 2 5402129. 4297818. 1791318. 1783497. 1725237.\n#&gt; 3 5066890. 4415547. 1741994. 1990458. 1785111.\n#&gt; 4 5372578. 4119759. 1768716. 1833665. 1905282.\n#&gt; 5 5022623. 4342021. 2004152. 1986356. 1644848.\n#&gt; 6 5287214. 4283855. 1876413. 1714759. 1837759.\n\nSumo votos de los bloques para cada una de las 12000 filas. Además. añado al bloque de la izquierda el 50% de los votos que están en resto.\n\nposterioris &lt;- posterioris |&gt; \n    mutate(\n        derecha = pp + vox, \n        izquierda = psoe + sumar + 0.5*resto) |&gt; \n    mutate(derecha_posterior = derecha / votantes, \n           izquierda_posterior = izquierda/votantes)\n\nposterioris |&gt; \n    head(20)\n#&gt; # A tibble: 20 × 9\n#&gt;          pp     psoe    sumar     vox  resto derecha izquierda derecha_posterior\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n#&gt;  1 5023147. 4400631. 1873502.  2.11e6 1.59e6  7.13e6  7071355.             0.475\n#&gt;  2 5402129. 4297818. 1791318.  1.78e6 1.73e6  7.19e6  6951755.             0.479\n#&gt;  3 5066890. 4415547. 1741994.  1.99e6 1.79e6  7.06e6  7050097.             0.470\n#&gt;  4 5372578. 4119759. 1768716.  1.83e6 1.91e6  7.21e6  6841116.             0.480\n#&gt;  5 5022623. 4342021. 2004152.  1.99e6 1.64e6  7.01e6  7168597.             0.467\n#&gt;  6 5287214. 4283855. 1876413.  1.71e6 1.84e6  7.00e6  7079147.             0.467\n#&gt;  7 5017396. 4110837. 2085376.  2.23e6 1.56e6  7.25e6  6974129.             0.483\n#&gt;  8 4793459. 4479950. 2223135.  2.01e6 1.49e6  6.80e6  7449523.             0.454\n#&gt;  9 5345670. 4142111. 1795447.  2.03e6 1.69e6  7.38e6  6781142.             0.492\n#&gt; 10 5127698. 4210917. 1935521.  1.91e6 1.82e6  7.04e6  7055529.             0.469\n#&gt; 11 5486238. 4199075. 1588582.  1.86e6 1.87e6  7.35e6  6720877.             0.490\n#&gt; 12 5149876. 4219835. 2100181.  1.78e6 1.75e6  6.93e6  7193195.             0.462\n#&gt; 13 4759372. 4465230. 2329286.  1.84e6 1.60e6  6.60e6  7596283.             0.440\n#&gt; 14 5236625. 4338129. 1747429.  1.95e6 1.73e6  7.18e6  6950958.             0.479\n#&gt; 15 5206440. 4387667. 1749035.  1.99e6 1.67e6  7.19e6  6971347.             0.480\n#&gt; 16 4854678. 4329145. 2184347.  1.77e6 1.86e6  6.62e6  7444375.             0.442\n#&gt; 17 5250217. 4518969. 1701659.  1.75e6 1.78e6  7.00e6  7110288.             0.467\n#&gt; 18 5179197. 4451222. 1814213.  1.79e6 1.76e6  6.97e6  7147712.             0.465\n#&gt; 19 4807230. 4425101. 1768384.  2.44e6 1.56e6  7.25e6  6974064.             0.483\n#&gt; 20 5001895. 3986637. 2109063.  2.05e6 1.85e6  7.05e6  7022137.             0.470\n#&gt; # ℹ 1 more variable: izquierda_posterior &lt;dbl&gt;\n\nAhora nos podemos hacer preguntas como , ¿en cuántas de estas posterioris gana el bloque de la derecha así construido al de la izquierda? o ¿en cuántas la diferencia que le saca el bloque de la derecha es mayor a un punto porcentual?\n\nposterioris |&gt; \n    mutate(gana_derecha = derecha_posterior&gt;izquierda_posterior, \n           gana_derecha_mas1_pct = (derecha_posterior - izquierda_posterior) &gt;= 0.01) |&gt; \n    summarise(\n        mean(gana_derecha),\n        mean(gana_derecha_mas1_pct)\n        )\n#&gt; # A tibble: 1 × 2\n#&gt;   `mean(gana_derecha)` `mean(gana_derecha_mas1_pct)`\n#&gt;                  &lt;dbl&gt;                         &lt;dbl&gt;\n#&gt; 1                0.673                         0.480"
  },
  {
    "objectID": "2023/metanalisis.html#coda",
    "href": "2023/metanalisis.html#coda",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Coda",
    "text": "Coda\nBueno, pues así es como he pasado el sábado. ¿Cosas que le faltaría a esto para ser algo serio?\n\nQue tuviera en cuenta más encuestas y analizara mejor qué tipo de encuestas son, sus cambios de estimación según el tiempo\nQue añadiera estimación de escaños, lo cual no es trivial.\nAñadir encuestas a nivel autónomico o datos de las municipales y poder hacer un modelo jerárquico en condiciones.\nHablando con Virgilio o Ana, queda claro que al ser encuestas a nivel nacional y tener un error muy grande a nivel provincial, es muy complicado hacer luego la predicción de escaños.\n\nY como decía al principio, seguramente esto del meta-análisis se pueda hacer de otra manera, mucho mejor, así que si alguien sabe, que lo ponga en los comentarios."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#intro",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#intro",
    "title": "Conformal prediction. Estilo compadre",
    "section": "",
    "text": "El jueves pasado asistí al más que recomendable meetup de PyData Madrid, que cuenta entre sus organizadores con el gran Juan Luis Cano Rodríguez, antiguo compañero mío de curro y tocayo de iniciales.\nEl caso es que en una de las charlas, Ignacio Peletier, mencionó de pasada lo del “Conformal Prediction”. Y siendo que Ignacio es un gran científico de datos, y que hacía unos meses que había tenido varias charlas con Carlos sobre el particular, pues he decidido ver un poco más en detalle de qué iba el asunto ."
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#intro",
    "href": "2023/02/19/Arrow_y_s3/index.html#intro",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#intro",
    "href": "2023/01/29/explicatividad_no_usual/index.html#intro",
    "title": "Explicatividad no usual",
    "section": "",
    "text": "Buscando en el portátil sobre otras cosas me he encontrado un pequeño ejercicio para implementar la idea que se comenta aquí\nLa idea es muy sencilla, tal y como comenta Carlos. Si tengo un modelo que sea tipo randomForest\n\nDe cada observación a predecir se anota en qué nodo terminal ha caído en cada árbol\nPara cada nodo terminal en cada árbol se recorre el camino hacia “arriba” para saber qué variables están implicadas en ese nodo\nSe cuenta para cada nodo terminal el número de veces que aparece una variable en cada árbol (o se da más importancia a las qeu estén más altos en el árbol)\nSe agrega de alguna manera para cada observación qué variables y cómo de importantes están en los caminos de los nodos terminales en los que han caído.\nEsa info sería la importancia de las variables a nivel individual\nSe podría clusterizar observaciones con similares variables importantes\n\nAntes de nada, sí, ya sé que existen cosas como los shap values y que a partir de ellos se puede hacer algo parecido. Pero no está de más de vez en cuando buscarse uno las habichueleas de forma más artesanal.."
  },
  {
    "objectID": "2023/metanalisis.html#el-código-que-genera-en-stan",
    "href": "2023/metanalisis.html#el-código-que-genera-en-stan",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "El código que genera en Stan",
    "text": "El código que genera en Stan\nCon la función stancode podemos ver las tripas de lo qeu se ha mandado a Stan\n\nstancode(model_multinomial)\n#&gt; // generated with brms 2.19.0\n#&gt; functions {\n#&gt;   /* compute correlated group-level effects\n#&gt;    * Args:\n#&gt;    *   z: matrix of unscaled group-level effects\n#&gt;    *   SD: vector of standard deviation parameters\n#&gt;    *   L: cholesky factor correlation matrix\n#&gt;    * Returns:\n#&gt;    *   matrix of scaled group-level effects\n#&gt;    */\n#&gt;   matrix scale_r_cor(matrix z, vector SD, matrix L) {\n#&gt;     // r is stored in another dimension order than z\n#&gt;     return transpose(diag_pre_multiply(SD, L) * z);\n#&gt;   }\n#&gt;   /* multinomial-logit log-PMF\n#&gt;    * Args:\n#&gt;    *   y: array of integer response values\n#&gt;    *   mu: vector of category logit probabilities\n#&gt;    * Returns:\n#&gt;    *   a scalar to be added to the log posterior\n#&gt;    */\n#&gt;   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#&gt;     return multinomial_lpmf(y | softmax(mu));\n#&gt;   }\n#&gt; }\n#&gt; data {\n#&gt;   int&lt;lower=1&gt; N; // total number of observations\n#&gt;   int&lt;lower=2&gt; ncat; // number of categories\n#&gt;   array[N, ncat] int Y; // response array\n#&gt;   array[N] int trials; // number of trials\n#&gt;   // data for group-level effects of ID 1\n#&gt;   int&lt;lower=1&gt; N_1; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_1; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_1; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_1_mupsoe_1;\n#&gt;   vector[N] Z_1_mupsoe_2;\n#&gt;   int&lt;lower=1&gt; NC_1; // number of group-level correlations\n#&gt;   // data for group-level effects of ID 2\n#&gt;   int&lt;lower=1&gt; N_2; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_2; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_2; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_2_musumar_1;\n#&gt;   vector[N] Z_2_musumar_2;\n#&gt;   int&lt;lower=1&gt; NC_2; // number of group-level correlations\n#&gt;   // data for group-level effects of ID 3\n#&gt;   int&lt;lower=1&gt; N_3; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_3; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_3; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_3_muvox_1;\n#&gt;   vector[N] Z_3_muvox_2;\n#&gt;   int&lt;lower=1&gt; NC_3; // number of group-level correlations\n#&gt;   // data for group-level effects of ID 4\n#&gt;   int&lt;lower=1&gt; N_4; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_4; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_4; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_4_muresto_1;\n#&gt;   vector[N] Z_4_muresto_2;\n#&gt;   int&lt;lower=1&gt; NC_4; // number of group-level correlations\n#&gt;   int prior_only; // should the likelihood be ignored?\n#&gt; }\n#&gt; transformed data {\n#&gt;   \n#&gt; }\n#&gt; parameters {\n#&gt;   real Intercept_mupsoe; // temporary intercept for centered predictors\n#&gt;   real Intercept_musumar; // temporary intercept for centered predictors\n#&gt;   real Intercept_muvox; // temporary intercept for centered predictors\n#&gt;   real Intercept_muresto; // temporary intercept for centered predictors\n#&gt;   vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations\n#&gt;   matrix[M_1, N_1] z_1; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_1] L_1; // cholesky factor of correlation matrix\n#&gt;   vector&lt;lower=0&gt;[M_2] sd_2; // group-level standard deviations\n#&gt;   matrix[M_2, N_2] z_2; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_2] L_2; // cholesky factor of correlation matrix\n#&gt;   vector&lt;lower=0&gt;[M_3] sd_3; // group-level standard deviations\n#&gt;   matrix[M_3, N_3] z_3; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_3] L_3; // cholesky factor of correlation matrix\n#&gt;   vector&lt;lower=0&gt;[M_4] sd_4; // group-level standard deviations\n#&gt;   matrix[M_4, N_4] z_4; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_4] L_4; // cholesky factor of correlation matrix\n#&gt; }\n#&gt; transformed parameters {\n#&gt;   matrix[N_1, M_1] r_1; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_1] r_1_mupsoe_1;\n#&gt;   vector[N_1] r_1_mupsoe_2;\n#&gt;   matrix[N_2, M_2] r_2; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_2] r_2_musumar_1;\n#&gt;   vector[N_2] r_2_musumar_2;\n#&gt;   matrix[N_3, M_3] r_3; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_3] r_3_muvox_1;\n#&gt;   vector[N_3] r_3_muvox_2;\n#&gt;   matrix[N_4, M_4] r_4; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_4] r_4_muresto_1;\n#&gt;   vector[N_4] r_4_muresto_2;\n#&gt;   real lprior = 0; // prior contributions to the log posterior\n#&gt;   // compute actual group-level effects\n#&gt;   r_1 = scale_r_cor(z_1, sd_1, L_1);\n#&gt;   r_1_mupsoe_1 = r_1[ : , 1];\n#&gt;   r_1_mupsoe_2 = r_1[ : , 2];\n#&gt;   // compute actual group-level effects\n#&gt;   r_2 = scale_r_cor(z_2, sd_2, L_2);\n#&gt;   r_2_musumar_1 = r_2[ : , 1];\n#&gt;   r_2_musumar_2 = r_2[ : , 2];\n#&gt;   // compute actual group-level effects\n#&gt;   r_3 = scale_r_cor(z_3, sd_3, L_3);\n#&gt;   r_3_muvox_1 = r_3[ : , 1];\n#&gt;   r_3_muvox_2 = r_3[ : , 2];\n#&gt;   // compute actual group-level effects\n#&gt;   r_4 = scale_r_cor(z_4, sd_4, L_4);\n#&gt;   r_4_muresto_1 = r_4[ : , 1];\n#&gt;   r_4_muresto_2 = r_4[ : , 2];\n#&gt;   lprior += student_t_lpdf(Intercept_mupsoe | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(Intercept_musumar | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(Intercept_muvox | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(Intercept_muresto | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n#&gt;   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n#&gt;   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_3 | 1);\n#&gt;   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_4 | 1);\n#&gt; }\n#&gt; model {\n#&gt;   // likelihood including constants\n#&gt;   if (!prior_only) {\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] mupsoe = rep_vector(0.0, N);\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] musumar = rep_vector(0.0, N);\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] muvox = rep_vector(0.0, N);\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] muresto = rep_vector(0.0, N);\n#&gt;     // linear predictor matrix\n#&gt;     array[N] vector[ncat] mu;\n#&gt;     mupsoe += Intercept_mupsoe;\n#&gt;     musumar += Intercept_musumar;\n#&gt;     muvox += Intercept_muvox;\n#&gt;     muresto += Intercept_muresto;\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       mupsoe[n] += r_1_mupsoe_1[J_1[n]] * Z_1_mupsoe_1[n]\n#&gt;                    + r_1_mupsoe_2[J_1[n]] * Z_1_mupsoe_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       musumar[n] += r_2_musumar_1[J_2[n]] * Z_2_musumar_1[n]\n#&gt;                     + r_2_musumar_2[J_2[n]] * Z_2_musumar_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       muvox[n] += r_3_muvox_1[J_3[n]] * Z_3_muvox_1[n]\n#&gt;                   + r_3_muvox_2[J_3[n]] * Z_3_muvox_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       muresto[n] += r_4_muresto_1[J_4[n]] * Z_4_muresto_1[n]\n#&gt;                     + r_4_muresto_2[J_4[n]] * Z_4_muresto_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       mu[n] = transpose([0, mupsoe[n], musumar[n], muvox[n], muresto[n]]);\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#&gt;     }\n#&gt;   }\n#&gt;   // priors including constants\n#&gt;   target += lprior;\n#&gt;   target += std_normal_lpdf(to_vector(z_1));\n#&gt;   target += std_normal_lpdf(to_vector(z_2));\n#&gt;   target += std_normal_lpdf(to_vector(z_3));\n#&gt;   target += std_normal_lpdf(to_vector(z_4));\n#&gt; }\n#&gt; generated quantities {\n#&gt;   // actual population-level intercept\n#&gt;   real b_mupsoe_Intercept = Intercept_mupsoe;\n#&gt;   // actual population-level intercept\n#&gt;   real b_musumar_Intercept = Intercept_musumar;\n#&gt;   // actual population-level intercept\n#&gt;   real b_muvox_Intercept = Intercept_muvox;\n#&gt;   // actual population-level intercept\n#&gt;   real b_muresto_Intercept = Intercept_muresto;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_1] cor_1;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_2] cor_2;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_3] Cor_3 = multiply_lower_tri_self_transpose(L_3);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_3] cor_3;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_4] Cor_4 = multiply_lower_tri_self_transpose(L_4);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_4] cor_4;\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_1) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n#&gt;     }\n#&gt;   }\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_2) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n#&gt;     }\n#&gt;   }\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_3) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_3[choose(k - 1, 2) + j] = Cor_3[j, k];\n#&gt;     }\n#&gt;   }\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_4) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_4[choose(k - 1, 2) + j] = Cor_4[j, k];\n#&gt;     }\n#&gt;   }\n#&gt; }"
  },
  {
    "objectID": "2023/metanalisis.html#actualización-2023-07-24",
    "href": "2023/metanalisis.html#actualización-2023-07-24",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Actualización [2023-07-24]",
    "text": "Actualización [2023-07-24]\nYa fueron las elecciones generales y escrutado al 100% se tiene que\n\n\n\n\n\n\n\nY claramente el promedio de encuestas infraestimó el voto al psoe. Para pp, sumar o o vox si que acertaron relativamente bien, aunque pp y vox se han quedado más cerda del extremo inferior del intervalo que del punto medio.\n\n¿Si hubiera considerado más encuestas habría cambiado algo?\n\ndf_update &lt;-  read_csv(here::here(\"data/encuestas_agregadas_39.csv\")) |&gt; \n    select(empresa, time, partido, everything())\n# Añado botones para descarga de datos \ndatatable(df, extensions = \"Buttons\",\n          options = list(\n              dom = 'Bfrtip',\n              buttons = c('copy', 'csv', 'excel', 'pdf') )\n          )\n\n\n\n\n\n\n\n\ndf_wider &lt;- df_update |&gt; \n    select(-estim) |&gt; \n    pivot_wider( id_cols = c(empresa, n, time),\n                 names_from = partido, \n                 values_from = votos) |&gt; \n    arrange(empresa)\n\ndf_wider$cell_counts &lt;- with(df_wider, cbind(resto,pp, psoe,sumar, vox))\n\nDT::datatable(df_wider)\n\n\n\n\n\n\n\n\n# cambio un poco la formla pq quiero efecto fijo del tiempo además del varying \n# slope\nformula &lt;- brmsformula(\n    cell_counts | trials(n) ~ time +  (time |empresa))\n\nmodel_multinomial_update &lt;- brm(formula, df_wider, multinomial(),\n                          iter = 4000, warmup = 1000, cores = 4, chains = 4,\n                          seed = 3,\n                          backend = \"cmdstanr\", \n                         control = list(adapt_delta = 0.95)\n)\n#&gt; Running MCMC with 4 parallel chains...\n#&gt; \n#&gt; Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 3 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 3 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 1 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 2 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 3 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 1 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 2 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 3 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 4 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 1 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 2 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 3 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 4 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 2 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 1 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 3 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 4 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 2 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 1 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 3 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 4 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 1 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 2 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 3 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 4 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 1 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 2 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 3 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 4 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 1 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 2 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 3 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 4 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 2 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 1 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 4 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 2 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 1 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 4 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 4 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 3 finished in 24.5 seconds.\n#&gt; Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 1 finished in 25.4 seconds.\n#&gt; Chain 2 finished in 25.4 seconds.\n#&gt; Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 4 finished in 26.1 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 25.3 seconds.\n#&gt; Total execution time: 26.2 seconds.\n\n\nestimaciones_update &lt;-  newdata %&gt;% \n    add_epred_draws(model_multinomial_update, allow_new_levels = TRUE) %&gt;% \n    mutate(partido = as_factor(.category)) |&gt; \n    select(-.category)\n\nPues parece que no cambia mucho. Si acaso añade más incertidumbre y ahora el resultado del psoe está dentro del intervalo de credibilidad del 90%\n\n\nestimaciones_update |&gt; \n    group_by(partido) |&gt; \n    summarise(\n        media = mean(.epred), \n        mediana= median(.epred), \n        low = quantile(.epred, 0.05), \n        high= quantile(.epred, 0.95)\n    )\n#&gt; # A tibble: 5 × 5\n#&gt;   partido media mediana    low  high\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 resto   0.115   0.115 0.108  0.126\n#&gt; 2 pp      0.343   0.343 0.310  0.375\n#&gt; 3 psoe    0.292   0.290 0.275  0.322\n#&gt; 4 sumar   0.125   0.124 0.113  0.139\n#&gt; 5 vox     0.125   0.128 0.0789 0.150\n\n\n\nestimaciones_update %&gt;% \n    ggplot(aes(x=.epred, fill = partido)) +\n    geom_density(alpha = 0.5 ) +\n    scale_x_continuous(labels = scales::percent) +\n    scale_fill_manual(values = colores) +\n    labs(title=\"Agregando encuestas por diversión. Resultado\",\n         subtitle = \"Actualización con más encuestas\",\n        x = \"Porcentaje estimado\", \n        y = \"Density\")\n\n\n\n\n\n\n\n\nCosas interesantes que puedo sacar del modelo. Por ejemplo los efectos aleatorios asociados a las diferentes empresas encuestadoras. Por ejemplo con mupsoe_Intercept se ve por ejemplo que la de prisa(40db) ,simple_logica (para el diario.es) sigma2 o el cis fueron las más favorables hacia el psoe.\nEsto parece sugerir que los resultados de las diferentes encuestas parecen estar afectados por la mano que les da de comer.\n\nranef(model_multinomial_update)\n#&gt; $empresa\n#&gt; , , mupp_Intercept\n#&gt; \n#&gt;                  Estimate  Est.Error        Q2.5      Q97.5\n#&gt; celeste_tel    0.04983451 0.05847759 -0.05785386 0.17452635\n#&gt; cis           -0.11269392 0.07921043 -0.28268055 0.01887259\n#&gt; electo_mania   0.01266378 0.05495308 -0.08769483 0.12890022\n#&gt; gad3           0.12708807 0.04538762  0.04470506 0.22400860\n#&gt; gesop         -0.09336239 0.06633783 -0.23445718 0.02701568\n#&gt; invymark      -0.08845348 0.08144761 -0.26558825 0.06216075\n#&gt; nc_report      0.06977150 0.05506073 -0.03028796 0.18586357\n#&gt; ok_diario      0.04076435 0.05285644 -0.05784015 0.15009835\n#&gt; prisa         -0.02726022 0.04755849 -0.11914163 0.07052230\n#&gt; sigma2         0.03550888 0.04398473 -0.04942665 0.12580797\n#&gt; simple_logica -0.02710374 0.04962525 -0.12671515 0.06897185\n#&gt; socio_metrica -0.02427102 0.05004341 -0.12195298 0.07523895\n#&gt; target_point   0.03521051 0.09644025 -0.16213082 0.23375662\n#&gt; \n#&gt; , , mupp_time\n#&gt; \n#&gt;                    Estimate    Est.Error         Q2.5        Q97.5\n#&gt; celeste_tel    5.601833e-04 0.0010752752 -0.001462165 0.0028845335\n#&gt; cis           -1.858769e-03 0.0014818803 -0.005107945 0.0003605464\n#&gt; electo_mania   8.326875e-04 0.0013083572 -0.001403193 0.0038301443\n#&gt; gad3           6.870950e-04 0.0018649834 -0.003052465 0.0044324950\n#&gt; gesop         -6.383980e-04 0.0018839697 -0.004635597 0.0029928842\n#&gt; invymark       3.140981e-05 0.0019661982 -0.004053128 0.0040827335\n#&gt; nc_report      6.845445e-04 0.0014938625 -0.002058980 0.0040297510\n#&gt; ok_diario      5.167656e-04 0.0012166924 -0.001778642 0.0031886107\n#&gt; prisa          6.750862e-04 0.0009813955 -0.001163746 0.0027679542\n#&gt; sigma2        -3.516238e-04 0.0012475810 -0.003085294 0.0019836857\n#&gt; simple_logica -1.126181e-03 0.0010989386 -0.003559223 0.0006837115\n#&gt; socio_metrica  3.710038e-04 0.0012584726 -0.002067272 0.0031104447\n#&gt; target_point  -3.996651e-04 0.0017999138 -0.004385952 0.0030570600\n#&gt; \n#&gt; , , mupsoe_Intercept\n#&gt; \n#&gt;                   Estimate  Est.Error        Q2.5      Q97.5\n#&gt; celeste_tel   -0.002276998 0.02808656 -0.06507048 0.05559588\n#&gt; cis            0.019702197 0.04093116 -0.05067533 0.11701995\n#&gt; electo_mania   0.011364728 0.02912626 -0.03663608 0.08641640\n#&gt; gad3           0.015223792 0.02628927 -0.02780848 0.07836367\n#&gt; gesop         -0.016066261 0.03623679 -0.10444262 0.04282177\n#&gt; invymark      -0.028486511 0.04541277 -0.14301735 0.04049496\n#&gt; nc_report     -0.007220812 0.02850193 -0.07316730 0.04673424\n#&gt; ok_diario     -0.030797624 0.03886857 -0.12636292 0.02279682\n#&gt; prisa          0.009762308 0.02631351 -0.03663878 0.07402995\n#&gt; sigma2         0.011121029 0.02579794 -0.03593884 0.07250188\n#&gt; simple_logica  0.016102746 0.03142286 -0.03983507 0.08999656\n#&gt; socio_metrica -0.008354206 0.02858118 -0.07234223 0.04903350\n#&gt; target_point   0.009988969 0.03623881 -0.05608511 0.09806407\n#&gt; \n#&gt; , , mupsoe_time\n#&gt; \n#&gt;                    Estimate   Est.Error          Q2.5         Q97.5\n#&gt; celeste_tel    6.351476e-05 0.001199362 -2.456705e-03  2.396963e-03\n#&gt; cis           -3.270677e-03 0.001262601 -5.934026e-03 -9.475839e-04\n#&gt; electo_mania   1.584091e-04 0.001340476 -2.472783e-03  2.911473e-03\n#&gt; gad3          -1.278420e-03 0.001959843 -5.409934e-03  2.486353e-03\n#&gt; gesop          1.001596e-03 0.002732244 -4.253542e-03  6.961284e-03\n#&gt; invymark       3.399821e-03 0.001909472  1.128583e-05  7.474699e-03\n#&gt; nc_report      9.755046e-04 0.001750050 -2.300850e-03  4.728513e-03\n#&gt; ok_diario      2.827258e-03 0.001539038 -1.292829e-05  5.969975e-03\n#&gt; prisa         -4.973800e-04 0.001138912 -2.882389e-03  1.753342e-03\n#&gt; sigma2        -1.277384e-03 0.001435137 -4.328619e-03  1.434950e-03\n#&gt; simple_logica -2.356142e-03 0.001293664 -5.124468e-03 -7.625871e-05\n#&gt; socio_metrica  1.594284e-03 0.001489799 -1.132845e-03  4.691291e-03\n#&gt; target_point  -1.411879e-03 0.001590356 -4.812724e-03  1.507244e-03\n#&gt; \n#&gt; , , musumar_Intercept\n#&gt; \n#&gt;                    Estimate  Est.Error         Q2.5      Q97.5\n#&gt; celeste_tel   -0.0358445986 0.04957444 -0.150229100 0.04743706\n#&gt; cis            0.0132971726 0.04651727 -0.069758368 0.12317857\n#&gt; electo_mania   0.0070341761 0.04248783 -0.083039127 0.09161290\n#&gt; gad3          -0.0353641132 0.03666900 -0.115859275 0.02688924\n#&gt; gesop          0.0373218097 0.05536161 -0.054462743 0.16499042\n#&gt; invymark      -0.0089751397 0.04854726 -0.111963050 0.08780055\n#&gt; nc_report     -0.0406315429 0.04748837 -0.150190250 0.03561324\n#&gt; ok_diario     -0.0102878996 0.04171049 -0.098574120 0.07398172\n#&gt; prisa          0.0225994343 0.04031133 -0.054578180 0.10777645\n#&gt; sigma2        -0.0082246939 0.03496083 -0.080907860 0.06160265\n#&gt; simple_logica  0.0751187593 0.05402766 -0.009799251 0.18905942\n#&gt; socio_metrica -0.0002657433 0.03985462 -0.084348557 0.08278825\n#&gt; target_point  -0.0165754581 0.05666641 -0.142857825 0.09367493\n#&gt; \n#&gt; , , musumar_time\n#&gt; \n#&gt;                    Estimate    Est.Error         Q2.5        Q97.5\n#&gt; celeste_tel    3.103610e-04 0.0008602449 -0.001373319 0.0022302135\n#&gt; cis            3.347250e-04 0.0008624901 -0.001128795 0.0024411762\n#&gt; electo_mania  -6.618639e-04 0.0011008107 -0.003507239 0.0008992042\n#&gt; gad3           2.429828e-05 0.0012769107 -0.002911007 0.0026252740\n#&gt; gesop         -8.011793e-05 0.0013149711 -0.002928823 0.0027984967\n#&gt; invymark       1.338833e-04 0.0011282724 -0.002083171 0.0027425217\n#&gt; nc_report      2.097078e-04 0.0011791976 -0.002183350 0.0028841215\n#&gt; ok_diario      3.148319e-04 0.0009717670 -0.001482126 0.0026558482\n#&gt; prisa         -6.035426e-04 0.0008646081 -0.002661508 0.0007422272\n#&gt; sigma2         4.400805e-04 0.0010397252 -0.001275172 0.0030627337\n#&gt; simple_logica -7.506860e-04 0.0010609271 -0.003165005 0.0009917626\n#&gt; socio_metrica  1.864726e-05 0.0009601066 -0.002061034 0.0020858427\n#&gt; target_point   3.113929e-04 0.0011178886 -0.001753471 0.0030024427\n#&gt; \n#&gt; , , muvox_Intercept\n#&gt; \n#&gt;                   Estimate  Est.Error        Q2.5      Q97.5\n#&gt; celeste_tel   -0.027563257 0.10163227 -0.22536030  0.1727010\n#&gt; cis           -0.565764453 0.13587027 -0.84565335 -0.3140112\n#&gt; electo_mania   0.066364066 0.09618556 -0.12254760  0.2570871\n#&gt; gad3          -0.008670455 0.07859782 -0.15700788  0.1495601\n#&gt; gesop          0.147990408 0.10824879 -0.05956420  0.3635356\n#&gt; invymark       0.033535523 0.14445059 -0.24711252  0.3263141\n#&gt; nc_report     -0.070586246 0.09473895 -0.25996155  0.1156743\n#&gt; ok_diario      0.054799569 0.09046204 -0.12134520  0.2336971\n#&gt; prisa          0.119210914 0.08427223 -0.04170232  0.2898681\n#&gt; sigma2         0.054515552 0.08308845 -0.10485597  0.2270518\n#&gt; simple_logica  0.138785392 0.08591571 -0.02525560  0.3128597\n#&gt; socio_metrica  0.078037685 0.08942748 -0.09257302  0.2565346\n#&gt; target_point  -0.011986902 0.20139152 -0.41809745  0.3930733\n#&gt; \n#&gt; , , muvox_time\n#&gt; \n#&gt;                    Estimate   Est.Error         Q2.5        Q97.5\n#&gt; celeste_tel    7.203336e-04 0.001784125 -0.002744172  0.004351591\n#&gt; cis           -7.170722e-03 0.002312615 -0.011891795 -0.002824443\n#&gt; electo_mania  -1.162150e-03 0.002241739 -0.005934080  0.002965156\n#&gt; gad3           4.264346e-04 0.002470284 -0.004084816  0.005839640\n#&gt; gesop          1.762831e-03 0.002799074 -0.003799805  0.007700488\n#&gt; invymark       3.921615e-04 0.003261884 -0.006150279  0.007273604\n#&gt; nc_report     -1.506608e-03 0.002381401 -0.006585859  0.002923375\n#&gt; ok_diario      8.105272e-05 0.001922182 -0.003800808  0.003791748\n#&gt; prisa          2.484377e-03 0.001586320 -0.000466300  0.005714418\n#&gt; sigma2         2.661615e-03 0.002236228 -0.001143362  0.007573518\n#&gt; simple_logica  1.070490e-03 0.001608866 -0.002146405  0.004289484\n#&gt; socio_metrica  2.899506e-04 0.002028250 -0.003889796  0.004245027\n#&gt; target_point   7.433332e-06 0.003551183 -0.007202358  0.007231719"
  },
  {
    "objectID": "2023/pyrotecnia-1.html",
    "href": "2023/pyrotecnia-1.html",
    "title": "Pyrotecnia. Full luxury bayes con numpyro",
    "section": "",
    "text": "Ahora que va a salir el tan esperado Tutorial de numpyro creo que es momento de empezar una serie de post sobre lo que estoy aprendiendo con numpyro\nLo primero que se me ocurre decir de numpyro es que va muy bien, es muy rápido y es relativamente sencillo. Su velocidad se debe al uso de jax , una librería de google que ha tomado una parte pequeña de numpy y la ha optimizado, gran parte de su desempeño se debe a xla Acdelerated Linear Algebra. Gracias a xla se están entrenando algunos de los modelos de LLM.\nnumpyro es un dsl de programación probabilística, es decir, para hacer cosas bayesianas y ahora mismo es el estado del arte para este tema, al menos en lo concerniente a velocidad. Con numpyro he tardado en obtener las posterioris de un modelo sencillo con unas 20 mil filas en torno a 6 o 7 minutos mientras que con Stan se tardaba 1 hora. Así que con el uso de numpyro ya no puedo decir a algún amigo bayesiano aquello de “si, lo bayesiano está muy bien, pero se acaba el universo antes de que tengas tus 4 cadenas MCMC”."
  },
  {
    "objectID": "2023/pyrotecnia-1.html#introducción",
    "href": "2023/pyrotecnia-1.html#introducción",
    "title": "Pyrotecnia. Full luxury bayes con numpyro",
    "section": "",
    "text": "Ahora que va a salir el tan esperado Tutorial de numpyro creo que es momento de empezar una serie de post sobre lo que estoy aprendiendo con numpyro\nLo primero que se me ocurre decir de numpyro es que va muy bien, es muy rápido y es relativamente sencillo. Su velocidad se debe al uso de jax , una librería de google que ha tomado una parte pequeña de numpy y la ha optimizado, gran parte de su desempeño se debe a xla Acdelerated Linear Algebra. Gracias a xla se están entrenando algunos de los modelos de LLM.\nnumpyro es un dsl de programación probabilística, es decir, para hacer cosas bayesianas y ahora mismo es el estado del arte para este tema, al menos en lo concerniente a velocidad. Con numpyro he tardado en obtener las posterioris de un modelo sencillo con unas 20 mil filas en torno a 6 o 7 minutos mientras que con Stan se tardaba 1 hora. Así que con el uso de numpyro ya no puedo decir a algún amigo bayesiano aquello de “si, lo bayesiano está muy bien, pero se acaba el universo antes de que tengas tus 4 cadenas MCMC”."
  },
  {
    "objectID": "2023/pyrotecnia-1.html#ejemplo",
    "href": "2023/pyrotecnia-1.html#ejemplo",
    "title": "Pyrotecnia. Full luxury bayes con numpyro",
    "section": "Ejemplo",
    "text": "Ejemplo\nEn mi post de pluralista comentaba el siguiente ejemplo\nDiagrama causal:\n\nM: Número de hijos de la madre\nD: Número de hijos de la hija\nB1: Orden de nacimiento de la madre\nB2: Orden de nacimiento de la hija\nU: Variable no medida en los datos, que pudiera ser cosas como influencia del entorno social y económico dónde viven madre e hija, que influye en las decisión del número de hijos de ambas.\n\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(patchwork)\nlibrary(reticulate) # para poder convertir de R a python y a la inversa \n\ng &lt;- dagitty(\"dag{ \n  M -&gt; D ;\n  B2 -&gt; D;\n  B1 -&gt; M;\n  U -&gt; M;\n  U -&gt; D\n }\")\n\n\ncoords &lt;-  \n  list(\n  x = c(B1 = 1, M = 2,  U = 3.5, D = 5, B2 = 6),\n  y = c(B1 = 0, M = 0, U = 1, D = 0, B2 = 0)\n)\n\ncoordinates(g) &lt;- coords\n\nggdag(g) + \n  theme_void()\n\n\n\n\n\n\n\n\nY simulaba unos valores, reproduciendo el DAG anterior, añadiendo los valores de una variable de confusión que luego no puedo usar en el ajuste puesto que el ejemplo trata de que existe una variable de confusión no observada.\n\n\nset.seed(1908)\nN &lt;- 1000 # número de pares, 1000 madres y 1000 hijas\n\n\nU &lt;- rnorm(N,0,1) # Simulamos el confounder\n\n# orden de nacimiento y \nB1 &lt;- rbinom(N,size=1,prob=0.5)  # 50% de madres nacieeron en primer lugar\nM &lt;- rnorm( N , 2 * B1 + U )\n\nB2 &lt;- rbinom(N,size=1,prob=0.5) # 50% son las primogénitas\nD &lt;- rnorm( N , 2 * B2 + U + 0 * M )\n\nEn el post comentaba que si queremos ajustar el efecto global (o el directo ) de M sobre D , habría que condicionar por U , tal y como nos ha enseñado Pearl. Pero tanto Gelman como Richard McElreath nos ilustran en que si ajustamos el modelo generativo al completo no hay problema en condicionar por un collider (en los casos en que Pearl dice que no se puede) o incluso por una variable no observada. Condicionar en un marco bayesiano no es lo mismo que condicionar en el marco de modelos no bayesianos.\n\nadjustmentSets(g, exposure = \"M\", outcome = \"D\", effect = \"total\"  )\n#&gt; { U }\nadjustmentSets(g, exposure = \"M\", outcome = \"D\", effect = \"direct\"  )\n#&gt; { U }\n\n\n\nggdag_adjustment_set(g, exposure = \"M\", outcome = \"D\", effect = \"direct\")\n\n\n\n\n\n\n\n\nEn este orden de cosas, ¿como ajustamos este DAG con numpyro?\nLibrerías necesarias\n\n\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sn\n\n\nnumpyro.set_platform(\"cpu\")\nnumpyro.set_host_device_count(4)\n\njax.local_device_count()\n#&gt; 4\n\nUso reticulate para pasar las variables simuladas a pyhon.\n\ndat_list = dict(\n    id = np.array(range(0,1000)),\n    D  = np.array(r.D), # con r. accedo a lo que está en el enviromment de R , \n    M  = np.array(r.M), \n    B1 = np.array(r.B1),\n    B2 = np.array(r.B2)\n)\n\n# vemos los 4 primeros valores de D por ejemplo \n\n\ndat_list['D'][0:4]\n#&gt; array([ 2.56309969,  1.46316491,  0.68597284, -0.24294441])\n\nPues siguiendo como ajustamos el DAG con ulam lo hacemos ahora con numpyro\nEs relativamente faćil, lo único qeu hay que tener cuidado porque la variable U no observada la simulamos dentro del modelo y que tiene que tener de dimensión el número de datos que tenemos, es decir, 1000.\nLuego en la parte de\nmu1 = a1 + b * B1 + k * U[id]\nmu2 = a2 + b * B2 + m * M + k * U[id]\nvemos que metemos el id que no es más que un índice para saber que observación se trata . Es decir, tener un dato perdido ( o todos en el caso de U ) se reduce a estimar tantos parámetros como datos perdidos tengo. Es un pensamiento interesante, un dato perdido se puede ver como un parámetro que hay que estimar.\nY el resto de la definición que tenemos en el modelo es igual de sencilla que cuándo lo hacía en el post original, al fin y al cabo esto es sólo sintaxis.\n\n\n\ndef model(id,D, M, B1, B2):\n    \n    # variable no observada\n    U = numpyro.sample(\"U\", dist.Normal(0, 1), sample_shape= D.shape) \n   \n      # Prior coeficientes\n    a1 = numpyro.sample(\"a1\", dist.Normal(0, 0.5))\n    a2 = numpyro.sample(\"a2\", dist.Normal(0, 0.5))\n    \n    m = numpyro.sample(\"m\", dist.Normal(0, 0.5))\n    b1 = numpyro.sample(\"b1\", dist.Normal(0, 0.5))\n    b2 = numpyro.sample(\"b2\", dist.Normal(0, 0.5))\n    p = numpyro.sample(\"p\", dist.Beta(2, 2))\n\n    k = numpyro.sample(\"k\", dist.Exponential(1))\n    sigma1 = numpyro.sample(\"sigma1\", dist.Exponential(1))\n    sigma2 = numpyro.sample(\"sigma2\", dist.Exponential(1))\n    \n    # verosimilitud\n    \n    B1_obs = numpyro.sample(\"B1_obs\", dist.Bernoulli(probs = p), obs = B1  )\n    B2_obs = numpyro.sample(\"B2_obs\", dist.Bernoulli(probs = p), obs = B2  )\n\n    \n    #  transformed parameters\n    mu1 = a1 + b1 * B1 + k * U[id]\n    mu2 = a2 + b2 * B2 + m * M + k * U[id]\n    \n    M_obs = numpyro.sample(\"M_obs\", dist.Normal(mu1, sigma1), obs = M)\n    D_obs = numpyro.sample(\"D_obs\", dist.Normal(mu2, sigma2), obs = D)\n\ny nada, vamos a ajustar.\n\n# Ajusto\n\n\nmcmc = MCMC(NUTS(model), num_warmup=500, num_samples=2500, num_chains=4)\n# fijaros qeu en dat_list en ningún momento está la variable no observada U, pero \n# en el modelo si se tiene en cuenta\nmcmc.run(jax.random.PRNGKey(0), **dat_list)\n#&gt; \n  0%|          | 0/3000 [00:00&lt;?, ?it/s]\nCompiling.. :   0%|          | 0/3000 [00:00&lt;?, ?it/s]\n#&gt; \n  0%|          | 0/3000 [00:00&lt;?, ?it/s]\u001b[A\n#&gt; \nCompiling.. :   0%|          | 0/3000 [00:00&lt;?, ?it/s]\u001b[A\n#&gt; \n#&gt; \n  0%|          | 0/3000 [00:00&lt;?, ?it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \nCompiling.. :   0%|          | 0/3000 [00:00&lt;?, ?it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \n  0%|          | 0/3000 [00:00&lt;?, ?it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nCompiling.. :   0%|          | 0/3000 [00:00&lt;?, ?it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:   0%|          | 0/3000 [00:06&lt;?, ?it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:   0%|          | 0/3000 [00:06&lt;?, ?it/s]\u001b[A\u001b[A\nRunning chain 0:   0%|          | 0/3000 [00:06&lt;?, ?it/s]\n#&gt; \nRunning chain 1:   0%|          | 0/3000 [00:06&lt;?, ?it/s]\u001b[A\n#&gt; \nRunning chain 1:   5%|5         | 150/3000 [00:07&lt;00:14, 197.29it/s]\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:   5%|5         | 150/3000 [00:07&lt;00:14, 192.17it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:   5%|5         | 150/3000 [00:07&lt;00:16, 175.63it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  10%|#         | 300/3000 [00:07&lt;00:07, 382.57it/s]\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  10%|#         | 300/3000 [00:07&lt;00:07, 373.19it/s]\u001b[A\u001b[A\u001b[A\nRunning chain 0:   5%|5         | 150/3000 [00:07&lt;00:17, 162.13it/s]\n#&gt; \n#&gt; \nRunning chain 2:  10%|#         | 300/3000 [00:07&lt;00:08, 335.26it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  15%|#5        | 450/3000 [00:07&lt;00:04, 531.64it/s]\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  15%|#5        | 450/3000 [00:07&lt;00:04, 537.20it/s]\u001b[A\u001b[A\u001b[A\nRunning chain 0:  10%|#         | 300/3000 [00:07&lt;00:08, 320.42it/s]\n#&gt; \n#&gt; \nRunning chain 2:  15%|#5        | 450/3000 [00:08&lt;00:05, 478.20it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  20%|##        | 600/3000 [00:08&lt;00:03, 664.78it/s]\u001b[A\u001b[A\u001b[A\nRunning chain 0:  15%|#5        | 450/3000 [00:08&lt;00:05, 482.19it/s]\n#&gt; \nRunning chain 1:  20%|##        | 600/3000 [00:08&lt;00:04, 591.33it/s]\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  20%|##        | 600/3000 [00:08&lt;00:03, 632.06it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  25%|##5       | 750/3000 [00:08&lt;00:02, 822.05it/s]\u001b[A\u001b[A\u001b[A\nRunning chain 0:  20%|##        | 600/3000 [00:08&lt;00:04, 599.32it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  30%|###       | 900/3000 [00:08&lt;00:02, 934.54it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  25%|##5       | 750/3000 [00:08&lt;00:03, 645.28it/s]\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  30%|###       | 900/3000 [00:08&lt;00:02, 872.87it/s]\u001b[A\u001b[A\nRunning chain 0:  25%|##5       | 750/3000 [00:08&lt;00:03, 679.54it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  35%|###5      | 1050/3000 [00:08&lt;00:01, 1020.07it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  35%|###5      | 1050/3000 [00:08&lt;00:02, 966.75it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  30%|###       | 900/3000 [00:08&lt;00:03, 691.37it/s]\u001b[A\nRunning chain 0:  30%|###       | 900/3000 [00:08&lt;00:02, 753.08it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  45%|####5     | 1350/3000 [00:08&lt;00:01, 1168.10it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  40%|####      | 1200/3000 [00:08&lt;00:01, 944.09it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  35%|###5      | 1050/3000 [00:08&lt;00:02, 725.91it/s]\u001b[A\nRunning chain 0:  35%|###5      | 1050/3000 [00:08&lt;00:02, 799.45it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  50%|#####     | 1500/3000 [00:08&lt;00:01, 1126.99it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  45%|####5     | 1350/3000 [00:08&lt;00:01, 967.83it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  40%|####      | 1200/3000 [00:08&lt;00:02, 747.38it/s]\u001b[A\nRunning chain 0:  40%|####      | 1200/3000 [00:08&lt;00:02, 805.48it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  60%|######    | 1800/3000 [00:08&lt;00:00, 1271.76it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  55%|#####5    | 1650/3000 [00:08&lt;00:01, 1138.21it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  45%|####5     | 1350/3000 [00:09&lt;00:02, 771.97it/s]\u001b[A\nRunning chain 0:  45%|####5     | 1350/3000 [00:09&lt;00:01, 829.49it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  65%|######5   | 1950/3000 [00:09&lt;00:00, 1273.00it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  60%|######    | 1800/3000 [00:09&lt;00:01, 1187.49it/s]\u001b[A\u001b[A\nRunning chain 0:  50%|#####     | 1500/3000 [00:09&lt;00:01, 830.23it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  70%|#######   | 2100/3000 [00:09&lt;00:00, 1144.14it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  50%|#####     | 1500/3000 [00:09&lt;00:01, 756.05it/s]\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  65%|######5   | 1950/3000 [00:09&lt;00:00, 1121.13it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  75%|#######5  | 2250/3000 [00:09&lt;00:00, 1130.74it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  70%|#######   | 2100/3000 [00:09&lt;00:00, 1137.82it/s]\u001b[A\u001b[A\nRunning chain 0:  55%|#####5    | 1650/3000 [00:09&lt;00:01, 851.76it/s]\n#&gt; \nRunning chain 1:  55%|#####5    | 1650/3000 [00:09&lt;00:01, 758.15it/s]\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  80%|########  | 2400/3000 [00:09&lt;00:00, 1178.24it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  75%|#######5  | 2250/3000 [00:09&lt;00:00, 1059.20it/s]\u001b[A\u001b[A\nRunning chain 0:  60%|######    | 1800/3000 [00:09&lt;00:01, 861.45it/s]\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  85%|########5 | 2550/3000 [00:09&lt;00:00, 1244.56it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  60%|######    | 1800/3000 [00:09&lt;00:01, 774.94it/s]\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  80%|########  | 2400/3000 [00:09&lt;00:00, 1140.27it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  90%|######### | 2700/3000 [00:09&lt;00:00, 1282.87it/s]\u001b[A\u001b[A\u001b[A\nRunning chain 0:  65%|######5   | 1950/3000 [00:09&lt;00:01, 819.33it/s]\n#&gt; \n#&gt; \nRunning chain 2:  85%|########5 | 2550/3000 [00:09&lt;00:00, 1175.97it/s]\u001b[A\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3:  95%|#########5| 2850/3000 [00:09&lt;00:00, 1317.20it/s]\u001b[A\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  65%|######5   | 1950/3000 [00:09&lt;00:01, 770.49it/s]\u001b[A\n#&gt; \n#&gt; \n#&gt; \nRunning chain 3: 100%|##########| 3000/3000 [00:09&lt;00:00, 1230.50it/s]\u001b[A\u001b[A\u001b[A\nRunning chain 0:  70%|#######   | 2100/3000 [00:09&lt;00:01, 818.74it/s]\n#&gt; \n#&gt; \nRunning chain 2:  90%|######### | 2700/3000 [00:09&lt;00:00, 1074.69it/s]\u001b[A\u001b[A\n#&gt; \nRunning chain 1:  70%|#######   | 2100/3000 [00:10&lt;00:01, 778.59it/s]\u001b[A\n#&gt; \n#&gt; \nRunning chain 2:  95%|#########5| 2850/3000 [00:10&lt;00:00, 1147.26it/s]\u001b[A\u001b[A\nRunning chain 0:  75%|#######5  | 2250/3000 [00:10&lt;00:00, 852.80it/s]\n#&gt; \nRunning chain 1:  75%|#######5  | 2250/3000 [00:10&lt;00:00, 816.51it/s]\u001b[A\n#&gt; \n#&gt; \nRunning chain 2: 100%|##########| 3000/3000 [00:10&lt;00:00, 1150.86it/s]\u001b[A\u001b[A\nRunning chain 0:  80%|########  | 2400/3000 [00:10&lt;00:00, 847.21it/s]\n#&gt; \nRunning chain 1:  80%|########  | 2400/3000 [00:10&lt;00:00, 809.55it/s]\u001b[A\nRunning chain 0:  85%|########5 | 2550/3000 [00:10&lt;00:00, 910.19it/s]\n#&gt; \nRunning chain 1:  85%|########5 | 2550/3000 [00:10&lt;00:00, 830.15it/s]\u001b[A\nRunning chain 0:  90%|######### | 2700/3000 [00:10&lt;00:00, 988.98it/s]\nRunning chain 0:  95%|#########5| 2850/3000 [00:10&lt;00:00, 1005.88it/s]\n#&gt; \nRunning chain 1:  90%|######### | 2700/3000 [00:10&lt;00:00, 859.81it/s]\u001b[A\nRunning chain 0: 100%|##########| 3000/3000 [00:10&lt;00:00, 1062.46it/s]\n#&gt; \nRunning chain 1:  95%|#########5| 2850/3000 [00:10&lt;00:00, 881.31it/s]\u001b[A\n#&gt; \nRunning chain 1: 100%|##########| 3000/3000 [00:11&lt;00:00, 878.21it/s]\u001b[A\nRunning chain 0: 100%|##########| 3000/3000 [00:11&lt;00:00, 272.13it/s] \n#&gt; \nRunning chain 1: 100%|##########| 3000/3000 [00:11&lt;00:00, 272.14it/s]\n#&gt; \nRunning chain 2: 100%|##########| 3000/3000 [00:11&lt;00:00, 272.14it/s] \n#&gt; \nRunning chain 3: 100%|##########| 3000/3000 [00:11&lt;00:00, 272.14it/s]\n\nY vemos que lo que en Stan tardaba unos 18 segundos, en numpyro se queda en unos 10. No parece mucha mejora, pero cuando aumenta el número de datos y la complejidad del modelo se nota y mucho.\nBien, veamos el resumen del modelo . El resumen muestra también los valores estimados para la variable no observada\n\n# si pongo exclude_deterministic=True no veo las estimaciones de mu1 y mu2. \nmcmc.print_summary(exclude_deterministic=False)\n#&gt; \n#&gt;                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n#&gt;       U[0]      0.50      0.60      0.49     -0.48      1.46  12860.51      1.00\n#&gt;       U[1]      0.22      0.58      0.22     -0.76      1.13  12871.14      1.00\n#&gt;       U[2]     -0.58      0.59     -0.57     -1.58      0.35  13943.01      1.00\n#&gt;       U[3]     -0.34      0.58     -0.35     -1.33      0.58  13443.15      1.00\n#&gt;       U[4]     -0.51      0.59     -0.52     -1.42      0.49  11688.25      1.00\n#&gt;       U[5]     -0.75      0.59     -0.75     -1.72      0.24  15168.33      1.00\n#&gt;       U[6]      1.11      0.60      1.13      0.13      2.12   8250.75      1.00\n#&gt;       U[7]     -0.44      0.58     -0.43     -1.33      0.57  12422.57      1.00\n#&gt;       U[8]      0.74      0.59      0.73     -0.19      1.71  10471.35      1.00\n#&gt;       U[9]      0.12      0.58      0.12     -0.85      1.06  12241.08      1.00\n#&gt;      U[10]     -0.23      0.59     -0.23     -1.18      0.77   9734.89      1.00\n#&gt;      U[11]      1.28      0.58      1.27      0.35      2.25  12928.53      1.00\n#&gt;      U[12]     -0.44      0.58     -0.44     -1.41      0.48  15420.94      1.00\n#&gt;      U[13]      1.33      0.59      1.33      0.38      2.32  11126.21      1.00\n#&gt;      U[14]      0.92      0.59      0.91     -0.07      1.88  13056.45      1.00\n#&gt;      U[15]     -0.06      0.60     -0.06     -1.09      0.88  10528.39      1.00\n#&gt;      U[16]      0.81      0.59      0.81     -0.15      1.79  12663.62      1.00\n#&gt;      U[17]      0.18      0.58      0.18     -0.77      1.13  15376.88      1.00\n#&gt;      U[18]     -0.62      0.59     -0.62     -1.56      0.36  12924.49      1.00\n#&gt;      U[19]      0.88      0.58      0.89     -0.13      1.78  14485.13      1.00\n#&gt;      U[20]     -1.08      0.60     -1.09     -2.08     -0.13   9141.35      1.00\n#&gt;      U[21]      0.39      0.60      0.39     -0.55      1.41  12648.56      1.00\n#&gt;      U[22]      0.59      0.58      0.59     -0.35      1.56  13053.85      1.00\n#&gt;      U[23]     -1.46      0.60     -1.46     -2.43     -0.45   9105.82      1.00\n#&gt;      U[24]     -0.77      0.59     -0.77     -1.72      0.19  15243.37      1.00\n#&gt;      U[25]     -0.49      0.59     -0.50     -1.47      0.47  11294.14      1.00\n#&gt;      U[26]     -1.31      0.58     -1.31     -2.23     -0.35  14659.16      1.00\n#&gt;      U[27]     -1.54      0.60     -1.54     -2.53     -0.56   7438.22      1.00\n#&gt;      U[28]      0.28      0.59      0.28     -0.67      1.28  12669.26      1.00\n#&gt;      U[29]     -1.25      0.60     -1.25     -2.27     -0.32  10963.02      1.00\n#&gt;      U[30]      0.15      0.59      0.15     -0.83      1.10  10448.26      1.00\n#&gt;      U[31]      1.17      0.60      1.17      0.13      2.10   8396.85      1.00\n#&gt;      U[32]      0.49      0.58      0.49     -0.46      1.42  14968.57      1.00\n#&gt;      U[33]     -0.30      0.59     -0.30     -1.27      0.64  12644.12      1.00\n#&gt;      U[34]      0.01      0.59      0.01     -0.96      0.98  11198.40      1.00\n#&gt;      U[35]     -1.31      0.58     -1.32     -2.31     -0.39  10300.60      1.00\n#&gt;      U[36]     -0.49      0.59     -0.50     -1.43      0.48  12618.59      1.00\n#&gt;      U[37]     -0.11      0.60     -0.11     -1.10      0.87  11842.77      1.00\n#&gt;      U[38]      1.20      0.59      1.20      0.24      2.17  10367.93      1.00\n#&gt;      U[39]     -0.47      0.59     -0.47     -1.45      0.50  15461.71      1.00\n#&gt;      U[40]     -0.08      0.59     -0.07     -1.08      0.87  10930.57      1.00\n#&gt;      U[41]     -0.63      0.58     -0.63     -1.55      0.38  12890.26      1.00\n#&gt;      U[42]     -0.61      0.58     -0.62     -1.54      0.36  11702.50      1.00\n#&gt;      U[43]      0.43      0.59      0.43     -0.52      1.41  13690.63      1.00\n#&gt;      U[44]      0.49      0.59      0.49     -0.47      1.45  11238.50      1.00\n#&gt;      U[45]     -0.03      0.59     -0.04     -0.97      0.96  11840.42      1.00\n#&gt;      U[46]     -1.69      0.59     -1.70     -2.69     -0.77  14033.04      1.00\n#&gt;      U[47]      0.10      0.59      0.11     -0.88      1.05  11824.14      1.00\n#&gt;      U[48]     -0.18      0.59     -0.19     -1.15      0.77   8511.45      1.00\n#&gt;      U[49]      0.37      0.60      0.37     -0.59      1.37  12087.98      1.00\n#&gt;      U[50]     -0.56      0.58     -0.56     -1.52      0.37  13209.47      1.00\n#&gt;      U[51]      0.06      0.58      0.06     -0.86      1.03  12666.16      1.00\n#&gt;      U[52]     -0.10      0.59     -0.09     -1.07      0.84  12525.88      1.00\n#&gt;      U[53]      0.13      0.59      0.14     -0.80      1.13  11189.64      1.00\n#&gt;      U[54]      0.05      0.59      0.06     -0.94      0.99  12382.78      1.00\n#&gt;      U[55]      0.27      0.59      0.27     -0.69      1.25  10025.66      1.00\n#&gt;      U[56]      0.14      0.59      0.14     -0.83      1.07  14000.12      1.00\n#&gt;      U[57]      0.14      0.59      0.15     -0.76      1.16  10683.09      1.00\n#&gt;      U[58]     -0.36      0.58     -0.35     -1.35      0.55  10952.31      1.00\n#&gt;      U[59]     -0.30      0.58     -0.30     -1.21      0.70  13582.04      1.00\n#&gt;      U[60]     -0.02      0.59     -0.02     -1.01      0.92  12206.84      1.00\n#&gt;      U[61]     -0.90      0.60     -0.90     -1.85      0.09  13165.60      1.00\n#&gt;      U[62]      0.58      0.59      0.58     -0.42      1.51  11896.14      1.00\n#&gt;      U[63]      0.04      0.60      0.05     -0.94      1.02  10098.42      1.00\n#&gt;      U[64]      0.09      0.58      0.08     -0.88      1.04  14011.39      1.00\n#&gt;      U[65]      0.03      0.59      0.04     -0.93      0.99  11286.75      1.00\n#&gt;      U[66]      1.12      0.59      1.13      0.13      2.07  11269.78      1.00\n#&gt;      U[67]     -1.26      0.58     -1.26     -2.21     -0.32  12066.69      1.00\n#&gt;      U[68]      0.35      0.59      0.35     -0.66      1.30  13870.54      1.00\n#&gt;      U[69]      1.19      0.58      1.19      0.28      2.17  12341.05      1.00\n#&gt;      U[70]     -0.48      0.59     -0.49     -1.48      0.45  12404.10      1.00\n#&gt;      U[71]     -0.71      0.57     -0.71     -1.63      0.24  11986.85      1.00\n#&gt;      U[72]     -0.89      0.58     -0.89     -1.81      0.07  13666.83      1.00\n#&gt;      U[73]      1.23      0.59      1.23      0.25      2.17  12336.82      1.00\n#&gt;      U[74]     -0.18      0.59     -0.18     -1.12      0.81  12545.44      1.00\n#&gt;      U[75]      1.26      0.58      1.26      0.30      2.19  12844.00      1.00\n#&gt;      U[76]     -0.20      0.59     -0.20     -1.17      0.76  11698.53      1.00\n#&gt;      U[77]     -0.21      0.58     -0.21     -1.16      0.74  12358.45      1.00\n#&gt;      U[78]      0.45      0.60      0.45     -0.52      1.45  12103.07      1.00\n#&gt;      U[79]      1.15      0.59      1.15      0.22      2.14   9660.98      1.00\n#&gt;      U[80]     -1.14      0.59     -1.14     -2.10     -0.17  13378.12      1.00\n#&gt;      U[81]     -1.22      0.58     -1.23     -2.17     -0.27  12092.24      1.00\n#&gt;      U[82]      0.32      0.59      0.32     -0.64      1.29  13847.49      1.00\n#&gt;      U[83]     -2.13      0.58     -2.13     -3.09     -1.20  11433.13      1.00\n#&gt;      U[84]     -0.39      0.58     -0.39     -1.39      0.52  13003.93      1.00\n#&gt;      U[85]      0.17      0.61      0.18     -0.83      1.16  12147.86      1.00\n#&gt;      U[86]     -0.70      0.58     -0.69     -1.66      0.26  13247.53      1.00\n#&gt;      U[87]     -0.88      0.58     -0.88     -1.89      0.02  13375.80      1.00\n#&gt;      U[88]      0.64      0.60      0.64     -0.33      1.63  14753.49      1.00\n#&gt;      U[89]     -1.10      0.59     -1.10     -2.06     -0.14  13158.03      1.00\n#&gt;      U[90]      0.88      0.59      0.88     -0.10      1.82  13778.42      1.00\n#&gt;      U[91]     -1.01      0.59     -1.01     -1.98     -0.05  11237.97      1.00\n#&gt;      U[92]      0.01      0.59      0.02     -0.98      0.96  13643.96      1.00\n#&gt;      U[93]     -0.69      0.60     -0.69     -1.67      0.30  13084.90      1.00\n#&gt;      U[94]     -0.00      0.59     -0.00     -1.01      0.93  12732.53      1.00\n#&gt;      U[95]      0.81      0.57      0.81     -0.11      1.79  12694.11      1.00\n#&gt;      U[96]     -0.33      0.59     -0.33     -1.31      0.62  15692.59      1.00\n#&gt;      U[97]     -1.09      0.59     -1.09     -2.07     -0.10  12472.36      1.00\n#&gt;      U[98]     -1.22      0.59     -1.23     -2.20     -0.28   8838.72      1.00\n#&gt;      U[99]     -1.11      0.58     -1.11     -2.10     -0.17  10466.06      1.00\n#&gt;     U[100]     -0.83      0.59     -0.82     -1.80      0.15  11928.80      1.00\n#&gt;     U[101]     -0.49      0.58     -0.49     -1.43      0.46  14035.03      1.00\n#&gt;     U[102]     -0.66      0.59     -0.66     -1.64      0.28  14393.13      1.00\n#&gt;     U[103]      0.19      0.59      0.18     -0.82      1.14   7924.52      1.00\n#&gt;     U[104]      0.89      0.57      0.89     -0.05      1.82  11962.30      1.00\n#&gt;     U[105]      0.73      0.58      0.72     -0.16      1.74  12317.05      1.00\n#&gt;     U[106]     -0.57      0.58     -0.56     -1.51      0.40  11928.89      1.00\n#&gt;     U[107]     -0.23      0.59     -0.23     -1.21      0.75  11338.78      1.00\n#&gt;     U[108]      0.04      0.58      0.04     -0.95      0.97  13271.67      1.00\n#&gt;     U[109]      0.40      0.58      0.39     -0.52      1.35  10169.48      1.00\n#&gt;     U[110]      0.51      0.57      0.51     -0.47      1.40  13637.87      1.00\n#&gt;     U[111]      1.12      0.59      1.12      0.19      2.12  11497.25      1.00\n#&gt;     U[112]     -0.11      0.58     -0.11     -1.06      0.84  13513.47      1.00\n#&gt;     U[113]      1.31      0.59      1.31      0.37      2.30   9924.64      1.00\n#&gt;     U[114]     -0.81      0.58     -0.81     -1.78      0.15  12868.99      1.00\n#&gt;     U[115]     -1.03      0.58     -1.03     -1.98     -0.07  14744.68      1.00\n#&gt;     U[116]      1.32      0.58      1.31      0.38      2.29  12470.25      1.00\n#&gt;     U[117]      0.88      0.59      0.89     -0.12      1.83  12804.00      1.00\n#&gt;     U[118]     -1.09      0.59     -1.09     -2.01     -0.09  10777.46      1.00\n#&gt;     U[119]      0.90      0.60      0.90     -0.05      1.91   9138.98      1.00\n#&gt;     U[120]      0.74      0.59      0.74     -0.27      1.67  10310.43      1.00\n#&gt;     U[121]     -0.18      0.58     -0.18     -1.18      0.73  11969.19      1.00\n#&gt;     U[122]      1.94      0.59      1.95      1.00      2.93  11160.77      1.00\n#&gt;     U[123]      1.50      0.60      1.50      0.49      2.44  10927.63      1.00\n#&gt;     U[124]      1.16      0.58      1.17      0.20      2.13  12306.13      1.00\n#&gt;     U[125]     -0.38      0.59     -0.38     -1.33      0.61  10683.76      1.00\n#&gt;     U[126]     -0.34      0.59     -0.34     -1.36      0.60  14485.48      1.00\n#&gt;     U[127]      0.93      0.59      0.93     -0.06      1.88  12883.29      1.00\n#&gt;     U[128]     -0.24      0.59     -0.24     -1.19      0.74  12373.53      1.00\n#&gt;     U[129]      0.11      0.58      0.11     -0.85      1.06  15170.94      1.00\n#&gt;     U[130]     -0.90      0.59     -0.91     -1.95     -0.01  15306.65      1.00\n#&gt;     U[131]      0.48      0.59      0.48     -0.51      1.41  13110.90      1.00\n#&gt;     U[132]     -0.19      0.58     -0.19     -1.17      0.77  13913.09      1.00\n#&gt;     U[133]     -1.25      0.59     -1.24     -2.21     -0.27  13904.50      1.00\n#&gt;     U[134]      0.15      0.59      0.15     -0.84      1.11  10850.29      1.00\n#&gt;     U[135]     -0.67      0.60     -0.67     -1.63      0.34  14721.89      1.00\n#&gt;     U[136]      0.38      0.59      0.38     -0.66      1.30  10986.46      1.00\n#&gt;     U[137]     -0.89      0.59     -0.89     -1.86      0.07  10068.80      1.00\n#&gt;     U[138]      0.47      0.58      0.47     -0.47      1.45  13443.37      1.00\n#&gt;     U[139]     -0.90      0.60     -0.91     -1.92      0.04  13741.88      1.00\n#&gt;     U[140]      1.10      0.59      1.10      0.14      2.08  11619.80      1.00\n#&gt;     U[141]      1.66      0.59      1.66      0.71      2.66  11570.02      1.00\n#&gt;     U[142]      0.27      0.60      0.27     -0.73      1.23  13717.42      1.00\n#&gt;     U[143]     -1.63      0.60     -1.64     -2.61     -0.67   9427.97      1.00\n#&gt;     U[144]     -0.04      0.59     -0.03     -0.99      0.96  11811.20      1.00\n#&gt;     U[145]     -0.29      0.60     -0.29     -1.20      0.76  13313.65      1.00\n#&gt;     U[146]     -0.92      0.58     -0.93     -1.86      0.06  12563.63      1.00\n#&gt;     U[147]      0.12      0.59      0.12     -0.86      1.07  13182.98      1.00\n#&gt;     U[148]     -0.39      0.58     -0.39     -1.38      0.53  13617.31      1.00\n#&gt;     U[149]     -0.90      0.58     -0.90     -1.83      0.05  14978.40      1.00\n#&gt;     U[150]      0.05      0.59      0.05     -0.91      1.03  12645.81      1.00\n#&gt;     U[151]     -1.28      0.59     -1.28     -2.25     -0.33  13428.76      1.00\n#&gt;     U[152]      0.06      0.59      0.04     -0.89      1.06  11111.32      1.00\n#&gt;     U[153]      1.22      0.58      1.23      0.27      2.18  12885.03      1.00\n#&gt;     U[154]     -0.17      0.59     -0.18     -1.15      0.80  11530.06      1.00\n#&gt;     U[155]      0.37      0.58      0.36     -0.60      1.30  13530.66      1.00\n#&gt;     U[156]      2.09      0.58      2.09      1.15      3.05  13852.77      1.00\n#&gt;     U[157]      0.49      0.60      0.48     -0.48      1.50   9804.98      1.00\n#&gt;     U[158]      1.15      0.57      1.15      0.22      2.10  11514.93      1.00\n#&gt;     U[159]     -0.35      0.59     -0.34     -1.34      0.59  13028.59      1.00\n#&gt;     U[160]      1.36      0.59      1.36      0.36      2.29  10520.79      1.00\n#&gt;     U[161]      0.30      0.59      0.30     -0.62      1.30  11782.84      1.00\n#&gt;     U[162]     -0.85      0.59     -0.85     -1.79      0.13  14349.73      1.00\n#&gt;     U[163]      0.40      0.59      0.40     -0.55      1.39  12865.53      1.00\n#&gt;     U[164]      0.95      0.58      0.95     -0.01      1.88  14255.03      1.00\n#&gt;     U[165]     -0.30      0.58     -0.30     -1.22      0.70  12373.07      1.00\n#&gt;     U[166]      0.15      0.58      0.15     -0.80      1.12  14312.39      1.00\n#&gt;     U[167]      0.26      0.60      0.26     -0.73      1.25  12285.91      1.00\n#&gt;     U[168]     -0.39      0.58     -0.39     -1.30      0.59  12002.92      1.00\n#&gt;     U[169]     -0.07      0.57     -0.07     -1.03      0.87  15105.86      1.00\n#&gt;     U[170]     -0.05      0.59     -0.04     -1.00      0.93   8541.24      1.00\n#&gt;     U[171]      1.08      0.59      1.09      0.10      2.06  13168.95      1.00\n#&gt;     U[172]      0.40      0.58      0.40     -0.59      1.33  10106.27      1.00\n#&gt;     U[173]     -0.25      0.58     -0.25     -1.18      0.73  14602.68      1.00\n#&gt;     U[174]     -0.11      0.58     -0.11     -1.10      0.80  11096.23      1.00\n#&gt;     U[175]      0.24      0.58      0.25     -0.68      1.21  11934.20      1.00\n#&gt;     U[176]     -0.58      0.59     -0.58     -1.55      0.37  13756.38      1.00\n#&gt;     U[177]      1.09      0.59      1.09      0.17      2.10  11548.47      1.00\n#&gt;     U[178]     -0.58      0.59     -0.58     -1.54      0.42  12597.36      1.00\n#&gt;     U[179]     -0.01      0.59     -0.01     -0.98      0.97  13612.65      1.00\n#&gt;     U[180]      1.14      0.60      1.14      0.18      2.15  12123.83      1.00\n#&gt;     U[181]      0.93      0.59      0.93     -0.05      1.88  11352.85      1.00\n#&gt;     U[182]     -0.00      0.59     -0.00     -0.96      0.96  12502.24      1.00\n#&gt;     U[183]     -0.16      0.60     -0.16     -1.11      0.86  10178.72      1.00\n#&gt;     U[184]     -0.37      0.58     -0.37     -1.35      0.54  12734.02      1.00\n#&gt;     U[185]      0.65      0.58      0.65     -0.27      1.64  12631.99      1.00\n#&gt;     U[186]     -1.47      0.59     -1.46     -2.44     -0.50  11059.01      1.00\n#&gt;     U[187]     -0.67      0.59     -0.66     -1.67      0.27   9788.89      1.00\n#&gt;     U[188]     -0.33      0.59     -0.34     -1.31      0.61  12356.98      1.00\n#&gt;     U[189]      0.63      0.59      0.64     -0.34      1.58  13301.83      1.00\n#&gt;     U[190]     -0.24      0.59     -0.24     -1.20      0.74  10879.36      1.00\n#&gt;     U[191]      0.13      0.58      0.13     -0.83      1.07  13282.46      1.00\n#&gt;     U[192]     -0.20      0.59     -0.20     -1.21      0.75   9960.53      1.00\n#&gt;     U[193]      0.28      0.59      0.28     -0.65      1.27  13071.30      1.00\n#&gt;     U[194]     -0.12      0.59     -0.12     -1.04      0.90  14620.62      1.00\n#&gt;     U[195]      1.17      0.59      1.17      0.18      2.11   9312.39      1.00\n#&gt;     U[196]      0.26      0.59      0.26     -0.72      1.22  11501.06      1.00\n#&gt;     U[197]     -0.84      0.59     -0.84     -1.81      0.12  12769.89      1.00\n#&gt;     U[198]     -0.49      0.59     -0.49     -1.41      0.52  13427.34      1.00\n#&gt;     U[199]     -0.70      0.59     -0.69     -1.68      0.24  13431.23      1.00\n#&gt;     U[200]      0.38      0.59      0.38     -0.59      1.35  11132.34      1.00\n#&gt;     U[201]     -0.03      0.58     -0.04     -0.96      0.94  12539.79      1.00\n#&gt;     U[202]     -0.16      0.58     -0.17     -1.07      0.85  13512.13      1.00\n#&gt;     U[203]     -0.14      0.59     -0.14     -1.10      0.82  12450.76      1.00\n#&gt;     U[204]      0.13      0.58      0.14     -0.85      1.05  12175.55      1.00\n#&gt;     U[205]     -1.35      0.59     -1.35     -2.31     -0.38  12687.69      1.00\n#&gt;     U[206]     -0.03      0.59     -0.03     -0.96      0.94  12114.60      1.00\n#&gt;     U[207]     -0.78      0.59     -0.79     -1.78      0.13  12476.86      1.00\n#&gt;     U[208]      0.04      0.59      0.04     -0.84      1.08  14123.41      1.00\n#&gt;     U[209]     -0.25      0.58     -0.25     -1.23      0.68  14388.18      1.00\n#&gt;     U[210]      0.03      0.60      0.02     -0.95      1.02   8139.46      1.00\n#&gt;     U[211]      0.35      0.59      0.35     -0.62      1.34  11449.96      1.00\n#&gt;     U[212]      0.07      0.59      0.07     -0.85      1.07  10400.93      1.00\n#&gt;     U[213]      1.96      0.59      1.96      0.96      2.91  11961.33      1.00\n#&gt;     U[214]      0.12      0.58      0.12     -0.85      1.03  13457.63      1.00\n#&gt;     U[215]      0.70      0.60      0.71     -0.28      1.70  11104.02      1.00\n#&gt;     U[216]      0.09      0.58      0.10     -0.90      1.03  12337.94      1.00\n#&gt;     U[217]     -1.03      0.57     -1.04     -1.94     -0.06  12580.81      1.00\n#&gt;     U[218]      1.31      0.58      1.31      0.33      2.24  11447.63      1.00\n#&gt;     U[219]     -0.83      0.60     -0.84     -1.80      0.14  13151.71      1.00\n#&gt;     U[220]     -0.84      0.59     -0.84     -1.78      0.16  13722.77      1.00\n#&gt;     U[221]      0.20      0.60      0.21     -0.79      1.18  13758.94      1.00\n#&gt;     U[222]     -0.77      0.58     -0.78     -1.69      0.21  10855.41      1.00\n#&gt;     U[223]     -0.70      0.59     -0.70     -1.67      0.27  12255.26      1.00\n#&gt;     U[224]     -0.33      0.59     -0.33     -1.27      0.63  12311.16      1.00\n#&gt;     U[225]     -0.28      0.58     -0.27     -1.26      0.62  12315.86      1.00\n#&gt;     U[226]      0.28      0.58      0.27     -0.67      1.24  14780.88      1.00\n#&gt;     U[227]     -0.10      0.59     -0.09     -1.04      0.90   9898.39      1.00\n#&gt;     U[228]      1.06      0.59      1.06      0.08      2.03  11553.76      1.00\n#&gt;     U[229]      0.37      0.59      0.37     -0.61      1.34  10324.40      1.00\n#&gt;     U[230]     -0.34      0.59     -0.35     -1.28      0.63  12025.01      1.00\n#&gt;     U[231]     -0.37      0.59     -0.38     -1.37      0.53  13633.08      1.00\n#&gt;     U[232]     -0.16      0.58     -0.16     -1.10      0.80  12826.48      1.00\n#&gt;     U[233]      0.90      0.57      0.90     -0.01      1.87  12082.30      1.00\n#&gt;     U[234]     -0.19      0.58     -0.19     -1.12      0.81  13799.25      1.00\n#&gt;     U[235]      1.21      0.60      1.21      0.20      2.18  10358.07      1.00\n#&gt;     U[236]      2.40      0.58      2.40      1.45      3.37   7544.78      1.00\n#&gt;     U[237]      0.20      0.58      0.20     -0.75      1.15  14363.50      1.00\n#&gt;     U[238]     -1.04      0.59     -1.04     -1.99     -0.06  16554.52      1.00\n#&gt;     U[239]     -1.11      0.60     -1.11     -2.05     -0.12  12873.16      1.00\n#&gt;     U[240]     -0.15      0.59     -0.15     -1.12      0.81   8565.71      1.00\n#&gt;     U[241]      0.55      0.59      0.55     -0.39      1.54  13030.71      1.00\n#&gt;     U[242]     -0.64      0.58     -0.64     -1.60      0.31  13221.54      1.00\n#&gt;     U[243]      0.08      0.59      0.08     -0.90      1.04  11692.27      1.00\n#&gt;     U[244]      0.28      0.59      0.28     -0.66      1.28  12858.59      1.00\n#&gt;     U[245]      0.72      0.59      0.73     -0.24      1.70  12500.04      1.00\n#&gt;     U[246]     -0.73      0.59     -0.73     -1.66      0.26  12717.40      1.00\n#&gt;     U[247]      0.08      0.60      0.07     -0.88      1.07  12960.24      1.00\n#&gt;     U[248]     -0.53      0.59     -0.53     -1.55      0.38  13319.87      1.00\n#&gt;     U[249]      0.77      0.58      0.78     -0.19      1.71  13274.38      1.00\n#&gt;     U[250]     -1.24      0.59     -1.25     -2.16     -0.25  13571.58      1.00\n#&gt;     U[251]      0.59      0.59      0.58     -0.42      1.52  10683.81      1.00\n#&gt;     U[252]     -1.11      0.59     -1.12     -2.10     -0.15   8364.42      1.00\n#&gt;     U[253]     -0.46      0.59     -0.46     -1.39      0.58  12855.43      1.00\n#&gt;     U[254]      1.03      0.60      1.03      0.04      2.00   9939.35      1.00\n#&gt;     U[255]      0.51      0.58      0.51     -0.46      1.44  12773.67      1.00\n#&gt;     U[256]     -0.80      0.60     -0.80     -1.74      0.23  11032.24      1.00\n#&gt;     U[257]     -0.58      0.59     -0.58     -1.55      0.38  15300.66      1.00\n#&gt;     U[258]      1.92      0.58      1.92      0.99      2.89  12025.58      1.00\n#&gt;     U[259]     -0.28      0.58     -0.28     -1.18      0.74  12904.97      1.00\n#&gt;     U[260]      0.02      0.60      0.02     -0.96      1.00  12452.41      1.00\n#&gt;     U[261]     -1.71      0.58     -1.71     -2.66     -0.73  14107.61      1.00\n#&gt;     U[262]      1.06      0.59      1.07      0.06      2.00  11191.22      1.00\n#&gt;     U[263]      0.30      0.60      0.29     -0.69      1.27   7406.29      1.00\n#&gt;     U[264]     -0.26      0.58     -0.26     -1.25      0.66  12780.44      1.00\n#&gt;     U[265]     -0.25      0.58     -0.24     -1.21      0.67  12804.31      1.00\n#&gt;     U[266]      1.07      0.58      1.07      0.10      2.01  13126.02      1.00\n#&gt;     U[267]     -0.36      0.60     -0.36     -1.33      0.64  13683.62      1.00\n#&gt;     U[268]      0.15      0.60      0.15     -0.89      1.08  13740.85      1.00\n#&gt;     U[269]      0.32      0.57      0.33     -0.61      1.27  11606.54      1.00\n#&gt;     U[270]      0.96      0.58      0.96     -0.01      1.90  11056.35      1.00\n#&gt;     U[271]     -0.23      0.58     -0.23     -1.20      0.71  12784.81      1.00\n#&gt;     U[272]      1.56      0.58      1.56      0.55      2.47  12928.06      1.00\n#&gt;     U[273]      0.52      0.59      0.51     -0.49      1.47  11668.35      1.00\n#&gt;     U[274]      0.16      0.59      0.16     -0.83      1.09  11545.89      1.00\n#&gt;     U[275]      1.46      0.59      1.45      0.53      2.45  12078.51      1.00\n#&gt;     U[276]      0.21      0.58      0.22     -0.71      1.21  14120.93      1.00\n#&gt;     U[277]      1.81      0.58      1.82      0.88      2.80  12174.92      1.00\n#&gt;     U[278]     -0.24      0.58     -0.24     -1.14      0.78  12455.34      1.00\n#&gt;     U[279]     -0.64      0.59     -0.65     -1.62      0.30  12811.67      1.00\n#&gt;     U[280]     -0.28      0.59     -0.29     -1.24      0.70  11894.67      1.00\n#&gt;     U[281]      0.24      0.59      0.24     -0.73      1.20  13934.58      1.00\n#&gt;     U[282]      0.03      0.59      0.04     -0.93      1.00  13446.72      1.00\n#&gt;     U[283]      0.35      0.57      0.35     -0.58      1.30  12492.87      1.00\n#&gt;     U[284]      0.54      0.58      0.54     -0.41      1.52  12001.27      1.00\n#&gt;     U[285]     -0.56      0.58     -0.56     -1.48      0.40  11657.50      1.00\n#&gt;     U[286]     -0.79      0.58     -0.79     -1.74      0.18  12871.04      1.00\n#&gt;     U[287]      0.19      0.59      0.20     -0.76      1.16  12924.71      1.00\n#&gt;     U[288]     -1.36      0.58     -1.36     -2.31     -0.39  15304.16      1.00\n#&gt;     U[289]      1.95      0.59      1.95      0.96      2.92  11218.10      1.00\n#&gt;     U[290]      0.17      0.59      0.17     -0.80      1.14  10590.40      1.00\n#&gt;     U[291]     -1.13      0.60     -1.13     -2.10     -0.14   6834.80      1.00\n#&gt;     U[292]     -1.07      0.58     -1.07     -2.06     -0.14  13923.23      1.00\n#&gt;     U[293]     -0.24      0.59     -0.24     -1.21      0.72  13592.64      1.00\n#&gt;     U[294]     -1.08      0.59     -1.08     -2.02     -0.08  13524.68      1.00\n#&gt;     U[295]     -0.08      0.58     -0.08     -1.02      0.86  11756.73      1.00\n#&gt;     U[296]     -1.27      0.59     -1.27     -2.25     -0.32  12176.58      1.00\n#&gt;     U[297]      0.55      0.59      0.55     -0.42      1.51  12442.86      1.00\n#&gt;     U[298]      0.94      0.60      0.94     -0.03      1.94  13583.01      1.00\n#&gt;     U[299]     -0.35      0.58     -0.36     -1.34      0.56  13882.18      1.00\n#&gt;     U[300]     -0.29      0.58     -0.28     -1.25      0.64  12070.29      1.00\n#&gt;     U[301]     -0.77      0.59     -0.78     -1.74      0.21  12006.59      1.00\n#&gt;     U[302]     -0.42      0.58     -0.42     -1.36      0.53  11897.29      1.00\n#&gt;     U[303]      0.84      0.59      0.84     -0.14      1.80  11475.28      1.00\n#&gt;     U[304]      0.02      0.58      0.02     -0.96      0.94  12353.85      1.00\n#&gt;     U[305]     -1.12      0.58     -1.13     -2.09     -0.19  12172.69      1.00\n#&gt;     U[306]     -0.94      0.59     -0.94     -1.92      0.01  12668.70      1.00\n#&gt;     U[307]     -0.84      0.58     -0.85     -1.82      0.10   8248.82      1.00\n#&gt;     U[308]      0.63      0.59      0.63     -0.31      1.63  11022.01      1.00\n#&gt;     U[309]     -1.39      0.59     -1.39     -2.36     -0.40  11446.68      1.00\n#&gt;     U[310]     -0.08      0.59     -0.08     -1.04      0.92  12137.85      1.00\n#&gt;     U[311]      0.01      0.59      0.01     -0.94      1.02   9753.12      1.00\n#&gt;     U[312]     -0.24      0.58     -0.24     -1.19      0.72   8412.05      1.00\n#&gt;     U[313]     -0.32      0.58     -0.32     -1.23      0.69  12094.94      1.00\n#&gt;     U[314]     -1.03      0.58     -1.04     -2.00     -0.08  13199.30      1.00\n#&gt;     U[315]      0.07      0.59      0.08     -0.88      1.05  13115.95      1.00\n#&gt;     U[316]     -1.89      0.59     -1.89     -2.83     -0.88  12321.19      1.00\n#&gt;     U[317]      0.72      0.60      0.72     -0.25      1.73  14303.69      1.00\n#&gt;     U[318]      1.10      0.60      1.10      0.13      2.11   7664.78      1.00\n#&gt;     U[319]      1.00      0.59      1.00      0.02      1.97  11615.20      1.00\n#&gt;     U[320]     -0.49      0.58     -0.49     -1.43      0.47   8733.67      1.00\n#&gt;     U[321]     -0.82      0.59     -0.82     -1.76      0.15  13258.53      1.00\n#&gt;     U[322]     -0.24      0.58     -0.24     -1.20      0.71  13118.09      1.00\n#&gt;     U[323]      0.46      0.58      0.46     -0.45      1.43  10230.53      1.00\n#&gt;     U[324]      0.87      0.59      0.88     -0.10      1.81  10055.12      1.00\n#&gt;     U[325]      0.03      0.58      0.04     -0.90      0.99  13478.46      1.00\n#&gt;     U[326]     -0.37      0.59     -0.37     -1.37      0.55  11370.02      1.00\n#&gt;     U[327]      0.13      0.58      0.14     -0.79      1.14  13399.86      1.00\n#&gt;     U[328]      0.27      0.58      0.27     -0.72      1.19  11489.27      1.00\n#&gt;     U[329]     -1.42      0.60     -1.42     -2.44     -0.46  12674.05      1.00\n#&gt;     U[330]     -0.28      0.59     -0.28     -1.24      0.67  13391.06      1.00\n#&gt;     U[331]     -0.84      0.60     -0.85     -1.83      0.13  12969.87      1.00\n#&gt;     U[332]      0.73      0.58      0.72     -0.21      1.70   9327.25      1.00\n#&gt;     U[333]     -0.83      0.58     -0.82     -1.81      0.08  13521.97      1.00\n#&gt;     U[334]      0.77      0.58      0.76     -0.22      1.67  13779.84      1.00\n#&gt;     U[335]     -0.03      0.59     -0.04     -1.00      0.93  13258.84      1.00\n#&gt;     U[336]     -0.67      0.60     -0.67     -1.68      0.29   9515.27      1.00\n#&gt;     U[337]     -0.41      0.59     -0.41     -1.41      0.53  12407.53      1.00\n#&gt;     U[338]      0.05      0.59      0.04     -0.95      0.99  11437.37      1.00\n#&gt;     U[339]     -1.11      0.60     -1.12     -2.07     -0.12   8116.53      1.00\n#&gt;     U[340]     -0.13      0.58     -0.13     -1.07      0.83  15405.20      1.00\n#&gt;     U[341]      0.23      0.58      0.23     -0.72      1.19   9983.91      1.00\n#&gt;     U[342]      0.69      0.60      0.69     -0.26      1.72  12528.65      1.00\n#&gt;     U[343]      0.02      0.60      0.02     -0.98      0.98   9499.49      1.00\n#&gt;     U[344]      2.43      0.59      2.44      1.43      3.37   9620.44      1.00\n#&gt;     U[345]      0.50      0.59      0.51     -0.44      1.50  13613.61      1.00\n#&gt;     U[346]      0.66      0.59      0.67     -0.33      1.61  10430.11      1.00\n#&gt;     U[347]      0.53      0.58      0.52     -0.44      1.48  13696.29      1.00\n#&gt;     U[348]      0.11      0.59      0.11     -0.88      1.06  12143.06      1.00\n#&gt;     U[349]      0.66      0.57      0.66     -0.22      1.64  12472.91      1.00\n#&gt;     U[350]     -0.52      0.59     -0.53     -1.43      0.51   8613.52      1.00\n#&gt;     U[351]      0.75      0.57      0.76     -0.21      1.66  13631.38      1.00\n#&gt;     U[352]     -1.90      0.59     -1.91     -2.87     -0.96  12857.42      1.00\n#&gt;     U[353]     -0.14      0.59     -0.14     -1.14      0.80  12752.39      1.00\n#&gt;     U[354]      0.04      0.58      0.05     -0.92      0.96  12365.87      1.00\n#&gt;     U[355]      0.70      0.58      0.71     -0.27      1.64  13293.08      1.00\n#&gt;     U[356]     -0.03      0.59     -0.02     -1.00      0.94  12399.13      1.00\n#&gt;     U[357]      1.04      0.60      1.04     -0.00      1.97   9946.34      1.00\n#&gt;     U[358]      0.83      0.61      0.83     -0.21      1.81   9919.16      1.00\n#&gt;     U[359]      0.33      0.60      0.33     -0.68      1.31   5532.99      1.00\n#&gt;     U[360]      0.34      0.58      0.34     -0.60      1.29  13596.44      1.00\n#&gt;     U[361]     -0.37      0.58     -0.38     -1.31      0.56  12242.07      1.00\n#&gt;     U[362]     -0.57      0.58     -0.57     -1.50      0.38  11309.66      1.00\n#&gt;     U[363]     -0.91      0.58     -0.91     -1.91     -0.01  12271.88      1.00\n#&gt;     U[364]      0.35      0.58      0.36     -0.64      1.26  12653.10      1.00\n#&gt;     U[365]      1.09      0.59      1.10      0.08      2.00  10599.77      1.00\n#&gt;     U[366]     -0.12      0.60     -0.13     -1.12      0.84  12461.51      1.00\n#&gt;     U[367]     -0.73      0.58     -0.73     -1.64      0.27  13220.86      1.00\n#&gt;     U[368]     -0.07      0.59     -0.06     -1.07      0.88  11894.29      1.00\n#&gt;     U[369]      0.51      0.57      0.51     -0.44      1.42  12089.61      1.00\n#&gt;     U[370]      0.09      0.59      0.09     -0.89      1.03  12338.97      1.00\n#&gt;     U[371]      0.23      0.59      0.23     -0.74      1.20  12326.06      1.00\n#&gt;     U[372]      0.26      0.57      0.26     -0.65      1.24  13746.78      1.00\n#&gt;     U[373]     -1.74      0.58     -1.74     -2.69     -0.79  13217.61      1.00\n#&gt;     U[374]      0.94      0.59      0.94     -0.04      1.87  13286.25      1.00\n#&gt;     U[375]      0.06      0.58      0.06     -0.89      1.02  11834.92      1.00\n#&gt;     U[376]     -0.81      0.59     -0.81     -1.78      0.15  11200.35      1.00\n#&gt;     U[377]     -0.55      0.59     -0.55     -1.54      0.39  13532.42      1.00\n#&gt;     U[378]      0.16      0.59      0.15     -0.80      1.12  11956.09      1.00\n#&gt;     U[379]      1.12      0.58      1.12      0.13      2.04  12594.93      1.00\n#&gt;     U[380]      0.39      0.58      0.39     -0.56      1.35  11367.18      1.00\n#&gt;     U[381]     -0.27      0.57     -0.26     -1.19      0.66  12903.09      1.00\n#&gt;     U[382]     -0.73      0.58     -0.73     -1.66      0.25  12222.95      1.00\n#&gt;     U[383]      1.15      0.60      1.15      0.17      2.10  12792.96      1.00\n#&gt;     U[384]     -0.67      0.59     -0.66     -1.62      0.34  10580.35      1.00\n#&gt;     U[385]      1.01      0.58      1.02      0.07      1.98  10839.79      1.00\n#&gt;     U[386]      0.25      0.59      0.24     -0.70      1.21  11055.16      1.00\n#&gt;     U[387]      0.66      0.61      0.66     -0.35      1.65   8521.55      1.00\n#&gt;     U[388]      0.35      0.57      0.35     -0.60      1.27  11991.65      1.00\n#&gt;     U[389]     -0.44      0.58     -0.44     -1.39      0.50  13551.91      1.00\n#&gt;     U[390]      0.43      0.59      0.43     -0.51      1.41  13205.40      1.00\n#&gt;     U[391]      0.42      0.59      0.41     -0.53      1.41  12147.62      1.00\n#&gt;     U[392]      0.27      0.59      0.27     -0.69      1.23  10853.26      1.00\n#&gt;     U[393]     -0.13      0.59     -0.14     -1.07      0.85  12444.05      1.00\n#&gt;     U[394]      0.07      0.60      0.07     -0.95      1.03  11183.61      1.00\n#&gt;     U[395]     -0.45      0.59     -0.45     -1.42      0.52  10406.10      1.00\n#&gt;     U[396]      0.17      0.59      0.16     -0.78      1.15  13547.67      1.00\n#&gt;     U[397]      1.24      0.61      1.23      0.24      2.24   7921.25      1.00\n#&gt;     U[398]     -1.77      0.57     -1.77     -2.70     -0.83  16355.62      1.00\n#&gt;     U[399]      0.81      0.59      0.81     -0.16      1.78  12846.81      1.00\n#&gt;     U[400]      0.24      0.59      0.23     -0.69      1.25   9787.17      1.00\n#&gt;     U[401]     -0.08      0.58     -0.08     -1.05      0.87  12999.88      1.00\n#&gt;     U[402]      0.73      0.58      0.73     -0.21      1.69  13454.55      1.00\n#&gt;     U[403]      1.15      0.58      1.15      0.21      2.09  11408.58      1.00\n#&gt;     U[404]     -0.43      0.60     -0.44     -1.47      0.52  12336.51      1.00\n#&gt;     U[405]      0.51      0.60      0.52     -0.48      1.51  11158.67      1.00\n#&gt;     U[406]      0.69      0.59      0.69     -0.27      1.65   8711.94      1.00\n#&gt;     U[407]      1.41      0.60      1.41      0.44      2.42  12849.57      1.00\n#&gt;     U[408]     -1.20      0.59     -1.21     -2.14     -0.21  14740.91      1.00\n#&gt;     U[409]     -0.26      0.58     -0.26     -1.21      0.69  11003.77      1.00\n#&gt;     U[410]      0.18      0.59      0.18     -0.78      1.15  11036.82      1.00\n#&gt;     U[411]     -0.74      0.59     -0.74     -1.70      0.24  10431.76      1.00\n#&gt;     U[412]      0.46      0.60      0.45     -0.53      1.45   8007.81      1.00\n#&gt;     U[413]      0.21      0.60      0.21     -0.80      1.15  12222.35      1.00\n#&gt;     U[414]     -1.52      0.62     -1.53     -2.49     -0.47   9486.41      1.00\n#&gt;     U[415]      0.42      0.60      0.43     -0.60      1.37  10751.20      1.00\n#&gt;     U[416]      0.08      0.60      0.08     -0.92      1.02  10155.81      1.00\n#&gt;     U[417]      0.92      0.58      0.92     -0.01      1.89  14106.48      1.00\n#&gt;     U[418]      1.03      0.59      1.04      0.04      1.97  10104.92      1.00\n#&gt;     U[419]     -0.87      0.58     -0.87     -1.83      0.07  12832.60      1.00\n#&gt;     U[420]     -0.70      0.59     -0.70     -1.68      0.26  10601.15      1.00\n#&gt;     U[421]     -1.12      0.59     -1.12     -2.08     -0.14  12851.73      1.00\n#&gt;     U[422]     -0.26      0.58     -0.26     -1.20      0.70  12830.76      1.00\n#&gt;     U[423]      1.63      0.59      1.62      0.65      2.58  11922.33      1.00\n#&gt;     U[424]     -1.91      0.58     -1.91     -2.87     -0.99  14552.48      1.00\n#&gt;     U[425]      1.20      0.59      1.19      0.21      2.17  11042.18      1.00\n#&gt;     U[426]      1.84      0.59      1.84      0.87      2.80  13401.04      1.00\n#&gt;     U[427]     -1.19      0.60     -1.19     -2.15     -0.18  11839.04      1.00\n#&gt;     U[428]     -1.75      0.59     -1.75     -2.73     -0.78  13746.05      1.00\n#&gt;     U[429]      0.14      0.59      0.14     -0.88      1.07  13384.92      1.00\n#&gt;     U[430]     -0.31      0.58     -0.32     -1.26      0.67  11674.92      1.00\n#&gt;     U[431]     -0.77      0.58     -0.77     -1.76      0.17  13262.43      1.00\n#&gt;     U[432]     -1.38      0.59     -1.37     -2.35     -0.40  12342.40      1.00\n#&gt;     U[433]     -1.33      0.59     -1.34     -2.32     -0.40  14373.06      1.00\n#&gt;     U[434]      0.98      0.60      0.98     -0.01      1.92  12318.99      1.00\n#&gt;     U[435]     -0.40      0.58     -0.40     -1.34      0.58  11703.87      1.00\n#&gt;     U[436]     -0.26      0.58     -0.26     -1.23      0.66  13971.77      1.00\n#&gt;     U[437]      0.77      0.60      0.78     -0.22      1.72   9286.14      1.00\n#&gt;     U[438]     -1.47      0.60     -1.47     -2.41     -0.48  13094.42      1.00\n#&gt;     U[439]     -0.08      0.58     -0.07     -1.03      0.86  13435.51      1.00\n#&gt;     U[440]     -1.34      0.58     -1.34     -2.30     -0.38  12984.85      1.00\n#&gt;     U[441]     -0.22      0.61     -0.22     -1.22      0.78  11339.79      1.00\n#&gt;     U[442]     -0.05      0.60     -0.05     -1.03      0.95  10156.91      1.00\n#&gt;     U[443]     -0.09      0.59     -0.09     -1.07      0.85  12431.54      1.00\n#&gt;     U[444]     -0.77      0.60     -0.76     -1.76      0.23  10669.00      1.00\n#&gt;     U[445]      0.41      0.57      0.42     -0.49      1.39  13644.38      1.00\n#&gt;     U[446]      1.26      0.58      1.25      0.35      2.27  10647.88      1.00\n#&gt;     U[447]     -1.92      0.61     -1.92     -2.90     -0.92   9504.32      1.00\n#&gt;     U[448]     -0.12      0.59     -0.12     -1.10      0.86  13051.01      1.00\n#&gt;     U[449]      0.31      0.60      0.31     -0.62      1.33  12514.50      1.00\n#&gt;     U[450]      1.43      0.58      1.43      0.49      2.39  13456.22      1.00\n#&gt;     U[451]      1.00      0.57      1.00      0.02      1.89  11559.73      1.00\n#&gt;     U[452]     -1.59      0.59     -1.59     -2.58     -0.64   9653.87      1.00\n#&gt;     U[453]      0.41      0.59      0.41     -0.53      1.41  14173.97      1.00\n#&gt;     U[454]     -0.42      0.59     -0.42     -1.37      0.53   9499.98      1.00\n#&gt;     U[455]      0.74      0.58      0.75     -0.23      1.66   9384.40      1.00\n#&gt;     U[456]      0.29      0.58      0.30     -0.69      1.22  13214.30      1.00\n#&gt;     U[457]      0.49      0.59      0.49     -0.49      1.44  13305.41      1.00\n#&gt;     U[458]     -0.28      0.60     -0.28     -1.27      0.66  13703.11      1.00\n#&gt;     U[459]     -0.40      0.58     -0.39     -1.33      0.60  12742.78      1.00\n#&gt;     U[460]      0.81      0.58      0.81     -0.16      1.73  13145.57      1.00\n#&gt;     U[461]     -0.70      0.59     -0.70     -1.67      0.28  13865.46      1.00\n#&gt;     U[462]      0.20      0.59      0.20     -0.80      1.12  10691.42      1.00\n#&gt;     U[463]      0.37      0.58      0.37     -0.59      1.31  14398.92      1.00\n#&gt;     U[464]     -0.69      0.60     -0.70     -1.64      0.35  10816.96      1.00\n#&gt;     U[465]     -0.41      0.59     -0.42     -1.35      0.59  13068.95      1.00\n#&gt;     U[466]     -0.63      0.59     -0.63     -1.60      0.35  12661.12      1.00\n#&gt;     U[467]     -1.22      0.59     -1.22     -2.20     -0.28  11509.65      1.00\n#&gt;     U[468]      0.31      0.57      0.31     -0.65      1.22  13620.26      1.00\n#&gt;     U[469]     -0.28      0.58     -0.28     -1.28      0.62  11093.25      1.00\n#&gt;     U[470]      0.20      0.58      0.20     -0.74      1.18  12344.95      1.00\n#&gt;     U[471]      0.12      0.60      0.12     -0.85      1.12  14343.69      1.00\n#&gt;     U[472]     -0.88      0.59     -0.88     -1.84      0.10  13781.89      1.00\n#&gt;     U[473]      0.45      0.58      0.45     -0.51      1.38  14220.89      1.00\n#&gt;     U[474]      0.21      0.58      0.21     -0.79      1.12  14064.46      1.00\n#&gt;     U[475]      0.33      0.58      0.33     -0.65      1.25  11517.17      1.00\n#&gt;     U[476]     -0.44      0.57     -0.45     -1.39      0.48  11939.92      1.00\n#&gt;     U[477]     -0.12      0.58     -0.12     -1.08      0.82  11535.72      1.00\n#&gt;     U[478]     -1.02      0.59     -1.03     -1.98     -0.07  13389.66      1.00\n#&gt;     U[479]     -1.96      0.59     -1.96     -2.92     -0.97  12265.09      1.00\n#&gt;     U[480]     -0.41      0.59     -0.42     -1.36      0.57   9788.78      1.00\n#&gt;     U[481]     -0.35      0.59     -0.35     -1.35      0.58  13504.08      1.00\n#&gt;     U[482]     -0.17      0.59     -0.18     -1.15      0.78   9224.85      1.00\n#&gt;     U[483]      0.43      0.59      0.43     -0.59      1.35  12573.96      1.00\n#&gt;     U[484]     -1.75      0.59     -1.76     -2.69     -0.74  12957.91      1.00\n#&gt;     U[485]     -0.52      0.60     -0.52     -1.49      0.48   9686.98      1.00\n#&gt;     U[486]      0.60      0.58      0.60     -0.38      1.54  12152.93      1.00\n#&gt;     U[487]      1.25      0.59      1.26      0.27      2.19  12661.34      1.00\n#&gt;     U[488]      1.01      0.59      1.02      0.04      1.97  12799.68      1.00\n#&gt;     U[489]     -0.58      0.59     -0.58     -1.53      0.40  12203.01      1.00\n#&gt;     U[490]     -0.70      0.59     -0.70     -1.65      0.30   8913.30      1.00\n#&gt;     U[491]     -2.54      0.59     -2.54     -3.54     -1.60  14101.97      1.00\n#&gt;     U[492]      0.95      0.59      0.96     -0.03      1.90  10656.72      1.00\n#&gt;     U[493]      0.96      0.58      0.96     -0.02      1.92  12597.25      1.00\n#&gt;     U[494]     -0.62      0.60     -0.63     -1.60      0.37  11538.90      1.00\n#&gt;     U[495]      0.57      0.58      0.57     -0.39      1.56  13765.70      1.00\n#&gt;     U[496]     -0.42      0.58     -0.43     -1.38      0.52  11830.30      1.00\n#&gt;     U[497]      0.26      0.58      0.27     -0.71      1.20  11293.71      1.00\n#&gt;     U[498]      0.84      0.59      0.84     -0.11      1.80  12403.20      1.00\n#&gt;     U[499]      1.28      0.58      1.29      0.31      2.23  14402.89      1.00\n#&gt;     U[500]     -0.32      0.59     -0.33     -1.31      0.60   9758.22      1.00\n#&gt;     U[501]     -0.25      0.59     -0.25     -1.20      0.73  13770.03      1.00\n#&gt;     U[502]     -0.79      0.58     -0.79     -1.78      0.12  13172.57      1.00\n#&gt;     U[503]     -1.46      0.56     -1.45     -2.39     -0.55  15181.34      1.00\n#&gt;     U[504]     -1.39      0.59     -1.39     -2.33     -0.40  10216.67      1.00\n#&gt;     U[505]      0.20      0.60      0.21     -0.79      1.15  12407.52      1.00\n#&gt;     U[506]      0.75      0.57      0.75     -0.20      1.68  12067.76      1.00\n#&gt;     U[507]      1.11      0.59      1.12      0.11      2.05   9159.21      1.00\n#&gt;     U[508]     -0.55      0.59     -0.55     -1.50      0.42  12372.61      1.00\n#&gt;     U[509]     -0.57      0.57     -0.58     -1.46      0.43  10299.20      1.00\n#&gt;     U[510]     -0.81      0.57     -0.80     -1.77      0.13  14652.82      1.00\n#&gt;     U[511]     -1.10      0.59     -1.11     -2.04     -0.08  14029.27      1.00\n#&gt;     U[512]     -0.39      0.59     -0.39     -1.38      0.54  11427.16      1.00\n#&gt;     U[513]     -0.45      0.59     -0.45     -1.40      0.55  12796.08      1.00\n#&gt;     U[514]     -0.04      0.59     -0.03     -0.99      0.95  12190.68      1.00\n#&gt;     U[515]     -0.05      0.60     -0.05     -1.05      0.90  11292.64      1.00\n#&gt;     U[516]     -0.15      0.59     -0.15     -1.09      0.85  11743.15      1.00\n#&gt;     U[517]     -1.07      0.59     -1.07     -2.01     -0.06   9000.28      1.00\n#&gt;     U[518]     -0.50      0.58     -0.50     -1.50      0.42  13325.82      1.00\n#&gt;     U[519]     -0.23      0.58     -0.22     -1.18      0.73  13809.36      1.00\n#&gt;     U[520]      0.57      0.60      0.57     -0.41      1.55  12545.69      1.00\n#&gt;     U[521]     -0.40      0.59     -0.40     -1.40      0.52  11252.15      1.00\n#&gt;     U[522]      1.63      0.58      1.64      0.68      2.59  11041.84      1.00\n#&gt;     U[523]     -0.19      0.57     -0.19     -1.13      0.73  12881.47      1.00\n#&gt;     U[524]     -0.22      0.59     -0.22     -1.12      0.78  12379.76      1.00\n#&gt;     U[525]      1.14      0.59      1.15      0.15      2.09  12905.90      1.00\n#&gt;     U[526]     -0.07      0.59     -0.07     -1.02      0.92  14284.69      1.00\n#&gt;     U[527]      1.09      0.58      1.09      0.17      2.08  11165.32      1.00\n#&gt;     U[528]     -0.71      0.59     -0.70     -1.70      0.25  14146.24      1.00\n#&gt;     U[529]     -0.66      0.58     -0.67     -1.63      0.29  14276.23      1.00\n#&gt;     U[530]      0.97      0.58      0.99      0.04      1.95  14085.96      1.00\n#&gt;     U[531]     -0.09      0.59     -0.10     -1.07      0.87  13267.12      1.00\n#&gt;     U[532]      1.83      0.59      1.82      0.88      2.78  13247.83      1.00\n#&gt;     U[533]     -0.22      0.59     -0.22     -1.19      0.76  11543.46      1.00\n#&gt;     U[534]      0.92      0.58      0.93     -0.04      1.84  10059.31      1.00\n#&gt;     U[535]     -0.98      0.60     -0.98     -1.95     -0.00  11535.46      1.00\n#&gt;     U[536]     -1.21      0.60     -1.22     -2.15     -0.19   9075.58      1.00\n#&gt;     U[537]      0.25      0.59      0.25     -0.66      1.28  12240.71      1.00\n#&gt;     U[538]      1.23      0.59      1.23      0.24      2.18  12795.25      1.00\n#&gt;     U[539]     -0.34      0.59     -0.34     -1.32      0.64  11372.96      1.00\n#&gt;     U[540]     -0.55      0.59     -0.55     -1.49      0.44  15017.37      1.00\n#&gt;     U[541]     -1.41      0.60     -1.42     -2.38     -0.41  11793.10      1.00\n#&gt;     U[542]      1.55      0.60      1.55      0.55      2.51  11007.70      1.00\n#&gt;     U[543]      1.51      0.58      1.51      0.58      2.50  12449.36      1.00\n#&gt;     U[544]      0.17      0.59      0.17     -0.82      1.12  13829.54      1.00\n#&gt;     U[545]      0.68      0.58      0.68     -0.19      1.72  13208.38      1.00\n#&gt;     U[546]      0.04      0.58      0.05     -0.94      0.98  13139.22      1.00\n#&gt;     U[547]     -0.73      0.59     -0.73     -1.70      0.22  13067.44      1.00\n#&gt;     U[548]      1.05      0.60      1.06      0.08      2.04   9932.12      1.00\n#&gt;     U[549]      0.79      0.60      0.80     -0.24      1.76  11270.32      1.00\n#&gt;     U[550]     -0.43      0.58     -0.43     -1.41      0.50  12783.71      1.00\n#&gt;     U[551]      1.94      0.59      1.93      0.98      2.90  13426.83      1.00\n#&gt;     U[552]     -0.76      0.59     -0.76     -1.71      0.21  13991.64      1.00\n#&gt;     U[553]     -0.12      0.59     -0.11     -1.07      0.86  10254.58      1.00\n#&gt;     U[554]     -0.82      0.59     -0.82     -1.80      0.12  13332.22      1.00\n#&gt;     U[555]      0.78      0.59      0.78     -0.20      1.73  11886.46      1.00\n#&gt;     U[556]     -0.26      0.60     -0.26     -1.20      0.75  11896.32      1.00\n#&gt;     U[557]      0.77      0.59      0.77     -0.22      1.73  13816.35      1.00\n#&gt;     U[558]     -0.30      0.59     -0.30     -1.23      0.68  14020.21      1.00\n#&gt;     U[559]      1.62      0.59      1.63      0.66      2.62  13149.90      1.00\n#&gt;     U[560]      1.53      0.59      1.54      0.56      2.48  14112.68      1.00\n#&gt;     U[561]      1.19      0.57      1.19      0.24      2.10  11032.19      1.00\n#&gt;     U[562]      0.49      0.59      0.49     -0.50      1.43   8911.21      1.00\n#&gt;     U[563]      0.22      0.59      0.22     -0.78      1.18  10122.46      1.00\n#&gt;     U[564]      0.32      0.58      0.32     -0.63      1.29  13641.73      1.00\n#&gt;     U[565]      0.22      0.57      0.23     -0.71      1.14  12037.45      1.00\n#&gt;     U[566]      0.15      0.59      0.15     -0.80      1.12  11211.73      1.00\n#&gt;     U[567]     -2.01      0.59     -2.02     -2.98     -1.05  12184.60      1.00\n#&gt;     U[568]     -1.72      0.59     -1.72     -2.67     -0.73  11628.55      1.00\n#&gt;     U[569]      0.14      0.59      0.14     -0.84      1.09  12749.36      1.00\n#&gt;     U[570]     -1.35      0.59     -1.35     -2.32     -0.40   9137.02      1.00\n#&gt;     U[571]      1.87      0.60      1.87      0.88      2.83  11329.97      1.00\n#&gt;     U[572]     -0.20      0.59     -0.20     -1.18      0.78  11542.83      1.00\n#&gt;     U[573]     -0.78      0.59     -0.77     -1.76      0.16  12822.03      1.00\n#&gt;     U[574]      1.46      0.59      1.47      0.44      2.39  13244.25      1.00\n#&gt;     U[575]      0.19      0.59      0.18     -0.79      1.15  11483.00      1.00\n#&gt;     U[576]      0.29      0.59      0.29     -0.70      1.26  13614.14      1.00\n#&gt;     U[577]      0.11      0.59      0.11     -0.87      1.07  12950.37      1.00\n#&gt;     U[578]      0.80      0.57      0.80     -0.09      1.78  14573.06      1.00\n#&gt;     U[579]     -0.49      0.58     -0.48     -1.49      0.42  12991.66      1.00\n#&gt;     U[580]     -1.62      0.59     -1.62     -2.56     -0.64  12195.17      1.00\n#&gt;     U[581]     -0.80      0.58     -0.80     -1.75      0.15  13906.27      1.00\n#&gt;     U[582]     -0.26      0.58     -0.26     -1.22      0.69  12594.05      1.00\n#&gt;     U[583]     -0.26      0.59     -0.26     -1.20      0.73  16095.47      1.00\n#&gt;     U[584]     -0.27      0.59     -0.27     -1.26      0.67  12130.10      1.00\n#&gt;     U[585]     -0.42      0.58     -0.42     -1.36      0.56  14090.59      1.00\n#&gt;     U[586]      0.14      0.59      0.14     -0.86      1.08  13292.05      1.00\n#&gt;     U[587]      0.57      0.60      0.57     -0.42      1.55  12871.18      1.00\n#&gt;     U[588]      1.11      0.59      1.11      0.15      2.06  10841.67      1.00\n#&gt;     U[589]      0.88      0.59      0.89     -0.07      1.86  12166.82      1.00\n#&gt;     U[590]      0.94      0.58      0.93     -0.05      1.84  11710.67      1.00\n#&gt;     U[591]     -0.13      0.59     -0.13     -1.09      0.84  14762.65      1.00\n#&gt;     U[592]      1.00      0.60      1.01      0.03      1.98  10940.15      1.00\n#&gt;     U[593]      0.64      0.58      0.63     -0.35      1.59  12058.88      1.00\n#&gt;     U[594]      0.85      0.59      0.85     -0.14      1.79  10580.93      1.00\n#&gt;     U[595]     -0.11      0.57     -0.12     -1.05      0.82  12914.79      1.00\n#&gt;     U[596]     -0.91      0.58     -0.91     -1.85      0.05  16036.00      1.00\n#&gt;     U[597]     -0.04      0.60     -0.04     -1.03      0.95  13950.67      1.00\n#&gt;     U[598]     -0.29      0.59     -0.29     -1.23      0.67  12969.37      1.00\n#&gt;     U[599]      1.56      0.59      1.56      0.62      2.58  13727.98      1.00\n#&gt;     U[600]      0.50      0.57      0.50     -0.47      1.40  12593.65      1.00\n#&gt;     U[601]      0.38      0.59      0.39     -0.55      1.38  14369.76      1.00\n#&gt;     U[602]      0.80      0.58      0.80     -0.11      1.76  11591.84      1.00\n#&gt;     U[603]     -0.48      0.59     -0.48     -1.44      0.51  13491.33      1.00\n#&gt;     U[604]      0.18      0.58      0.17     -0.79      1.11  11519.48      1.00\n#&gt;     U[605]     -1.13      0.60     -1.14     -2.10     -0.12  14111.74      1.00\n#&gt;     U[606]     -0.65      0.58     -0.66     -1.57      0.31  12445.59      1.00\n#&gt;     U[607]     -0.30      0.59     -0.29     -1.27      0.67  12959.59      1.00\n#&gt;     U[608]     -1.59      0.60     -1.59     -2.61     -0.65  14391.90      1.00\n#&gt;     U[609]      0.38      0.59      0.37     -0.55      1.38  11794.54      1.00\n#&gt;     U[610]      0.12      0.58      0.12     -0.83      1.07  12847.28      1.00\n#&gt;     U[611]      0.41      0.58      0.41     -0.58      1.35  14560.69      1.00\n#&gt;     U[612]     -0.87      0.58     -0.87     -1.86      0.05  13966.40      1.00\n#&gt;     U[613]      1.04      0.58      1.05      0.06      1.99  12395.25      1.00\n#&gt;     U[614]     -1.21      0.61     -1.22     -2.16     -0.20   8226.24      1.00\n#&gt;     U[615]     -0.69      0.59     -0.69     -1.64      0.28  13433.42      1.00\n#&gt;     U[616]      0.11      0.58      0.12     -0.84      1.07  11015.20      1.00\n#&gt;     U[617]      1.68      0.59      1.68      0.72      2.64  11268.38      1.00\n#&gt;     U[618]      0.32      0.58      0.32     -0.65      1.26  12029.81      1.00\n#&gt;     U[619]      0.42      0.59      0.42     -0.51      1.42  14806.23      1.00\n#&gt;     U[620]      0.63      0.58      0.63     -0.33      1.57  14123.87      1.00\n#&gt;     U[621]      0.07      0.59      0.08     -0.90      1.02  12143.11      1.00\n#&gt;     U[622]     -0.12      0.59     -0.12     -1.12      0.79  11559.07      1.00\n#&gt;     U[623]     -0.50      0.59     -0.51     -1.41      0.54  11697.86      1.00\n#&gt;     U[624]      0.76      0.58      0.77     -0.19      1.71   8430.68      1.00\n#&gt;     U[625]      0.61      0.58      0.61     -0.33      1.58  12780.25      1.00\n#&gt;     U[626]     -1.07      0.60     -1.07     -2.05     -0.08  15134.25      1.00\n#&gt;     U[627]     -1.28      0.59     -1.28     -2.26     -0.32  12161.10      1.00\n#&gt;     U[628]     -0.53      0.60     -0.53     -1.47      0.47  11278.93      1.00\n#&gt;     U[629]      1.24      0.59      1.24      0.24      2.18  13359.84      1.00\n#&gt;     U[630]     -1.03      0.59     -1.03     -1.95      0.00  14178.34      1.00\n#&gt;     U[631]     -0.97      0.59     -0.96     -1.99     -0.04  12720.45      1.00\n#&gt;     U[632]      0.82      0.60      0.82     -0.17      1.81  13179.87      1.00\n#&gt;     U[633]     -1.12      0.60     -1.13     -2.09     -0.13  13341.54      1.00\n#&gt;     U[634]     -1.02      0.60     -1.03     -2.01     -0.06  12347.75      1.00\n#&gt;     U[635]      0.91      0.57      0.91      0.01      1.87  12928.51      1.00\n#&gt;     U[636]      0.75      0.58      0.75     -0.18      1.71  14234.57      1.00\n#&gt;     U[637]      1.43      0.59      1.43      0.46      2.42  11153.05      1.00\n#&gt;     U[638]     -0.15      0.58     -0.15     -1.10      0.81  14543.76      1.00\n#&gt;     U[639]      0.53      0.58      0.53     -0.40      1.50  14435.54      1.00\n#&gt;     U[640]     -1.14      0.58     -1.14     -2.12     -0.20  12462.42      1.00\n#&gt;     U[641]     -0.61      0.59     -0.60     -1.61      0.33  12702.63      1.00\n#&gt;     U[642]     -1.11      0.59     -1.11     -2.08     -0.17  12690.25      1.00\n#&gt;     U[643]      0.35      0.58      0.35     -0.62      1.29  12795.18      1.00\n#&gt;     U[644]     -0.03      0.59     -0.02     -0.98      0.96  16243.42      1.00\n#&gt;     U[645]      0.56      0.59      0.56     -0.42      1.53  11992.17      1.00\n#&gt;     U[646]      1.44      0.59      1.44      0.54      2.48  13103.01      1.00\n#&gt;     U[647]     -0.20      0.60     -0.20     -1.16      0.79  10841.14      1.00\n#&gt;     U[648]     -0.38      0.58     -0.39     -1.38      0.52  13604.52      1.00\n#&gt;     U[649]     -0.02      0.59     -0.01     -0.96      0.97  14556.01      1.00\n#&gt;     U[650]     -0.21      0.59     -0.20     -1.15      0.80  12335.62      1.00\n#&gt;     U[651]     -0.05      0.59     -0.06     -1.03      0.90  13313.91      1.00\n#&gt;     U[652]      0.27      0.58      0.26     -0.64      1.26  13279.55      1.00\n#&gt;     U[653]      1.12      0.58      1.12      0.19      2.08  10875.21      1.00\n#&gt;     U[654]     -0.65      0.58     -0.66     -1.59      0.31  12440.61      1.00\n#&gt;     U[655]     -0.39      0.60     -0.39     -1.39      0.58  12919.09      1.00\n#&gt;     U[656]      1.19      0.58      1.19      0.24      2.14  11960.76      1.00\n#&gt;     U[657]     -0.51      0.57     -0.51     -1.44      0.44  11525.22      1.00\n#&gt;     U[658]     -1.68      0.58     -1.68     -2.66     -0.73  10853.40      1.00\n#&gt;     U[659]      0.80      0.60      0.81     -0.23      1.76  13624.81      1.00\n#&gt;     U[660]     -1.45      0.60     -1.45     -2.48     -0.51  13487.80      1.00\n#&gt;     U[661]     -0.30      0.59     -0.30     -1.31      0.63  11555.96      1.00\n#&gt;     U[662]     -0.51      0.58     -0.51     -1.46      0.46  11536.04      1.00\n#&gt;     U[663]     -0.76      0.58     -0.77     -1.74      0.17  12931.27      1.00\n#&gt;     U[664]     -1.15      0.59     -1.16     -2.10     -0.19  11891.74      1.00\n#&gt;     U[665]      0.90      0.58      0.90     -0.03      1.90  14469.25      1.00\n#&gt;     U[666]      0.58      0.59      0.59     -0.40      1.53  13261.87      1.00\n#&gt;     U[667]      0.45      0.58      0.45     -0.49      1.41  13281.13      1.00\n#&gt;     U[668]      0.34      0.59      0.35     -0.67      1.25  13348.07      1.00\n#&gt;     U[669]      0.02      0.59      0.03     -0.96      0.97  14517.16      1.00\n#&gt;     U[670]      0.22      0.59      0.22     -0.74      1.19  12305.68      1.00\n#&gt;     U[671]     -0.54      0.58     -0.53     -1.50      0.41  13543.98      1.00\n#&gt;     U[672]     -0.27      0.59     -0.27     -1.23      0.70  11962.37      1.00\n#&gt;     U[673]     -0.94      0.59     -0.94     -1.92      0.03  12195.95      1.00\n#&gt;     U[674]      0.66      0.58      0.67     -0.33      1.56  11780.59      1.00\n#&gt;     U[675]      0.82      0.61      0.83     -0.11      1.88  12765.54      1.00\n#&gt;     U[676]      0.22      0.59      0.22     -0.72      1.19  11646.20      1.00\n#&gt;     U[677]     -1.35      0.59     -1.35     -2.28     -0.36  12263.00      1.00\n#&gt;     U[678]     -0.69      0.58     -0.69     -1.66      0.24  11258.36      1.00\n#&gt;     U[679]      0.93      0.62      0.94     -0.12      1.93   5521.87      1.00\n#&gt;     U[680]     -0.35      0.60     -0.35     -1.31      0.64  14167.62      1.00\n#&gt;     U[681]     -0.17      0.58     -0.17     -1.12      0.79  13494.86      1.00\n#&gt;     U[682]     -0.33      0.59     -0.33     -1.29      0.65  11924.16      1.00\n#&gt;     U[683]      0.97      0.58      0.96      0.00      1.90  13208.64      1.00\n#&gt;     U[684]     -0.46      0.58     -0.46     -1.39      0.50  11686.21      1.00\n#&gt;     U[685]      0.11      0.58      0.11     -0.84      1.06  12962.51      1.00\n#&gt;     U[686]     -0.55      0.58     -0.56     -1.54      0.38  13831.88      1.00\n#&gt;     U[687]      0.29      0.57      0.29     -0.69      1.19  11764.48      1.00\n#&gt;     U[688]     -0.74      0.60     -0.75     -1.73      0.24  11925.40      1.00\n#&gt;     U[689]      0.02      0.59      0.02     -0.92      0.98  13950.75      1.00\n#&gt;     U[690]      1.12      0.60      1.12      0.13      2.08  12709.14      1.00\n#&gt;     U[691]     -0.16      0.59     -0.16     -1.15      0.77  14532.80      1.00\n#&gt;     U[692]      0.84      0.59      0.84     -0.09      1.84  13909.94      1.00\n#&gt;     U[693]      0.24      0.59      0.25     -0.74      1.20  13988.05      1.00\n#&gt;     U[694]      1.03      0.59      1.04      0.05      1.98  14919.09      1.00\n#&gt;     U[695]     -0.15      0.58     -0.16     -1.11      0.77  12812.88      1.00\n#&gt;     U[696]      0.73      0.60      0.72     -0.23      1.73  11344.47      1.00\n#&gt;     U[697]      0.57      0.60      0.57     -0.40      1.56  12248.67      1.00\n#&gt;     U[698]     -0.40      0.59     -0.41     -1.34      0.57  12979.71      1.00\n#&gt;     U[699]     -0.37      0.59     -0.37     -1.37      0.58  11964.69      1.00\n#&gt;     U[700]      0.73      0.59      0.74     -0.23      1.67  13417.28      1.00\n#&gt;     U[701]      1.50      0.60      1.50      0.49      2.45  12526.30      1.00\n#&gt;     U[702]      0.34      0.58      0.34     -0.59      1.32  11231.11      1.00\n#&gt;     U[703]      0.19      0.58      0.19     -0.77      1.13  12636.94      1.00\n#&gt;     U[704]     -0.10      0.59     -0.10     -1.08      0.85  11403.26      1.00\n#&gt;     U[705]     -1.03      0.58     -1.03     -1.98     -0.05  10596.41      1.00\n#&gt;     U[706]     -1.34      0.60     -1.33     -2.33     -0.37  13211.95      1.00\n#&gt;     U[707]     -0.30      0.59     -0.30     -1.28      0.64  12286.46      1.00\n#&gt;     U[708]      0.49      0.58      0.50     -0.46      1.45  11634.29      1.00\n#&gt;     U[709]     -0.66      0.58     -0.66     -1.60      0.31  11759.81      1.00\n#&gt;     U[710]      0.03      0.59      0.03     -0.92      0.99  11969.39      1.00\n#&gt;     U[711]     -0.33      0.57     -0.33     -1.24      0.64  12516.63      1.00\n#&gt;     U[712]     -0.63      0.58     -0.63     -1.57      0.33  13298.55      1.00\n#&gt;     U[713]      0.51      0.59      0.50     -0.45      1.50  13096.07      1.00\n#&gt;     U[714]      0.08      0.59      0.08     -0.91      1.01  15385.56      1.00\n#&gt;     U[715]     -1.08      0.58     -1.09     -1.99     -0.08  11438.61      1.00\n#&gt;     U[716]      0.50      0.60      0.50     -0.46      1.49  13092.63      1.00\n#&gt;     U[717]      0.57      0.58      0.56     -0.45      1.46  12830.47      1.00\n#&gt;     U[718]     -1.00      0.59     -1.01     -1.97     -0.01  11560.26      1.00\n#&gt;     U[719]      0.98      0.59      0.98      0.04      1.97  14042.18      1.00\n#&gt;     U[720]     -0.17      0.58     -0.16     -1.16      0.72  13736.40      1.00\n#&gt;     U[721]     -0.04      0.60     -0.04     -0.99      0.97  11880.10      1.00\n#&gt;     U[722]     -1.12      0.58     -1.12     -2.08     -0.18  11778.21      1.00\n#&gt;     U[723]      0.87      0.60      0.88     -0.14      1.83  10972.97      1.00\n#&gt;     U[724]      0.48      0.59      0.48     -0.46      1.49  12709.96      1.00\n#&gt;     U[725]     -0.08      0.57     -0.08     -0.99      0.90  14409.50      1.00\n#&gt;     U[726]     -0.05      0.57     -0.06     -0.99      0.89  11637.55      1.00\n#&gt;     U[727]      0.47      0.59      0.48     -0.47      1.47  13168.31      1.00\n#&gt;     U[728]     -0.32      0.61     -0.33     -1.29      0.70  11983.78      1.00\n#&gt;     U[729]     -0.21      0.59     -0.21     -1.21      0.73  14285.12      1.00\n#&gt;     U[730]      0.11      0.57      0.11     -0.83      1.06  12453.43      1.00\n#&gt;     U[731]      1.21      0.59      1.20      0.26      2.19  13479.40      1.00\n#&gt;     U[732]      0.37      0.58      0.36     -0.59      1.31  12953.29      1.00\n#&gt;     U[733]     -0.97      0.58     -0.98     -1.90      0.02  12962.94      1.00\n#&gt;     U[734]     -0.46      0.59     -0.46     -1.35      0.58  13727.35      1.00\n#&gt;     U[735]     -0.37      0.59     -0.36     -1.33      0.62  12303.72      1.00\n#&gt;     U[736]     -0.19      0.59     -0.19     -1.12      0.79  12913.78      1.00\n#&gt;     U[737]      0.35      0.58      0.35     -0.61      1.30  13250.25      1.00\n#&gt;     U[738]      0.26      0.58      0.26     -0.66      1.24  13599.53      1.00\n#&gt;     U[739]     -1.64      0.59     -1.64     -2.56     -0.62  13427.57      1.00\n#&gt;     U[740]     -0.17      0.58     -0.16     -1.09      0.79  14381.48      1.00\n#&gt;     U[741]     -1.03      0.59     -1.03     -2.00     -0.06  14213.48      1.00\n#&gt;     U[742]     -0.80      0.59     -0.81     -1.76      0.18  12132.65      1.00\n#&gt;     U[743]      0.53      0.58      0.53     -0.40      1.50  14150.36      1.00\n#&gt;     U[744]      0.18      0.58      0.17     -0.80      1.10  12122.25      1.00\n#&gt;     U[745]     -1.39      0.59     -1.38     -2.36     -0.43   9659.47      1.00\n#&gt;     U[746]     -1.34      0.58     -1.34     -2.29     -0.39  11830.58      1.00\n#&gt;     U[747]      0.50      0.59      0.50     -0.53      1.40  12857.09      1.00\n#&gt;     U[748]      0.56      0.58      0.57     -0.39      1.53  11945.49      1.00\n#&gt;     U[749]     -0.15      0.58     -0.15     -1.12      0.78  14011.60      1.00\n#&gt;     U[750]     -0.18      0.59     -0.18     -1.15      0.78  12365.52      1.00\n#&gt;     U[751]     -0.03      0.58     -0.03     -1.02      0.90  13040.24      1.00\n#&gt;     U[752]      0.63      0.58      0.64     -0.30      1.61  13639.99      1.00\n#&gt;     U[753]     -0.20      0.59     -0.20     -1.15      0.77   8052.56      1.00\n#&gt;     U[754]     -0.38      0.60     -0.39     -1.36      0.62  12504.20      1.00\n#&gt;     U[755]      1.37      0.59      1.37      0.40      2.35  11863.09      1.00\n#&gt;     U[756]     -1.07      0.59     -1.07     -2.03     -0.08  12919.17      1.00\n#&gt;     U[757]     -1.58      0.59     -1.59     -2.60     -0.65   9069.29      1.00\n#&gt;     U[758]      0.21      0.58      0.20     -0.72      1.19  12854.63      1.00\n#&gt;     U[759]     -0.21      0.59     -0.21     -1.15      0.78  12442.30      1.00\n#&gt;     U[760]      0.26      0.59      0.25     -0.70      1.23  12271.16      1.00\n#&gt;     U[761]     -0.14      0.59     -0.15     -1.09      0.82   9949.49      1.00\n#&gt;     U[762]     -0.44      0.59     -0.43     -1.41      0.53  12624.94      1.00\n#&gt;     U[763]     -0.03      0.60     -0.03     -1.02      0.96  11105.15      1.00\n#&gt;     U[764]      0.89      0.60      0.89     -0.12      1.83  12758.92      1.00\n#&gt;     U[765]     -1.32      0.59     -1.32     -2.26     -0.35  11339.76      1.00\n#&gt;     U[766]     -0.63      0.59     -0.63     -1.60      0.34  10904.68      1.00\n#&gt;     U[767]      0.94      0.59      0.95     -0.01      1.95  10779.38      1.00\n#&gt;     U[768]     -0.29      0.58     -0.30     -1.28      0.62  12933.22      1.00\n#&gt;     U[769]      0.51      0.59      0.52     -0.45      1.46   8965.17      1.00\n#&gt;     U[770]     -1.06      0.57     -1.06     -2.03     -0.15  12241.49      1.00\n#&gt;     U[771]     -0.38      0.60     -0.37     -1.37      0.60  11181.49      1.00\n#&gt;     U[772]     -0.76      0.59     -0.76     -1.74      0.21  10374.46      1.00\n#&gt;     U[773]      0.56      0.58      0.56     -0.41      1.50  14334.64      1.00\n#&gt;     U[774]     -1.19      0.58     -1.19     -2.14     -0.22  12683.36      1.00\n#&gt;     U[775]     -0.14      0.58     -0.14     -1.13      0.81  12980.08      1.00\n#&gt;     U[776]      0.54      0.60      0.54     -0.42      1.54  11477.96      1.00\n#&gt;     U[777]     -0.87      0.60     -0.87     -1.82      0.12  12900.75      1.00\n#&gt;     U[778]      0.46      0.60      0.47     -0.49      1.46  11131.66      1.00\n#&gt;     U[779]     -0.04      0.59     -0.05     -1.01      0.90  13731.50      1.00\n#&gt;     U[780]     -0.65      0.59     -0.65     -1.63      0.27  12139.39      1.00\n#&gt;     U[781]     -0.15      0.59     -0.15     -1.13      0.82  11249.00      1.00\n#&gt;     U[782]     -0.86      0.59     -0.87     -1.82      0.12  10948.69      1.00\n#&gt;     U[783]     -1.08      0.58     -1.08     -2.03     -0.13  10607.14      1.00\n#&gt;     U[784]      0.41      0.59      0.40     -0.52      1.39  13336.31      1.00\n#&gt;     U[785]     -0.27      0.59     -0.28     -1.19      0.75  11995.32      1.00\n#&gt;     U[786]      0.06      0.58      0.07     -0.90      1.02  12808.13      1.00\n#&gt;     U[787]      1.24      0.60      1.25      0.29      2.26  14210.87      1.00\n#&gt;     U[788]     -1.74      0.59     -1.74     -2.71     -0.78  14278.58      1.00\n#&gt;     U[789]     -0.39      0.58     -0.39     -1.35      0.55  11918.33      1.00\n#&gt;     U[790]     -0.16      0.59     -0.15     -1.14      0.80  10797.06      1.00\n#&gt;     U[791]     -0.36      0.59     -0.36     -1.35      0.58  13674.96      1.00\n#&gt;     U[792]     -0.39      0.58     -0.39     -1.37      0.54  13495.43      1.00\n#&gt;     U[793]      0.60      0.59      0.60     -0.37      1.57  10249.55      1.00\n#&gt;     U[794]      0.81      0.60      0.81     -0.23      1.74  10047.28      1.00\n#&gt;     U[795]      1.59      0.59      1.60      0.63      2.57  12656.69      1.00\n#&gt;     U[796]      0.12      0.57      0.11     -0.81      1.07  14759.71      1.00\n#&gt;     U[797]     -0.59      0.59     -0.60     -1.55      0.39  15234.20      1.00\n#&gt;     U[798]     -0.52      0.58     -0.52     -1.45      0.47  11961.21      1.00\n#&gt;     U[799]     -0.29      0.59     -0.28     -1.27      0.65  11363.52      1.00\n#&gt;     U[800]     -0.13      0.60     -0.13     -1.12      0.86  11855.32      1.00\n#&gt;     U[801]     -0.33      0.58     -0.33     -1.29      0.62  15013.59      1.00\n#&gt;     U[802]     -1.15      0.60     -1.14     -2.17     -0.18  11781.22      1.00\n#&gt;     U[803]      0.63      0.59      0.64     -0.33      1.59  13116.52      1.00\n#&gt;     U[804]      0.20      0.59      0.21     -0.77      1.14  11598.55      1.00\n#&gt;     U[805]      0.06      0.58      0.06     -0.87      1.02  13977.51      1.00\n#&gt;     U[806]      0.37      0.59      0.36     -0.61      1.31  10452.07      1.00\n#&gt;     U[807]      0.56      0.57      0.56     -0.36      1.54  14833.72      1.00\n#&gt;     U[808]     -1.02      0.59     -1.02     -1.99     -0.07  13223.11      1.00\n#&gt;     U[809]     -1.46      0.59     -1.46     -2.43     -0.50  11971.97      1.00\n#&gt;     U[810]      0.80      0.59      0.81     -0.17      1.78   9773.12      1.00\n#&gt;     U[811]      0.58      0.59      0.58     -0.36      1.54  13153.27      1.00\n#&gt;     U[812]      1.24      0.58      1.24      0.27      2.19  11276.10      1.00\n#&gt;     U[813]     -0.71      0.58     -0.72     -1.69      0.22  12363.30      1.00\n#&gt;     U[814]      0.24      0.57      0.24     -0.70      1.18  13283.51      1.00\n#&gt;     U[815]      0.65      0.60      0.65     -0.30      1.64  11343.30      1.00\n#&gt;     U[816]      0.18      0.58      0.18     -0.81      1.09  13008.89      1.00\n#&gt;     U[817]     -0.22      0.59     -0.21     -1.12      0.80  13729.62      1.00\n#&gt;     U[818]      0.23      0.58      0.24     -0.76      1.16  11536.73      1.00\n#&gt;     U[819]     -0.20      0.58     -0.19     -1.16      0.76  13636.70      1.00\n#&gt;     U[820]      0.67      0.58      0.68     -0.25      1.64  12038.22      1.00\n#&gt;     U[821]      0.01      0.57      0.02     -0.93      0.93  12973.23      1.00\n#&gt;     U[822]     -0.20      0.57     -0.20     -1.10      0.75  13588.41      1.00\n#&gt;     U[823]      0.67      0.60      0.67     -0.29      1.67  12961.93      1.00\n#&gt;     U[824]     -0.05      0.58     -0.05     -1.02      0.87  11440.29      1.00\n#&gt;     U[825]     -0.99      0.59     -0.99     -1.99     -0.05   9902.70      1.00\n#&gt;     U[826]     -1.12      0.60     -1.12     -2.12     -0.14  12517.76      1.00\n#&gt;     U[827]     -1.63      0.58     -1.63     -2.59     -0.68  11952.17      1.00\n#&gt;     U[828]      1.56      0.59      1.56      0.59      2.51  13664.23      1.00\n#&gt;     U[829]      0.99      0.58      0.99      0.02      1.95  12636.73      1.00\n#&gt;     U[830]     -1.23      0.59     -1.23     -2.20     -0.28  10209.23      1.00\n#&gt;     U[831]     -0.77      0.59     -0.77     -1.73      0.20  15087.36      1.00\n#&gt;     U[832]      0.63      0.59      0.63     -0.38      1.57  14691.65      1.00\n#&gt;     U[833]      0.03      0.58      0.03     -0.89      1.01  14178.70      1.00\n#&gt;     U[834]     -0.13      0.59     -0.14     -1.08      0.86  12249.07      1.00\n#&gt;     U[835]      0.59      0.59      0.59     -0.43      1.51  13578.09      1.00\n#&gt;     U[836]      0.37      0.60      0.36     -0.57      1.38  13043.05      1.00\n#&gt;     U[837]      0.31      0.57      0.31     -0.61      1.26  11478.04      1.00\n#&gt;     U[838]     -0.82      0.59     -0.82     -1.75      0.18  13747.26      1.00\n#&gt;     U[839]      1.29      0.58      1.29      0.36      2.23  14354.43      1.00\n#&gt;     U[840]     -0.10      0.59     -0.09     -1.06      0.87  13892.96      1.00\n#&gt;     U[841]     -1.54      0.60     -1.54     -2.51     -0.54  13262.77      1.00\n#&gt;     U[842]      0.13      0.59      0.12     -0.87      1.06  12080.94      1.00\n#&gt;     U[843]     -0.17      0.59     -0.17     -1.15      0.79  13723.58      1.00\n#&gt;     U[844]      0.32      0.59      0.32     -0.64      1.29  14400.19      1.00\n#&gt;     U[845]      0.04      0.58      0.04     -0.91      0.99  12070.64      1.00\n#&gt;     U[846]     -0.16      0.59     -0.16     -1.14      0.82   8626.14      1.00\n#&gt;     U[847]      0.12      0.58      0.12     -0.80      1.10  10268.37      1.00\n#&gt;     U[848]     -0.52      0.58     -0.52     -1.46      0.42  11188.02      1.00\n#&gt;     U[849]      0.39      0.60      0.39     -0.61      1.37  10431.49      1.00\n#&gt;     U[850]      1.52      0.59      1.52      0.51      2.47  11547.84      1.00\n#&gt;     U[851]     -1.97      0.60     -1.97     -2.98     -1.02  10995.34      1.00\n#&gt;     U[852]      0.01      0.58      0.01     -0.95      0.96  13027.37      1.00\n#&gt;     U[853]     -1.65      0.59     -1.65     -2.59     -0.65  15045.21      1.00\n#&gt;     U[854]      0.18      0.60      0.18     -0.79      1.16  14380.81      1.00\n#&gt;     U[855]     -0.75      0.58     -0.74     -1.71      0.21  14646.04      1.00\n#&gt;     U[856]     -0.54      0.59     -0.55     -1.50      0.44  12638.16      1.00\n#&gt;     U[857]     -0.28      0.59     -0.28     -1.26      0.67  14096.09      1.00\n#&gt;     U[858]     -0.26      0.58     -0.26     -1.22      0.69  12444.06      1.00\n#&gt;     U[859]      0.53      0.60      0.53     -0.44      1.55  14019.03      1.00\n#&gt;     U[860]     -0.87      0.59     -0.88     -1.83      0.10  11203.44      1.00\n#&gt;     U[861]     -0.70      0.59     -0.72     -1.63      0.29  11127.68      1.00\n#&gt;     U[862]      1.20      0.58      1.19      0.27      2.19  13078.93      1.00\n#&gt;     U[863]      1.24      0.58      1.23      0.29      2.20  13047.31      1.00\n#&gt;     U[864]      0.43      0.58      0.42     -0.52      1.36  11825.89      1.00\n#&gt;     U[865]     -0.03      0.58     -0.02     -0.96      0.96  14911.16      1.00\n#&gt;     U[866]      0.75      0.59      0.75     -0.26      1.68  11577.88      1.00\n#&gt;     U[867]     -0.15      0.58     -0.15     -1.12      0.79  12893.93      1.00\n#&gt;     U[868]      0.08      0.58      0.08     -0.90      1.02  12283.35      1.00\n#&gt;     U[869]      0.61      0.59      0.61     -0.32      1.61  14164.51      1.00\n#&gt;     U[870]     -1.02      0.60     -1.03     -2.05     -0.06  10982.40      1.00\n#&gt;     U[871]      0.25      0.60      0.25     -0.75      1.21  12148.07      1.00\n#&gt;     U[872]     -0.14      0.58     -0.14     -1.07      0.81  14142.04      1.00\n#&gt;     U[873]      1.05      0.58      1.05      0.12      2.03  12686.14      1.00\n#&gt;     U[874]     -0.58      0.59     -0.58     -1.51      0.41  13148.46      1.00\n#&gt;     U[875]      0.39      0.60      0.39     -0.57      1.37  12107.63      1.00\n#&gt;     U[876]     -0.77      0.58     -0.77     -1.73      0.19  13604.79      1.00\n#&gt;     U[877]      0.39      0.59      0.39     -0.55      1.37  11225.53      1.00\n#&gt;     U[878]      1.19      0.59      1.19      0.27      2.19  12859.41      1.00\n#&gt;     U[879]     -0.78      0.59     -0.78     -1.79      0.14  12561.92      1.00\n#&gt;     U[880]     -0.23      0.59     -0.23     -1.20      0.71  11720.42      1.00\n#&gt;     U[881]      0.00      0.59      0.00     -0.98      0.95  10783.17      1.00\n#&gt;     U[882]     -0.65      0.60     -0.66     -1.64      0.29  12781.39      1.00\n#&gt;     U[883]      1.08      0.59      1.09      0.14      2.07  11271.23      1.00\n#&gt;     U[884]     -0.82      0.59     -0.82     -1.79      0.11  10766.62      1.00\n#&gt;     U[885]      0.61      0.59      0.61     -0.34      1.59  12671.88      1.00\n#&gt;     U[886]     -0.19      0.58     -0.20     -1.13      0.75  12337.52      1.00\n#&gt;     U[887]      0.38      0.59      0.37     -0.60      1.34  12451.74      1.00\n#&gt;     U[888]     -0.49      0.59     -0.48     -1.48      0.47  12131.36      1.00\n#&gt;     U[889]     -0.23      0.59     -0.23     -1.19      0.73  11902.83      1.00\n#&gt;     U[890]      0.18      0.58      0.17     -0.75      1.18  14856.25      1.00\n#&gt;     U[891]     -0.08      0.59     -0.07     -1.05      0.88  15044.39      1.00\n#&gt;     U[892]      1.14      0.58      1.15      0.16      2.09   9600.08      1.00\n#&gt;     U[893]      0.22      0.58      0.22     -0.71      1.19  13299.69      1.00\n#&gt;     U[894]     -1.91      0.60     -1.92     -2.89     -0.93  11706.63      1.00\n#&gt;     U[895]     -1.62      0.59     -1.61     -2.61     -0.68  11747.83      1.00\n#&gt;     U[896]      0.04      0.59      0.03     -0.96      0.97  10040.22      1.00\n#&gt;     U[897]     -0.36      0.58     -0.37     -1.33      0.57  11797.92      1.00\n#&gt;     U[898]     -0.64      0.59     -0.64     -1.67      0.26  14673.66      1.00\n#&gt;     U[899]      0.26      0.59      0.26     -0.72      1.22  12863.56      1.00\n#&gt;     U[900]     -0.96      0.59     -0.97     -1.94     -0.00  12851.43      1.00\n#&gt;     U[901]      0.10      0.60      0.10     -0.89      1.08  13087.47      1.00\n#&gt;     U[902]     -0.79      0.60     -0.79     -1.80      0.16  12059.18      1.00\n#&gt;     U[903]     -0.59      0.58     -0.59     -1.48      0.42  11874.58      1.00\n#&gt;     U[904]      0.97      0.58      0.97      0.01      1.90  13192.80      1.00\n#&gt;     U[905]      0.10      0.58      0.10     -0.88      1.04  14119.12      1.00\n#&gt;     U[906]     -0.47      0.60     -0.47     -1.44      0.52  12169.03      1.00\n#&gt;     U[907]     -0.41      0.60     -0.41     -1.37      0.59  12734.92      1.00\n#&gt;     U[908]     -0.22      0.60     -0.21     -1.20      0.79  13882.34      1.00\n#&gt;     U[909]      0.47      0.59      0.47     -0.49      1.42  12193.58      1.00\n#&gt;     U[910]      0.35      0.59      0.35     -0.63      1.30  13537.50      1.00\n#&gt;     U[911]     -1.06      0.58     -1.05     -2.03     -0.14  12435.29      1.00\n#&gt;     U[912]      0.61      0.58      0.62     -0.35      1.56  11625.48      1.00\n#&gt;     U[913]      0.16      0.59      0.16     -0.79      1.14  12443.07      1.00\n#&gt;     U[914]     -0.58      0.59     -0.59     -1.58      0.35  13396.17      1.00\n#&gt;     U[915]     -0.29      0.60     -0.30     -1.30      0.67   9966.67      1.00\n#&gt;     U[916]      0.97      0.58      0.97      0.03      1.92  11737.92      1.00\n#&gt;     U[917]     -0.75      0.60     -0.75     -1.72      0.25  11770.67      1.00\n#&gt;     U[918]      0.54      0.59      0.54     -0.46      1.52  15692.14      1.00\n#&gt;     U[919]     -1.00      0.59     -1.01     -1.94      0.01   8610.11      1.00\n#&gt;     U[920]      0.19      0.58      0.19     -0.75      1.18  13303.70      1.00\n#&gt;     U[921]      0.96      0.58      0.96      0.03      1.93  14315.07      1.00\n#&gt;     U[922]      0.20      0.59      0.20     -0.78      1.14  10544.62      1.00\n#&gt;     U[923]      1.44      0.58      1.43      0.49      2.42  12403.20      1.00\n#&gt;     U[924]      0.70      0.58      0.70     -0.20      1.69  14658.35      1.00\n#&gt;     U[925]      1.16      0.60      1.16      0.18      2.15  11994.38      1.00\n#&gt;     U[926]      0.85      0.58      0.85     -0.09      1.82  15237.50      1.00\n#&gt;     U[927]     -0.76      0.59     -0.76     -1.71      0.21  10134.26      1.00\n#&gt;     U[928]      1.05      0.59      1.06      0.06      2.01  11215.76      1.00\n#&gt;     U[929]     -0.24      0.59     -0.23     -1.21      0.74   9164.47      1.00\n#&gt;     U[930]      0.54      0.58      0.53     -0.42      1.49  11564.10      1.00\n#&gt;     U[931]     -0.67      0.59     -0.67     -1.60      0.33  13877.97      1.00\n#&gt;     U[932]      0.77      0.59      0.77     -0.24      1.69  12011.27      1.00\n#&gt;     U[933]      0.23      0.59      0.23     -0.75      1.17  13068.34      1.00\n#&gt;     U[934]      1.58      0.58      1.58      0.64      2.53  12223.93      1.00\n#&gt;     U[935]     -0.24      0.60     -0.24     -1.17      0.78  11304.33      1.00\n#&gt;     U[936]      0.30      0.60      0.31     -0.67      1.29  12863.41      1.00\n#&gt;     U[937]     -0.74      0.59     -0.74     -1.68      0.25  12982.29      1.00\n#&gt;     U[938]     -0.02      0.59     -0.02     -0.97      0.95  13541.41      1.00\n#&gt;     U[939]     -0.50      0.59     -0.50     -1.46      0.48  12883.27      1.00\n#&gt;     U[940]     -0.81      0.58     -0.81     -1.76      0.14  13165.92      1.00\n#&gt;     U[941]     -0.48      0.58     -0.49     -1.41      0.48  13979.24      1.00\n#&gt;     U[942]      1.78      0.58      1.79      0.80      2.73  14024.36      1.00\n#&gt;     U[943]      0.88      0.60      0.88     -0.12      1.84  11993.43      1.00\n#&gt;     U[944]      0.74      0.60      0.74     -0.26      1.70  14228.20      1.00\n#&gt;     U[945]      0.35      0.59      0.35     -0.57      1.34  11251.19      1.00\n#&gt;     U[946]     -0.39      0.58     -0.39     -1.36      0.53  12276.39      1.00\n#&gt;     U[947]      0.48      0.59      0.48     -0.53      1.40  11715.89      1.00\n#&gt;     U[948]      1.18      0.59      1.18      0.26      2.19  11371.76      1.00\n#&gt;     U[949]      1.73      0.60      1.73      0.71      2.66  12723.34      1.00\n#&gt;     U[950]      2.82      0.60      2.82      1.88      3.83   8890.82      1.00\n#&gt;     U[951]     -0.73      0.58     -0.73     -1.68      0.22  11916.12      1.00\n#&gt;     U[952]      1.95      0.58      1.96      1.00      2.89  13048.36      1.00\n#&gt;     U[953]     -0.33      0.58     -0.34     -1.26      0.64  12824.61      1.00\n#&gt;     U[954]      1.05      0.57      1.05      0.08      1.96  11992.68      1.00\n#&gt;     U[955]     -0.34      0.59     -0.33     -1.28      0.66  13954.68      1.00\n#&gt;     U[956]      0.23      0.58      0.23     -0.71      1.18  13758.78      1.00\n#&gt;     U[957]     -1.48      0.58     -1.49     -2.45     -0.54  11316.99      1.00\n#&gt;     U[958]     -0.58      0.60     -0.58     -1.58      0.38  11935.67      1.00\n#&gt;     U[959]     -0.33      0.58     -0.34     -1.24      0.64  12510.94      1.00\n#&gt;     U[960]     -0.19      0.58     -0.19     -1.14      0.76  12086.79      1.00\n#&gt;     U[961]      0.71      0.61      0.71     -0.29      1.71  13449.14      1.00\n#&gt;     U[962]     -0.78      0.59     -0.77     -1.70      0.25  12452.61      1.00\n#&gt;     U[963]      0.35      0.60      0.34     -0.65      1.31  12980.81      1.00\n#&gt;     U[964]     -1.60      0.59     -1.61     -2.54     -0.61  11376.19      1.00\n#&gt;     U[965]      0.25      0.59      0.25     -0.76      1.19  13332.58      1.00\n#&gt;     U[966]      0.86      0.58      0.86     -0.11      1.80  11166.35      1.00\n#&gt;     U[967]      0.33      0.59      0.33     -0.62      1.31  11491.01      1.00\n#&gt;     U[968]     -0.17      0.58     -0.16     -1.12      0.77  11585.04      1.00\n#&gt;     U[969]      0.43      0.58      0.43     -0.52      1.39  13740.37      1.00\n#&gt;     U[970]      0.86      0.58      0.85     -0.08      1.83  11582.33      1.00\n#&gt;     U[971]      0.53      0.59      0.53     -0.45      1.50   9096.17      1.00\n#&gt;     U[972]     -1.02      0.60     -1.02     -1.98     -0.03  12193.37      1.00\n#&gt;     U[973]     -0.28      0.59     -0.28     -1.20      0.72   9240.19      1.00\n#&gt;     U[974]      0.71      0.59      0.71     -0.20      1.73  12943.65      1.00\n#&gt;     U[975]     -0.50      0.58     -0.50     -1.45      0.45  14639.88      1.00\n#&gt;     U[976]      0.33      0.59      0.34     -0.67      1.26  12165.45      1.00\n#&gt;     U[977]      0.41      0.58      0.41     -0.52      1.40  10229.29      1.00\n#&gt;     U[978]     -0.47      0.58     -0.47     -1.45      0.47  14130.64      1.00\n#&gt;     U[979]      0.63      0.59      0.64     -0.36      1.58  11353.03      1.00\n#&gt;     U[980]     -0.39      0.58     -0.38     -1.37      0.54  13252.09      1.00\n#&gt;     U[981]     -0.67      0.58     -0.67     -1.60      0.31  13049.74      1.00\n#&gt;     U[982]      1.59      0.59      1.59      0.57      2.50  13283.70      1.00\n#&gt;     U[983]      0.54      0.57      0.54     -0.36      1.50  12470.61      1.00\n#&gt;     U[984]      0.57      0.59      0.56     -0.43      1.51  14576.05      1.00\n#&gt;     U[985]      0.71      0.59      0.72     -0.27      1.64  10617.97      1.00\n#&gt;     U[986]      1.17      0.60      1.17      0.20      2.16  11287.34      1.00\n#&gt;     U[987]      1.21      0.58      1.21      0.24      2.13  14315.95      1.00\n#&gt;     U[988]     -0.04      0.59     -0.05     -1.00      0.94  14458.60      1.00\n#&gt;     U[989]     -1.20      0.57     -1.21     -2.10     -0.24  14147.97      1.00\n#&gt;     U[990]     -0.09      0.58     -0.10     -1.02      0.88  12347.97      1.00\n#&gt;     U[991]      0.10      0.59      0.09     -0.87      1.08  11560.09      1.00\n#&gt;     U[992]      1.32      0.57      1.32      0.42      2.31  12121.23      1.00\n#&gt;     U[993]      0.15      0.58      0.15     -0.78      1.15  13128.86      1.00\n#&gt;     U[994]     -1.39      0.59     -1.40     -2.33     -0.37   9881.38      1.00\n#&gt;     U[995]      2.40      0.60      2.40      1.43      3.37  10272.16      1.00\n#&gt;     U[996]      0.65      0.59      0.66     -0.33      1.62  15164.67      1.00\n#&gt;     U[997]      0.23      0.59      0.23     -0.71      1.24  12960.72      1.00\n#&gt;     U[998]      0.81      0.59      0.82     -0.15      1.78  14299.27      1.00\n#&gt;     U[999]      1.18      0.58      1.18      0.26      2.17  12518.06      1.00\n#&gt;         a1      0.08      0.06      0.08     -0.02      0.18   2966.95      1.00\n#&gt;         a2      0.05      0.08      0.05     -0.07      0.17   1249.36      1.00\n#&gt;         b1      1.95      0.09      1.95      1.80      2.10   1753.96      1.00\n#&gt;         b2      1.97      0.08      1.97      1.84      2.09   6711.91      1.00\n#&gt;          k      1.00      0.06      1.00      0.90      1.10    602.40      1.00\n#&gt;          m     -0.00      0.05     -0.00     -0.08      0.07    598.44      1.00\n#&gt;          p      0.48      0.01      0.48      0.46      0.50  12217.26      1.00\n#&gt;     sigma1      1.06      0.06      1.06      0.96      1.16    604.62      1.00\n#&gt;     sigma2      0.98      0.04      0.98      0.92      1.04   2708.98      1.00\n#&gt; \n#&gt; Number of divergences: 0"
  },
  {
    "objectID": "2023/pyrotecnia-1.html#efecto-de-m-sobre-d",
    "href": "2023/pyrotecnia-1.html#efecto-de-m-sobre-d",
    "title": "Pyrotecnia. Full luxury bayes con numpyro",
    "section": "Efecto de M sobre D",
    "text": "Efecto de M sobre D\nComo hemos simulado sabemos que el efecto de D sobre M es 0. Y que Pearl nos advierte que para estimarlo habría que ajustar por U\nEfectivamente, sin ajustar tenemos\n\nlm(D ~ M)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ M)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            M  \n#&gt;      0.7108       0.2930\n\nY haciendo trampas , ajustando por U\n\nlm(D ~ M + U)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ M + U)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            M            U  \n#&gt;     1.05524     -0.05078      1.03985\n\nPero en la vida real no podemos ajustar por U puesto que no la hemos observado. Sin embargo al incorporar esa variable no observada dentro de un modelo generativo, si que podemos tener en cuenta que existe aunque no la hayamos observado.\nVeamos la posterior que nos da numpyro\nSe trata de la distribución de m\n\nres = mcmc.get_samples()\n\nm_posterior = res['m']\nnp.quantile(m_posterior, q = [0, 0.25, 0.5, 0.75, 1])\n#&gt; array([-0.25327477, -0.03099037, -0.00064275,  0.02862958,  0.17461409])\n\nY vemos que efectivamente la distribución de m está centrada en 0.\n\nsn.kdeplot(m_posterior)"
  },
  {
    "objectID": "2023/pyrotecnia-1.html#efecto-de-b1-sobre-d",
    "href": "2023/pyrotecnia-1.html#efecto-de-b1-sobre-d",
    "title": "Pyrotecnia. Full luxury bayes con numpyro",
    "section": "Efecto de B1 sobre D",
    "text": "Efecto de B1 sobre D\nPara ver el efecto de B1 sobre D podemos hacer tal y como dice Richard una intervención , de hecho, el efecto causal no es más que eso. Para eso, simplemente utilizo las posterioris y vamos siguiendo las flechas del DAG. y utilizar las expresiones de mu1 y mu2 que puse en el modelo. También fijamos B2 = 0 en ambos casos\n\n\n\n\n\n\nTip\n\n\n\nEn este caso al ser un modelo lineal el efecto de B1 sobre D, se puede estimar simplemente multiplicando la posterior de B1 por la posterior de M, pero en modelos no lineales no se puede\n\n\n\nPara B1 = 0 , B2= 0\n\nM_B1_0 = res['a1'] + res['b1']* 0 # pongo el 0 para que quede claro qeu B1 = 0 \n\ny ahora utilzo la posteriori obtenida de M cuando B1 es igual a 0 para obtener la de D\n\nD_B1_0 =  res['a2'] + res['b2'] * 0  + res['m']*M_B1_0 \n\n\n\nPara B1 = 1, B2= 0\n\nM_B1_1 = res['a1'] + res['b1']* 1 # pongo el 0 para que quede claro qeu B1 = 0 \n\n\nD_B1_1 =  res['a2'] + res['b2'] * 0  + res['m']*M_B1_1 \n\nY el efecto causal de B1 sobre M sería simplemente restar esas dos posterioris.\n\n\n\nd_D_B1 = D_B1_1 - D_B1_0\nnp.quantile(d_D_B1, q = [0, 0.25, 0.5, 0.75, 1])\n#&gt; array([-0.45246571, -0.05934101, -0.00127174,  0.05669086,  0.3790037 ])\n\nsn.kdeplot(d_D_B1)"
  },
  {
    "objectID": "2023/pyrotecnia-1.html#notas-finales",
    "href": "2023/pyrotecnia-1.html#notas-finales",
    "title": "Pyrotecnia. Full luxury bayes con numpyro",
    "section": "Notas finales",
    "text": "Notas finales\nEl uso de numpyro es relativamente sencillo y permite expresar de forma fácil los modelos generativos. Además, es muy rápido y se puede usar con GPu’s . En mi opinión estamos ante un avance importante en la computación de modelos bayesianos.\nHasta la próxima entrada de pyrotecnia . Feliz verano !!"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html",
    "href": "2022/02/06/pluralista/index.html",
    "title": "Pluralista",
    "section": "",
    "text": "Ando viendo los vídeos de Richard McElreath , Statistical Rethinking 2022 y ciertamente me están gustando mucho. En la segunda edición de su libro hace hincapié en temas de inferencia causal. Cuenta bastante bien todo el tema de los “confounders”, “forks”, “colliders” y demás. Además lo hace simulando datos, por lo que entiende todo de forma muy sencilla. Un par de conceptos que me han llamado la atención son por ejemplo cuando dice que condicionar por una variable no significa lo mismo en un modelo de regresión al uso que en uno bayesiano, en el segundo caso significa incluir esa variable en la distribución conjunta. Esto permite por ejemplo que bajo el marco de un modelo bayesiano se pueda condicionar incluso por un “collider” cosa que los entendidos de la inferencia causal prohíben expresamente pues eso abre un camino no causal en el DAG definido.\nSegún la RAE , pluralismo significa\ny en los videos se toma dicha postura, por ejemplo, se especifica el modelo teórico utilizando los diagramas causales y el Back door criterio para ver sobre qué variables hay que condicionar o no , para ver el efecto total de X sobre Y o para estimar el efecto directo.\nHay un ejemplo muy bueno en este post de Richard.\nNota: Este post es simplemente para entender un poco el post de Richard, el mérito es totalmente de él.\nBásicamente es una situación dónde se quiere estimar el efecto que tiene sobre el número de hijos que tiene una mujer, el número de hijos que tuvo su madre. En el diagrama causal también se indica la influencia que tiene el orden de nacimiento de de la madre y de la hija.\nDiagrama causal:\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(patchwork)\n\ng &lt;- dagitty(\"dag{ \n  M -&gt; D ;\n  B2 -&gt; D;\n  B1 -&gt; M;\n  U -&gt; M;\n  U -&gt; D\n }\")\n\n\ncoords &lt;-  \n  list(\n  x = c(B1 = 1, M = 2,  U = 3.5, D = 5, B2 = 6),\n  y = c(B1 = 0, M = 0, U = 1, D = 0, B2 = 0)\n)\n\ncoordinates(g) &lt;- coords\n\nggdag(g) + \n  theme_void()\nSi queremos estimar el efecto global o el directo de M sobre D, habría que condicionar por U (siguiendo el backdoor criterio), y al ser no observable, no se puede estimar.\nadjustmentSets(g, exposure = \"M\", outcome = \"D\", effect = \"total\"  )\n#&gt; { U }\nadjustmentSets(g, exposure = \"M\", outcome = \"D\", effect = \"direct\"  )\n#&gt; { U }\nggdag_adjustment_set(g, exposure = \"M\", outcome = \"D\", effect = \"direct\")\n¿Cómo podemos “estimar” el efecto de M sobre D dado que no podemos condicionar (en el sentido clásico) sobre U? .\nRichard propone lo que el llama “full luxury bayesian” que consiste en estimar a la vez todo el DAG y luego generar simulaciones usando la distribución conjunta obtenida para medir el efecto de la “intervención” y poder obtener el efecto causal.\nNótese que cuando en el DAG las relaciones se pueden expresar como modelos lineales, se puede estimar todo el DAG usando técnicas como los modelos de ecuaciones estructurales o el path analysis."
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#simulación",
    "href": "2022/02/06/pluralista/index.html#simulación",
    "title": "Pluralista",
    "section": "Simulación",
    "text": "Simulación\nSimulamos unos datos de forma qué vamos a conocer la “verdad” de la relaciones entre variables, que para eso simulamos.\n\n\nset.seed(1908)\nN &lt;- 1000 # número de pares, 1000 madres y 1000 hijas\n\n\nU &lt;- rnorm(N,0,1) # Simulamos el confounder\n\n# orden de nacimiento y \nB1 &lt;- rbinom(N,size=1,prob=0.5)  # 50% de madres nacieeron en primer lugar\nM &lt;- rnorm( N , 2 * B1 + U )\n\nB2 &lt;- rbinom(N,size=1,prob=0.5) # 50% son las primogénitas\nD &lt;- rnorm( N , 2  *B2 + U + 0 * M )\n\nEn esta simulación se ha forzado que el efecto del número de hijos de la madre sobre el núemro de hijos de la hija sea nulo. Por tanto sabemos que el efecto de M sobre D es 0..\nSi hacemos un modelo sin condicionar, tenemos sesgo\n\nlm(D ~ M)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ M)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            M  \n#&gt;      0.7108       0.2930\n\nCondicionando por B1 también, de hecho tenemos la situación de amplificación del sesgo\n\nlm(D ~ M + B1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ M + B1)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            M           B1  \n#&gt;      1.0356       0.4606      -1.0441\n\n\nlm(D ~ M + B1 +B2 )\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ M + B1 + B2)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            M           B1           B2  \n#&gt;    -0.01621      0.46913     -0.91307      2.01487\n\n\nlm(D ~ M + B2)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ M + B2)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            M           B2  \n#&gt;     -0.3204       0.3231       2.0550\n\nEn esta situación, no podemos estimar el efecto de M sobre D utilizando un solo modelo.\nUna forma de estimar el efecto de M sobre D es tirar de path analysis, que en este caso se puede al ser las relaciones lineales.\nSea:\n\nb: Efecto de B1 sobre M\nm: Efecto de M sobre D\n\nSe tiene que\n\\[Cov(B1, D ) = b\\cdot m \\cdot Var(B1)\\] Y como\n\\[b = \\dfrac{Cov(B1,M)}{Var(B1)} \\]\nPodemos estimar \\(m\\) como\n\\[m = \\dfrac{Cov(B1,D)}{b \\cdot Var(B1)} = \\dfrac{Cov(B1,D)}{Cov(B1,M)} \\] Y\n\n(m_hat = cov(B1,D) / cov(B1,M))\n#&gt; [1] -0.0563039\n\ny esta estimación está menos sesgada, antes era del orden de 0.1 o 0.2 y ahora la estimación es del orden 0.01. Pero con esta estimación no tenemos información de su distribución sino sólo de esta estimación puntual. Y si las relaciones no fueran lineales no podría usarse, en cambio la siguiente aproximación si funciona"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#full-luxury-bayes",
    "href": "2022/02/06/pluralista/index.html#full-luxury-bayes",
    "title": "Pluralista",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nUtilizamos la librería de Richard rethinking y también cmdstanr para expresar el modelo causal completo y ajustarlo con Stan.\nAhora estimamos el DAG completo, aquí es dónde es diferente de la aproximación causal de Pearl, de esta forma podemos “condicionar” incluso por los colliders, porque condicionar en este marco significa meter esa información dentro de la distribución conjunta.\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\n# No metemos U al ser no observable\ndat &lt;- list(\n  N = N,\n  M = M,\n  D = D,\n  B1 = B1,\n  B2 = B2\n)\nset.seed(1908)\n\nflbi &lt;- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu &lt;- a1 + b*B1 + k*U[i],\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu &lt;- a2 + b*B2 + m*M + k*U[i],\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n    # unmeasured confound\n    vector[N]:U ~ normal(0,1),\n    # priors\n    c(a1,a2,b,m) ~ normal( 0 , 0.5 ),\n    c(k,sigma,tau) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n#&gt; Running MCMC with 4 parallel chains, with 1 thread(s) per chain...\n#&gt; \n#&gt; Chain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \n#&gt; Chain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n#&gt; Chain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n#&gt; Chain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \n#&gt; Chain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \n#&gt; Chain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \n#&gt; Chain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \n#&gt; Chain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \n#&gt; Chain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \n#&gt; Chain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \n#&gt; Chain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#&gt; Chain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \n#&gt; Chain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#&gt; Chain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#&gt; Chain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#&gt; Chain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#&gt; Chain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#&gt; Chain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#&gt; Chain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#&gt; Chain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#&gt; Chain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#&gt; Chain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#&gt; Chain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#&gt; Chain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#&gt; Chain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#&gt; Chain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#&gt; Chain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#&gt; Chain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#&gt; Chain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#&gt; Chain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#&gt; Chain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#&gt; Chain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#&gt; Chain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#&gt; Chain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#&gt; Chain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#&gt; Chain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#&gt; Chain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#&gt; Chain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#&gt; Chain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#&gt; Chain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#&gt; Chain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#&gt; Chain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#&gt; Chain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#&gt; Chain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#&gt; Chain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#&gt; Chain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#&gt; Chain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#&gt; Chain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#&gt; Chain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#&gt; Chain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#&gt; Chain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#&gt; Chain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#&gt; Chain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#&gt; Chain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#&gt; Chain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#&gt; Chain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#&gt; Chain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#&gt; Chain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#&gt; Chain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#&gt; Chain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#&gt; Chain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#&gt; Chain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#&gt; Chain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#&gt; Chain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#&gt; Chain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#&gt; Chain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#&gt; Chain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#&gt; Chain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#&gt; Chain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#&gt; Chain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#&gt; Chain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#&gt; Chain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#&gt; Chain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#&gt; Chain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#&gt; Chain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#&gt; Chain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#&gt; Chain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#&gt; Chain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#&gt; Chain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#&gt; Chain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#&gt; Chain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#&gt; Chain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#&gt; Chain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#&gt; Chain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#&gt; Chain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#&gt; Chain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#&gt; Chain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#&gt; Chain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#&gt; Chain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#&gt; Chain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#&gt; Chain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#&gt; Chain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#&gt; Chain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#&gt; Chain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#&gt; Chain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#&gt; Chain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#&gt; Chain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#&gt; Chain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#&gt; Chain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#&gt; Chain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#&gt; Chain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#&gt; Chain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#&gt; Chain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \n#&gt; Chain 4 finished in 17.1 seconds.\n#&gt; Chain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#&gt; Chain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \n#&gt; Chain 3 finished in 17.5 seconds.\n#&gt; Chain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \n#&gt; Chain 1 finished in 17.6 seconds.\n#&gt; Chain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \n#&gt; Chain 2 finished in 17.9 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 17.6 seconds.\n#&gt; Total execution time: 18.0 seconds.\n\n\npost &lt;- extract.samples(flbi)\nprecis(flbi)\n#&gt;             mean         sd        5.5%      94.5%      n_eff     Rhat4\n#&gt; m     0.01029393 0.04047150 -0.05430275 0.07409054   890.4789 1.0018319\n#&gt; b     1.98864256 0.05811342  1.89487725 2.08168275  3042.1496 1.0007825\n#&gt; a2    0.02836364 0.07310509 -0.08729441 0.14659932  1335.7564 1.0027553\n#&gt; a1    0.06834714 0.05394291 -0.01785119 0.15621615  3747.6036 1.0007151\n#&gt; tau   0.98041340 0.03617180  0.92352906 1.03914055  2745.6394 1.0007016\n#&gt; sigma 1.07286195 0.05249312  0.98902178 1.15557275   909.1510 1.0015771\n#&gt; k     0.98545837 0.05638324  0.89516212 1.07461055   828.2759 1.0013545\n#&gt; p     0.48012384 0.01111044  0.46233213 0.49808700 16117.4910 0.9996995\n\nVemos que no aparece la estimación de U, pero en la posterior se ha estimado un valor de U para cada uno de las observaciones. 1000 observaciones y\n\n\ndim(post$U)\n#&gt; [1] 8000 1000\npost$U[1:4, 1:5]\n#&gt;          [,1]       [,2]       [,3]      [,4]        [,5]\n#&gt; [1,] 0.382053 -0.4087260 -1.5231200  0.486262  0.00381229\n#&gt; [2,] 0.733459  0.2971870  0.0441252 -1.424160 -0.98469300\n#&gt; [3,] 0.254011 -0.0201819 -0.7499000 -1.444340 -0.51772400\n#&gt; [4,] 0.468004  0.1923180 -1.8694400 -1.314570 -0.43902700"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#efecto-de-m-sobre-d.",
    "href": "2022/02/06/pluralista/index.html#efecto-de-m-sobre-d.",
    "title": "Pluralista",
    "section": "Efecto de M sobre D.",
    "text": "Efecto de M sobre D.\nEste era el efecto que queríamos obtener y el cuál no podíamos estimar al no poder condicionar sobre U. Aquí es tan sencillo como ver su distribución a posteriori.\n\n\nquantile(post$m)\n#&gt;          0%         25%         50%         75%        100% \n#&gt; -0.14804300 -0.01706778  0.01080760  0.03813360  0.15027000\n\nggplot() +\n  geom_density(aes(post$m)) + \n  labs(title = \"Efecto directo de M sobre D\", \n       x = \"m\")\n\n\n\n\n\n\n\n\n\nEfecto de B1 sobre D\nComo ya sabíamos, al haber simulado los datos de forma que las relaciones entre las variables sean lineales, el efecto de B1 sobre D no es más que el efecto de B1 sobre M multiplicado por el efecto de M sobre D.\nUtilizando la distribución a posteriori.\n\n# Efecto de B1 sobre D \nquantile( with(post,b*m) )\n#&gt;          0%         25%         50%         75%        100% \n#&gt; -0.27728158 -0.03379382  0.02148739  0.07658919  0.30614657\n\nggplot() +\n  geom_density(aes(post$b * post$m))+\n  labs(title = \"Efecto de B1 sobre D\", \n       x = \"b1 x m\")\n\n\n\n\n\n\n\n\n\n\nEfecto de B1 sobre D, simulando\nTal y como dice en su curso, el efecto causal puede ser visto como hacer una intervención supuesto cierto el modelo causal.\nSimplemente utilizamos las posterioris obtenidas y vamos simulando , en primer lugar B1 = 0 y simulamos qué M se obtendría, y lo hacemos también para B1 = 1 y restamos para obtener el efecto causal, que coindice con b * m\n\n# \n\n# B1 = 0\n# B1 -&gt; M\nM_B1_0 &lt;- with( post , a1 + b*0 + k*0 )\n# M -&gt; D\nD_B1_0 &lt;- with( post , a2 + b*0 + m*M_B1_0 + k*0 )\n\n# now same but with B1 = 1\nM_B1_1 &lt;- with( post , a1 + b*1 + k*0 )\nD_B1_1 &lt;- with( post , a2 + b*0 + m*M_B1_1 + k*0 )\n\n# difference to get causal effect\nd_D_B1 &lt;- D_B1_1 - D_B1_0\nquantile(d_D_B1)\n#&gt;          0%         25%         50%         75%        100% \n#&gt; -0.27728158 -0.03379382  0.02148739  0.07658919  0.30614657\n\nPues como dice el título , ser pluralista no está tan mal, puedes usar el DAG y el backdoor criterio para entender qué variables ha de tener en cuenta para estimar tu efecto causal, y a partir de ahí podrías usar el “full luxury bayesian” en situaciones más complicadas."
  },
  {
    "objectID": "2019/06/16/agile/index.html",
    "href": "2019/06/16/agile/index.html",
    "title": "¿Agile?",
    "section": "",
    "text": "Reconozco que no tengo mucha idea de lo que es esto del Agile, he leído el manifiesto y parecen cosas bastante lógicas, no obstante mi crítica no va tanto al agile sino más bien a cómo está implementado.\nEn primer lugar, estoy más que harto de la mezcolanza de términos en inglés y en español, el sprint, la daily, la planning, la retro, el refinamiento, la tribu, el townhall, os juro que me entran ganas de matar a alguien cuando me dicen -tengo una call para hablar del kickoff de la próxima semana-. ¡Arghh, que me den un cuchillo por favor!. Otro de los temas es la duración de las “ceremonias”, es bastante común que la reunión diaria se convierta en una especie de reporte al propietaria del producto, PO para los amantes de las abreviaturas, dónde se te puede ir media mañana en charlas que se salen fuera del objetivo de simplemente explicar qué hice ayer, qué voy a hacer hoy y qué puñetas me tiene amargado porque no lo saco y mira qué es fácil. Un cuarto de lo mismo pasa con las reuniones de planificación o de refinamiento, la mayoría de las veces vas a esas reuniones pensando que ese día es posible que hagas más bien poco.\nFuera bromas, hay una cosa que todavía no he acabado de entender, entiendo que este tipo de metodologías surgieron en proyectos de desarrollo de software dónde hacer una serie de tareas a,b,c llevan irremediablemente a d si se hacen correctamente. Sin embargo, en los proyectos de ciencia de datos tú puedes hacer correctamente a, b y c y sin embargo no llegas a d sino que vuelves a a o peor aún, se desecha completamente todo lo que has hecho, por lo que parece que no has avanzado nada. Lo que quiero transmitir es que en los procesos de ciencia de datos, las tareas están menos detalladas, son más amplias y están sujetas a una mayor incertidumbre que las propias de desarrollo de software y, que por tanto, parece que estas metodologías ágiles no sean las más adecuadas.\nA lo largo de mi escasa experiencia en esto de currar en organizaciones ágiles lo que me he encontrado es que la implementación de estas metodologías no es más que un barniz de apariencia lleno de postits. En realidad, detrás de esta apariencia de modernización de la estructura organizativa se suelen esconder los mismos demonios, a saber: fechas de entrega inamovibles, jefes de proyecto tradicionales reconvertidos a “bussiness owners” o a “product owners”, burocracia infernal o competitividad extrema disfrazada de “juego”.\nPero no todo es malo, tengo que decir que una vez vi bien aplicado esto de las metodologías agile y fue en un proyecto con la gente de un conocido retail de venta de ropa y accesorios deportivos. El “propietario del producto” tenía muy claro a dónde queríamos llegar, trabajó con nosotros día a día y cuando los problemas de los datos salieron a la luz (los datos siempre están regular o mal en todos sitios) se cambió el alcance del proyecto y se enfocó en solucionar en la medida de lo posible dichos problemas, para acabar dando una herramienta útil a la gente de negocio. Todos salimos contentos a pesar de que lo proyectado y lo conseguido no se parecían en nada.\nLo dicho, no acabo de entender muy bien toda la obsesión con las metodologías ágiles en proyectos de ciencia de datos, pero quizá tenga que ver con qué sólo nos quedamos con la forma y no con el fondo. Seguro que alguno de mis lectores sabría explicarme las bondades del agile más allá de los tableros llenos de postits, las mil herramientas de software para gestionar las tareas y las agotadoras ceremonias llenas de palabrejas en inglés.\nNada más, solo desearos feliz sprint a todos, que vuestras plannings se cumplan en al menos un 25%, que no os digáis muchas burradas en las retrospectivas, que los inceptions sean provechosos, que la tribu os acompañe y que el dios de los postit os proteja."
  },
  {
    "objectID": "2019/06/07/agua-con-gas/index.html",
    "href": "2019/06/07/agua-con-gas/index.html",
    "title": "Agua con gas",
    "section": "",
    "text": "O mejor dicho Sparkling Water , que es una librería de la buena gente de h2o que permite aunar el mundo spark con el mundo de h2o.\nEn un post anterior ya comentaba cómo poner modelos de h2o en producción en un cluster de spark, pero tengo que rectificar el punto en el que decía que con sparkling water había un cuello de botella al pasar de sparkdataframe a h2oframe, ese cuello ya no es tal, puesto que la conversión se hace en distribuido."
  },
  {
    "objectID": "2019/06/07/agua-con-gas/index.html#antecedentes",
    "href": "2019/06/07/agua-con-gas/index.html#antecedentes",
    "title": "Agua con gas",
    "section": "Antecedentes",
    "text": "Antecedentes\nEn mi corta experiencia en el mundo del big data (2016- actualidad), en todos los sitios por los que paso se me presenta la disyuntiva de cómo pasar modelos a producción. Yo vengo del mundo de R y otra mucha gente del mundo de python y las soluciones que se nos ocurren son cosas basadas en docker, hacer udf’s dentro de spark o cosas así. Otra gente siempre me dice que lo que hay que usar es MLlib ya sea usando pyspark, sparkr, sparklyr o directamente la API con Scala (no conozco a nadie que use la API de Java), pero seamos honestos, los modelos implementados en MLlib son una basura, son lentos, consumen muchos recursos y dan peores resultados que los implementados con R o python por ejemplo.\nComo dice el título de este blog “muestrear no es pecado” y a la hora de entrenar un modelo prefiero hacer 1000 muestreos aleatorios con reemplazamiento y ver la estabilidad de las predicciones e incluso sacar intervalos de confianza que cualquier otra cosa. Pero a todos nos piden implementar nuestros modelos (entrenamiento y predicción) en un entorno productivo."
  },
  {
    "objectID": "2019/06/07/agua-con-gas/index.html#una-posible-solución-buena-bonita-y-barata",
    "href": "2019/06/07/agua-con-gas/index.html#una-posible-solución-buena-bonita-y-barata",
    "title": "Agua con gas",
    "section": "Una posible solución (buena, bonita y barata)",
    "text": "Una posible solución (buena, bonita y barata)\nTal y como se cuenta en la comparativa que hace Szilard H2O es más eficiente que R, Python y Spark y con resultados iguales o mejores. Y un ingeniero me diría, -sí vale, muy bonito, pero yo lo que uso es spark y es lo que tenemos en el entorno de producción y no nos dejan instalar ni R, ni python, ni montar un cluster de h2o en los workers- . Pues aquí es dónde entra sparkling-water.\nCon Sparkling Water tenemos lo mejor de los dos mundos, Spark para tratar los datos y h2o para entrenar modelos en distribuido y para predicción, y lo mejor de todo, podemos utilizar sparkling-water como si fuera una librería más de SPARK e incluirlo dentro de nuestras Apps sin ningún problema, con lo que el paso a producción es tan simple como con cualquier otro proceso de spark.\nOtra ventaja es qué h2o y sparkling-water están ampliamente documentados, y por ejemplo aquí viene información de cómo utilizar en un EMR de Amazon, en Azure o en Google Cloud. Os recomiendo encarecidamente que leáis la docu, es una joya, así como los códigos de ejemplo que tienen en su github\nY bueno, después de tanto rollo y que parezco el comercial de la empresa voy a poner un ejemplo de cómo usar sparkling-water en spark con scala. En otro post lo comentaré con R."
  },
  {
    "objectID": "2019/06/07/agua-con-gas/index.html#ejemplo-con-spark-scala",
    "href": "2019/06/07/agua-con-gas/index.html#ejemplo-con-spark-scala",
    "title": "Agua con gas",
    "section": "Ejemplo con spark-scala",
    "text": "Ejemplo con spark-scala\nVamos a probar funcionalidad con spark-shell, para construir una app en condiciones una opción sería crear el jar ya sea mediante sbt o con gradle, en un post anterior conté más o menos como era.\nLo primero es lanzar el spark-shell añadiendo el jar correspondiente de sparkling water, spark ya se encarga de distribuirlo entre los workers.\nLa prueba la hago sobre mi portátil aunque todo esto ya está probado sobre un cluster de spark tocho y funciona perfectamente. Habría que cambiar el modelo de ejecución a --master yarn y elegir más ejecutores y memoria.\nLa versión de sparkling-water correspondiente a cada versión de spark se puede consultar Aquí Yo voy a utilizar spark 2.4.0 y sparkling-water 2.4.10\nArrancamos un spark shell en modo local\n/home/jose/spark/spark-2.4.0-bin-hadoop2.7/bin/spark-shell \\\n--jars /home/jose/Descargas/sparkling-water-2.4.10/assembly/build/libs/sparkling-water-assembly_2.11-2.4.10-all.jar \\\n--conf \"spark.dynamicAllocation.enabled=false\" \\\n--conf \"spark.scheduler.minRegisteredResourcesRatio=1\"  --executor-memory 7G --executor-cores 2 --num-executors 1 \\\n--name sparkling_water_scala \\\n/\nPodemos ver el sparkui en http://127.0.0.1:4040/jobs/\nTodo lo necesario para ejecutar h2o y su enlace con spark está en el jar sparkling-water-assembly_2.11-2.4.10-all.jar y al añadirlo al lanzar el spark-shell ya podemos utilizar h2o dentro de spark como una librería más.\nImportamos librerías (no todas son necesarias)\nimport org.apache.spark.SparkFiles\nimport org.apache.spark.h2o._\nimport org.apache.spark.examples.h2o._\nimport org.apache.spark.ml.h2o.models._\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.sql.{DataFrame, SQLContext}\nimport water.Key\nimport java.io.File\nimport java.net.URI\n\nimport _root_.hex.tree.gbm.GBM\nimport _root_.hex.tree.gbm.GBMModel\nimport _root_.hex.tree.gbm.GBMModel.GBMParameters\nimport _root_.hex.ModelMetricsSupervised\nimport _root_.hex.Model\nimport _root_.hex.ScoreKeeper\n\n//import _root_.hex._\nimport _root_.hex.ModelMetricsBinomial\nimport water.support.{H2OFrameSupport, SparkContextSupport, ModelMetricsSupport}\nimport water.support.SparkContextSupport.addFiles\nimport water.support.H2OFrameSupport._\nimport water.support.ModelSerializationSupport\nimport org.apache.spark.sql.{SaveMode}\n\nLevantamos un h2ocontext sobre el sparkcontext, lo que hace es levantar un cluster de h2o dentro de spark, sin necesidad de tener que instalar nada en los workers, no R, no python 3.\n\n\n\nbackendh2o\n\n\n\nimplicit val sqlContext = spark.sqlContext\nval hc = H2OContext.getOrCreate(sc)\n\nimport hc._\nimport hc.implicits._\nimport sqlContext.implicits._\nY se nos levanta el flow de h2o en\nhc.flowURL\nres0: String = http://192.168.1.37:54321\nO lo abrimos de esta forma\nopenFlow\nimagen\nLeemos datos con spark\n\nval dataPath = \"mtcars.csv\"\n\nval df= spark.read.option(\"header\", \"true\").\noption(\"inferSchema\", \"true\").\ncsv(dataPath)\ndf.show(3,false)\n+----+---+----+---+----+-----+-----+---+---+----+----+-------------+\n|mpg |cyl|disp|hp |drat|wt   |qsec |vs |am |gear|carb|id           |\n+----+---+----+---+----+-----+-----+---+---+----+----+-------------+\n|21  |6  |160 |110|3.9 |2.62 |16.46|0  |1  |4   |4   |Mazda RX4    |\n|21  |6  |160 |110|3.9 |2.875|17.02|0  |1  |4   |4   |Mazda RX4 Wag|\n|22.8|4  |108 |93 |3.85|2.32 |18.61|1  |1  |4   |1   |Datsun 710   |\n+----+---+----+---+----+-----+-----+---+---+----+----+-------------+\nonly showing top 3 rows\nCreamos variable binaria que sea tener 6 cilindros vs 4 u 8\n\nval df2 = df.withColumn(\"cyl_cat\", when($\"cyl\" === 6 , \"1\").otherwise(\"0\")).drop($\"cyl\")\nConvertimos el sparkdataframe a h2oframe. Esto antes era un cuello de botella pero ahora el paso de sparkdataframe a h2oframe se hace en distribuido y es bastante rápido, incluso con datos de 40 millones de filas y más de 100 columnas\n\n// convertir a h2oframe\nval trainFrame:H2OFrame = df2\ntrainFrame.names\ntrainFrame.means\nConvertimos cyl_cat, gear, carb a Categorical, esto se puede hacer en spark previamente convirtiendo a string y luego pasar todos los strings a categorical en h2o con withLockAndUpdate(trainFrame){ allStringVecToCategorical(_) }\n\n\nH2OFrameSupport.withLockAndUpdate(trainFrame) { fr =&gt;\n  fr.replace(fr.find(\"gear\"), fr.vec(\"gear\").toCategoricalVec).remove()\n}\n\nH2OFrameSupport.withLockAndUpdate(trainFrame) { fr =&gt;\n  fr.replace(fr.find(\"cyl_cat\"), fr.vec(\"cyl_cat\").toCategoricalVec).remove()\n}\n\nH2OFrameSupport.withLockAndUpdate(trainFrame) { fr =&gt;\n  fr.replace(fr.find(\"carb\"), fr.vec(\"carb\").toCategoricalVec).remove()\n}\n\nres8: org.apache.spark.h2o.H2OFrame =\nFrame key: frame_rdd_44_864202a286d438fb206334d98079482a\n   cols: 12\n   rows: 32\n chunks: 1\n   size: 4896\nDividimos en traint, test y hold. H2O hace esta tarea bastante mejor que spark\n\nval keys = Seq[String](\"train.hex\", \"test.hex\", \"hold.hex\")\nval ratios = Seq[Double](0.6, 0.3, 0.1)\nval frs = splitFrame(trainFrame, keys , ratios)\n\nval (train, test, hold) = (frs(0), frs(1), frs(2))\nAjustamos un modelo gbm para predecir cyl_cat, cambiamos la métrica de early stopping a AUC\nval ignore_columns = Array[String](\"id\")\nval stopping_metric = ScoreKeeper.StoppingMetric.AUC\n\nval gbmParams = new GBMParameters()\ngbmParams._train = train\ngbmParams._valid = test\n\ngbmParams._response_column = \"cyl_cat\"\ngbmParams._ignored_columns = ignore_columns\ngbmParams._ntrees = 10\ngbmParams._max_depth = 2\ngbmParams._seed = 155\ngbmParams._min_rows = 4\ngbmParams._learn_rate_annealing = 0.9\ngbmParams._col_sample_rate = 0.9\ngbmParams._sample_rate = 0.98\ngbmParams._balance_classes = false\ngbmParams._stopping_metric = stopping_metric \ngbmParams._stopping_rounds = 2\ngbmParams._score_tree_interval = 3\nEntrenamos, (en la interfaz se puede ver como va avanzando el entrenamiento y las métricas en train y validación)\nval gbmModel = new GBM(gbmParams).trainModel.get\n\nval holdMetrics =  ModelMetricsSupport.modelMetrics[ModelMetricsBinomial](gbmModel, hold)\n\nprintln(holdMetrics.auc)\nEn la interfaz podemos ver todas las métricas, el gainlift, etc.\nprintln(holdMetrics.gainsLift)"
  },
  {
    "objectID": "2019/06/07/agua-con-gas/index.html#serializarguardar-el-modelo",
    "href": "2019/06/07/agua-con-gas/index.html#serializarguardar-el-modelo",
    "title": "Agua con gas",
    "section": "Serializar/guardar el modelo",
    "text": "Serializar/guardar el modelo\n¿Cómo guardamos el modelo? Hay dos formas\n\nSerializando con ModelSerializationSupport.exportH2OModel lo que implica que cuando queramos cargarlo y predecir tenemos que lanzar de nuevo un spark-shell o spark-submit con el jar de sparkling-water\nGuardarlo como mojo (model object java optimization) tal y como conté en post anterior\n\nAmbas formas de salvarlo se pueden hacer desde la interfaz y desde las diferentes apis. Voy a contar como sería con la primera\nCreamos un uri de dónde vamos a guardarlo, puede ser una ruta de hdfs, el filesystem o incluso un bucket de s3.\nval destinationURI = URI.create(\"file:///home/jose/mi_modelo\")\n//val uri_mojo = URI.create(\"file:///opt/datos2/jcanadar/modelo_sparkling_mojo.zip\")\nModelSerializationSupport.exportH2OModel(gbmModel, destinationURI)\nCargar modelo guardado y hacer predicciones\nval loadedModel: GBMModel = ModelSerializationSupport.loadH2OModel(destinationURI)\nAl cargarlo ya me da información de las métricas que se obtuvieron con ese modelo\nval loadedModel: GBMModel = ModelSerializationSupport.loadH2OModel(destinationURI)\nloadedModel: hex.tree.gbm.GBMModel =\nModel Metrics Type: Binomial\n Description: N/A\n model id: GBM_model_1559923164707_105\n frame id: train.hex\n MSE: 0.07837714\n RMSE: 0.27995917\n AUC: 1.0\n pr_auc: 0.0\n logloss: 0.30478087\n mean_per_class_error: 0.0\n default threshold: 0.6001753807067871\n CM: Confusion Matrix (Row labels: Actual class; Column labels: Predicted class):\n         0  1   Error    Rate\n     0  11  0  0,0000  0 / 11\n     1   0  5  0,0000   0 / 5\nTotals  11  5  0,0000  0 / 16\nGains/Lift Table (Avg response rate: 31,25 %, avg score:31,52 %):\n  Group  Cumulative Data Fraction  Lower Threshold      Lift  Cumulative Lift  Response Rate     Score  Cumulative Response Rate  Cumulative Score  Capture Rate  Cumulative Capture Rate         Gain  Cumulative Gain\n      1                ...\nPara predecir sería tan sencillo como pasar de sparkdataframe a h2oframe como ya hemos visto antes y aplicar un método.\n// ya tengo un conjunto de datos en formato h2oframe, \n\nval predicciones = loadedModel.score(hold)\nConvierto a sparkdataframe\nval predicciones_spark = hc.asDataFrame(predicciones)\npredicciones_spark.show(3, false)\n+-------+------------------+-------------------+\n|predict|p0                |p1                 |\n+-------+------------------+-------------------+\n|0      |0.6386240026776301|0.3613759973223699 |\n|1      |0.3998245858574603|0.6001754141425397 |\n|0      |0.849842626576839 |0.15015737342316104|\n+-------+------------------+-------------------+\nSalvar a tabla hive\npredicciones_spark.write.mode(SaveMode.Overwrite).saveAsTable(\"esquema.nombretabla\")\nOtra cosa interesante es que h2o incorpora explicatividad mediante shap values para los algoritmos de árboles.\n// hay que crear una clave aleatoria con Key.make para que funcione\nval explain_frame = loadedModel.scoreContributions(hold, Key.make())\nval explain_spark = hc.asDataFrame(explain_frame)\nexplain_spark.show(3,false)\nLa suma de todos los términos de la explicatividad da el logit de la probabilidad de tener cyl == 4.\n+-------------------+-------------------+---+----+---+----+---+---+----+-------------------+-------------------+\n|mpg                |disp               |hp |drat|wt |qsec|vs |am |gear|carb               |BiasTerm           |\n+-------------------+-------------------+---+----+---+----+---+---+----+-------------------+-------------------+\n|0.06274261325597763|0.7365164756774902 |0  |0   |0  |0   |0  |0  |0   |-0.4328688085079193|-0.9357871413230896|\n|0.17428503930568695|0.8345963358879089 |0  |0   |0  |0   |0  |0  |0   |0.33310189843177795|-0.9357871413230896|\n|-0.2034180760383606|-0.7214104533195496|0  |0   |0  |0   |0  |0  |0   |0.12724840641021729|-0.9357871413230896|\n+-------------------+-------------------+---+----+---+----+---+---+----+-------------------+-------------------+\nonly showing top 3 rows\nLo único que faltaría para tener este en un entorno de producción sería meterlo en un jar ya sea mediante sbt o con gradle. En un proyecto en la empresa lo hemos probado, y gracias a mi compañero de trabajo y sin embargo amigo Sergio Calderón y a mis queridos ingenazis lo hemos formalizado en un proyecto gradle.\nHuelga decir que con esta implementación hemos mejorado espectacularmente el tiempo de entrenamiento y predicción de nuestros modelos obteniendo AUC’s similares a los que se obtienen con xgboost y la ventaja de que pasar a producción es muchísimo más sencillo que con otras alternativas"
  },
  {
    "objectID": "2019/05/04/jornadas-de-usuarios-de-r-y-ya-van-11/index.html",
    "href": "2019/05/04/jornadas-de-usuarios-de-r-y-ya-van-11/index.html",
    "title": "Jornadas de usuarios de R (y ya van 11)",
    "section": "",
    "text": "Quién nos los iba a decir, allá por 2009 cuándo recién creada la lista de correo r-help-es nos llegó el mensaje de que iban a celebrarse las primeras jornadas.Creo que soy, salvo que me corrija Emilio o Carlos, el que a más jornadas ha ido (sólo falté a las míticas de Mieres), jejejeje.\nHoy en día hay muchos eventos relacionados con software libre, analítica, big data, machine learning etcétera. Sin quitar méritos a ninguno de ellos, en mi humilde (y sesgada) opinión las jornadas de R-Hispano son uno de los mejores eventos. Son unas jornadas asequibles para todo el mundo (45 € este año si te inscribes antes del 31 de Julio), y habrá charlas y talleres super interesantes. Se me olvidaba, una de las cosas que más me gusta es que hay cabida para todo tipo de charlas, desde las más avanzadas hasta primeras experiencias en el uso de R.\nPues poco más que decir, que este año son en Madrid, que serán del 14 al 16 de Noviembre, que vamos a traer a uno (o varios) de los “popes” en el mundo R a nivel internacional y que esperamos que os inscribáis y mandéis comunicaciones, presentaciones cortas y propuestas de talleres para el uso y disfrute de todos.\n\nEnlace a las jornadas\nInscripción\nEnvío de propuestas"
  },
  {
    "objectID": "2023/y-si-parte-iii.html",
    "href": "2023/y-si-parte-iii.html",
    "title": "¿Y si … ? Parte III",
    "section": "",
    "text": "Ya estuve hablando anteriormente de los Metalearners o como se diga aquí y aquí. Pero ahora vamos a ver si lo utilizamos en unos datos reales.\nLa encuestas de estructura salarial se ha usado muchas veces para ver la brecha salarial entre hombre y mujeres. No obstante yo me hago la pregunta de si es posible y cómo estimar la brecha salarial entre sector público y sector privado.\n¿Cómo podríamos hacerlo? Está claro que son dos sectores muy distintos y que comparar las medias, tal y como hacen (mal) algunos para comparar brecha salarial de género, no es lo más adecuado.\nMi idea aquí es contar un poco como lo haríamos estilo compadre usando un modelo lineal de toda la vida, luego ver como se haría utilizando metalearners y también usando doubly robust estimator ."
  },
  {
    "objectID": "2023/y-si-parte-iii.html#introducción",
    "href": "2023/y-si-parte-iii.html#introducción",
    "title": "¿Y si … ? Parte III",
    "section": "",
    "text": "Ya estuve hablando anteriormente de los Metalearners o como se diga aquí y aquí. Pero ahora vamos a ver si lo utilizamos en unos datos reales.\nLa encuestas de estructura salarial se ha usado muchas veces para ver la brecha salarial entre hombre y mujeres. No obstante yo me hago la pregunta de si es posible y cómo estimar la brecha salarial entre sector público y sector privado.\n¿Cómo podríamos hacerlo? Está claro que son dos sectores muy distintos y que comparar las medias, tal y como hacen (mal) algunos para comparar brecha salarial de género, no es lo más adecuado.\nMi idea aquí es contar un poco como lo haríamos estilo compadre usando un modelo lineal de toda la vida, luego ver como se haría utilizando metalearners y también usando doubly robust estimator ."
  },
  {
    "objectID": "2023/y-si-parte-iii.html#datos",
    "href": "2023/y-si-parte-iii.html#datos",
    "title": "¿Y si … ? Parte III",
    "section": "Datos",
    "text": "Datos\nVamos a utilizar los microdatos de la encuestas de estructura salarial del INE. A pesar de ser una encuesta anual, los últimos resultados publicados son de 2021 y los últimos microdatos disponibles los de 2018. La verdad es que me gustaría entender por qué el INE publica tan tarde los microdatos 😥. La nota de prensa con los resultados de 2021 es del 20 de junio de 2023. Y si ya tienen resultados de 2021, ¿por qué los últimos microdatos disponibles son los de hace 5 años?\nSea como fuere vamos a lo nuestro.\n\n\nCode\nlibrary(tidyverse)\nlibrary(haven) # Para leer los datos de SPSS\n\n\nlibrary(survey) # para obtener estimadores correctos por ser una muestra\nlibrary(sjPlot) # plot de los modelos, \n# library(causact) # usar alguna cosa de numpyro desde R , quiero probar este paquete\n\n\nAunque en el fichero comprimido que te descargas del INE viene script para leer los datos con R, no me gusta ese script que instala XLConnect y no sé qué más. Así que lo que he hecho es leer el fichero en formato de spss con haven::read_spss() y luego limpiarlos un poco con janitor::clean_names().\n\n\nCode\n\ness &lt;- read_sav(here::here(\"data/INE/datos_2018/SPSS/EES_2018.sav\"))\ness &lt;- janitor::clean_names(ess)\n\nhead(ess)\n#&gt; # A tibble: 6 × 56\n#&gt;   idenccc  ordentra nuts1   cnace    estrato2 control mercado regulacion sexo   \n#&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr+l&gt; &lt;chr+lb&gt; &lt;chr+lb&gt; &lt;chr+l&gt; &lt;chr+l&gt; &lt;chr+lbl&gt;  &lt;chr+l&gt;\n#&gt; 1 00000025 01       1 [NOR… H1 [Tra… 1 [DE 1… 2 [PRI… 3 [UNI… 2 [SECTOR… 1 [HOM…\n#&gt; 2 00000025 02       1 [NOR… H1 [Tra… 1 [DE 1… 2 [PRI… 3 [UNI… 2 [SECTOR… 1 [HOM…\n#&gt; 3 00000025 03       1 [NOR… H1 [Tra… 1 [DE 1… 2 [PRI… 3 [UNI… 2 [SECTOR… 6 [MUJ…\n#&gt; 4 00000025 04       1 [NOR… H1 [Tra… 1 [DE 1… 2 [PRI… 3 [UNI… 2 [SECTOR… 1 [HOM…\n#&gt; 5 00000025 05       1 [NOR… H1 [Tra… 1 [DE 1… 2 [PRI… 3 [UNI… 2 [SECTOR… 1 [HOM…\n#&gt; 6 00000025 06       1 [NOR… H1 [Tra… 1 [DE 1… 2 [PRI… 3 [UNI… 2 [SECTOR… 1 [HOM…\n#&gt; # ℹ 47 more variables: tipopais &lt;chr+lbl&gt;, cno1 &lt;chr+lbl&gt;, responsa &lt;chr+lbl&gt;,\n#&gt; #   estu &lt;chr+lbl&gt;, anoanti &lt;dbl&gt;, mesanti &lt;dbl&gt;, tipojor &lt;chr+lbl&gt;,\n#&gt; #   tipocon &lt;chr+lbl&gt;, fijodism &lt;dbl&gt;, fijodisd &lt;dbl&gt;, val &lt;dbl&gt;, van &lt;dbl&gt;,\n#&gt; #   puentes &lt;dbl&gt;, jap &lt;dbl&gt;, jsp1 &lt;dbl&gt;, jsp2 &lt;dbl&gt;, hextra &lt;dbl&gt;,\n#&gt; #   drelabm &lt;dbl&gt;, siespm1 &lt;chr+lbl&gt;, dsiespm1 &lt;dbl&gt;, siespm2 &lt;chr+lbl&gt;,\n#&gt; #   dsiespm2 &lt;dbl&gt;, salbase &lt;dbl&gt;, extraorm &lt;dbl&gt;, phextra &lt;dbl&gt;, comsal &lt;dbl&gt;,\n#&gt; #   comsaltt &lt;dbl&gt;, irpfmes &lt;dbl&gt;, cotiza &lt;dbl&gt;, base &lt;dbl&gt;, drelabam &lt;dbl&gt;, …\n\n\nDejo enlace al diseño del registro para ver qué es cada variable en los microdatos.\n dis_registro\nLo que quiero comparar es el salario neto mensual, ¿por qué? porque me da la gana y porque en la docu del INE explican como se calcula el salario neto partiendo de los microdatos.\n\n\nCode\n\ness &lt;- ess |&gt;\n  mutate(\n    diasmes    = drelabm - dsiespm2,\n    diasrelaba = drelabam * 30.42 + drelabad,\n    diasrelaba = ifelse(diasrelaba &gt; 365, 365, diasrelaba),\n    diasano    = diasrelaba - dsiespa2 - dsiespa4,\n    salbase    = ifelse(siespm1 == \"6\", (31 / diasmes) * salbase, salbase),\n    comsal     = ifelse(siespm1 == \"6\", (31 / diasmes) * comsal, comsal),\n    comsaltt   = ifelse(siespm1 == \"6\", (31 / diasmes) * comsaltt, comsaltt),\n    salmes     = salbase + comsal + extraorm + phextra,\n    salmor     = salbase + comsal + phextra,\n    salneto    = salmes - cotiza - irpfmes,\n    salanual   = (365 / diasano) * (retrinoin + retriin + vespnoin + vespin),\n    salaor     = (365 / diasano) * ((retrinoin + retriin) - gextra),\n    vespnoin   = (365 / diasano) * vespnoin,\n    jmp1       = (jsp1 + jsp2 / 60) * 4.35 + hextra,\n    salhora    = salmes / jmp1\n  )"
  },
  {
    "objectID": "2020/12/30/y-si-parte-ii/index.html",
    "href": "2020/12/30/y-si-parte-ii/index.html",
    "title": "¿Y si … ? Parte II",
    "section": "",
    "text": "Volvamos a nuestro ejemplo tonto, dónde habíamos visto que el T-learner cuando el modelo base es un modelo lineal equivale a tener un modelo saturado (con interacciones).\nEn estos de los “metalearners” tenemos entre otros, los T-learners vistos en el post anterior , los S-learner y los X-learners.\nLos S-learners no es más que usar un solo modelo “Single” para estimar el Conditional Average Treatment Effect , CATE.\nUsando el mismo ejemplo sencillo, se tiene que.\nCode\nset.seed(155)\n\nX &lt;- rnorm(100, 10,1)\nW &lt;- rbinom(100, 1, 0.6)\n\n# Me construyo la Y de forma que haya efectos principales e interacción\nY &lt;- 4 + 2 * X + 2 * W + 2 * W * X + rnorm(100, 0, sd = 2)\n\ndf &lt;- as.data.frame(cbind(Y,W,X))\n\ndf\n#&gt;            Y W         X\n#&gt; 1   48.78438 1 10.800067\n#&gt; 2   25.28644 0 10.707605\n#&gt; 3   28.39538 0  9.925625\n#&gt; 4   47.60225 1 10.652555\n#&gt; 5   46.72225 1  9.992698\n#&gt; 6   55.15008 1 11.514759\n#&gt; 7   40.46547 1  9.093717\n#&gt; 8   22.17879 0  9.157972\n#&gt; 9   49.44883 1  9.866499\n#&gt; 10  51.21602 1 11.100414\n#&gt; 11  46.90193 1 10.287350\n#&gt; 12  22.88517 0  9.295653\n#&gt; 13  39.44776 1  9.156142\n#&gt; 14  40.78560 1  8.513496\n#&gt; 15  48.04199 1 10.067613\n#&gt; 16  47.80314 1  9.898276\n#&gt; 17  25.33331 0 10.578513\n#&gt; 18  24.15651 0  9.253759\n#&gt; 19  25.13304 0 10.365123\n#&gt; 20  25.23243 0 11.040849\n#&gt; 21  30.45260 0 12.869587\n#&gt; 22  44.82112 1  9.319895\n#&gt; 23  25.11998 0  9.830254\n#&gt; 24  19.99574 0  9.635928\n#&gt; 25  43.48504 1  9.215349\n#&gt; 26  41.14271 1  8.356523\n#&gt; 27  22.81061 0  8.883480\n#&gt; 28  25.58288 0  9.784855\n#&gt; 29  44.41997 1  9.404844\n#&gt; 30  27.84046 0 10.414529\n#&gt; 31  39.59324 1  9.041776\n#&gt; 32  51.28215 1 10.442391\n#&gt; 33  38.53548 1  8.142158\n#&gt; 34  21.95668 0  9.042216\n#&gt; 35  46.84521 1  9.724798\n#&gt; 36  43.87810 1  9.013322\n#&gt; 37  42.12536 1  9.633154\n#&gt; 38  45.74959 1 10.873450\n#&gt; 39  18.78703 0  9.748465\n#&gt; 40  21.79664 0 10.607739\n#&gt; 41  37.35355 1  8.361663\n#&gt; 42  22.53808 0 10.303852\n#&gt; 43  42.48434 1  9.004360\n#&gt; 44  49.39156 1 10.580300\n#&gt; 45  47.92040 1 10.672659\n#&gt; 46  48.76256 1 11.773249\n#&gt; 47  23.67107 0  9.875302\n#&gt; 48  48.76949 1  9.921954\n#&gt; 49  41.39283 1  8.920843\n#&gt; 50  42.49853 1  8.688555\n#&gt; 51  48.09462 1 10.564605\n#&gt; 52  44.45942 1  9.194570\n#&gt; 53  45.84477 1  9.438857\n#&gt; 54  41.94149 1  9.888696\n#&gt; 55  47.26368 1  9.887931\n#&gt; 56  51.42203 1 11.055223\n#&gt; 57  39.17177 1  8.327467\n#&gt; 58  51.15275 1 10.320770\n#&gt; 59  50.40525 1 10.585048\n#&gt; 60  42.49727 1  9.336601\n#&gt; 61  28.05959 0 10.952144\n#&gt; 62  49.10409 1 10.562264\n#&gt; 63  27.15474 0 12.045244\n#&gt; 64  19.24901 0  8.091111\n#&gt; 65  47.67471 1 10.241636\n#&gt; 66  24.39380 0 10.824896\n#&gt; 67  26.49221 0 10.812256\n#&gt; 68  38.77565 1  8.358974\n#&gt; 69  45.05843 1  9.515578\n#&gt; 70  52.28683 1 11.800317\n#&gt; 71  23.36347 0  9.797133\n#&gt; 72  26.84582 0 10.470713\n#&gt; 73  42.10340 1  9.598281\n#&gt; 74  39.43318 1  8.326351\n#&gt; 75  44.69754 1  9.965926\n#&gt; 76  48.71043 1 10.870054\n#&gt; 77  24.30603 0  9.038770\n#&gt; 78  24.54690 0 11.097281\n#&gt; 79  22.08450 0 10.558284\n#&gt; 80  51.71144 1 11.264590\n#&gt; 81  53.69442 1 11.434979\n#&gt; 82  26.79476 0 12.390173\n#&gt; 83  40.80879 1  9.520336\n#&gt; 84  43.63049 1 10.081028\n#&gt; 85  20.06392 0  8.716013\n#&gt; 86  41.11569 1  8.556393\n#&gt; 87  24.45452 0  9.109263\n#&gt; 88  24.05505 0 10.779678\n#&gt; 89  41.82733 1  9.990715\n#&gt; 90  53.17613 1 11.501511\n#&gt; 91  49.50179 1 11.061493\n#&gt; 92  20.42382 0  7.543992\n#&gt; 93  41.57695 1  8.856854\n#&gt; 94  50.83502 1 11.004920\n#&gt; 95  41.66118 1  9.274137\n#&gt; 96  47.30987 1 10.771928\n#&gt; 97  20.74180 0  9.829798\n#&gt; 98  24.39354 0 10.412418\n#&gt; 99  53.71654 1 11.506078\n#&gt; 100 51.22245 1 10.711256"
  },
  {
    "objectID": "2020/12/30/y-si-parte-ii/index.html#extra-uso-de-causalml",
    "href": "2020/12/30/y-si-parte-ii/index.html#extra-uso-de-causalml",
    "title": "¿Y si … ? Parte II",
    "section": "Extra, uso de causalml",
    "text": "Extra, uso de causalml\nEn la librería causalml de Uber vienen implmentandos los metalearner entre otras cosas. Usando el mismo ejemplo veamos como se calcularía el CATE.\nNota: He sido incapaz de ver como predecir para mi nueva x, no hay o no he encontrado que funcione un método predict para aplicar el X learner a unos nuevos datos.\n\n\nCode\nfrom causalml.inference.meta import BaseXRegressor\n#&gt; The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\nfrom sklearn.linear_model import LinearRegression\n# llamamos al df que está en R\ndf_python = r.df[['Y','W','X','pesos']]\ndf_python\n#&gt;             Y    W          X     pesos\n#&gt; 0   48.784384  1.0  10.800067  0.608752\n#&gt; 1   25.286438  0.0  10.707605  0.612492\n#&gt; 2   28.395375  0.0   9.925625  0.643553\n#&gt; 3   47.602247  1.0  10.652555  0.614712\n#&gt; 4   46.722247  1.0   9.992698  0.640932\n#&gt; ..        ...  ...        ...       ...\n#&gt; 95  47.309873  1.0  10.771928  0.609891\n#&gt; 96  20.741797  0.0   9.829798  0.647284\n#&gt; 97  24.393540  0.0  10.412418  0.624340\n#&gt; 98  53.716540  1.0  11.506078  0.579804\n#&gt; 99  51.222453  1.0  10.711256  0.612344\n#&gt; \n#&gt; [100 rows x 4 columns]\n\n\n\n\nCode\nlearner_x = BaseXRegressor(learner=LinearRegression())\n\nX = df_python.X.values.reshape(-1,1)\ny = df_python.Y.values\ntreatment = df_python.W.values\ne = df_python.pesos.values\nnueva_X = r.df_nueva_x['X'].values.reshape(-1,1)\n\n# estimamos\ncate_x = learner_x.fit_predict(X=X, treatment=treatment, y=y, p=e)\n\nprint(cate_x)\n\n#&gt; [[24.18445071]\n#&gt;  [23.96165333]\n#&gt;  [22.07738827]\n#&gt;  [23.8290041 ]\n#&gt;  [22.23900667]\n#&gt;  [25.90657902]\n#&gt;  [20.07281545]\n#&gt;  [20.22764413]\n#&gt;  [21.93491718]\n#&gt;  [24.90817005]\n#&gt;  [22.94900375]\n#&gt;  [20.55940033]\n#&gt;  [20.22323467]\n#&gt;  [18.67470923]\n#&gt;  [22.41952385]\n#&gt;  [22.01148778]\n#&gt;  [23.65059347]\n#&gt;  [20.4584526 ]\n#&gt;  [23.13640524]\n#&gt;  [24.76464061]\n#&gt;  [29.17118624]\n#&gt;  [20.61781408]\n#&gt;  [21.84758166]\n#&gt;  [21.37933037]\n#&gt;  [20.36590009]\n#&gt;  [18.29646419]\n#&gt;  [19.56622485]\n#&gt;  [21.73818594]\n#&gt;  [20.82250793]\n#&gt;  [23.25545554]\n#&gt;  [19.9476568 ]\n#&gt;  [23.32259103]\n#&gt;  [17.77992951]\n#&gt;  [19.94871811]\n#&gt;  [21.59347327]\n#&gt;  [19.87909366]\n#&gt;  [21.37264782]\n#&gt;  [24.36127525]\n#&gt;  [21.65050016]\n#&gt;  [23.72101678]\n#&gt;  [18.30885076]\n#&gt;  [22.98876753]\n#&gt;  [19.85749887]\n#&gt;  [23.65489924]\n#&gt;  [23.8774463 ]\n#&gt;  [26.52943858]\n#&gt;  [21.9561297 ]\n#&gt;  [22.06854152]\n#&gt;  [19.65625568]\n#&gt;  [19.09653297]\n#&gt;  [23.61708013]\n#&gt;  [20.31583121]\n#&gt;  [20.90446626]\n#&gt;  [21.98840222]\n#&gt;  [21.98655993]\n#&gt;  [24.7992766 ]\n#&gt;  [18.2264522 ]\n#&gt;  [23.02953222]\n#&gt;  [23.66633862]\n#&gt;  [20.65807062]\n#&gt;  [24.55089614]\n#&gt;  [23.61143985]\n#&gt;  [27.18483957]\n#&gt;  [17.65692503]\n#&gt;  [22.83885017]\n#&gt;  [24.24427817]\n#&gt;  [24.21382276]\n#&gt;  [18.30237035]\n#&gt;  [21.08933438]\n#&gt;  [26.59466223]\n#&gt;  [21.76777266]\n#&gt;  [23.39083619]\n#&gt;  [21.28861592]\n#&gt;  [18.22376141]\n#&gt;  [22.17449599]\n#&gt;  [24.35309297]\n#&gt;  [19.94041482]\n#&gt;  [24.90061955]\n#&gt;  [23.60184813]\n#&gt;  [25.3037691 ]\n#&gt;  [25.71434126]\n#&gt;  [28.01598546]\n#&gt;  [21.10080014]\n#&gt;  [22.45184837]\n#&gt;  [19.16269587]\n#&gt;  [18.77807334]\n#&gt;  [20.1102733 ]\n#&gt;  [24.1353215 ]\n#&gt;  [22.23422848]\n#&gt;  [25.87465564]\n#&gt;  [24.81438579]\n#&gt;  [16.33858204]\n#&gt;  [19.50206724]\n#&gt;  [24.67806615]\n#&gt;  [20.5075564 ]\n#&gt;  [24.11664769]\n#&gt;  [21.84648138]\n#&gt;  [23.25036905]\n#&gt;  [25.88566055]\n#&gt;  [23.97045205]]"
  },
  {
    "objectID": "2020/11/15/y-si-parte-i/index.html",
    "href": "2020/11/15/y-si-parte-i/index.html",
    "title": "¿Y si … ? Parte I",
    "section": "",
    "text": "Lo de la inferencia causal está de moda, y motivos hay, es una herramienta que intenta dar respuesta a preguntas cómo las siguientes.\n\n¿Qué habría pasado si en vez de poner este precio a este producto hubiera puesto otro?\n¿Se habría vendido más?\n¿He mandado a mi campaña a aquellos para los que justo al mandar a campaña su probabilidad de compra se incrementa?\n\nTradicionalmente a esta pregunta, los estadísticos respondían con una de sus herramientas más potentes, el diseño de experimentos. Pero muchas veces lo único que tenemos son datos observacionales y se trata de estimar el tamaño del efecto.\nLeyendo sobre cosas de este tipo llegué a los “metalearners” y en particular al “T-learner”.\nSe trata de estimar el efecto de una variable, típicamente un tratamiento con 2 categorías sobre una variable respuesta, y con presencia de otras variables, de forma que el efecto del tratamiento puede ser diferente según el valor de las covariables, vamos, que haya interacción.\nSupongamos que tenemos una variable respuesta Y, un tratamiento W (con dos niveles, 0 y 1) y una o varias covariables X. El T-learner (La T es de two models) lo que propone básicamente es estimar dos modelos. Uno que estime \\(E[Y | X]\\) en el grupo de control (W=0) y otro que estime lo mismo pero en el grupo del tratamiento (W=1) y luego restar esas dos esperanzas. A esto lo llaman una estimación del CATE (Conditional Average Treatment Effects) ¿Fácil, verdad?\nSi estamos en el marco de los modelos lineales esta forma de proceder es idéntica a estimar un sólo modelo dónde W es otra variable más y además pondríamos todas las posibles interacciones entre W y X, casi podríamos decir que es el modelo saturado. De hecho en un modelo lineal, podríamos sacar el CATE simplemente utilizando los coeficientes estimados.\nEjemplo tonto\n\n\nCode\nset.seed(155)\n\nX &lt;- rnorm(100, 10,1)\nW &lt;- rbinom(100, 1, 0.6)\n\n# Me construyo la Y de forma que haya efectos principales e interacción\nY &lt;- 4 + 2 * X + 2 * W + 2 * W * X + rnorm(100, 0, sd = 2)\n\ndf &lt;- as.data.frame(cbind(Y,W,X))\n\n\nSi hacemos un modelo sólo sobre los que son W = 0 y otro para los que son W = 1 (He obviado la parte de hacer train, test, validación, etc).\n\n\nCode\nmod0 &lt;- lm(Y ~ X, data = df[W==0, ])\nmod1 &lt;- lm(Y ~ X, data = df[W==1, ])\n\n\nY si suponemos una nueva observación dónde X = 14 entonces laa estimación del CATE mediante un T -learner.\n\n\nCode\ndf_nuevo &lt;- data.frame(X = 14)\n(cate1 &lt;- predict(mod1, newdata = df_nuevo) - predict(mod0, newdata = df_nuevo))\n#&gt;        1 \n#&gt; 31.89504\n\n\nHaciendo el modelo con interacción\n\n\nCode\n\nmod_saturado &lt;-  lm(Y ~ W *X , data = df)\nsummary(mod_saturado)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ W * X, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.6786 -1.2138  0.1903  1.5419  4.6289 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   6.9118     3.1354   2.204   0.0299 *  \n#&gt; W            -1.8395     4.0511  -0.454   0.6508    \n#&gt; X             1.6981     0.3085   5.504 3.08e-07 ***\n#&gt; W:X           2.4096     0.4016   6.000 3.49e-08 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.011 on 96 degrees of freedom\n#&gt; Multiple R-squared:  0.9689, Adjusted R-squared:  0.9679 \n#&gt; F-statistic: 995.9 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nPara ver el efecto de W sobre una hipotética población sería tener la misma observación con X=14, pero en un caso con W= 0 y en otro con W=1\nUtilizando los coeficientes, el CATE sería simplemente tener en cuenta cuando interviene W (los otros términos se cancelan).\n\n\nCode\n# \n(cate_2 &lt;- coef(mod_saturado)[2] + coef(mod_saturado)[4] * 14 )\n#&gt;        W \n#&gt; 31.89504\n\n\nQue coincide con la estimación usando el “T - Learner”. Es decir, en este ejemplo sencillo, utilizando como modelo base del T-learner un modelo lineal, la estimación es la misma que considerar un solo modelo dónde tenemos las interacciones del tratamiento con las covariables.\nLa aproximación de T - Learner (y de otros metalearners ) cobra sentido cuando tenemos muchas covariables y un modelo lineal con interacciones se puede volver muy complicado. En el caso del T-learner se podría utilizar como modelo base cualquier modelo que estime la \\(E[Y|W=w_i,X=x]\\).\nSin meterme mucho en la parte de los “potential outcomes” , básicamente se trata de inferir con la población con W=0 lo que pasaría si todas las observaciones tuvieran \\(Y^{(0)}\\) y lo mismo con la población con W=1. Este tipo de estrategias funcionan bien mientras el grado de solape de tratamiento y control en los diferentes valores de X sea alto (en el diseño de experimentos se busca justo eso, jeje).\nEn fin, que creo que me he enrollado demasiado para algo que es muy simple. En próximos post a ver si explico mejor los S-learners, los X- learners, los causal tree y causal forest, modelos de uplift, y más cositas, y con algún ejemplo más claro."
  },
  {
    "objectID": "2023/y-si-parte-iii.html#estimando-cosas..",
    "href": "2023/y-si-parte-iii.html#estimando-cosas..",
    "title": "¿Y si … ? Parte III",
    "section": "Estimando cosas..",
    "text": "Estimando cosas..\nLa variable dónde se consigna si el sector es público o privado es control\n\n\nCode\n\ness |&gt; \n    group_by(control) |&gt;    \n    count()\n#&gt; # A tibble: 2 × 2\n#&gt; # Groups:   control [2]\n#&gt;   control          n\n#&gt;   &lt;chr+lbl&gt;    &lt;int&gt;\n#&gt; 1 1 [PUBLICO]  35553\n#&gt; 2 2 [PRIVADO] 181173\n\n\nVoy a crearme variable treatment que valga 1 cuando sea sector público y 0 para el sector privado\n\n\nCode\ness$treatment = ess$control\ness$treatment = ifelse(ess$control == \"1\", 1, 0)\n# también llamo outcome al salario neto\ness$outcome = ess$salneto\n\n\n\nGroup by\nLo más simple , hacemos un group by y calculamos medias\n\n\nCode\ness |&gt; \n    group_by(treatment) |&gt;  \n    summarise(\n        mean = mean(outcome),\n        n = n()\n    )   \n#&gt; # A tibble: 2 × 3\n#&gt;   treatment  mean      n\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n#&gt; 1         0 1561. 181173\n#&gt; 2         1 1864.  35553\n\n\nAsí de primeras, pues parece que se gana más en el sector público que en el privado, pero ¡ojo! que la encuesta tiene una variable de ponderación, que el INE ha calculado para que los resultados sean representativos de la población. En la nota metodológica el INE dice lo siguiente sobre el plan de muestreo\nEl procedimiento de selección aleatoria de unidades corresponde a un muestreo bietápico estratificado, donde las unidades de primera etapa son las cuentas de cotización a la Seguridad Social (CC), mientras que las de segunda etapa son los trabajadores (asalariados). En la primera etapa tanto el diseño muestral como la muestra obtenida de CC coincide con la ETCL (para una mayor información consultar la metodología de la ETCL). Las unidades de primera etapa se estratifican según las siguientes variables:\n\nComunidad autónoma\nRama de actividad económica (división de la CNAE-09)\nTamaño, medido por el número de asalariados en cada CC\n\nEn los microdatos tenemos la variable factotal que es la ponderación que el INE dice que hay que usar a la hora de hacer estimaciones.\n\n\nCode\n\ness |&gt; \n    group_by(treatment) |&gt;  \n    summarise(\n        media_ponderada = weighted.mean(outcome, w = factotal)\n    )   \n#&gt; # A tibble: 2 × 2\n#&gt;   treatment media_ponderada\n#&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1         0           1351.\n#&gt; 2         1           1799.\n\n\n\n\nModelo lineal\nPero sabemos que la media tal cual puede no ser un buen indicador, lo suyo sería controlar (condicionar) por otras variables, tales como el sexo, nivel de estudio, edad, años de antigüedad , tipo de jornada laboral, y cosas así.\nHagámoslo, pero usando que tenemos pesos en la encuesta.\n\n\nCode\n\ndisenno &lt;- svydesign(id = ~1, weight = ~factotal, data = ess)\n\n\nModelo simple dónde uso variables como edad, tipo contrato, área nuts, antigüedad, nivel de estudios, etc..\n\n\nCode\nmod_simple &lt;- svyglm(outcome ~ treatment + sexo + anos2 +   estu + cnace + cno1 + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, design = disenno)\n\n\n\n\nCode\nsummary(mod_simple)\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = outcome ~ treatment + sexo + anos2 + estu + \n#&gt;     cnace + cno1 + estrato2 + tipojor + anoanti + mesanti + tipocon + \n#&gt;     nuts1, design = disenno)\n#&gt; \n#&gt; Survey design:\n#&gt; svydesign(id = ~1, weight = ~factotal, data = ess)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error  t value Pr(&gt;|t|)    \n#&gt; (Intercept)  2267.4873    58.5408   38.733  &lt; 2e-16 ***\n#&gt; treatment     112.7778     8.1697   13.804  &lt; 2e-16 ***\n#&gt; sexo6        -158.0960     5.4276  -29.128  &lt; 2e-16 ***\n#&gt; anos202        21.0286    25.4780    0.825 0.409167    \n#&gt; anos203       101.1315    25.4521    3.973 7.09e-05 ***\n#&gt; anos204       149.8967    25.3798    5.906 3.51e-09 ***\n#&gt; anos205       176.1458    25.6102    6.878 6.09e-12 ***\n#&gt; anos206        52.5466    27.0543    1.942 0.052106 .  \n#&gt; estu2          29.6478    14.4793    2.048 0.040601 *  \n#&gt; estu3          38.1183    14.3801    2.651 0.008031 ** \n#&gt; estu4         142.3435    14.8437    9.590  &lt; 2e-16 ***\n#&gt; estu5         166.5131    15.9282   10.454  &lt; 2e-16 ***\n#&gt; estu6         265.4148    17.3323   15.313  &lt; 2e-16 ***\n#&gt; estu7         510.3787    18.1827   28.070  &lt; 2e-16 ***\n#&gt; cnaceC1      -324.1132    25.8960  -12.516  &lt; 2e-16 ***\n#&gt; cnaceC2      -278.7694    31.4639   -8.860  &lt; 2e-16 ***\n#&gt; cnaceC3      -366.8802    28.5581  -12.847  &lt; 2e-16 ***\n#&gt; cnaceC4      -107.0397    26.9400   -3.973 7.09e-05 ***\n#&gt; cnaceC5      -252.2397    27.0669   -9.319  &lt; 2e-16 ***\n#&gt; cnaceC6      -167.9342    26.4826   -6.341 2.28e-10 ***\n#&gt; cnaceC7      -180.7201    28.5478   -6.330 2.45e-10 ***\n#&gt; cnaceC8      -171.5305    25.6146   -6.697 2.14e-11 ***\n#&gt; cnaceD0       323.7104    36.5705    8.852  &lt; 2e-16 ***\n#&gt; cnaceE0      -282.7253    26.5709  -10.640  &lt; 2e-16 ***\n#&gt; cnaceF0      -222.3453    25.6119   -8.681  &lt; 2e-16 ***\n#&gt; cnaceG1      -266.4662    27.1602   -9.811  &lt; 2e-16 ***\n#&gt; cnaceG2      -409.9509    28.0075  -14.637  &lt; 2e-16 ***\n#&gt; cnaceH1      -231.9251    27.2267   -8.518  &lt; 2e-16 ***\n#&gt; cnaceH2      -288.5758    26.5848  -10.855  &lt; 2e-16 ***\n#&gt; cnaceI0      -314.6540    26.9575  -11.672  &lt; 2e-16 ***\n#&gt; cnaceJ0      -305.2619    27.9572  -10.919  &lt; 2e-16 ***\n#&gt; cnaceK0       -26.9416    29.4084   -0.916 0.359606    \n#&gt; cnaceL0      -426.2992    31.8821  -13.371  &lt; 2e-16 ***\n#&gt; cnaceM0      -382.3510    26.5815  -14.384  &lt; 2e-16 ***\n#&gt; cnaceN0      -426.7819    26.7070  -15.980  &lt; 2e-16 ***\n#&gt; cnaceO0      -489.0672    26.5529  -18.419  &lt; 2e-16 ***\n#&gt; cnaceP0      -677.0378    28.3344  -23.895  &lt; 2e-16 ***\n#&gt; cnaceQ0      -417.2582    26.2085  -15.921  &lt; 2e-16 ***\n#&gt; cnaceR0      -446.4843    27.0788  -16.488  &lt; 2e-16 ***\n#&gt; cnaceS0      -441.9862    25.9167  -17.054  &lt; 2e-16 ***\n#&gt; cno1B0       -564.2450    40.6680  -13.874  &lt; 2e-16 ***\n#&gt; cno1C0       -581.5582    40.3270  -14.421  &lt; 2e-16 ***\n#&gt; cno1D0       -772.4413    38.8858  -19.864  &lt; 2e-16 ***\n#&gt; cno1E0       -964.0718    38.7102  -24.905  &lt; 2e-16 ***\n#&gt; cno1F0       -963.2193    39.0883  -24.642  &lt; 2e-16 ***\n#&gt; cno1G0       -944.6769    40.6941  -23.214  &lt; 2e-16 ***\n#&gt; cno1H0      -1007.1839    39.2405  -25.667  &lt; 2e-16 ***\n#&gt; cno1I0       -805.5218    41.4801  -19.419  &lt; 2e-16 ***\n#&gt; cno1J0       -833.4489    50.5645  -16.483  &lt; 2e-16 ***\n#&gt; cno1K0       -971.3301    39.7779  -24.419  &lt; 2e-16 ***\n#&gt; cno1L0       -948.3725    38.9086  -24.374  &lt; 2e-16 ***\n#&gt; cno1M0       -977.4409    38.9573  -25.090  &lt; 2e-16 ***\n#&gt; cno1N0       -976.4017    40.1926  -24.293  &lt; 2e-16 ***\n#&gt; cno1O0      -1009.1702    40.1926  -25.108  &lt; 2e-16 ***\n#&gt; cno1P0      -1033.0883    39.0592  -26.449  &lt; 2e-16 ***\n#&gt; cno1Q0       -981.9190    65.8301  -14.916  &lt; 2e-16 ***\n#&gt; estrato21      25.7381    28.0871    0.916 0.359476    \n#&gt; estrato22     130.3521    28.3970    4.590 4.43e-06 ***\n#&gt; estrato23     208.3260    28.3907    7.338 2.18e-13 ***\n#&gt; estrato24     130.3223    29.7704    4.378 1.20e-05 ***\n#&gt; tipojor2     -593.8166     5.3784 -110.408  &lt; 2e-16 ***\n#&gt; anoanti        12.3393     0.3434   35.929  &lt; 2e-16 ***\n#&gt; mesanti         2.0881     0.6091    3.428 0.000608 ***\n#&gt; tipocon2     -119.7352     5.0449  -23.734  &lt; 2e-16 ***\n#&gt; nuts12        139.1728     8.4611   16.449  &lt; 2e-16 ***\n#&gt; nuts13         97.8708     9.3378   10.481  &lt; 2e-16 ***\n#&gt; nuts14         -4.8016     7.8750   -0.610 0.542040    \n#&gt; nuts15        104.8487     7.5335   13.918  &lt; 2e-16 ***\n#&gt; nuts16         48.8032     8.2790    5.895 3.76e-09 ***\n#&gt; nuts17         17.5482    10.3777    1.691 0.090848 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 373331.4)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n\n\nY el coeficiente asociaso al sector público indica que se gana en media unos 112 euros más que en el sector privado, según este modelo.\n¿Cuánto sería para alguien que trabaja a jornada completa, nivel de estudios superior o igual a licenciado?\nPara eso podemos hacer lo que se conoce como una “intervención”, que es crear dos conjuntos de datos copias del original, con la diferencia de que en uno todo el mundo es del sector privado y en el otro todos del sector público y comparamos las medias estimadas de salario neto que nos da el modelo para el subgrupo de población que queramos.\nA esto se le conoce por los modernos como un S-learner\n\n\nCode\n\ness_fake_publico  &lt;- ess  \n\ness_fake_publico$treatment  &lt;- 1\n\nestim_publico &lt;- predict(mod_simple, newdata = ess_fake_publico |&gt;  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\"  ) )\n\n\ness_fake_privado &lt;- ess \n\ness_fake_privado$treatment  &lt;- 0\n\nestim_privado &lt;- predict(mod_simple, newdata = ess_fake_privado |&gt;  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\") )\n\n\nmean(estim_publico)\n#&gt; [1] 2516.276\nmean(estim_privado)\n#&gt; [1] 2403.498\n\n(s_learner_with_pond &lt;- mean(estim_publico) - mean(estim_privado))\n#&gt; [1] 112.7778\n\n\nY coincide con el coeficiente que daba el modelo. Y eso es así porque no he metido interacciones en el modelo. Si metemos una simple interacción entre ser del sector público y privado con la zona Nuts1.\n\n\nCode\ness |&gt; \n    group_by(nuts1) |&gt;  \n    count()\n#&gt; # A tibble: 7 × 2\n#&gt; # Groups:   nuts1 [7]\n#&gt;   nuts1                       n\n#&gt;   &lt;chr+lbl&gt;               &lt;int&gt;\n#&gt; 1 1 [NOROESTE]            24806\n#&gt; 2 2 [NORESTE]             33624\n#&gt; 3 3 [COMUNIDAD DE MADRID] 34269\n#&gt; 4 4 [CENTRO]              26428\n#&gt; 5 5 [ESTE]                58852\n#&gt; 6 6 [SUR]                 29413\n#&gt; 7 7 [CANARIAS]             9334\n\n\n\n\nCode\nmod_inter_con_nuts &lt;- svyglm(outcome ~ treatment* nuts1 + sexo + anos2 +   estu + cnace + cno1 + estrato2  + tipojor  + anoanti + mesanti + tipocon , design = disenno)\n\nsummary(mod_inter_con_nuts)\n#&gt; \n#&gt; Call:\n#&gt; svyglm(formula = outcome ~ treatment * nuts1 + sexo + anos2 + \n#&gt;     estu + cnace + cno1 + estrato2 + tipojor + anoanti + mesanti + \n#&gt;     tipocon, design = disenno)\n#&gt; \n#&gt; Survey design:\n#&gt; svydesign(id = ~1, weight = ~factotal, data = ess)\n#&gt; \n#&gt; Coefficients:\n#&gt;                    Estimate Std. Error  t value Pr(&gt;|t|)    \n#&gt; (Intercept)       2267.0375    58.7580   38.583  &lt; 2e-16 ***\n#&gt; treatment          139.5808    16.4879    8.466  &lt; 2e-16 ***\n#&gt; nuts12             136.3734     9.6420   14.144  &lt; 2e-16 ***\n#&gt; nuts13             109.8222    10.5875   10.373  &lt; 2e-16 ***\n#&gt; nuts14             -13.0969     8.7817   -1.491 0.135862    \n#&gt; nuts15             115.8699     8.4975   13.636  &lt; 2e-16 ***\n#&gt; nuts16              50.2008     9.4214    5.328 9.92e-08 ***\n#&gt; nuts17              11.3094    11.6282    0.973 0.330765    \n#&gt; sexo6             -157.8517     5.4260  -29.092  &lt; 2e-16 ***\n#&gt; anos202             22.9645    25.5775    0.898 0.369272    \n#&gt; anos203            103.1094    25.5532    4.035 5.46e-05 ***\n#&gt; anos204            151.6792    25.4825    5.952 2.65e-09 ***\n#&gt; anos205            177.3203    25.7096    6.897 5.32e-12 ***\n#&gt; anos206             53.6148    27.1482    1.975 0.048282 *  \n#&gt; estu2               29.8615    14.4816    2.062 0.039207 *  \n#&gt; estu3               37.3601    14.3817    2.598 0.009384 ** \n#&gt; estu4              142.6627    14.8458    9.610  &lt; 2e-16 ***\n#&gt; estu5              167.2457    15.9296   10.499  &lt; 2e-16 ***\n#&gt; estu6              264.4340    17.3311   15.258  &lt; 2e-16 ***\n#&gt; estu7              510.9210    18.1818   28.101  &lt; 2e-16 ***\n#&gt; cnaceC1           -322.4719    25.8565  -12.472  &lt; 2e-16 ***\n#&gt; cnaceC2           -277.8307    31.4386   -8.837  &lt; 2e-16 ***\n#&gt; cnaceC3           -368.1116    28.5083  -12.912  &lt; 2e-16 ***\n#&gt; cnaceC4           -107.5481    26.8992   -3.998 6.38e-05 ***\n#&gt; cnaceC5           -251.5735    27.0238   -9.309  &lt; 2e-16 ***\n#&gt; cnaceC6           -165.5994    26.4505   -6.261 3.84e-10 ***\n#&gt; cnaceC7           -179.9980    28.5076   -6.314 2.72e-10 ***\n#&gt; cnaceC8           -170.7310    25.5703   -6.677 2.45e-11 ***\n#&gt; cnaceD0            323.6055    36.5367    8.857  &lt; 2e-16 ***\n#&gt; cnaceE0           -281.1503    26.5217  -10.601  &lt; 2e-16 ***\n#&gt; cnaceF0           -221.8244    25.5641   -8.677  &lt; 2e-16 ***\n#&gt; cnaceG1           -266.6874    27.1071   -9.838  &lt; 2e-16 ***\n#&gt; cnaceG2           -410.0799    27.9517  -14.671  &lt; 2e-16 ***\n#&gt; cnaceH1           -228.7886    27.1892   -8.415  &lt; 2e-16 ***\n#&gt; cnaceH2           -287.2581    26.5307  -10.827  &lt; 2e-16 ***\n#&gt; cnaceI0           -314.4745    26.9273  -11.679  &lt; 2e-16 ***\n#&gt; cnaceJ0           -307.8448    27.9093  -11.030  &lt; 2e-16 ***\n#&gt; cnaceK0            -25.9796    29.3654   -0.885 0.376318    \n#&gt; cnaceL0           -428.7358    31.8459  -13.463  &lt; 2e-16 ***\n#&gt; cnaceM0           -382.9944    26.5201  -14.442  &lt; 2e-16 ***\n#&gt; cnaceN0           -427.5663    26.6470  -16.046  &lt; 2e-16 ***\n#&gt; cnaceO0           -491.1736    26.4855  -18.545  &lt; 2e-16 ***\n#&gt; cnaceP0           -679.4070    28.3151  -23.995  &lt; 2e-16 ***\n#&gt; cnaceQ0           -418.4000    26.1469  -16.002  &lt; 2e-16 ***\n#&gt; cnaceR0           -446.9767    27.0305  -16.536  &lt; 2e-16 ***\n#&gt; cnaceS0           -441.7225    25.8682  -17.076  &lt; 2e-16 ***\n#&gt; cno1B0            -563.9700    40.6603  -13.870  &lt; 2e-16 ***\n#&gt; cno1C0            -580.4722    40.3078  -14.401  &lt; 2e-16 ***\n#&gt; cno1D0            -771.5835    38.8634  -19.854  &lt; 2e-16 ***\n#&gt; cno1E0            -963.6523    38.6908  -24.907  &lt; 2e-16 ***\n#&gt; cno1F0            -962.4620    39.0754  -24.631  &lt; 2e-16 ***\n#&gt; cno1G0            -943.1722    40.6791  -23.186  &lt; 2e-16 ***\n#&gt; cno1H0           -1006.9339    39.2172  -25.676  &lt; 2e-16 ***\n#&gt; cno1I0            -799.1017    41.4442  -19.281  &lt; 2e-16 ***\n#&gt; cno1J0            -833.4729    50.5531  -16.487  &lt; 2e-16 ***\n#&gt; cno1K0            -970.9199    39.7730  -24.412  &lt; 2e-16 ***\n#&gt; cno1L0            -946.7735    38.8851  -24.348  &lt; 2e-16 ***\n#&gt; cno1M0            -975.6476    38.9286  -25.062  &lt; 2e-16 ***\n#&gt; cno1N0            -974.9346    40.1732  -24.268  &lt; 2e-16 ***\n#&gt; cno1O0           -1008.8982    40.1716  -25.115  &lt; 2e-16 ***\n#&gt; cno1P0           -1032.8826    39.0379  -26.458  &lt; 2e-16 ***\n#&gt; cno1Q0            -941.5737    66.0825  -14.248  &lt; 2e-16 ***\n#&gt; estrato21           18.7235    28.1838    0.664 0.506476    \n#&gt; estrato22          122.1463    28.5034    4.285 1.83e-05 ***\n#&gt; estrato23          201.3305    28.4886    7.067 1.59e-12 ***\n#&gt; estrato24          126.1192    29.8559    4.224 2.40e-05 ***\n#&gt; tipojor2          -593.1639     5.3768 -110.318  &lt; 2e-16 ***\n#&gt; anoanti             12.3334     0.3434   35.921  &lt; 2e-16 ***\n#&gt; mesanti              2.0871     0.6094    3.425 0.000615 ***\n#&gt; tipocon2          -119.0692     5.0451  -23.601  &lt; 2e-16 ***\n#&gt; treatment:nuts12    15.2013    19.3060    0.787 0.431056    \n#&gt; treatment:nuts13   -71.6951    20.4396   -3.508 0.000452 ***\n#&gt; treatment:nuts14    27.4429    19.1259    1.435 0.151329    \n#&gt; treatment:nuts15   -66.6916    17.9164   -3.722 0.000197 ***\n#&gt; treatment:nuts16    -8.9706    19.3186   -0.464 0.642398    \n#&gt; treatment:nuts17    39.0070    25.1314    1.552 0.120634    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 373097.8)\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n\n\nEstimamos diferencias entre sector público y privado para Madrid y Andalucía, para un hombre a jornada completa y con estudios de licenciatura o superior.\n\n\nCode\n\nestim_publico_madrid &lt;- predict(mod_inter_con_nuts, newdata = ess_fake_publico |&gt;  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\" , nuts1 == \"3\" ) )\n\nestim_privado_madrid &lt;- predict(mod_inter_con_nuts, newdata = ess_fake_privado |&gt;  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"3\") )\n\n\nmean(estim_publico_madrid)\n#&gt; [1] 2533.962\nmean(estim_privado_madrid)\n#&gt; [1] 2466.076\n\n(s_learner_with_pond_madrid &lt;- mean(estim_publico_madrid) - mean(estim_privado_madrid))\n#&gt; [1] 67.88572\n\n\n\nestim_publico_sur &lt;- predict(mod_inter_con_nuts, newdata = ess_fake_publico |&gt;  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\" , nuts1 == \"1\" ) )\n\nestim_privado_sur &lt;- predict(mod_inter_con_nuts, newdata = ess_fake_privado |&gt;  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"1\") )\n\n\nmean(estim_publico_sur)\n#&gt; [1] 2449.94\nmean(estim_privado_sur)\n#&gt; [1] 2310.359\n\n(s_learner_with_pond_sur &lt;- mean(estim_publico_sur) - mean(estim_privado_sur))\n#&gt; [1] 139.5808\n\n\nBueno, pues según esto, parece que para ese perfil, dónde se ha tenido en cuenta edad, años de antigüedad y demás, se gana un poco más en el sector público que en el privado, aunque esa diferencia es mayor en el Sur que en Madrid."
  },
  {
    "objectID": "2023/y-si-parte-iii.html#t--learner",
    "href": "2023/y-si-parte-iii.html#t--learner",
    "title": "¿Y si … ? Parte III",
    "section": "T- learner",
    "text": "T- learner\nOtro de los metalearners empleados es el T-learner, ya explicado en post anteriores. Aquí vamos a usarlo sin tener en cuenta la ponderación de la encuesta.\nEn el T-learner se ajusta un modelo para cuando sea sector público y otro para cuando sea sector privado y se ve la diferencia de las medias de sus estimaciones.\n\n\nCode\n\nmodpublico &lt;- lm(outcome ~ sexo  + anos2  +  estu + cno1 + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess[ess$treatment==1, ])\nmodprivado &lt;-  lm(outcome ~ sexo +  anos2  +  estu  + cno1 + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess[ess$treatment==0, ])\n\ness_sub &lt;- ess  %&gt;% \n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\") \n\n# t-learner\n(t_learner &lt;- mean(predict(modpublico, ess_sub)) - \n  mean(predict(modprivado, ess_sub)) )\n#&gt; [1] -175.951\n\ness_sub_madrid &lt;- ess  %&gt;% \n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"3\")\n\n# t-learner\n(t_learner_madrid &lt;- mean(predict(modpublico, ess_sub_madrid)) - \n  mean(predict(modprivado, ess_sub_madrid)) )\n#&gt; [1] -296.4373\n\n\n\ness_sub_sur &lt;- ess  %&gt;% \n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"1\")\n\n# t-learner\n(t_learner_sur &lt;- mean(predict(modpublico, ess_sub_sur)) - \n  mean(predict(modprivado, ess_sub_sur)) )\n#&gt; [1] -159.0581\n\n\nEn este caso, nos sale que se ganaría más en el sector público que en el privado. ¿Con qué nos quedamos?"
  },
  {
    "objectID": "2023/y-si-parte-iii.html#x-learner",
    "href": "2023/y-si-parte-iii.html#x-learner",
    "title": "¿Y si … ? Parte III",
    "section": "X-learner",
    "text": "X-learner\nYa expliqué en su día en que consiste un X-learner (../2020/12/30/y-si-parte-ii/#x-learner)\nBásicamente, usas el modelo ajustado con treatment = 1 para predecir las observaciones con treatment = 0 y al revés en un intento de estimar el potential outcome. Luego haces dos modelos para modelar las diferencias entre el outcome y las predicciones anteriores y otro modelo de propensity score que se usará para ponderar esas dos predicciones.\n\n\nCode\n\n\nm1 &lt;- lm(outcome ~ sexo   + anos2 +   estu + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess[ess$treatment==1, ])\nm2 &lt;- lm(outcome ~ sexo   + anos2 +   estu  + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess[ess$treatment==0, ])\n\n\n# Usamos modelo 1 para estimar cuando W=0 y el modelo 2 para estimar cuando W = 1\n\n# Con el viejo R-base sería \ness$Difer[ess$treatment==0] &lt;- ess$outcome[ess$treatment==0] - predict(m1, ess[ess$treatment==0, ])\nhead(ess[ess$treatment==0, c(\"outcome\", \"Difer\")])\n#&gt; # A tibble: 6 × 2\n#&gt;   outcome Difer\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    302. -267.\n#&gt; 2   1181. -263.\n#&gt; 3   1314. -784.\n#&gt; 4   1210. -285.\n#&gt; 5   1152. -201.\n#&gt; 6   1184. -237.\n\n\ness$Difer[ess$treatment==1] &lt;- ess$outcome[ess$treatment==1] - predict(m2, ess[ess$treatment==1, ])\nhead(ess[ess$treatment==1, c(\"outcome\", \"Difer\")])\n#&gt; # A tibble: 6 × 2\n#&gt;   outcome  Difer\n#&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   2112.  288. \n#&gt; 2   1989. -405. \n#&gt; 3   2183.  -36.2\n#&gt; 4   1954. -212. \n#&gt; 5   1779. -483. \n#&gt; 6   1738. -447.\n\n# Modelamos las diferencias\n\n\nm3 &lt;- lm(Difer ~ sexo   + anos2 +   estu + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess[ess$treatment==1, ])\nm4 &lt;- lm(Difer ~ sexo   + anos2 +   estu + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess[ess$treatment==0, ])\n\n# Combinamos\n\nglm1 &lt;- glm(treatment ~ sexo   + anos2 +   estu + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess, family= binomial)\ness$pesos &lt;- predict(glm1, ess, type = \"response\")\n\n\n\ness$combinado &lt;- ess$pesos * predict(m4, ess) + (1-ess$pesos) * predict(m3, ess) \n\nhead(ess[, c(\"outcome\", \"treatment\",\"Difer\", \"pesos\", \"combinado\")])\n#&gt; # A tibble: 6 × 5\n#&gt;   outcome treatment Difer  pesos combinado\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1    302.         0 -267. 0.111     -254. \n#&gt; 2   1181.         0 -263. 0.0238     125. \n#&gt; 3   1314.         0 -784. 0.0591      93.2\n#&gt; 4   1210.         0 -285. 0.0332      74.0\n#&gt; 5   1152.         0 -201. 0.0107     -19.7\n#&gt; 6   1184.         0 -237. 0.0201     -62.1\n\n(x_learner &lt;- ess  %&gt;% \n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\") |&gt; \n  group_by(treatment)  %&gt;%\n  summarise(mean = mean(outcome)) |&gt; \n  pivot_wider(names_from = treatment, values_from = mean, names_prefix = \"mean_\") |&gt; \n  mutate(\n    estim_xlearner = mean_1 - mean_0) |&gt; \n  pull(estim_xlearner))\n#&gt; [1] -228.745\n\n\n\n(x_learner_madrid &lt;- ess  %&gt;% \n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1==\"3\") |&gt; \n  group_by(treatment)  %&gt;%\n  summarise(mean = mean(outcome)) |&gt; \n  pivot_wider(names_from = treatment, values_from = mean, names_prefix = \"mean_\") |&gt; \n  mutate(\n    estim_xlearner = mean_1 - mean_0) |&gt; \n  pull(estim_xlearner))\n#&gt; [1] -501.2355\n\n\n(x_learner_sur &lt;- ess  %&gt;% \n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1==\"1\") |&gt; \n  group_by(treatment)  %&gt;%\n  summarise(mean = mean(outcome)) |&gt; \n  pivot_wider(names_from = treatment, values_from = mean, names_prefix = \"mean_\") |&gt; \n  mutate(\n    estim_xlearner = mean_1 - mean_0) |&gt; \n  pull(estim_xlearner))\n#&gt; [1] -55.67715"
  },
  {
    "objectID": "2023/y-si-parte-iii.html#doubly-robust-estimator",
    "href": "2023/y-si-parte-iii.html#doubly-robust-estimator",
    "title": "¿Y si … ? Parte III",
    "section": "Doubly robust estimator",
    "text": "Doubly robust estimator\nCon idea parecida al X-learner , en el sentido de mezclar las estrategias de usar Inverse probability weighting y el de hacer un modelo de la respuesta condicionando por los counfounders.\nDe nuevo, al igual que con el T-Learner o el X-Learner no vamos a tener en cuenta la variable de ponderación de casos.\nEl estimador sería algo así como\n\\[\\dfrac{1}{n} \\sum_{i=1}^n \\left[ \\dfrac{Y_i \\cdot A_i - \\color{red}{ \\left(A_i -\\pi(X_i)\\right) \\mu(X_i, A_i)})} {\\pi(X_i)}  - \\dfrac{Y_i \\cdot (1-A_i) - \\color{red}{ \\left(A_i -\\pi(X_i)\\right) \\mu(X_i,A_i)})} {1-\\pi(X_i)}  \\right ] \\tag{1}\\]\nDónde \\(\\mu\\) hace referencia al modelo para estimar el outcome y \\(\\pi\\) al modelo de propensity score.\nEste Doubly robust estimator es una combinación entre usar inverse probability weighting y el modelo de la media del outcome. Este estimador suele ser consistnete si al menos uno de los dos modelos es correcto. A la expresión coloreda en rojo se le denomina augmented IPW estimator\nEn código es bastante sencillo.\n\n\nCode\ndr_estimator &lt;- function(data, prop_model, mean_model){\ndata %&gt;% \nmutate(\n  prob = predict(prop_model, newdata = data, type = \"response\"),\n  pred = predict(mean_model, newdata = data, type = \"response\"), \n  augm = (treatment - prob) * pred \n  ) %&gt;%\nsummarise(\n  EYpublico = mean((outcome * treatment -augm) / prob),\n  EYprivado= mean((outcome * (1 - treatment) - augm) / (1 - prob))\n)  %&gt;%\nmutate(dre = EYpublico - EYprivado)\n}\n\n\nY si usamos ese estimador tenemos\n\n\nCode\n\nprop_model  &lt;- glm(treatment ~  sexo + anos2+ cnace + cno1 + estrato2 +  estu + tipojor  + anoanti + mesanti + tipocon  + nuts1, data = ess, family = binomial)\nmean_model &lt;- glm(outcome ~ treatment +  sexo + anos2+ cnace + cno1 + estrato2 +  estu + tipojor  + anoanti + mesanti + tipocon  + nuts1 , data = ess, family = gaussian)\n\n\nsummary(prop_model)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = treatment ~ sexo + anos2 + cnace + cno1 + estrato2 + \n#&gt;     estu + tipojor + anoanti + mesanti + tipocon + nuts1, family = binomial, \n#&gt;     data = ess)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -8.090e+00  5.934e-01 -13.634  &lt; 2e-16 ***\n#&gt; sexo6        2.674e-01  2.116e-02  12.637  &lt; 2e-16 ***\n#&gt; anos202      8.244e-01  5.243e-01   1.572 0.115866    \n#&gt; anos203      1.488e+00  5.233e-01   2.844 0.004459 ** \n#&gt; anos204      1.817e+00  5.233e-01   3.473 0.000515 ***\n#&gt; anos205      1.981e+00  5.235e-01   3.785 0.000154 ***\n#&gt; anos206      2.064e+00  5.242e-01   3.937 8.24e-05 ***\n#&gt; cnaceC1     -2.069e+00  1.909e-01 -10.835  &lt; 2e-16 ***\n#&gt; cnaceC2     -2.340e+00  2.854e-01  -8.200 2.41e-16 ***\n#&gt; cnaceC3      4.823e-01  1.773e-01   2.720 0.006533 ** \n#&gt; cnaceC4     -1.680e+01  1.053e+02  -0.160 0.873244    \n#&gt; cnaceC5     -1.634e+01  1.897e+02  -0.086 0.931349    \n#&gt; cnaceC6     -2.705e+00  2.597e-01 -10.416  &lt; 2e-16 ***\n#&gt; cnaceC7     -1.649e+01  1.280e+02  -0.129 0.897495    \n#&gt; cnaceC8     -6.740e-01  1.545e-01  -4.363 1.28e-05 ***\n#&gt; cnaceD0     -5.031e+00  7.223e-01  -6.964 3.30e-12 ***\n#&gt; cnaceE0      1.948e+00  1.483e-01  13.139  &lt; 2e-16 ***\n#&gt; cnaceF0      2.271e-01  1.511e-01   1.503 0.132789    \n#&gt; cnaceG1     -1.622e+01  1.124e+02  -0.144 0.885252    \n#&gt; cnaceG2     -1.600e+01  9.413e+01  -0.170 0.865012    \n#&gt; cnaceH1      1.437e+00  1.507e-01   9.534  &lt; 2e-16 ***\n#&gt; cnaceH2      2.936e+00  1.482e-01  19.808  &lt; 2e-16 ***\n#&gt; cnaceI0     -7.434e-01  1.783e-01  -4.170 3.04e-05 ***\n#&gt; cnaceJ0      3.013e-01  1.472e-01   2.047 0.040683 *  \n#&gt; cnaceK0      1.752e-02  1.484e-01   0.118 0.905995    \n#&gt; cnaceL0      6.883e-01  1.787e-01   3.852 0.000117 ***\n#&gt; cnaceM0      1.066e+00  1.455e-01   7.326 2.38e-13 ***\n#&gt; cnaceN0     -7.697e-01  1.550e-01  -4.966 6.85e-07 ***\n#&gt; cnaceO0      2.246e+01  9.488e+01   0.237 0.812845    \n#&gt; cnaceP0      2.748e+00  1.487e-01  18.482  &lt; 2e-16 ***\n#&gt; cnaceQ0      2.013e+00  1.464e-01  13.750  &lt; 2e-16 ***\n#&gt; cnaceR0      1.759e+00  1.487e-01  11.834  &lt; 2e-16 ***\n#&gt; cnaceS0      5.370e-01  1.554e-01   3.455 0.000550 ***\n#&gt; cno1B0       1.487e+00  6.226e-02  23.887  &lt; 2e-16 ***\n#&gt; cno1C0       8.397e-01  5.779e-02  14.529  &lt; 2e-16 ***\n#&gt; cno1D0       4.886e-01  5.826e-02   8.386  &lt; 2e-16 ***\n#&gt; cno1E0       3.641e-01  6.314e-02   5.767 8.08e-09 ***\n#&gt; cno1F0       2.401e-02  6.783e-02   0.354 0.723379    \n#&gt; cno1G0      -2.950e-01  1.191e-01  -2.476 0.013269 *  \n#&gt; cno1H0       4.008e-01  6.743e-02   5.944 2.79e-09 ***\n#&gt; cno1I0       5.683e-01  1.077e-01   5.277 1.31e-07 ***\n#&gt; cno1J0       1.599e+00  1.437e-01  11.121  &lt; 2e-16 ***\n#&gt; cno1K0       4.359e-01  9.292e-02   4.691 2.72e-06 ***\n#&gt; cno1L0       2.333e-01  7.777e-02   2.999 0.002705 ** \n#&gt; cno1M0      -8.548e-01  1.435e-01  -5.956 2.59e-09 ***\n#&gt; cno1N0       2.546e-01  7.800e-02   3.264 0.001098 ** \n#&gt; cno1O0       3.076e-01  7.608e-02   4.043 5.27e-05 ***\n#&gt; cno1P0      -8.458e-01  9.808e-02  -8.624  &lt; 2e-16 ***\n#&gt; cno1Q0       3.002e+00  1.482e+03   0.002 0.998384    \n#&gt; estrato21   -9.996e-01  2.206e-01  -4.531 5.87e-06 ***\n#&gt; estrato22    9.719e-02  2.202e-01   0.441 0.658908    \n#&gt; estrato23    9.132e-01  2.202e-01   4.147 3.37e-05 ***\n#&gt; estrato24    5.801e-01  2.244e-01   2.585 0.009724 ** \n#&gt; estu2        2.584e-01  1.589e-01   1.626 0.103944    \n#&gt; estu3        1.217e+00  1.575e-01   7.727 1.10e-14 ***\n#&gt; estu4        1.253e+00  1.582e-01   7.917 2.43e-15 ***\n#&gt; estu5        1.644e+00  1.600e-01  10.275  &lt; 2e-16 ***\n#&gt; estu6        1.470e+00  1.607e-01   9.145  &lt; 2e-16 ***\n#&gt; estu7        1.858e+00  1.603e-01  11.593  &lt; 2e-16 ***\n#&gt; tipojor2    -1.526e+00  3.257e-02 -46.857  &lt; 2e-16 ***\n#&gt; anoanti      7.184e-02  1.209e-03  59.437  &lt; 2e-16 ***\n#&gt; mesanti      3.078e-02  2.627e-03  11.718  &lt; 2e-16 ***\n#&gt; tipocon2     1.800e+00  2.660e-02  67.697  &lt; 2e-16 ***\n#&gt; nuts12      -7.224e-02  3.819e-02  -1.891 0.058560 .  \n#&gt; nuts13      -5.637e-01  3.770e-02 -14.952  &lt; 2e-16 ***\n#&gt; nuts14       5.716e-01  3.905e-02  14.636  &lt; 2e-16 ***\n#&gt; nuts15      -3.778e-01  3.500e-02 -10.795  &lt; 2e-16 ***\n#&gt; nuts16       3.272e-01  3.778e-02   8.660  &lt; 2e-16 ***\n#&gt; nuts17       1.661e-02  5.427e-02   0.306 0.759563    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 193458  on 216725  degrees of freedom\n#&gt; Residual deviance:  76006  on 216657  degrees of freedom\n#&gt; AIC: 76144\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 18\nsummary(mean_model)\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = outcome ~ treatment + sexo + anos2 + cnace + cno1 + \n#&gt;     estrato2 + estu + tipojor + anoanti + mesanti + tipocon + \n#&gt;     nuts1, family = gaussian, data = ess)\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error  t value Pr(&gt;|t|)    \n#&gt; (Intercept)  2877.5791    61.1896   47.027  &lt; 2e-16 ***\n#&gt; treatment      17.9283     8.3590    2.145  0.03197 *  \n#&gt; sexo6        -240.4080     4.9123  -48.940  &lt; 2e-16 ***\n#&gt; anos202        22.1784    43.0544    0.515  0.60647    \n#&gt; anos203        89.1187    42.8391    2.080  0.03750 *  \n#&gt; anos204       178.3491    42.8356    4.164 3.13e-05 ***\n#&gt; anos205       240.2115    42.9841    5.588 2.29e-08 ***\n#&gt; anos206        99.7613    43.5939    2.288  0.02211 *  \n#&gt; cnaceC1      -362.2154    26.7350  -13.548  &lt; 2e-16 ***\n#&gt; cnaceC2      -336.6893    29.3593  -11.468  &lt; 2e-16 ***\n#&gt; cnaceC3      -407.7262    33.1492  -12.300  &lt; 2e-16 ***\n#&gt; cnaceC4      -142.3327    26.9322   -5.285 1.26e-07 ***\n#&gt; cnaceC5      -320.6788    30.9230  -10.370  &lt; 2e-16 ***\n#&gt; cnaceC6      -214.7569    28.1272   -7.635 2.26e-14 ***\n#&gt; cnaceC7      -295.1496    27.9070  -10.576  &lt; 2e-16 ***\n#&gt; cnaceC8      -304.6001    26.3723  -11.550  &lt; 2e-16 ***\n#&gt; cnaceD0       315.2232    33.1870    9.498  &lt; 2e-16 ***\n#&gt; cnaceE0      -368.5855    27.9188  -13.202  &lt; 2e-16 ***\n#&gt; cnaceF0      -251.8171    26.4790   -9.510  &lt; 2e-16 ***\n#&gt; cnaceG1      -317.1349    27.2848  -11.623  &lt; 2e-16 ***\n#&gt; cnaceG2      -443.6205    28.1035  -15.785  &lt; 2e-16 ***\n#&gt; cnaceH1       -24.2486    28.2536   -0.858  0.39076    \n#&gt; cnaceH2      -252.9940    28.3037   -8.939  &lt; 2e-16 ***\n#&gt; cnaceI0      -364.2829    27.7660  -13.120  &lt; 2e-16 ***\n#&gt; cnaceJ0      -399.0452    26.5212  -15.046  &lt; 2e-16 ***\n#&gt; cnaceK0      -184.2903    27.1146   -6.797 1.07e-11 ***\n#&gt; cnaceL0      -376.1283    34.1175  -11.025  &lt; 2e-16 ***\n#&gt; cnaceM0      -481.0621    26.1363  -18.406  &lt; 2e-16 ***\n#&gt; cnaceN0      -465.0980    26.1408  -17.792  &lt; 2e-16 ***\n#&gt; cnaceO0      -498.8197    27.9612  -17.840  &lt; 2e-16 ***\n#&gt; cnaceP0      -785.3254    28.6574  -27.404  &lt; 2e-16 ***\n#&gt; cnaceQ0      -448.4801    26.8000  -16.734  &lt; 2e-16 ***\n#&gt; cnaceR0      -286.1541    27.6853  -10.336  &lt; 2e-16 ***\n#&gt; cnaceS0      -523.3793    28.0319  -18.671  &lt; 2e-16 ***\n#&gt; cno1B0      -1061.3503    16.4387  -64.564  &lt; 2e-16 ***\n#&gt; cno1C0      -1133.4003    13.6831  -82.832  &lt; 2e-16 ***\n#&gt; cno1D0      -1275.7747    13.3916  -95.267  &lt; 2e-16 ***\n#&gt; cno1E0      -1533.8695    14.7719 -103.837  &lt; 2e-16 ***\n#&gt; cno1F0      -1503.8142    15.1869  -99.020  &lt; 2e-16 ***\n#&gt; cno1G0      -1478.8524    17.2092  -85.934  &lt; 2e-16 ***\n#&gt; cno1H0      -1541.8639    16.5003  -93.444  &lt; 2e-16 ***\n#&gt; cno1I0      -1498.1010    20.0039  -74.890  &lt; 2e-16 ***\n#&gt; cno1J0      -1517.7995    36.9359  -41.093  &lt; 2e-16 ***\n#&gt; cno1K0      -1527.8727    18.7324  -81.563  &lt; 2e-16 ***\n#&gt; cno1L0      -1494.9651    15.1867  -98.439  &lt; 2e-16 ***\n#&gt; cno1M0      -1549.5391    16.7938  -92.269  &lt; 2e-16 ***\n#&gt; cno1N0      -1589.9336    17.9925  -88.367  &lt; 2e-16 ***\n#&gt; cno1O0      -1570.6894    16.5873  -94.692  &lt; 2e-16 ***\n#&gt; cno1P0      -1595.2569    16.1226  -98.945  &lt; 2e-16 ***\n#&gt; cno1Q0      -1530.0462   138.3092  -11.063  &lt; 2e-16 ***\n#&gt; estrato21     -26.9047    30.6473   -0.878  0.38001    \n#&gt; estrato22      93.7567    30.8162    3.042  0.00235 ** \n#&gt; estrato23     189.6530    30.7961    6.158 7.36e-10 ***\n#&gt; estrato24      99.0138    33.0210    2.999  0.00271 ** \n#&gt; estu2          17.6977    22.9002    0.773  0.43963    \n#&gt; estu3          31.8206    22.8239    1.394  0.16326    \n#&gt; estu4         146.0304    23.0680    6.330 2.45e-10 ***\n#&gt; estu5         188.1811    23.7531    7.922 2.34e-15 ***\n#&gt; estu6         340.0061    24.1380   14.086  &lt; 2e-16 ***\n#&gt; estu7         629.4307    23.9967   26.230  &lt; 2e-16 ***\n#&gt; tipojor2     -632.4750     6.1453 -102.921  &lt; 2e-16 ***\n#&gt; anoanti        13.7806     0.2783   49.511  &lt; 2e-16 ***\n#&gt; mesanti         1.9103     0.5911    3.232  0.00123 ** \n#&gt; tipocon2     -105.5518     5.9315  -17.795  &lt; 2e-16 ***\n#&gt; nuts12        112.1766     8.2021   13.677  &lt; 2e-16 ***\n#&gt; nuts13        159.6686     8.4096   18.986  &lt; 2e-16 ***\n#&gt; nuts14         -0.7868     8.6804   -0.091  0.92778    \n#&gt; nuts15        115.6430     7.5034   15.412  &lt; 2e-16 ***\n#&gt; nuts16         70.6160     8.4813    8.326  &lt; 2e-16 ***\n#&gt; nuts17         13.0092    12.1986    1.066  0.28622    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 955907.9)\n#&gt; \n#&gt;     Null deviance: 3.0241e+11  on 216725  degrees of freedom\n#&gt; Residual deviance: 2.0710e+11  on 216656  degrees of freedom\n#&gt; AIC: 3599521\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2\n(dre_estimator &lt;-  ess  %&gt;%\n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\") %&gt;%\n  dr_estimator(prop_model, mean_model) |&gt; \n  pull(dre))\n#&gt; [1] 153.843\n\n\n(dre_estimator_madrid &lt;-  ess  %&gt;%\n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"3\") %&gt;%\n  dr_estimator(prop_model, mean_model) |&gt; \n  pull(dre))\n#&gt; [1] 168.0452\n\n\n\n(dre_estimator_sur &lt;-  ess  %&gt;%\n  filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"1\") %&gt;%\n  dr_estimator(prop_model, mean_model) |&gt; \n  pull(dre))\n#&gt; [1] 183.358"
  },
  {
    "objectID": "2023/y-si-parte-iii.html#resumiendo",
    "href": "2023/y-si-parte-iii.html#resumiendo",
    "title": "¿Y si … ? Parte III",
    "section": "Resumiendo",
    "text": "Resumiendo\nEl S-learner usando ponderación de observaciones y el doubly robust estimator (sin usar ponderaciones) nos dan estimaciones diciendo que se gana más en el sector público que en el privado, mientras que el t-learner y el x-learner nos dicen lo contrario.\nAsí que, no me queda claro la respueta a la pregunta inicial.\n\n\nCode\n\nres &lt;- data.frame(s_learner_with_pond_madrid = s_learner_with_pond_madrid,\n           s_learner_with_pond_sur = s_learner_with_pond_sur,\n           t_learner_madrid= t_learner_madrid, t_learner_sur = t_learner_sur, \n           x_learner_madrid = x_learner_madrid, x_learner_sur = x_learner_sur, \n           dre_estimator_madrid = dre_estimator_madrid, dre_estimator_sur = dre_estimator_sur )\n\n\nres |&gt; \n    pivot_longer(everything(), names_to = \"estimador\", values_to = \"valor\") \n#&gt; # A tibble: 8 × 2\n#&gt;   estimador                   valor\n#&gt;   &lt;chr&gt;                       &lt;dbl&gt;\n#&gt; 1 s_learner_with_pond_madrid   67.9\n#&gt; 2 s_learner_with_pond_sur     140. \n#&gt; 3 t_learner_madrid           -296. \n#&gt; 4 t_learner_sur              -159. \n#&gt; 5 x_learner_madrid           -501. \n#&gt; 6 x_learner_sur               -55.7\n#&gt; 7 dre_estimator_madrid        168. \n#&gt; 8 dre_estimator_sur           183."
  },
  {
    "objectID": "2023/y-si-parte-iii.html#coda",
    "href": "2023/y-si-parte-iii.html#coda",
    "title": "¿Y si … ? Parte III",
    "section": "Coda",
    "text": "Coda\n\nEs complicado dar una respuesta concluyente a la pregunta inicial.\nMi objetivo era sólo contaros algunas formas de estimar “efectos causales”, o si es gusta más, diferencias entre grupos condicionando por variables\nLa inferencia causal es complicada, ha de sustentarse en un análisis teórico previo. Yo he decidido que no había colliders por ejemplo\nHe obviado variables que podrían influir tanto en la variable respuesta como en el tratamiento (sector público o privado), pero estos son datos reales, no una simulación ad hoc, y en el mundo real tienes que tomar decisiones y apechugar con ellas.\nPara escribir este post lo he hecho con Rstudio y con el github copilot activado y la verdad es que ayuda bastante, incluso a completar las fórmulas en latex."
  },
  {
    "objectID": "2023/y-si-parte-iii.html#actualización",
    "href": "2023/y-si-parte-iii.html#actualización",
    "title": "¿Y si … ? Parte III",
    "section": "Actualización",
    "text": "Actualización\nMe acabo de acordar de otra forma de estimar esto. Consiste en:\n\nHago un modelo para estimar el salario neto pero sólo usando la población que trabaja en el sector privado.\nAplico ese modelo para obtener estimaciones sobre la población que trabaja en el sector público.\nComparo la estimación obtenida con el salario neto de esa subpoblación.\n\nEs parecido al X-learner, pero sin tanta complicación. Es como decir ¿cuánto ganarían los que están en el sector público si estuvieran en el privado?\nAl hacerlo así hay que obviar en el modelo en la subpoblación para el sector privado, las variables de cnace y de cno1 puesto que tienen niveles en el sector público que no están en el privado y el modelo daría error por niveles nuevos. Un modelo mixto si podría hacer eso.\n\n\nCode\n\nmod_sector_privado &lt;- svyglm(outcome ~  sexo + anos2 +   estu  + estrato2  + tipojor  + anoanti + mesanti + tipocon  + nuts1, design = disenno, subset = treatment == 0)\n\ness_sub_publico_madrid &lt;- ess |&gt; \n  filter(treatment == 1,tipojor == \"1\", estu == \"7\", sexo == \"1\", nuts1 == \"3\" ) \n\nestim_publico_con_mod_privado_madrid &lt;-  predict(mod_sector_privado,\n                                               ess_sub_publico_madrid)\n\n(media_estimada_sector_publico_madrid &lt;- weighted.mean(estim_publico_con_mod_privado_madrid, ess_sub_publico_madrid$factotal) )  \n#&gt; [1] 2412.159\n\n\n(media_observada_sector_publico &lt;- weighted.mean(ess_sub_publico_madrid$outcome    , \n                                                 ess_sub_publico_madrid$factotal) )\n#&gt; [1] 2535.845\n\n(diferencia &lt;- media_observada_sector_publico - media_estimada_sector_publico_madrid)\n#&gt; [1] 123.6857\n\n\nY haciéndolo así se tendría que si los trabajadores del sector público hombres a jornada completa y con estudios de licenciados o superiores lse cambiaran al privado, manteniendo el resto igual ganarían unos 124 euros menos al mes de media.\nAsí que tengo varias preguntas\n\n¿Qué piensan mis escasos lectores a la vista de estas estimaciones? ¿Hay brecha salarial?\nSea cual sea la respuesta, ¿no os parece que podría utilizar un método u otro según lo que me interese contar? Dan ganas de escribir un manual sobre como “engañar con estadística de forma avanzada”, pero ya conozco a quien tiene esa idea en mente\n¿Cuál sería la metodología correcta si es que existe? ¿Quizá adentrándonos en el mundo bayesiano? ¿o es todo un artificio “técnico”?"
  },
  {
    "objectID": "2023/y-si-parte-iii.html#otra-actualización",
    "href": "2023/y-si-parte-iii.html#otra-actualización",
    "title": "¿Y si … ? Parte III",
    "section": "Otra actualización",
    "text": "Otra actualización\nUna de las hipótesis es que en el sector privado la distribución es más dispersa.\nTodo esto debería haberse hecho antes que todos los modelos. EL EDA es lo primero.\n\n\nCode\n# En general parece que no \ness |&gt; \n    mutate(nuts1 = as_factor(nuts1)) |&gt;\n    ggplot(aes(x =outcome,fill = as_factor(treatment))) +\n    geom_density(alpha = 0.5) + scale_x_continuous(limits = c(0,8000)) +\n    facet_wrap( ~ nuts1)\n\n\n\n\n\n\n\n\n\nPero y si la vemos, para los licenciados, hombres y a jornada completa.\nEn la comunidad de Madrid y en Canarias si se aprecia que en la cola de la derecha es superior la función de densidad en el sector privado.\n\n\nCode\ness |&gt; \n    filter(tipojor == \"1\", estu == \"7\", sexo == \"1\") |&gt;\n    mutate(nuts1 = as_factor(nuts1)) |&gt;\n    ggplot(aes(x =outcome,fill = as_factor(treatment))) +\n    geom_density(alpha = 0.4) + scale_x_continuous(limits = c(0,8000)) +\n    facet_wrap( ~ nuts1)\n\n\n\n\n\n\n\n\n\nY si lo vemos para un par de ocupaciones, tales como Directores y gerentes (A0) y para técnicos y profesionales científicos e intelectuales de la salud y la enseñanza(B0).\nGerentes (A0) mejor en el privado, curritos de la enseñanza y la salud pues..\n\n\nCode\n\ness |&gt;\n    filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", cno1 %in% c(\"A0\", \"B0\")) |&gt;\n    mutate(nuts1 = as_factor(nuts1)) |&gt;\n    ggplot(aes(x = outcome, fill = as_factor(treatment))) +\n    geom_density(alpha = 0.4) + scale_x_continuous(limits = c(0, 10000)) +\n    facet_grid( nuts1 ~ cno1)  \n\n\n\n\n\n\n\n\n\nY si usamos el primer modelo que vimos\n\n\nCode\n\ness |&gt;\n    mutate(estim = predict(mod_sector_privado, ess)) |&gt; \n    filter(tipojor == \"1\", estu == \"7\", sexo == \"1\", cno1 %in% c(\"A0\", \"B0\")) |&gt;\n    mutate(nuts1 = as_factor(nuts1)) |&gt;\n    ggplot(aes(x = estim, fill = as_factor(treatment))) +\n    geom_density(alpha = 0.4) + scale_x_continuous(limits = c(0, 10000)) +\n    facet_grid( nuts1 ~ cno1)  \n\n\n\n\n\n\n\n\n\nPero claramente como lo que predice es la media condicionada queda todo muy “centrado”. Lo suyo sería un modelo bayesiano o hacer boostraping y tener la posterior predictive, para incorporar correctamente la variabilidad. A ver si lo hago en otro post, pero con numpyro, que stan no puede con estos datos"
  },
  {
    "objectID": "2023/shinylive-python.html",
    "href": "2023/shinylive-python.html",
    "title": "Shinylive-python",
    "section": "",
    "text": "Ayer asistí a una charla de David Durey en el grupo de R-madrid. Era sobre como migraron de Tableau a Shiny y fue una charla excepcional, gracias David.\nEn shiny está habiendo avances, tales como shiny en python o la extensión shinylive para Quarto. Está extensión permite ejecutar código de python y de R (pero está más avanzado en python) en el navegador, gracias a WebAssembly que permite correr ese código en el navegador en vez de tener un servidor corriendo\nAsí que ni corto ni perezoso, voy a probarlo, copiando un código de ejemplo y poniendo una aplicación shiny dentro de este documento. Si la ves y se ejecuta, que sepas que no hay un servidor detrás y que eres tú en tu navegador quién está ejecutando esta aplicación de shiny para python.\n\n\n\n\n\n\nCaution\n\n\n\nDespués de desplegar en netlify parece que no funciona. Si a alguien le va que lo ponga en los comentarios\n\n\n#| standalone: true\n#| components: [editor, viewer]\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom shiny import App, render, ui\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"n\", \"N\", 0, 100, 20),\n        ),\n        ui.panel_main(\n            ui.output_plot(\"histogram\"),\n        ),\n    ),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def histogram():\n        np.random.seed(19680801)\n        x = 100 + 15 * np.random.randn(437)\n        plt.hist(x, input.n(), density=True)\n\n\napp = App(app_ui, server, debug=True)\n\nPara R usaría webry una versión en desarrollo de shinylive que he sido incapaz de compilar correctamente, y mira que se me dan bien estas cosas.\nAsí que quizá toque esperar un poco y ver como avanzan este tipo de proyectos de ejecutar cosas de python y R e incluso shinyapps en el navegador y por tanto en la máquina del usuario sin necesidad de un servidor."
  },
  {
    "objectID": "2023/shinylive-r.html",
    "href": "2023/shinylive-r.html",
    "title": "shiny live en R",
    "section": "",
    "text": "Esto tampoco tiene pinta de funcionar. ups.\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\n\n# Define UI for app that draws a histogram ----\nui &lt;- page_sidebar(\n  sidebar = sidebar(open = \"open\",\n    numericInput(\"n\", \"Sample count\", 100),\n    checkboxInput(\"pause\", \"Pause\", FALSE),\n  ),\n  plotOutput(\"plot\", width=1100)\n)\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    input$resample\n    if (!isTRUE(input$pause)) {\n      invalidateLater(1000)\n    }\n    rnorm(input$n)\n  })\n  \n  output$plot &lt;- renderPlot({\n    hist(data(),\n      breaks = 40,\n      xlim = c(-2, 2),\n      ylim = c(0, 1),\n      lty = \"blank\",\n      xlab = \"value\",\n      freq = FALSE,\n      main = \"\"\n    )\n    \n    x &lt;- seq(from = -2, to = 2, length.out = 500)\n    y &lt;- dnorm(x)\n    lines(x, y, lwd=1.5)\n    \n    lwd &lt;- 5\n    abline(v=0, col=\"red\", lwd=lwd, lty=2)\n    abline(v=mean(data()), col=\"blue\", lwd=lwd, lty=1)\n\n    legend(legend = c(\"Normal\", \"Mean\", \"Sample mean\"),\n      col = c(\"black\", \"red\", \"blue\"),\n      lty = c(1, 2, 1),\n      lwd = c(1, lwd, lwd),\n      x = 1,\n      y = 0.9\n    )\n  }, res=140)\n}\n\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "2023/01/02/cachitos-2022-primera-parte/index.html",
    "href": "2023/01/02/cachitos-2022-primera-parte/index.html",
    "title": "Cachitos 2022. Primera parte",
    "section": "",
    "text": "Retomando la entrada de cachitos de la nochevieja de 2020\nActualizamos el script que se baja el vídeo, recorta y extrae los subtítulos.\nSea como sea, siempre ha dejado perlas como estas\nEjemplo:\nEl fotograma se obtiene usando mplayer y sacando uno de 200 fotogramas como imagen, luego se recorta la parte del súbtitulo, se invierten los colores y se usa tesseract para el ocr\nY tesseract hace el trabajo de pasarlo a texto\nVeamos las partes importantes de extract_subtitles.sh"
  },
  {
    "objectID": "2023/01/02/cachitos-2022-primera-parte/index.html#bajar-el-vídeo",
    "href": "2023/01/02/cachitos-2022-primera-parte/index.html#bajar-el-vídeo",
    "title": "Cachitos 2022. Primera parte",
    "section": "Bajar el vídeo",
    "text": "Bajar el vídeo\n#!/bin/bash\n\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n\n\nif [ \"$ANNO\" == \"2022\" ] ;\nthen\n    wget https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/1/5/1672556504451.mp4\n    mv 1672556504451.mp4 $video_file\nfi\n\n\nComo se contaba en el post original hay que ir a la web de rtve play, ir a reproducir el cachitos correspondiente, y luego darle a inspeccionar código y ver de qué url está haciendo el request. Para 2021 por ejemplo estaba en https://lote5-vod-hls-geoblockurl.akamaized.net/resources/TE_GLUCA/mp4/4/0/1641020001504.mp4 . Cómo vemos, no hay mucha coherencia en dónde dejan los archivos."
  },
  {
    "objectID": "2023/01/02/cachitos-2022-primera-parte/index.html#convertir-a-jpg-cambiar-tamaño-y-extraer-subtítulos.",
    "href": "2023/01/02/cachitos-2022-primera-parte/index.html#convertir-a-jpg-cambiar-tamaño-y-extraer-subtítulos.",
    "title": "Cachitos 2022. Primera parte",
    "section": "Convertir a jpg , cambiar tamaño y extraer subtítulos.",
    "text": "Convertir a jpg , cambiar tamaño y extraer subtítulos.\nGracias a software de linux como mplayer parallel o magick se puede hacer un montón de cosas con los vídeos e imágenes.\n# Pasar a jpg uno de cada 200 fotogramas\n\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 8 mogrify -resize 642x480 {}\n\n# Seleccionar cacho dond están subtitulos\nfind . -name '*.jpg' |  parallel -j 8 convert {} -crop 460x50+90+295 +repage -compress none -depth 8 {}.subtitulo.tif\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 8 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\nCon mplayer extraemso uno de cada 200 fotogramas y los guardamos como formato jpeg en la carpeta especificada.\nCon la utilidad mogrify de magick reescalamos cada uno de los archivos anteriores a 642x480 píxeles. Con parallel ( sudo apt install parallel si no lo tienes instalado) le decimos que haga esto en paralelo usando 8 hilos de ejecución\nCon converty opción crop indicamos y recortamos la zona de la imagen dónde creemos que están los rótulos y creamos archivos tif que sólo tienen esa zona.\nCon la opción negate de convert invertimos los colores de forma que sea más fácil para tesseract hacer el reconomicimiento óptico de caracteres."
  },
  {
    "objectID": "2023/01/02/cachitos-2022-primera-parte/index.html#ocr.",
    "href": "2023/01/02/cachitos-2022-primera-parte/index.html#ocr.",
    "title": "Cachitos 2022. Primera parte",
    "section": "OCR.",
    "text": "OCR.\nCon tesseract simplemente pasamos los archivos tif generados y le indicamos que lo haga en paralelo usando 8 hilos y que como lenguaje utilizando el español -l spa\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 8 tesseract -l spa {} {}\ntesseract creará tantos archivos txt como fotogramas hayamos extraído (1302 ficheros para cachitos nochevieja de 2022), la mayoría estarán sin rótulos.\nEl nombre que hemos puesto a los archivos sigue este patrón 00000946.jpg.subtitulo.tif.txt indicando el número de fotograma así como las diferentes extensiones, así será más fácil luego identificar un texto con la imagen de la que viene.\nEn la siguiente entrada veremos algo de limpieza de texto."
  },
  {
    "objectID": "2023/Jornadas_R_barcelona.html",
    "href": "2023/Jornadas_R_barcelona.html",
    "title": "XIII Jornadas de R. Barcelona",
    "section": "",
    "text": "Salvo en 2010 he ido a todas las jornadas de R que se han celebrado en España. Que han sido en:\n\n2009 Murcia\n2010 Mieres\n2011 Madrid\n2012 Barcelona\n2013 Zaragoza\n2014 Santiago de Compostela\n2015 Salamanca\n2016 Albacete\n2017 Granada\n2018 Murcia\n2019 Madrid\n2022 Córdoba\n2023 Barcelona\n\nY las de 2024 serán en ¡¡Sevilla!!.\nY qué queréis que os diga, es de mis momentos favoritos del año. Reencuentros con gente, conocer gente nueva y joven que hacen cosas increíbles con R. Ver cómo se usa R en multitud de ámbitos como el periodismo de datos, geocomputación, análisis bayesiano, educación, psicología etc..\nYo por mi parte aporté mi granito de arena contando cosas sobre cómo poner R en producción, aquí os dejo mi taller dónde conté cosas sobre apis, docker, la nube, crontab o incluso sobre h2o y sparkling water y código en scala. Seguramente fue demasiado denso, pero para eso he dejado los materiales y creo que se puede seguir bastante bien en Github.\nUna pena que coincidiera con el taller de tidymodels que impartió Aitor González Amezcua\nPor otro lado, la organización fue excepcional, muchas gracias a Eli Vivas de Story Data, al grupo de usuarios de Barcelona, a la Universidad Pompeu Fabra y a todos los que han colaborado o patrocinado dichas jornadas, incluyendo la empresa dónde trabajo (Orange Spain)..\nY qué decir de la mejor parte de cualquier evento de este tipo , por supuesto me refiero a la parte social. Hablć con un montón de gente, conocí gente nueva y buena, me reencontré con algunos que no veía hace años. Echamos de menos a alguno que no pudo venir del grupo de R Madrid, pero nos lo pasamos genial igualmente.\nPor mencionar a algunos de todos los que he tenido trato, tengo que agradecer la compañía de Aitor y de Mireia que nos han tratado fenomenalmente. También he podido desvirtualizar al gran David Ibarra y a su pareja, ambos grandes profesionales y gente que sabe mucho más que yo de casi todo. Y no quiero olvidarme de los que nos vemos más habitualmente por Madrid, gente como Leonardo, Emilio, Jesús, Francisco, Carlos y Santiago. Ha sido un placer como siempre compartir momentos y conocimientos con vosotros. Lástima que Pedro , Raúl y alguno más no hayan podido venir.\nY bueno, si tengo que poner una pega, o más bien una petición para las siguientes jornadas. ¡Por favor, no me pongáis más un taller o comunicación a las 9 de la mañana del viernes!.\n¡ Nos vemos en Sevilla 2024 !"
  },
  {
    "objectID": "cachitos.html",
    "href": "cachitos.html",
    "title": "Cachitos nochevieja",
    "section": "",
    "text": "Cachitos 2024. Cuarta parte. The python way\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2023. Tercera parte\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2023. Segunda parte\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2023. Primera parte\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2022. Tercera parte\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2022. Segunda parte\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2021/12/12/Modelos-mixtos-spark/index.html",
    "href": "2021/12/12/Modelos-mixtos-spark/index.html",
    "title": "Modelos mixtos en spark. Intento 1",
    "section": "",
    "text": "A los que nos dedicamos a esto siempre echamos de menos un lme4 en python o en Spark. En Julia afortunadamente tenemos MixedModels.jl.\nTotal que buscando alguna posible solución para poder usar esto en spark me encuentro con dos posibles soluciones.\n\nphoton-ml\nMomentMixedModels\n\nAmbos repos llevan un tiempo sin actualizarse así que no sé yo.\nphoton-ml es de linkedin y tiene buena pinta, al menos el tutorial, que tienes que bajarte un docker y tal, funciona. Aunque la sintaxis es rara. Aún tengo que probarlo más y probar a crear el jar del proyecto ya que no está en maven central y tal (y no me funcionó)\n\nEjemplo de sintaxis de photon-ml\n\n// Define another feature shard for our random effect coordinate, and create a new mapping\n// with both our 'global' and 'perUser' shards.\nval perUserFeatureShardId = \"perUser\"\nval perUserFeatureShard = Set(\"genreFeatures\", \"movieLatentFactorFeatures\")\nval mixedFeatureShardBags = Map(\n    globalFeatureShardId -&gt; globalFeatureShard,\n    perUserFeatureShardId -&gt; perUserFeatureShard)\n\n// Since we have a new shard, re-read the training and validation data into a new DataFrame\n// (and a new index map for the new feature shard).\nval (mixedInputData, mixedFeatureShardInputIndexMaps) = dataReader.readMerged(\n    Seq(\"/data/movielens/trainData.avro\"),\n    mixedFeatureShardBags,\n    numPartitions)\nval mixedValidateData = dataReader.readMerged(\n    Seq(\"/data/movielens/validateData.avro\"),\n    mixedFeatureShardInputIndexMaps,\n    mixedFeatureShardBags,\n    numPartitions)\nDonde mixedInputData es un dataframe de spark con esta pinta.\nmixedInputData.show()\n\n+----+--------+------+-------+------+------+--------------------+--------------------+\n| uid|response|userId|movieId|weight|offset|              global|             perUser|\n+----+--------+------+-------+------+------+--------------------+--------------------+\n|null|     4.0|     1|   1215|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|     1|   1350|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     1|   2193|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|     1|   3476|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|     1|   4993|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     3|   1544|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|    440|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|    914|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|   1894|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|   2112|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|   3524|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|   3911|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    11|    256|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    11|   1200|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.5|    11|  48394|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     1.0|    11|  56003|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     0.5|    11|  64508|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    14|    471|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|    14|   2018|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|    14|   6936|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n\nDonde las columna global y perUser son iguales, pero una se usa para estimar la parte de los efectos fijos y la otra para los aleatorios.\nY luego sigue con\n// A 'RandomEffectDataConfiguration' requires an identifier field to use for grouping data from the\n// same entity, in addition to the fields that a 'FixedEffectDataConfiguration' requires. It also has\n// some additional optional parameters not covered in this tutorial.\nval perUserRandomEffectId = \"userId\"\nval perUserDataConfig = RandomEffectDataConfiguration(\n    perUserRandomEffectId,\n    perUserFeatureShardId,\n    numPartitions,\n    projectorType = IndexMapProjection)\n\n// A 'RandomEffectOptimizationConfiguration' is defined much like a\n// 'FixedEffectOptimizationConfiguration'. The options below are varied from those above primarily\n// for variety and demonstration.\nval perUserOptimizerConfig = OptimizerConfig(\n    optimizerType = TRON,\n    tolerance = 1e-3,\n    maximumIterations = 4)\nval perUserRegularizationContext = L2RegularizationContext\nval perUserRegularizationWeight = 1\nval perUserOptimizationConfig = RandomEffectOptimizationConfiguration(\n    perUserOptimizerConfig,\n    perUserRegularizationContext,\n    perUserRegularizationWeight)\n\n// Assign a coordinate ID to the random effect configurations we defined above. This time, we have\n// multiple coordinates and need to determine the update sequence. In general, it's recommended to\n// order coordinates from least to most granular, i.e. those that correlate most with the response to\n// those that correlate least.\nval perUserCoordinateId = \"perUser\"\nval mixedCoordinateDataConfigs = Map(\n    globalCoordinateId -&gt; globalDataConfig,\n    perUserCoordinateId -&gt; perUserDataConfig)\nval mixedCoordinateOptConfigs = Map(\n    globalCoordinateId -&gt; globalOptimizationConfig,\n    perUserCoordinateId -&gt; perUserOptimizationConfig)\nval mixedUpdateSequence = Seq(globalCoordinateId, perUserCoordinateId)\n\n// Reset our estimator. The training task hasn't changed, but the data configurations and update\n// sequence have. Furthermore, since there are now multiple coordinates, we should try multiple\n// passes of coordinate descent.\nestimator.setCoordinateDataConfigurations(mixedCoordinateDataConfigs)\nestimator.setCoordinateUpdateSequence(mixedUpdateSequence)\nestimator.setCoordinateDescentIterations(2)\n\n// Train a new model.\nval (mixedModel, _, mixedModelConfig) = estimator.fit(\n    mixedInputData,\n    Some(mixedValidateData),\n    Seq(mixedCoordinateOptConfigs)).head\n\n// Save the trained model.\nModelProcessingUtils.saveGameModelToHDFS(\n    sc,\n    new Path(\"output/mixed\"),\n    mixedModel,\n    trainingTask,\n    mixedModelConfig,\n    None,\n    mixedFeatureShardInputIndexMaps)\n    \nY guarda los coeficientes en avro\n\"avro cat -n 1 ./output/mixed/random-effect/perUser/coefficients/part-00000.avro\" #| \"jq .\" !\n{\n  \"variances\": null,\n  \"means\": [\n    {\n      \"term\": \"Drama\",\n      \"name\": \"Genre\",\n      \"value\": -0.35129547272878503\n    },\n    {\n      \"term\": \"Musical\",\n      \"name\": \"Genre\",\n      \"value\": -0.2967514108349342\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"7\",\n      \"value\": -0.13789947075029355\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"14\",\n      \"value\": -0.13577029316450503\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"8\",\n      \"value\": -0.12850130065314527\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"26\",\n      \"value\": -0.11646520581859549\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"15\",\n      \"value\": -0.09620039918539182\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"6\",\n      \"value\": 0.08934738779979344\n    },\n    {\n      \"term\": \"Comedy\",\n      \"name\": \"Genre\",\n      \"value\": 0.08833383209245319\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"2\",\n      \"value\": -0.08756438537931642\n    },\n\nmore coefficients\n    \"modelClass\": \"com.linkedin.photon.ml.supervised.regression.LinearRegressionModel\",\n  \"lossFunction\": \"\",\n  \"modelId\": \"7\"\n}\nLo dicho, no tiene mala pinta y ajusta rápido, me falta probar a crear el jar del proyecto\nPor otro lado MomentMixedModels también parecía prometedora pero al intentar crear el jar con sbt (tampoco está en maven central) peta con (*:update) sbt.ResolveException: unresolved dependency: com.stitchfix.algorithms.spark#sfs3_2.11;0.7.0-spark2.2.0: not found y viendo el build.sbt hace referencia a http://artifactory.vertigo.stitchfix.com/artifactory/releases que parece que ya no existe, así que mi gozo en un pozo. La sintaxis parecía sencilla.\nval linearModelFitter = {\n    new MixedEffectsRegression()\n      .setResponseCol(\"Reaction\")\n      .setFixedEffectCols(Seq(\"Days\"))\n      .setRandomEffectCols(Seq(\"Days\"))\n      .setFamilyParam(\"gaussian\")\n      .setGroupCol(\"Subject\")\n  }\n\n  val linearModel = linearModelFitter.fit(sleepstudyData)\n  println(linearModel.β)\nPues nada, a ver si algún ingenazi con alma de analista se digna a hacer una implementación de lme4 en Spark , porque, reconozcámoslo Spark-ml es una ñapa. Lo único que medio funciona bien es usar los algoritmos de h2o sobre spark con sparkling-water y me falta probar un poco más su implementación de modelos jerárquicos\nHasta otra."
  },
  {
    "objectID": "2023/Vuelta_a_la_facultad_svd.html",
    "href": "2023/Vuelta_a_la_facultad_svd.html",
    "title": "Vuelta a la facultad. SVD",
    "section": "",
    "text": "Unos compis del trabajo están haciendo cosas muy chulas utilizando SVD (Descomposición en valores singulares) y me ha recordado a los tiempos de la universidad.\nLa SVD es una factorización de una matriz rectangular tal que así.\n\\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^T} \\]\nVeamos un ejemplo, dónde tenemos una matriz de 5x4\n\nX &lt;- matrix(c( 10, -1, -2, 2,\n               4,  8, -1, 3,\n               -2,  5, -3, 4,\n               -7, -8,  6, -2,\n               -2 , 5, 4, -4\n               ),\n            byrow = T, nrow = 5, ncol = 4,\n            dimnames = list(c(\"P1\", \"P2\", \"P3\", \"P4\",\"P5\"),\n                            c(\"V1\", \"V2\", \"V3\",\"V4\")))\nX\n#&gt;    V1 V2 V3 V4\n#&gt; P1 10 -1 -2  2\n#&gt; P2  4  8 -1  3\n#&gt; P3 -2  5 -3  4\n#&gt; P4 -7 -8  6 -2\n#&gt; P5 -2  5  4 -4\n\nHacemos la svd\n\ndvs &lt;- svd(X)\ndvs\n#&gt; $d\n#&gt; [1] 17.033392 10.982941  6.873957  2.997880\n#&gt; \n#&gt; $u\n#&gt;             [,1]        [,2]        [,3]        [,4]\n#&gt; [1,] -0.41759401  0.66860794  0.28085132 -0.28960722\n#&gt; [2,] -0.50584653 -0.30661800  0.13614448 -0.62790360\n#&gt; [3,] -0.23820970 -0.40000698 -0.61431429 -0.21204097\n#&gt; [4,]  0.71395758  0.08354597 -0.00214809 -0.68888308\n#&gt; [5,]  0.05705485 -0.54033629  0.72470722  0.04840934\n#&gt; \n#&gt; $v\n#&gt;            [,1]        [,2]       [,3]        [,4]\n#&gt; [1,] -0.6360870  0.61508773  0.4578647 -0.08614145\n#&gt; [2,] -0.6015603 -0.77316598  0.2003873 -0.01358129\n#&gt; [3,]  0.3855737 -0.13572378  0.5864215 -0.69930092\n#&gt; [4,] -0.2912926  0.07389525 -0.6374282 -0.70948815\n\nY vemos que efectivamente\n\ndvs$u %*% diag(dvs$d) %*% t(dvs$v)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]   10   -1   -2    2\n#&gt; [2,]    4    8   -1    3\n#&gt; [3,]   -2    5   -3    4\n#&gt; [4,]   -7   -8    6   -2\n#&gt; [5,]   -2    5    4   -4\n\nAhora, la proyección de las filas en el espacio vectorial definido por la descomposión en valores singulares sería\n\\[  Proj = \\mathbf{X} \\mathbf{V} \\]\nDe forma que si tenemos unos nuevos datos y queremos proyectarlos sobre la estructura definida por la DVS ya calculada se haría así.\n\nnuevaX &lt;- matrix(c( 20, -2, -3, 2.1,\n               5,  2, -1, 3\n               ),\n            byrow = T, nrow = 2, ncol = 4,\n            dimnames = list(c(\"P6\", \"P7\"),\n                            c(\"V1\", \"V2\", \"V3\",\"V4\")))\n\nnuevaX\n#&gt;    V1 V2 V3  V4\n#&gt; P6 20 -2 -3 2.1\n#&gt; P7  5  2 -1 3.0\n\n(proyeccion = nuevaX %*% dvs$v)\n#&gt;          [,1]      [,2]      [,3]      [,4]\n#&gt; P6 -13.287055 14.410438 5.6586547 -1.087689\n#&gt; P7  -5.643007  1.886516 0.1913919 -1.887033\n\nY para proyectar los valores de X originales, podemos hacer lo mismo o como sabemos que\n\\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^T} \\]\nentonces\n\\[ Proj = \\mathbf{X} \\mathbf{V} =  \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^T} \\mathbf{V} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{I} = \\mathbf{U} \\mathbf{\\Sigma} \\]\nEs decir, la proyección de las filas son los vectores singulares izquierdos por la matriz diagonal de los valores singulares.\n¿Y para qué sirve esto? Pues una cosa interesante de la SVD es que nos sirve para reducir la dimensión de los datos, quedándonos con los k primeros valores singulares y los respectivos k primeros vectores singulares.\nSupongamos que queremos quedarnos con k = 3\n\ndvs_red &lt;- svd(X, nu = 3, nv = 3)\ndvs_red\n#&gt; $d\n#&gt; [1] 17.033392 10.982941  6.873957  2.997880\n#&gt; \n#&gt; $u\n#&gt;             [,1]        [,2]        [,3]\n#&gt; [1,] -0.41759401  0.66860794  0.28085132\n#&gt; [2,] -0.50584653 -0.30661800  0.13614448\n#&gt; [3,] -0.23820970 -0.40000698 -0.61431429\n#&gt; [4,]  0.71395758  0.08354597 -0.00214809\n#&gt; [5,]  0.05705485 -0.54033629  0.72470722\n#&gt; \n#&gt; $v\n#&gt;            [,1]        [,2]       [,3]\n#&gt; [1,] -0.6360870  0.61508773  0.4578647\n#&gt; [2,] -0.6015603 -0.77316598  0.2003873\n#&gt; [3,]  0.3855737 -0.13572378  0.5864215\n#&gt; [4,] -0.2912926  0.07389525 -0.6374282\n\nPodemos reconstruir la matriz original tal que así\n\n# matriz original\nX\n#&gt;    V1 V2 V3 V4\n#&gt; P1 10 -1 -2  2\n#&gt; P2  4  8 -1  3\n#&gt; P3 -2  5 -3  4\n#&gt; P4 -7 -8  6 -2\n#&gt; P5 -2  5  4 -4\n\n# reconstrucción \n\ndvs_red$u %*% diag(dvs_red$d[1:3]) %*% t(dvs_red$v)\n#&gt;           [,1]      [,2]      [,3]      [,4]\n#&gt; [1,]  9.925211 -1.011791 -2.607138  1.384017\n#&gt; [2,]  3.837849  7.974435 -2.316350  1.664474\n#&gt; [3,] -2.054758  4.991367 -3.444527  3.548997\n#&gt; [4,] -7.177898 -8.028048  4.555812 -3.465227\n#&gt; [5,] -1.987499  5.001971  4.101486 -3.897035\n\nY así almacenando U, d y V podemos tener una aproximación de X ocupando menos espacio.\nUna utilidad de esto es que si pensamos en las filas como observaciones y las columnnas como variables, la SVD nos sirve para hacer una reducción de la dimensionalidad, de hecho los valores singulares al cuadrado son los autovalores de \\(\\mathbf{X^T} \\mathbf{X}\\)\n\nZ = t(X) %*% X\n\neigen(Z)\n#&gt; eigen() decomposition\n#&gt; $values\n#&gt; [1] 290.136446 120.624987  47.251282   8.987284\n#&gt; \n#&gt; $vectors\n#&gt;            [,1]        [,2]       [,3]       [,4]\n#&gt; [1,]  0.6360870  0.61508773 -0.4578647 0.08614145\n#&gt; [2,]  0.6015603 -0.77316598 -0.2003873 0.01358129\n#&gt; [3,] -0.3855737 -0.13572378 -0.5864215 0.69930092\n#&gt; [4,]  0.2912926  0.07389525  0.6374282 0.70948815\n\ndvs_red$d^2\n#&gt; [1] 290.136446 120.624987  47.251282   8.987284\n\nSi nos quedamos con los k primeros singulares podríamos querer tener por ejemplo la proyección de las filas en ese subespacio, y usar esas coordenadas para ver distancias entre filas. Una propiedad deseable de ese subespacio es que las distancias entre los individuos sean parecidas o al menos ordenen de la misma forma que en el espacio original.\nPor ejemplo las distancias euclídeas entre las filas de X\n\ndist(X)\n#&gt;           P1        P2        P3        P4\n#&gt; P2 10.908712                              \n#&gt; P3 13.601471  7.071068                    \n#&gt; P4 20.445048 21.236761 17.635192          \n#&gt; P5 15.874508 10.908712 10.630146 14.212670\n\nVemos que P5 está más cerca de P3 que de P2.\nSi usamos sólo la matriz \\(\\mathbf{U_{n \\times k}}\\) para ver distancias tenemos que no se mantiene esa relación entre las distancias\n\ndist(dvs_red$u)\n#&gt;           1         2         3         4\n#&gt; 2 0.9898455                              \n#&gt; 3 1.4055028 0.8022090                    \n#&gt; 4 1.3049119 1.2881284 1.2309319          \n#&gt; 5 1.3725326 0.8472829 1.3783512 1.1614943\n\nEn cambio si utilizamos la proyección tenemos que las distancias son del mismo orden que en los datos originales y que se mantiene la relación entre las diferentes filas.\n\n\nproyeccion &lt;- (dvs_red$u %*% diag(dvs_red$d[1:3]))\n\ndist(proyeccion)\n#&gt;           1         2         3         4\n#&gt; 2 10.861467                              \n#&gt; 3 13.599483  6.960296                    \n#&gt; 4 20.409979 21.235974 17.577158          \n#&gt; 5 15.842132 10.718639 10.601432 14.039748\n\nPor eso, si queremos utilizar SVD para hacer una reducción de dimensionalidad y posteriormente utilizar técnicas que utilicen distancias puede tener más sentido utilizar la proyección \\(\\mathbf{U_{n\\times k}} \\mathbf{\\Sigma_{k \\times k}}\\) que sólo \\(\\mathbf{U_{n\\times k}}\\)"
  },
  {
    "objectID": "2023/shinylive-R.html",
    "href": "2023/shinylive-R.html",
    "title": "Shinylive-r",
    "section": "",
    "text": "Están saliendo cosas como webr que permiten ejecutar R en el navegador que junto con shinylive nos van a permitir hacer aplicaciones de Shiny (en R y en Python) que sean serverless. Es decir, el código se ejecuta en el navegador del usuario, no en un servidor.\nUn post dónde probé con shinylive en python. Shinylive en python \n\nCosas que hay que hacer para que funcione en mi documento de quarto, que es con lo que hago el blog\n\nInstalar la librería de python shinylive pip install shinylive\nInstalar las extensiones de quarto para shinylive quarto add quarto-ext/shinylive\nInstalar la librería de shinylive de https://github.com/posit-dev/r-shinylive\n\nEl chunk se especifica poniendo shinylive-r y se ponen algunas opciones como estas\n#| standalone: true\n#| viewerHeight: 600\n#| components: [editor, viewer]\n\nEstas librerías aún están en fase temprana de desarrollo y por ejemplo no está resuelto como leer un fichero que tengas en local y que ese fichero se suba al sitio web estático y se pueda leer con read.csv . Pero podemos hacer una ñapa como he hecho en el ejemplo, que es subir el fichero de datos a github y leer de la url dónde lo he subido.\n\n\nmpgData &lt;- read.csv(\"https://raw.githubusercontent.com/joscani/blog_quarto/master/mtcars.csv\")\n\nSe abre el camino pues para tener pequeñas aplicaciones shiny que no necesiten de servidor.\n#| standalone: true\n#| viewerHeight: 800\n#| components: [editor, viewer]\n\nlibrary(shiny)\nlibrary(datasets)\n\n# Data pre-processing ----\n# Tweak the \"am\" variable to have nicer factor labels -- since this\n# doesn't rely on any user inputs, we can do this once at startup\n# and then use the value throughout the lifetime of the app\nmpgData &lt;- read.csv(\"https://raw.githubusercontent.com/joscani/blog_quarto/master/mtcars.csv\")\nmpgData$am &lt;- factor(mpgData$am, labels = c(\"Automatic\", \"Manual\"))\n\n\n# Define UI for miles per gallon app ----\nui &lt;- fluidPage(\n\n  # App title ----\n  titlePanel(\"Miles Per Gallon\"),\n\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n\n      # Input: Selector for variable to plot against mpg ----\n      selectInput(\"variable\", \"Elige la variable:\",\n                  c(\"Cylinders\" = \"cyl\",\n                    \"Transmission\" = \"am\",\n                    \"Gears\" = \"gear\")),\n\n      # Input: Checkbox for whether outliers should be included ----\n      checkboxInput(\"outliers\", \"Show outliers\", TRUE)\n\n    ),\n\n    # Main panel for displaying outputs ----\n    mainPanel(\n\n      # Output: Formatted text for caption ----\n      h3(textOutput(\"caption\")),\n\n      # Output: Plot of the requested variable against mpg ----\n      plotOutput(\"mpgPlot\")\n\n    )\n  )\n)\n\n# Define server logic to plot various variables against mpg ----\nserver &lt;- function(input, output) {\n\n  # Compute the formula text ----\n  # This is in a reactive expression since it is shared by the\n  # output$caption and output$mpgPlot functions\n  formulaText &lt;- reactive({\n    paste(\"mpg ~\", input$variable)\n  })\n\n  # Return the formula text for printing as a caption ----\n  output$caption &lt;- renderText({\n    formulaText()\n  })\n\n  # Generate a plot of the requested variable against mpg ----\n  # and only exclude outliers if requested\n  output$mpgPlot &lt;- renderPlot({\n    boxplot(as.formula(formulaText()),\n            data = mpgData,\n            outline = input$outliers,\n            col = \"#75AADB\", pch = 19)\n  })\n\n}\n\n# Create Shiny app ----\nshinyApp(ui, server)"
  },
  {
    "objectID": "2023/shinylive-R.html#cosas-que-hay-que-hacer-para-que-funcione-en-mi-documento-de-quarto-que-es-con-lo-que-hago-el-blog",
    "href": "2023/shinylive-R.html#cosas-que-hay-que-hacer-para-que-funcione-en-mi-documento-de-quarto-que-es-con-lo-que-hago-el-blog",
    "title": "Shinylive-r",
    "section": "Cosas que hay que hacer para que funcione en mi documento de quarto, que es con lo que hago el blog",
    "text": "Cosas que hay que hacer para que funcione en mi documento de quarto, que es con lo que hago el blog\n\nInstalar la librería de python shinylive pip install shinylive\nInstalar las extensiones de quarto para shinylive quarto add quarto-ext/shinylive\nInstalar la librería de shinylive de https://github.com/posit-dev/r-shinylive\n\nEl chunk se especifica como\n\n```{shinylive-r}\n\nel código\n\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 600\n#| components: [editor, viewer]\n\nlibrary(shiny)\nlibrary(datasets)\n\n# Data pre-processing ----\n# Tweak the \"am\" variable to have nicer factor labels -- since this\n# doesn't rely on any user inputs, we can do this once at startup\n# and then use the value throughout the lifetime of the app\nmpgData &lt;- read.csv(\"https://raw.githubusercontent.com/joscani/blog_quarto/master/mtcars.csv\")\nmpgData$am &lt;- factor(mpgData$am, labels = c(\"Automatic\", \"Manual\"))\n\n\n# Define UI for miles per gallon app ----\nui &lt;- fluidPage(\n\n  # App title ----\n  titlePanel(\"Miles Per Gallon\"),\n\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n\n      # Input: Selector for variable to plot against mpg ----\n      selectInput(\"variable\", \"Variable:\",\n                  c(\"Cylinders\" = \"cyl\",\n                    \"Transmission\" = \"am\",\n                    \"Gears\" = \"gear\")),\n\n      # Input: Checkbox for whether outliers should be included ----\n      checkboxInput(\"outliers\", \"Show outliers\", TRUE)\n\n    ),\n\n    # Main panel for displaying outputs ----\n    mainPanel(\n\n      # Output: Formatted text for caption ----\n      h3(textOutput(\"caption\")),\n\n      # Output: Plot of the requested variable against mpg ----\n      plotOutput(\"mpgPlot\")\n\n    )\n  )\n)\n\n# Define server logic to plot various variables against mpg ----\nserver &lt;- function(input, output) {\n\n  # Compute the formula text ----\n  # This is in a reactive expression since it is shared by the\n  # output$caption and output$mpgPlot functions\n  formulaText &lt;- reactive({\n    paste(\"mpg ~\", input$variable)\n  })\n\n  # Return the formula text for printing as a caption ----\n  output$caption &lt;- renderText({\n    formulaText()\n  })\n\n  # Generate a plot of the requested variable against mpg ----\n  # and only exclude outliers if requested\n  output$mpgPlot &lt;- renderPlot({\n    boxplot(as.formula(formulaText()),\n            data = mpgData,\n            outline = input$outliers,\n            col = \"#75AADB\", pch = 19)\n  })\n\n}\n\n# Create Shiny app ----\nshinyApp(ui, server)"
  },
  {
    "objectID": "2023/webr.html",
    "href": "2023/webr.html",
    "title": "Web-R",
    "section": "",
    "text": "Hay nueva tecnología, usar R dentro de la web. ¡Dale a Run Code!\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPero es que puedes modificar el código que hay puesto. Por ejemplo, prueba a cambiar el fit de arrib, simplemente pon el cursor en dónde está el código y empieza a escribir. por ejemplo cambia la fórmula a lm(mpg ~ am + disp, data = mtcars) y dale a run\nWebR se ejecuta en el navegador, es decir es en el ordenador de ti lector dónde se ejecuta, aunque no tengas R instalado. Es como cuando el navegador ejecuta código de JavaScript, de hecho webR es librería de JavaScript pero que tiene R compilado en Web Assembly.\nSi queremos leer un fichero que yo tenga en local es más complicado, pero puedo subirlo a un github y hacer download.file dentro de webR y así ya es accesible. Nota: Esto ya no funciona\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTambién puedo instalar librerías, no todas están disponibles, pero hay más de 20 mil ya compiladas en webR\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPodemos pintar , y como antes, modificar el código para hacer lo que queramos. Os animo a modificar el plot de abajo, por ejemplo poniendo lo siguiente, o cualquier otra cosa que se os ocurra, como hacer group bys y summarises o lo que queráis, Lo dicho, se ejecuta en vuestro ordenador y en ningún otro sitio más.\n\ndem_women %&gt;% \nfilter(country %in% c(\"Mexico\",\"Algeria\")) %&gt;% \nggplot(aes(x = year, y = flfp)) +\ngeom_smooth() +\nfacet_wrap(~ country)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "2019/04/01/index.html",
    "href": "2019/04/01/index.html",
    "title": "Burbuja o no burbuja, esa es la cuestión",
    "section": "",
    "text": "Llevo un tiempo que me llegan noticias tales como “Con el big data predecimos cuál va a ser tu próxima tienda y cuánto te vas a gastar” o “predecimos los rebotes por partido de un jugador con un margen de error de un rebote cada 6 partidos” y cosas aún más peregrinas.\nLa verdad es que entre la gente más o menos seria que nos dedicamos a esto creo que está bastante claro que nuestra labor es reducir (y medir) la incertidumbre de algunos procesos de toma de decisiones, lean esto si quieren saber más en qué debería consistir nuestro trabajo.\n¡No hacemos magia, no tenemos ingestado todo Internet, no sabemos si te vas a tomar esta tarde una caña con bocata de calamares o 2 vinos !\nOtro amigo mío dice que el “big data son los padres”, y no le falta razón, sobre todo por el problema de gestión de expectativas, entre todos estamos vendiendo que con la Inteligencia Artificial o el deep learning vamos a hacer maravillas. En realidad todos suspiramos por unos datos limpios y por unos modelos sencillos, pero útiles.\nY otro tema es el de la cantidad de gente que quiere entrar en el sector de la analítica pidiendo 50k con 1 año de experiencia, cuándo no saben ni interpretar un coeficiente de una regresión lineal. Mejor me callo que me enervo.\nPero bueno, veamos qué tal ha ido la evolución de búsquedas en Google de ciertos términos relacionados con estos temas.\nVamos a usar la librería gtrendsR que ataca a la API de googleTrends\n\nlibrary(gtrendsR)\nlibrary(tidyverse)\nlibrary(patchwork)\n\nA petición de lector del blog, compañero de trabajo y sin embargo amigo pongo las búsquedas desde más atrás, pero sólo para las globales\n\nfecha_inicio &lt;- '2009-01-01'\nintervalo &lt;- paste(fecha_inicio, '2019-04-01', sep = \" \")\n\nhype_words_all &lt;- gtrends(c('big data','machine learning','artificial intelligence','deep learning'), time = intervalo)\n\n\n\n# hype_words_spain &lt;- gtrends(c(\"big data\",\"machine learning\",\"artificial intelligence\",\"deep learning\"), geo=\"ES\", time = intervalo)\n\n\ninterest_over_time_all &lt;-  hype_words_all$interest_over_time \ninterest_over_time_all &lt;- interest_over_time_all %&gt;% filter(hits &gt;0 & !is.na(hits))\n\n# \n# interest_over_time_spain &lt;-  hype_words_spain$interest_over_time \n# interest_over_time_spain &lt;- interest_over_time_spain %&gt;% filter(hits &gt;0 & !is.na(hits))\n\n\np1 &lt;- ggplot(interest_over_time_all, aes(x = date, y = hits, color=keyword)) +\n  geom_point(size = rel(0.5)) +\n  geom_smooth(span = 0.8) +\n  labs(title=\"World: Interest last ten years\", y = \"relative interest\")\n\np1\n\n\n\n\n\n\n\n# p2 &lt;- ggplot(interest_over_time_spain, aes(x = date, y = hits, color=keyword)) +\n#   geom_point(size = rel(0.5)) +\n#   geom_smooth() +\n#   labs(title=\"Spain: Interest last five years\", y = \"relative interest\")\n\n# p1 + p2  + plot_layout(ncol = 1)\n\nPues ¿qué les parece? ¿Vamos camino de un burbuja o ya estamos en ella?"
  },
  {
    "objectID": "2024/cachitos_2023_primera_parte.html",
    "href": "2024/cachitos_2023_primera_parte.html",
    "title": "Cachitos 2023. Primera parte",
    "section": "",
    "text": "Como todos los años toca hacer el análisis de de los subtítulos de Nochevieja a Cachitos\nEste año resulta que la gente de RTVE ha capado el poder bajarse los videos desde rtve usando curl o wget , pero gracias a Javi Fdez he podido bajarme el video tirando de la url del streaming ."
  },
  {
    "objectID": "2024/cachitos_2023_primera_parte.html#requisitos",
    "href": "2024/cachitos_2023_primera_parte.html#requisitos",
    "title": "Cachitos 2023. Primera parte",
    "section": "Requisitos",
    "text": "Requisitos\n\nUn plugin en el navegador que pueda identificar el m3u asociado al archivo mp4 del video de Cachitos que está en rtve play. Yo he usado live Stream Downloader , pero seguro que hay alguno mejor\nUsar un sistema operativo GNU/Linux, en mi caso Linux Mint 21.2 basada en Ubuntu Jammy\nffmpeg sudo apt install ffmpeg . ffmpeg nos va a servir para poder bajar el video\nmplayer sudo apt install mplayer. mplayer con mplayer vamos a extraer 1 de cada 200 fotogramas del video\nimagemagick con sudo apt install imagemagick. Con imagemagick vamos a cambiar el tamaño de los fotogramas, recortar el área dónde están los subtítulos, convertir a formato tif e invertir los colores para que el texto se vea mejor.\nparallel sudo apt install parallel . Con parallel vamos a poder utilizar varios núcleos e hilos del ordenador en paralelo y poder hacer la modificación de los fotogramas y el reconocimiento óptico de caracteres más rápido.\ntesseract sudo apt install tesseract-ocr y sudo apt install tesseract-ocr-spa . Con tesseract se va a hacer el reconocimiento óptico del texto y guardar en ficheros de texto."
  },
  {
    "objectID": "2024/cachitos_2023_primera_parte.html#script-de-bash",
    "href": "2024/cachitos_2023_primera_parte.html#script-de-bash",
    "title": "Cachitos 2023. Primera parte",
    "section": "Script de bash",
    "text": "Script de bash\nEl script es extract_subtitles.sh"
  },
  {
    "objectID": "2024/01/cachitos_2023_primera_parte.html",
    "href": "2024/01/cachitos_2023_primera_parte.html",
    "title": "Cachitos 2023. Primera parte",
    "section": "",
    "text": "Como todos los años toca hacer el análisis de de los subtítulos de Nochevieja a Cachitos\nEste año resulta que la gente de RTVE ha capado el poder bajarse los videos desde rtve usando curl o wget , pero gracias a Javi Fdez he podido bajarme el video tirando de la url del streaming ."
  },
  {
    "objectID": "2024/01/cachitos_2023_primera_parte.html#requisitos",
    "href": "2024/01/cachitos_2023_primera_parte.html#requisitos",
    "title": "Cachitos 2023. Primera parte",
    "section": "Requisitos",
    "text": "Requisitos\n\nUn plugin en el navegador que pueda identificar el m3u asociado al archivo mp4 del video de Cachitos que está en rtve play. Yo he usado live Stream Downloader , pero seguro que hay alguno mejor\nUsar un sistema operativo GNU/Linux, en mi caso Linux Mint 21.2 basada en Ubuntu Jammy\nffmpeg sudo apt install ffmpeg . ffmpeg nos va a servir para poder bajar el video\nmplayer sudo apt install mplayer. mplayer con mplayer vamos a extraer 1 de cada 200 fotogramas del video\nimagemagick con sudo apt install imagemagick. Con imagemagick vamos a cambiar el tamaño de los fotogramas, recortar el área dónde están los subtítulos, convertir a formato tif e invertir los colores para que el texto se vea mejor.\nparallel sudo apt install parallel . Con parallel vamos a poder utilizar varios núcleos e hilos del ordenador en paralelo y poder hacer la modificación de los fotogramas y el reconocimiento óptico de caracteres más rápido.\ntesseract sudo apt install tesseract-ocr y sudo apt install tesseract-ocr-spa . Con tesseract se va a hacer el reconocimiento óptico del texto y guardar en ficheros de texto.\nElimina ficheros de texto de menos de 10 bytes, puesto que ahí no hay texto ."
  },
  {
    "objectID": "2024/01/cachitos_2023_primera_parte.html#script-de-bash",
    "href": "2024/01/cachitos_2023_primera_parte.html#script-de-bash",
    "title": "Cachitos 2023. Primera parte",
    "section": "Script de bash",
    "text": "Script de bash\nEl script es extract_subtitles.sh , se ejecutaría con extract_subtitles.sh 2023 .\nEste script básicamente hace:\n\nSe baja el video del sitio de rtve play\nExtrae 1 de cada 200 fotogramas con mplayer\nReduce el tamaño de las imágenes con mogrify de imagemagick\nConvierte a tif y recorta el área dónde está el texto con convert y crop de imagemagick\nObtiene el negativo de ese tif con convert y negate\nObtiene el texto usando tesseract y guarda un archivo de texto por cada subtítulo\n\n#!/bin/bash\n\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n\n\nif [ \"$ANNO\" == \"2023\" ] ;\nthen\n    ffmpeg -i \"https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4/video.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=7047821\" -c copy $video_file\nfi\n\nif [ \"$ANNO\" == \"2022\" ] ;\nthen\n    ffmpeg -i \"https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/1/5/1672556504451.mp4/video.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=6767615\" -c copy $video_file\nfi \n \n\n# Pasar a jpg uno de cada 200 fotogramas\n\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 8 mogrify -resize 642x480 {}\n\n# Seleccionar cacho dond están subtitulos\nfind . -name '*.jpg' |  parallel -j 8 convert {} -crop 460x50+90+285 +repage -compress none -depth 8 {}.subtitulo.tif\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 8 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 8 tesseract -l spa {} {}\n\n# mover a directorio texto\nmkdir -p $root_directory/$ANNO$suffix_txt_dir\n\nmv *.txt $root_directory/$ANNO$suffix_txt_dir\n\ncd $root_directory/$ANNO$suffix_txt_dir\n\n# Borrar archivos de 10 bytes , subtítulos vacíos\nfind . -size -10c -exec rm -f {} \\;\n\ncd $root_directory"
  },
  {
    "objectID": "2024/01/cachitos_2023_primera_parte.html#algún-ejemplo",
    "href": "2024/01/cachitos_2023_primera_parte.html#algún-ejemplo",
    "title": "Cachitos 2023. Primera parte",
    "section": "Algún ejemplo",
    "text": "Algún ejemplo\n\nlibrary(tidyverse)\n\nroot_directory = \"~/proyecto_cachitos/\"\nanno &lt;- \"2023\"\nlibrary(magick)\n(directorio_imagenes &lt;- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#&gt; ~/proyecto_cachitos/video/2023_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00001297.jpg\"))\n\n\n\n\n\n\n\n\n\nimage_read(str_glue(\"{directorio_imagenes}00001297.jpg.subtitulo.tif\"))\n\n\n\n\n\n\n\n\n\n\n(directorio_texto &lt;- str_glue(\"{root_directory}{anno}_txt/\"))\n#&gt; ~/proyecto_cachitos/2023_txt/\n\nsystem(str_glue(\"cat {directorio_texto}00001297.jpg.subtitulo.tif.txt\"), intern = TRUE)\n#&gt; [1] \"Con el tiempo descubrimos que todas las letras de este\"\n#&gt; [2] \"exprofesor de Primaria tenian referencias sexuales.\"   \n#&gt; [3] \"\\f\"\n\nY poco más, mañana intentaré tener elpost similar al del año pasado dónde veíamos distancias de texto entre rótulos y demás."
  },
  {
    "objectID": "2024/01/cachitos_2023_segunda_parte.html",
    "href": "2024/01/cachitos_2023_segunda_parte.html",
    "title": "Cachitos 2023. Segunda parte",
    "section": "",
    "text": "Una vez que ya hemos visto en la entrada anterior como extraer los rótulos, vamos a juntarlos todos en un sólo csv y hacer algo de limpieza.\nDejo el enlace a los ficheros de texto construidos por tesseract enlace directorio"
  },
  {
    "objectID": "2024/01/cachitos_2023_segunda_parte.html#lectura-rótulos",
    "href": "2024/01/cachitos_2023_segunda_parte.html#lectura-rótulos",
    "title": "Cachitos 2023. Segunda parte",
    "section": "Lectura rótulos",
    "text": "Lectura rótulos\n\n\nlibrary(tidyverse)\n\nroot_directory = \"~/proyecto_cachitos/\"\nanno &lt;- \"2023\"\n\n\nnombre_ficheros &lt;- list.files(path = str_glue(\"{root_directory}{anno}_txt/\")) %&gt;% \n    enframe() %&gt;% \n    rename(n_fichero = value)\n\nnombre_ficheros\n#&gt; # A tibble: 687 × 2\n#&gt;     name n_fichero                     \n#&gt;    &lt;int&gt; &lt;chr&gt;                         \n#&gt;  1     1 00000004.jpg.subtitulo.tif.txt\n#&gt;  2     2 00000010.jpg.subtitulo.tif.txt\n#&gt;  3     3 00000011.jpg.subtitulo.tif.txt\n#&gt;  4     4 00000012.jpg.subtitulo.tif.txt\n#&gt;  5     5 00000018.jpg.subtitulo.tif.txt\n#&gt;  6     6 00000019.jpg.subtitulo.tif.txt\n#&gt;  7     7 00000020.jpg.subtitulo.tif.txt\n#&gt;  8     8 00000025.jpg.subtitulo.tif.txt\n#&gt;  9     9 00000026.jpg.subtitulo.tif.txt\n#&gt; 10    10 00000027.jpg.subtitulo.tif.txt\n#&gt; # ℹ 677 more rows\n\nAhora los podemos leer en orden\n\n\nsubtitulos &lt;-  list.files(path = str_glue(\"{root_directory}{anno}_txt/\"), \n                        pattern = \"*.txt\", full.names = TRUE) %&gt;% \n    map(~read_file(.)) %&gt;% \n    enframe() %&gt;%  \n    left_join(nombre_ficheros)\n\nglimpse(subtitulos)\n#&gt; Rows: 687\n#&gt; Columns: 3\n#&gt; $ name      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n#&gt; $ value     &lt;list&gt; \"ey ÁNGEL CARMONA | y\\n.».\\n\\n \\n\\f\", \"DAVID BISBAL - Llora…\n#&gt; $ n_fichero &lt;chr&gt; \"00000004.jpg.subtitulo.tif.txt\", \"00000010.jpg.subtitulo.ti…\nsubtitulos\n#&gt; # A tibble: 687 × 3\n#&gt;     name value     n_fichero                     \n#&gt;    &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                         \n#&gt;  1     1 &lt;chr [1]&gt; 00000004.jpg.subtitulo.tif.txt\n#&gt;  2     2 &lt;chr [1]&gt; 00000010.jpg.subtitulo.tif.txt\n#&gt;  3     3 &lt;chr [1]&gt; 00000011.jpg.subtitulo.tif.txt\n#&gt;  4     4 &lt;chr [1]&gt; 00000012.jpg.subtitulo.tif.txt\n#&gt;  5     5 &lt;chr [1]&gt; 00000018.jpg.subtitulo.tif.txt\n#&gt;  6     6 &lt;chr [1]&gt; 00000019.jpg.subtitulo.tif.txt\n#&gt;  7     7 &lt;chr [1]&gt; 00000020.jpg.subtitulo.tif.txt\n#&gt;  8     8 &lt;chr [1]&gt; 00000025.jpg.subtitulo.tif.txt\n#&gt;  9     9 &lt;chr [1]&gt; 00000026.jpg.subtitulo.tif.txt\n#&gt; 10    10 &lt;chr [1]&gt; 00000027.jpg.subtitulo.tif.txt\n#&gt; # ℹ 677 more rows\n\nTenemos 1302 rótulos de los cuales la mayoría estarán vacíos"
  },
  {
    "objectID": "2024/01/cachitos_2023_segunda_parte.html#contando-letras",
    "href": "2024/01/cachitos_2023_segunda_parte.html#contando-letras",
    "title": "Cachitos 2023. Segunda parte",
    "section": "Contando letras",
    "text": "Contando letras\nEn n_fichero tenemos el nombre y en value el texto. Si vemos alguno de los subtítulos.\n\n\nsubtitulos %&gt;%\n    pull(value) %&gt;%\n    pluck(200)\n#&gt; [1] \"Con todos los respetos a Antonio José, esperábamos que\\n\\nel dúo de Rufián fuera con Junqueras,\\n\\f\"\n\nMuchos de los ficheros no tienen texto (son fotogramas sin rótulos). Contemos letras.\n\n\nsubtitulos &lt;- subtitulos %&gt;% \n    mutate(n_caracteres = nchar(value)) \n\nsubtitulos %&gt;% \n    group_by(n_caracteres) %&gt;% \n    count()\n#&gt; # A tibble: 105 × 2\n#&gt; # Groups:   n_caracteres [105]\n#&gt;    n_caracteres     n\n#&gt;           &lt;int&gt; &lt;int&gt;\n#&gt;  1           10     5\n#&gt;  2           12     3\n#&gt;  3           13     1\n#&gt;  4           15     1\n#&gt;  5           18     1\n#&gt;  6           21     2\n#&gt;  7           22     1\n#&gt;  8           23     1\n#&gt;  9           29     1\n#&gt; 10           32     4\n#&gt; # ℹ 95 more rows\n\nsubtitulos %&gt;% \n    group_by(n_caracteres) %&gt;% \n    count() %&gt;% \n  ggplot(aes(x = n_caracteres, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\nY viendo el conteo podríamos ver cómo son los rótulos con menos de 25 caracteres. Y suele ser por haber pillado el nombre de la canción en vez del rótulo\n\n\nsubtitulos %&gt;% \n    filter(n_caracteres &lt;= 25, n_caracteres &gt; 0 ) %&gt;% \n    arrange(desc(n_caracteres)) %&gt;% \n    head(40) %&gt;%\n    pull(value)\n#&gt; [[1]]\n#&gt; [1] \"Para Crazy, su padre.\\n\\f\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"—\\nNATALIA LAFOURCADE\\n\\f\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"HO *\\n\\n= Roland 7-50\\n\\f\"\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"Os lo dijimos.\\n\\n   \\n\\f\"\n#&gt; \n#&gt; [[5]]\n#&gt; [1] \"pee\\n\\nES\\n\\no\\n\\nMIEL\\n\\f\"\n#&gt; \n#&gt; [[6]]\n#&gt; [1] \"¿Es ANO MN ss\\n\\f\"\n#&gt; \n#&gt; [[7]]\n#&gt; [1] \"The GodFary\\n\\f\"\n#&gt; \n#&gt; [[8]]\n#&gt; [1] \"Mar y sol.\\n\\f\"\n#&gt; \n#&gt; [[9]]\n#&gt; [1] \"NA PAL =&gt;:\\n\\f\"\n#&gt; \n#&gt; [[10]]\n#&gt; [1] \"O AA A\\nA E\\n\\f\"\n#&gt; \n#&gt; [[11]]\n#&gt; [1] \"WI ES MO\\n\\f\"\n#&gt; \n#&gt; [[12]]\n#&gt; [1] \"QA Y EPA\\n\\f\"\n#&gt; \n#&gt; [[13]]\n#&gt; [1] \"SN\\n1\\nO y\\n\\f\"\n#&gt; \n#&gt; [[14]]\n#&gt; [1] \" \\n\\nml\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[15]]\n#&gt; [1] \"ly\\nMi\\n\\n \\n\\f\"\n\n\nsubtitulos %&gt;% \n    filter(n_caracteres &gt;= 30) %&gt;% \n    arrange(n_caracteres)\n#&gt; # A tibble: 671 × 4\n#&gt;     name value     n_fichero                      n_caracteres\n#&gt;    &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                                 &lt;int&gt;\n#&gt;  1   151 &lt;chr [1]&gt; 00000309.jpg.subtitulo.tif.txt           32\n#&gt;  2   176 &lt;chr [1]&gt; 00000361.jpg.subtitulo.tif.txt           32\n#&gt;  3   513 &lt;chr [1]&gt; 00001015.jpg.subtitulo.tif.txt           32\n#&gt;  4   677 &lt;chr [1]&gt; 00001333.jpg.subtitulo.tif.txt           32\n#&gt;  5   466 &lt;chr [1]&gt; 00000917.jpg.subtitulo.tif.txt           34\n#&gt;  6   170 &lt;chr [1]&gt; 00000346.jpg.subtitulo.tif.txt           35\n#&gt;  7   241 &lt;chr [1]&gt; 00000496.jpg.subtitulo.tif.txt           35\n#&gt;  8   361 &lt;chr [1]&gt; 00000733.jpg.subtitulo.tif.txt           35\n#&gt;  9   446 &lt;chr [1]&gt; 00000877.jpg.subtitulo.tif.txt           35\n#&gt; 10   687 &lt;chr [1]&gt; 00001353.jpg.subtitulo.tif.txt           35\n#&gt; # ℹ 661 more rows\n\nUsando la librería magick en R que permite usar imagemagick en R, ver post de Raúl Vaquerizo y su homenaje a Sean Connery, podemos ver el fotgrama correspondiente\n\nlibrary(magick)\n(directorio_imagenes &lt;- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#&gt; ~/proyecto_cachitos/video/2023_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00000680.jpg\"))\n\n\n\n\n\n\n\n\nAsí que nos quedamos con los rótulos con más de 30 caracteres\n\nsubtitulos &lt;-  subtitulos %&gt;% \n    filter(n_caracteres &gt; 30)\n\ndim(subtitulos)\n#&gt; [1] 671   4"
  },
  {
    "objectID": "2024/01/cachitos_2023_segunda_parte.html#detección-duplicados",
    "href": "2024/01/cachitos_2023_segunda_parte.html#detección-duplicados",
    "title": "Cachitos 2023. Segunda parte",
    "section": "Detección duplicados",
    "text": "Detección duplicados\nMini limpieza de caracteres extraños y puntuación\n\nstring_mini_clean &lt;-  function(string){\n    string &lt;- gsub(\"?\\n|\\n\", \" \", string)\n    string &lt;- gsub(\"\\r|?\\f|=\", \" \", string)\n    string &lt;- gsub('“|”|—|&gt;',\" \", string)\n    \n    string &lt;- gsub(\"[[:punct:][:blank:]]+\", \" \", string)\n    string &lt;- tolower(string)\n    string &lt;- gsub(\"  \", \" \", string)\n    \n    return(string)\n}\n\n# Haciendo uso de programación funciona con purrr es muy fácil pasar esta función a cada elemento. y decirle que \n# el resultado es string con map_chr\n\nsubtitulos_proces &lt;- subtitulos %&gt;% \n    mutate(texto = map_chr(value, string_mini_clean)) %&gt;% \n    select(-value)\n\nsubtitulos_proces %&gt;% \n  select(texto)\n#&gt; # A tibble: 671 × 1\n#&gt;    texto                                                                        \n#&gt;    &lt;chr&gt;                                                                        \n#&gt;  1 \"david bisbal lloraré las penas noche de fiesta 2002 \"                       \n#&gt;  2 \"bueno cómo están los máquinas lo primero de todo estáis bien \"              \n#&gt;  3 \"pues eso que hemos venido a bailar a cantar y si hay que llorar las penas d…\n#&gt;  4 \"rayden calle de la llorería benidorm fest 2021 \"                            \n#&gt;  5 \"la verdad es que rayden por no ir a eurovisión con esta canción no lloró mu…\n#&gt;  6 \"anunció que se iba a retirar cuando lo veamos lloramos \"                    \n#&gt;  7 \"fangoria dramas y comedias feliz 2015 \"                                     \n#&gt;  8 \"para algunos el año ha sido más dramático que cómico y viceversa \"          \n#&gt;  9 \"aunque también ha habido muchos como nacho canut a su bola \"                \n#&gt; 10 \"rozalén con estopa vivir territorio estopa 2019 \"                           \n#&gt; # ℹ 661 more rows\n\nDistancia de texto entre rótulos consecutivos\n\n\nsubtitulos_proces &lt;- subtitulos_proces %&gt;% \n    mutate(texto_anterior = lag(texto)) %&gt;% \n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\"))\n    \n \nsubtitulos_proces %&gt;% \n    filter(!is.na(distancia)) %&gt;% \n    select(name,texto,distancia, texto_anterior, everything()) %&gt;% \n    arrange(distancia) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nDecidimos eliminar texto cuya distancia sea menor de 36\n\nsubtitulos_proces &lt;-  subtitulos_proces %&gt;% \n    filter(distancia &gt; 36) %&gt;% \n    select(-texto_anterior)\n\nsubtitulos_proces %&gt;% \n    select(name,texto, everything()) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nNo nos hemos quitado todos los duplicados pero sí algunos de ellos.\n\ndim(subtitulos_proces)\n#&gt; [1] 582   5\n\nGuardamos el fichero unido\n\n\nwrite_csv(subtitulos_proces,\n          file = str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\nY aquí os dejo el enlace con los rótulos definitivos"
  },
  {
    "objectID": "2023/01/03/cachitos-2022-segunda-parte/index.html",
    "href": "2023/01/03/cachitos-2022-segunda-parte/index.html",
    "title": "Cachitos 2022. Segunda parte",
    "section": "",
    "text": "Una vez que ya hemos visto en la entrada anterior como extraer los rótulos, vamos a juntarlos todos en un sólo csv y hacer algo de limpieza.\nDejo el enlace a los ficheros de texto construidos por tesseract enlace directorio"
  },
  {
    "objectID": "2023/01/03/cachitos-2022-segunda-parte/index.html#lectura-rótulos",
    "href": "2023/01/03/cachitos-2022-segunda-parte/index.html#lectura-rótulos",
    "title": "Cachitos 2022. Segunda parte",
    "section": "Lectura rótulos",
    "text": "Lectura rótulos\n\n\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno &lt;- \"2022\"\n\n\nnombre_ficheros &lt;- list.files(path = str_glue(\"{root_directory}{anno}_txt/\")) %&gt;% \n    enframe() %&gt;% \n    rename(n_fichero = value)\n\nnombre_ficheros\n#&gt; # A tibble: 1,302 × 2\n#&gt;     name n_fichero                     \n#&gt;    &lt;int&gt; &lt;chr&gt;                         \n#&gt;  1     1 00000001.jpg.subtitulo.tif.txt\n#&gt;  2     2 00000002.jpg.subtitulo.tif.txt\n#&gt;  3     3 00000003.jpg.subtitulo.tif.txt\n#&gt;  4     4 00000004.jpg.subtitulo.tif.txt\n#&gt;  5     5 00000005.jpg.subtitulo.tif.txt\n#&gt;  6     6 00000006.jpg.subtitulo.tif.txt\n#&gt;  7     7 00000007.jpg.subtitulo.tif.txt\n#&gt;  8     8 00000008.jpg.subtitulo.tif.txt\n#&gt;  9     9 00000009.jpg.subtitulo.tif.txt\n#&gt; 10    10 00000010.jpg.subtitulo.tif.txt\n#&gt; # … with 1,292 more rows\n\nAhora los podemos leer en orden\n\n\nsubtitulos &lt;-  list.files(path = str_glue(\"{root_directory}{anno}_txt/\"), \n                        pattern = \"*.txt\", full.names = TRUE) %&gt;% \n    map(~read_file(.)) %&gt;% \n    enframe() %&gt;%  \n    left_join(nombre_ficheros)\n\nglimpse(subtitulos)\n#&gt; Rows: 1,302\n#&gt; Columns: 3\n#&gt; $ name      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n#&gt; $ value     &lt;list&gt; \"\\f\", \"\\f\", \"   \\n\\f\", \"\\f\", \" \\n\\f\", \" \\n\\f\", \"\\f\", \"\\f\", …\n#&gt; $ n_fichero &lt;chr&gt; \"00000001.jpg.subtitulo.tif.txt\", \"00000002.jpg.subtitulo.ti…\nsubtitulos\n#&gt; # A tibble: 1,302 × 3\n#&gt;     name value     n_fichero                     \n#&gt;    &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                         \n#&gt;  1     1 &lt;chr [1]&gt; 00000001.jpg.subtitulo.tif.txt\n#&gt;  2     2 &lt;chr [1]&gt; 00000002.jpg.subtitulo.tif.txt\n#&gt;  3     3 &lt;chr [1]&gt; 00000003.jpg.subtitulo.tif.txt\n#&gt;  4     4 &lt;chr [1]&gt; 00000004.jpg.subtitulo.tif.txt\n#&gt;  5     5 &lt;chr [1]&gt; 00000005.jpg.subtitulo.tif.txt\n#&gt;  6     6 &lt;chr [1]&gt; 00000006.jpg.subtitulo.tif.txt\n#&gt;  7     7 &lt;chr [1]&gt; 00000007.jpg.subtitulo.tif.txt\n#&gt;  8     8 &lt;chr [1]&gt; 00000008.jpg.subtitulo.tif.txt\n#&gt;  9     9 &lt;chr [1]&gt; 00000009.jpg.subtitulo.tif.txt\n#&gt; 10    10 &lt;chr [1]&gt; 00000010.jpg.subtitulo.tif.txt\n#&gt; # … with 1,292 more rows\n\nTenemos 1302 rótulos de los cuales la mayoría estarán vacíos"
  },
  {
    "objectID": "2023/01/03/cachitos-2022-segunda-parte/index.html#contando-letras",
    "href": "2023/01/03/cachitos-2022-segunda-parte/index.html#contando-letras",
    "title": "Cachitos 2022. Segunda parte",
    "section": "Contando letras",
    "text": "Contando letras\nEn n_fichero tenemos el nombre y en value el texto. Si vemos alguno de los subtítulos.\n\n\nsubtitulos %&gt;%\n    pull(value) %&gt;%\n    pluck(946)\n#&gt; [1] \"Izquierda... Derecha... Como veis, tienen la postura tan\\nsólidamente definida como el gobierno respecto al Sahara.\\n\\n \\n\\f\"\n\nMuchos de los ficheros no tienen texto (son fotogramas sin rótulos). Contemos letras.\n\n\nsubtitulos &lt;- subtitulos %&gt;% \n    mutate(n_caracteres = nchar(value)) \n\nsubtitulos %&gt;% \n    group_by(n_caracteres) %&gt;% \n    count()\n#&gt; # A tibble: 131 × 2\n#&gt; # Groups:   n_caracteres [131]\n#&gt;    n_caracteres     n\n#&gt;           &lt;int&gt; &lt;int&gt;\n#&gt;  1            1   487\n#&gt;  2            3   118\n#&gt;  3            4    12\n#&gt;  4            5    14\n#&gt;  5            6     6\n#&gt;  6            7     4\n#&gt;  7            8     5\n#&gt;  8            9     3\n#&gt;  9           10     3\n#&gt; 10           11     4\n#&gt; # … with 121 more rows\n\nsubtitulos %&gt;% \n    group_by(n_caracteres) %&gt;% \n    count() %&gt;% \n  ggplot(aes(x = n_caracteres, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\nY viendo el conteo podríamos ver cómo son los rótulos con menos de 25 caracteres. Y suele ser por haber pillado el nombre de la canción en vez del rótulo\n\n\nsubtitulos %&gt;% \n    filter(n_caracteres &lt;= 25, n_caracteres &gt; 0 ) %&gt;% \n    arrange(desc(n_caracteres)) %&gt;% \n    head(40) %&gt;%\n    pull(value)\n#&gt; [[1]]\n#&gt; [1] \" \\n\\nMÁRQUEZ PIQUER YN\\n\\nY\\n\\f\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"DRES DO BARRO\\n\\n¡deirada\\n\\f\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \" \\n\\n¡A DORTA e\\nna O” A —\\n\\f\"\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"MBERROS UNIDOS\\n\\nIderete\\n\\f\"\n#&gt; \n#&gt; [[5]]\n#&gt; [1] \"LIE IMBRUGLIA Lp]\\n-n ]\\n\\f\"\n#&gt; \n#&gt; [[6]]\n#&gt; [1] \"E? EPR ENOAMADO\\nENCADo\\n\\f\"\n#&gt; \n#&gt; [[7]]\n#&gt; [1] \"IO DALLA\\n\\na La Vita\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[8]]\n#&gt; [1] \"O VENENO\\n\\n10 de menos\\n\\f\"\n#&gt; \n#&gt; [[9]]\n#&gt; [1] \"IL MCCARTNEY\\n\\nis One\\n\\f\"\n#&gt; \n#&gt; [[10]]\n#&gt; [1] \"'HAEL\\n\\n1é sabe nadie\\n\\f\"\n#&gt; \n#&gt; [[11]]\n#&gt; [1] \"   \\n\\nGEES ca z a\\njan\\n\\f\"\n#&gt; \n#&gt; [[12]]\n#&gt; [1] \" \\n\\nk\\nA BELÉN [\\napimó\\n\\f\"\n#&gt; \n#&gt; [[13]]\n#&gt; [1] \"RIGO CUEVAS\\n\\nbrujada\\n\\f\"\n#&gt; \n#&gt; [[14]]\n#&gt; [1] \"LETS\\n\\neased Lighting\\n\\f\"\n#&gt; \n#&gt; [[15]]\n#&gt; [1] \"3EL PANTOJA\\n\\nrlochí\\n\\f\"\n#&gt; \n#&gt; [[16]]\n#&gt; [1] \" \\n\\nMR banda LL Ú\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[17]]\n#&gt; [1] \"ANDA LEAR\\n\\nrfomania\\n\\f\"\n#&gt; \n#&gt; [[18]]\n#&gt; [1] \" \\n\\nOCOLATE\\n\\nyonesa\\n\\f\"\n#&gt; \n#&gt; [[19]]\n#&gt; [1] \"IS\\nFrom Désire\\n\\n  \\n\\f\"\n#&gt; \n#&gt; [[20]]\n#&gt; [1] \": ENEMIGOS\\n1 Vane\\n\\f\"\n#&gt; \n#&gt; [[21]]\n#&gt; [1] \"RAN DURAN\\nld Boys\\n\\f\"\n#&gt; \n#&gt; [[22]]\n#&gt; [1] \"SHOP BOYS\\n\\nsurbia\\n\\f\"\n#&gt; \n#&gt; [[23]]\n#&gt; [1] \"Sería el momen\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[24]]\n#&gt; [1] \"Boney M y Tigrete\\n\\f\"\n#&gt; \n#&gt; [[25]]\n#&gt; [1] \"IO FUTURA\\nGrados\\n\\f\"\n#&gt; \n#&gt; [[26]]\n#&gt; [1] \" \\n\\nSSIEL\\n\\nrinero\\n\\f\"\n#&gt; \n#&gt; [[27]]\n#&gt; [1] \"ACIA MONTES\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[28]]\n#&gt; [1] \" \\n\\n) SUAVES\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[29]]\n#&gt; [1] \" \\n\\nhe Ye ke\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[30]]\n#&gt; [1] \"UCambDatlacne\\n\\f\"\n#&gt; \n#&gt; [[31]]\n#&gt; [1] \"1,0616 EDAD |\\n\\f\"\n#&gt; \n#&gt; [[32]]\n#&gt; [1] \"¡GE URE\\n\\nathe\\n\\f\"\n#&gt; \n#&gt; [[33]]\n#&gt; [1] \"NA\\nMustafá\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[34]]\n#&gt; [1] \"AGO\\n\\nmbamanía\\n\\f\"\n#&gt; \n#&gt; [[35]]\n#&gt; [1] \",\\nLuftballons\\n\\f\"\n#&gt; \n#&gt; [[36]]\n#&gt; [1] \"SURE\\n\\nmetimes\\n\\f\"\n#&gt; \n#&gt; [[37]]\n#&gt; [1] \" \\n\\nDMA\\n\\nnbada\\n\\f\"\n#&gt; \n#&gt; [[38]]\n#&gt; [1] \"ENDI\\nEDI O\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[39]]\n#&gt; [1] \"oo “Brisp\\n\\nyr\\n\\f\"\n#&gt; \n#&gt; [[40]]\n#&gt; [1] \" \\n\\nPEEBLES\\nn\\n\\f\"\n\n\nsubtitulos %&gt;% \n    filter(n_caracteres == 30)\n#&gt; # A tibble: 5 × 4\n#&gt;    name value     n_fichero                      n_caracteres\n#&gt;   &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                                 &lt;int&gt;\n#&gt; 1    98 &lt;chr [1]&gt; 00000098.jpg.subtitulo.tif.txt           30\n#&gt; 2   409 &lt;chr [1]&gt; 00000409.jpg.subtitulo.tif.txt           30\n#&gt; 3   882 &lt;chr [1]&gt; 00000882.jpg.subtitulo.tif.txt           30\n#&gt; 4  1018 &lt;chr [1]&gt; 00001018.jpg.subtitulo.tif.txt           30\n#&gt; 5  1258 &lt;chr [1]&gt; 00001258.jpg.subtitulo.tif.txt           30\n\nUsando la librería magick en R que permite usar imagemagick en R, ver post de Raúl Vaquerizo y su homenaje a Sean Connery, podemos ver el fotgrama correspondiente\n\nlibrary(magick)\n(directorio_imagenes &lt;- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#&gt; /media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/video/2022_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00001258.jpg\"))\n\n\n\n\n\n\n\n\nAsí que nos quedamos con los rótulos con más de 30 caracteres\n\nsubtitulos &lt;-  subtitulos %&gt;% \n    filter(n_caracteres &gt; 30)\n\ndim(subtitulos)\n#&gt; [1] 567   4"
  },
  {
    "objectID": "2023/01/03/cachitos-2022-segunda-parte/index.html#detección-duplicados",
    "href": "2023/01/03/cachitos-2022-segunda-parte/index.html#detección-duplicados",
    "title": "Cachitos 2022. Segunda parte",
    "section": "Detección duplicados",
    "text": "Detección duplicados\nMini limpieza de caracteres extraños y puntuación\n\nstring_mini_clean &lt;-  function(string){\n    string &lt;- gsub(\"?\\n|\\n\", \" \", string)\n    string &lt;- gsub(\"\\r|?\\f|=\", \" \", string)\n    string &lt;- gsub('“|”|—|&gt;',\" \", string)\n    \n    string &lt;- gsub(\"[[:punct:][:blank:]]+\", \" \", string)\n    string &lt;- tolower(string)\n    string &lt;- gsub(\"  \", \" \", string)\n    \n    return(string)\n}\n\n# Haciendo uso de programación funciona con purrr es muy fácil pasar esta función a cada elemento. y decirle que \n# el resultado es string con map_chr\n\nsubtitulos_proces &lt;- subtitulos %&gt;% \n    mutate(texto = map_chr(value, string_mini_clean)) %&gt;% \n    select(-value)\n\nsubtitulos_proces %&gt;% \n  select(texto)\n#&gt; # A tibble: 567 × 1\n#&gt;    texto                                                                        \n#&gt;    &lt;chr&gt;                                                                        \n#&gt;  1 \"3 pues te vas a reír cuando sepas an lo que pasa en el xxi \"                \n#&gt;  2 \" bienvenidos si estáis todos es porque habéis recordado que tras los cuarto…\n#&gt;  3 \"bueno si eres colchonero al menos estás acostumbrado a los disgustos \"      \n#&gt;  4 \" faella carrá jn e el amor todo es empezar 3 \"                              \n#&gt;  5 \"en cachitos como en el amor y en el rascar todo es empezar os quedan tres h…\n#&gt;  6 \"esta actuación contiene más latigazos cervicales 32 que una colisión en cad…\n#&gt;  7 \"la oración pagana que todos hemos rezado mucha ms \"                         \n#&gt;  8 \"la oración pagana que todos hemos rezado muchas más veces de las que nos gu…\n#&gt;  9 \"aquel año el cádiz descendió a 2 b y creemos que bisbal no tuvo nada que ve…\n#&gt; 10 \" ha llegado ya el momento de abrir el melón de que este muchacho canta raro…\n#&gt; # … with 557 more rows\n\nDistancia de texto entre rótulos consecutivos\n\n\nsubtitulos_proces &lt;- subtitulos_proces %&gt;% \n    mutate(texto_anterior = lag(texto)) %&gt;% \n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\"))\n    \n \nsubtitulos_proces %&gt;% \n    filter(!is.na(distancia)) %&gt;% \n    select(name,texto,distancia, texto_anterior, everything()) %&gt;% \n    arrange(distancia) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nDecidimos eliminar texto cuya distancia sea menor de 36\n\nsubtitulos_proces &lt;-  subtitulos_proces %&gt;% \n    filter(distancia &gt; 36) %&gt;% \n    select(-texto_anterior)\n\nsubtitulos_proces %&gt;% \n    select(name,texto, everything()) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nNo nos hemos quitado todos los duplicados pero sí algunos de ellos.\n\ndim(subtitulos_proces)\n#&gt; [1] 488   5\n\nGuardamos el fichero unido\n\n\nwrite_csv(subtitulos_proces,\n          file = str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\nY aquí os dejo el enlace con los rótulos definitivos"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html",
    "href": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html",
    "title": "Cachitos 2023. Primera parte",
    "section": "",
    "text": "Como todos los años toca hacer el análisis de de los subtítulos de Nochevieja a Cachitos\nEste año resulta que la gente de RTVE ha capado el poder bajarse los videos desde rtve usando curl o wget , pero gracias a Javi Fdez he podido bajarme el video tirando de la url del streaming ."
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#requisitos",
    "href": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#requisitos",
    "title": "Cachitos 2023. Primera parte",
    "section": "Requisitos",
    "text": "Requisitos\n\nUn plugin en el navegador que pueda identificar el m3u asociado al archivo mp4 del video de Cachitos que está en rtve play. Yo he usado live Stream Downloader , pero seguro que hay alguno mejor\nUsar un sistema operativo GNU/Linux, en mi caso Linux Mint 21.2 basada en Ubuntu Jammy\nffmpeg sudo apt install ffmpeg . ffmpeg nos va a servir para poder bajar el video\nmplayer sudo apt install mplayer. mplayer con mplayer vamos a extraer 1 de cada 200 fotogramas del video\nimagemagick con sudo apt install imagemagick. Con imagemagick vamos a cambiar el tamaño de los fotogramas, recortar el área dónde están los subtítulos, convertir a formato tif e invertir los colores para que el texto se vea mejor.\nparallel sudo apt install parallel . Con parallel vamos a poder utilizar varios núcleos e hilos del ordenador en paralelo y poder hacer la modificación de los fotogramas y el reconocimiento óptico de caracteres más rápido.\ntesseract sudo apt install tesseract-ocr y sudo apt install tesseract-ocr-spa . Con tesseract se va a hacer el reconocimiento óptico del texto y guardar en ficheros de texto.\nElimina ficheros de texto de menos de 10 bytes, puesto que ahí no hay texto ."
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#script-de-bash",
    "href": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#script-de-bash",
    "title": "Cachitos 2023. Primera parte",
    "section": "Script de bash",
    "text": "Script de bash\nEl script es extract_subtitles.sh , se ejecutaría con extract_subtitles.sh 2023 .\nVamos a ir comentando el script poco a poco\n\nBajar video\nIndicamos el nombre del directorio donde queremos trabajar, lo creamos y también creamos un subdirectorio llamado video , nos situamos en él y creamos algunas variables como el nombre del video cuando nos lo bajemos o como se van a llamar los directorios dónde vamos a ir guardando los fotogramas o el texto, típicamente 2023_jpg y 2023_txt. El script está puesto para que se le pase el año y se construya el\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n\n\nif [ \"$ANNO\" == \"2023\" ] ;\nthen\n    ffmpeg -i \"https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4/video.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=7047821\" -c copy $video_file\nfi\n\n\nExtracción de fotogramas\nUsando mplayer que sirve para reproducir el vídeo, podemos indicar además que nos extraiga una imagen cada 200 fotogramas framestep=200, también le decimos que lo reproduzca sin sonido y a velocidad alta y que guarde esos fotogramas como archivos jpeg en la ruta relativa `2023_jpg dentro de nuestro directorio principal.\nEste es el comando que hace todo eso\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n\n\nRecorte de la zona de dónde está el rótulo\nDe años anteriores sé que si “achico” cada imagen anterior me vale de igual forma y por prueba y error vi en que parte de la imagen suelen colocar los rótulos. Así que con herramientas de imagemagick puedo manipular esas imágenes y extraer solo la zona del rótulo.\n\nReducción de la imagen\nCambiamos al directorio dónde hemos dejado las imágenes, y usando el comando find seleccionamos todos los archivos con extensión jpg y los reducimos a formato de 642x480.\nPara eso usamos mogrify que forma parte de imagemagick y la parte de usar parallel es para que se ejecute en paralelo usando 8 hilos de cpu.\n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 8 mogrify -resize 642x480 {}\n\n\nSeleccionar la zona dónde están los rótulos\nAhora usamos otras funciones de imagemagick que son convert y crop . Lo que hacemos es convertir a formato tif y recortamos justo la zona dónde están los rótulos.\nAsí, para cada imagen jpg creamos un archivo con nombre nombre_imagen.jpg.subtitulo.tif\n\n# Seleccionar cacho dond están subtitulos\nfind . -name '*.jpg' |  parallel -j 8 convert {} -crop 460x50+90+285 +repage -compress none -depth 8 {}.subtitulo.tif\n\n\nHacer el negativo de las imágenes\nPara ayudar al software de reconocimiento óptico de caractereres obtenemos el negativo de cada tif. Para eso usamos de nuevo convert pero con la opción negate\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 8 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n\n\nsubtitulo_negativo\n\n\n\n\n\nReconocimiento óptico de caracteres\nNosotros lo que queremos es tener el texto, no una imagen. Para eso utilizamos tesseract. Le pasamos cada una de las imágenes anteriores en formato tif y nos devolverá el texto que contienen. No siempre lo hará bien, pero el resultado es aceptable. También creamos directorio dónde vamos a mover todos los ficheros de texto creados y eliminamos los ficheros con menos de 10 bytes.\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 8 tesseract -l spa {} {}\n\n# mover a directorio texto\nmkdir -p $root_directory/$ANNO$suffix_txt_dir\n\nmv *.txt $root_directory/$ANNO$suffix_txt_dir\n\ncd $root_directory/$ANNO$suffix_txt_dir\n\n# Borrar archivos de 10 bytes , subtítulos vacíos, bytes se indican con la letra c, cosas de linux\nfind . -size -10c -exec rm -f {} \\;\nY podemos ver cómo ha convertido por ejemplo el rótulo asociado al fotograma 35.\n\ncmd &lt;-  \"cat ~/proyecto_cachitos/2023_txt/00000035.jpg.subtitulo.tif.txt\"\nsystem(cmd, intern = TRUE)\n#&gt; [1] \"ste tema se escribió con cartas de mujeres con cáncer de mama\"    \n#&gt; [2] \"\"                                                                 \n#&gt; [3] \"Si te ha tocado este año, lo de la amnistía igual lo relativizas.\"\n#&gt; [4] \"\\f\"\n\nComo vemos, la lógica de la creación de los ficheros de texto ha sido la de ir guardando el nombre original del fotograma e ir añadiendo extensiones, (.jpg, .subtitulo.tif, .txt).\nY sin más, os dejo también aquí el script entero\n#!/bin/bash\n\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n\n\nif [ \"$ANNO\" == \"2023\" ] ;\nthen\n    ffmpeg -i \"https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4/video.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=7047821\" -c copy $video_file\nfi\n\nif [ \"$ANNO\" == \"2022\" ] ;\nthen\n    ffmpeg -i \"https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/1/5/1672556504451.mp4/video.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=6767615\" -c copy $video_file\nfi \n \n\n# Pasar a jpg uno de cada 200 fotogramas\n\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 8 mogrify -resize 642x480 {}\n\n# Seleccionar cacho dond están subtitulos\nfind . -name '*.jpg' |  parallel -j 8 convert {} -crop 460x50+90+285 +repage -compress none -depth 8 {}.subtitulo.tif\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 8 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 8 tesseract -l spa {} {}\n\n# mover a directorio texto\nmkdir -p $root_directory/$ANNO$suffix_txt_dir\n\nmv *.txt $root_directory/$ANNO$suffix_txt_dir\n\ncd $root_directory/$ANNO$suffix_txt_dir\n\n# Borrar archivos de 10 bytes , subtítulos vacíos\nfind . -size -10c -exec rm -f {} \\;\n\ncd $root_directory"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#algún-ejemplo",
    "href": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#algún-ejemplo",
    "title": "Cachitos 2023. Primera parte",
    "section": "Algún ejemplo",
    "text": "Algún ejemplo\n\nlibrary(tidyverse)\n\nroot_directory = \"~/proyecto_cachitos/\"\nanno &lt;- \"2023\"\nlibrary(magick)\n(directorio_imagenes &lt;- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#&gt; ~/proyecto_cachitos/video/2023_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00001297.jpg\"))\n\n\n\n\n\n\n\n\n\nimage_read(str_glue(\"{directorio_imagenes}00001297.jpg.subtitulo.tif\"))\n\n\n\n\n\n\n\n\n\n\n(directorio_texto &lt;- str_glue(\"{root_directory}{anno}_txt/\"))\n#&gt; ~/proyecto_cachitos/2023_txt/\n\nsystem(str_glue(\"cat {directorio_texto}00001297.jpg.subtitulo.tif.txt\"), intern = TRUE)\n#&gt; [1] \"Con el tiempo descubrimos que todas las letras de este\"\n#&gt; [2] \"exprofesor de Primaria tenian referencias sexuales.\"   \n#&gt; [3] \"\\f\"\n\nY poco más, mañana intentaré tener elpost similar al del año pasado dónde veíamos distancias de texto entre rótulos y demás."
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html",
    "href": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html",
    "title": "Cachitos 2023. Segunda parte",
    "section": "",
    "text": "Una vez que ya hemos visto en la entrada anterior como extraer los rótulos, vamos a juntarlos todos en un sólo csv y hacer algo de limpieza.\nDejo el enlace a los ficheros de texto construidos por tesseract enlace directorio"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html#lectura-rótulos",
    "href": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html#lectura-rótulos",
    "title": "Cachitos 2023. Segunda parte",
    "section": "Lectura rótulos",
    "text": "Lectura rótulos\n\n\nlibrary(tidyverse)\n\nroot_directory = \"~/proyecto_cachitos/\"\nanno &lt;- \"2023\"\n\n\nnombre_ficheros &lt;- list.files(path = str_glue(\"{root_directory}{anno}_txt/\")) %&gt;% \n    enframe() %&gt;% \n    rename(n_fichero = value)\n\nnombre_ficheros\n#&gt; # A tibble: 687 × 2\n#&gt;     name n_fichero                     \n#&gt;    &lt;int&gt; &lt;chr&gt;                         \n#&gt;  1     1 00000004.jpg.subtitulo.tif.txt\n#&gt;  2     2 00000010.jpg.subtitulo.tif.txt\n#&gt;  3     3 00000011.jpg.subtitulo.tif.txt\n#&gt;  4     4 00000012.jpg.subtitulo.tif.txt\n#&gt;  5     5 00000018.jpg.subtitulo.tif.txt\n#&gt;  6     6 00000019.jpg.subtitulo.tif.txt\n#&gt;  7     7 00000020.jpg.subtitulo.tif.txt\n#&gt;  8     8 00000025.jpg.subtitulo.tif.txt\n#&gt;  9     9 00000026.jpg.subtitulo.tif.txt\n#&gt; 10    10 00000027.jpg.subtitulo.tif.txt\n#&gt; # ℹ 677 more rows\n\nAhora los podemos leer en orden\n\n\nsubtitulos &lt;-  list.files(path = str_glue(\"{root_directory}{anno}_txt/\"), \n                        pattern = \"*.txt\", full.names = TRUE) %&gt;% \n    map(~read_file(.)) %&gt;% \n    enframe() %&gt;%  \n    left_join(nombre_ficheros)\n\nglimpse(subtitulos)\n#&gt; Rows: 687\n#&gt; Columns: 3\n#&gt; $ name      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n#&gt; $ value     &lt;list&gt; \"ey ÁNGEL CARMONA | y\\n.».\\n\\n \\n\\f\", \"DAVID BISBAL - Llora…\n#&gt; $ n_fichero &lt;chr&gt; \"00000004.jpg.subtitulo.tif.txt\", \"00000010.jpg.subtitulo.ti…\nsubtitulos\n#&gt; # A tibble: 687 × 3\n#&gt;     name value     n_fichero                     \n#&gt;    &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                         \n#&gt;  1     1 &lt;chr [1]&gt; 00000004.jpg.subtitulo.tif.txt\n#&gt;  2     2 &lt;chr [1]&gt; 00000010.jpg.subtitulo.tif.txt\n#&gt;  3     3 &lt;chr [1]&gt; 00000011.jpg.subtitulo.tif.txt\n#&gt;  4     4 &lt;chr [1]&gt; 00000012.jpg.subtitulo.tif.txt\n#&gt;  5     5 &lt;chr [1]&gt; 00000018.jpg.subtitulo.tif.txt\n#&gt;  6     6 &lt;chr [1]&gt; 00000019.jpg.subtitulo.tif.txt\n#&gt;  7     7 &lt;chr [1]&gt; 00000020.jpg.subtitulo.tif.txt\n#&gt;  8     8 &lt;chr [1]&gt; 00000025.jpg.subtitulo.tif.txt\n#&gt;  9     9 &lt;chr [1]&gt; 00000026.jpg.subtitulo.tif.txt\n#&gt; 10    10 &lt;chr [1]&gt; 00000027.jpg.subtitulo.tif.txt\n#&gt; # ℹ 677 more rows\n\nTenemos 687 rótulos de los cuales la mayoría estarán vacíos"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html#contando-letras",
    "href": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html#contando-letras",
    "title": "Cachitos 2023. Segunda parte",
    "section": "Contando letras",
    "text": "Contando letras\nEn n_fichero tenemos el nombre y en value el texto. Si vemos alguno de los subtítulos.\n\n\nsubtitulos %&gt;%\n    pull(value) %&gt;%\n    pluck(239)\n#&gt; [1] \"El Darth Vader español: tiene la voz profunda,\\n\\nun lado oscuro y podría ser tu padre\\n\\n \\n\\f\"\n\nContemos letras.\n\n\nsubtitulos &lt;- subtitulos %&gt;% \n    mutate(n_caracteres = nchar(value)) \n\nsubtitulos %&gt;% \n    group_by(n_caracteres) %&gt;% \n    count()\n#&gt; # A tibble: 105 × 2\n#&gt; # Groups:   n_caracteres [105]\n#&gt;    n_caracteres     n\n#&gt;           &lt;int&gt; &lt;int&gt;\n#&gt;  1           10     5\n#&gt;  2           12     3\n#&gt;  3           13     1\n#&gt;  4           15     1\n#&gt;  5           18     1\n#&gt;  6           21     2\n#&gt;  7           22     1\n#&gt;  8           23     1\n#&gt;  9           29     1\n#&gt; 10           32     4\n#&gt; # ℹ 95 more rows\n\nsubtitulos %&gt;% \n    group_by(n_caracteres) %&gt;% \n    count() %&gt;% \n  ggplot(aes(x = n_caracteres, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\nY viendo el conteo podríamos ver cómo son los rótulos con menos de 25 caracteres. Y suele ser por haber pillado el nombre de la canción en vez del rótulo\n\n\nsubtitulos %&gt;% \n    filter(n_caracteres &lt;= 25, n_caracteres &gt; 0 ) %&gt;% \n    arrange(desc(n_caracteres)) %&gt;% \n    head(40) %&gt;%\n    pull(value)\n#&gt; [[1]]\n#&gt; [1] \"Para Crazy, su padre.\\n\\f\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"—\\nNATALIA LAFOURCADE\\n\\f\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"HO *\\n\\n= Roland 7-50\\n\\f\"\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"Os lo dijimos.\\n\\n   \\n\\f\"\n#&gt; \n#&gt; [[5]]\n#&gt; [1] \"pee\\n\\nES\\n\\no\\n\\nMIEL\\n\\f\"\n#&gt; \n#&gt; [[6]]\n#&gt; [1] \"¿Es ANO MN ss\\n\\f\"\n#&gt; \n#&gt; [[7]]\n#&gt; [1] \"The GodFary\\n\\f\"\n#&gt; \n#&gt; [[8]]\n#&gt; [1] \"Mar y sol.\\n\\f\"\n#&gt; \n#&gt; [[9]]\n#&gt; [1] \"NA PAL =&gt;:\\n\\f\"\n#&gt; \n#&gt; [[10]]\n#&gt; [1] \"O AA A\\nA E\\n\\f\"\n#&gt; \n#&gt; [[11]]\n#&gt; [1] \"WI ES MO\\n\\f\"\n#&gt; \n#&gt; [[12]]\n#&gt; [1] \"QA Y EPA\\n\\f\"\n#&gt; \n#&gt; [[13]]\n#&gt; [1] \"SN\\n1\\nO y\\n\\f\"\n#&gt; \n#&gt; [[14]]\n#&gt; [1] \" \\n\\nml\\n\\n \\n\\f\"\n#&gt; \n#&gt; [[15]]\n#&gt; [1] \"ly\\nMi\\n\\n \\n\\f\"\n\n\nsubtitulos %&gt;% \n    filter(n_caracteres &gt;= 30) %&gt;% \n    arrange(n_caracteres)\n#&gt; # A tibble: 671 × 4\n#&gt;     name value     n_fichero                      n_caracteres\n#&gt;    &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                                 &lt;int&gt;\n#&gt;  1   151 &lt;chr [1]&gt; 00000309.jpg.subtitulo.tif.txt           32\n#&gt;  2   176 &lt;chr [1]&gt; 00000361.jpg.subtitulo.tif.txt           32\n#&gt;  3   513 &lt;chr [1]&gt; 00001015.jpg.subtitulo.tif.txt           32\n#&gt;  4   677 &lt;chr [1]&gt; 00001333.jpg.subtitulo.tif.txt           32\n#&gt;  5   466 &lt;chr [1]&gt; 00000917.jpg.subtitulo.tif.txt           34\n#&gt;  6   170 &lt;chr [1]&gt; 00000346.jpg.subtitulo.tif.txt           35\n#&gt;  7   241 &lt;chr [1]&gt; 00000496.jpg.subtitulo.tif.txt           35\n#&gt;  8   361 &lt;chr [1]&gt; 00000733.jpg.subtitulo.tif.txt           35\n#&gt;  9   446 &lt;chr [1]&gt; 00000877.jpg.subtitulo.tif.txt           35\n#&gt; 10   687 &lt;chr [1]&gt; 00001353.jpg.subtitulo.tif.txt           35\n#&gt; # ℹ 661 more rows\n\nUsando la librería magick en R que permite usar imagemagick en R, ver post de Raúl Vaquerizo y su homenaje a Sean Connery, podemos ver el fotgrama correspondiente\n\nlibrary(magick)\n(directorio_imagenes &lt;- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#&gt; ~/proyecto_cachitos/video/2023_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00000383.jpg\"))\n\n\n\n\n\n\n\n\nAsí que nos quedamos con los rótulos con más de 30 caracteres\n\nsubtitulos &lt;-  subtitulos %&gt;% \n    filter(n_caracteres &gt; 30)\n\ndim(subtitulos)\n#&gt; [1] 671   4"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html#detección-duplicados",
    "href": "2024/01/cachitos_2023/cachitos_2023_segunda_parte.html#detección-duplicados",
    "title": "Cachitos 2023. Segunda parte",
    "section": "Detección duplicados",
    "text": "Detección duplicados\nMini limpieza de caracteres extraños y puntuación\n\nstring_mini_clean &lt;-  function(string){\n    string &lt;- gsub(\"?\\n|\\n\", \" \", string)\n    string &lt;- gsub(\"\\r|?\\f|=\", \" \", string)\n    string &lt;- gsub('“|”|—|&gt;',\" \", string)\n    \n    string &lt;- gsub(\"[[:punct:][:blank:]]+\", \" \", string)\n    string &lt;- tolower(string)\n    string &lt;- gsub(\"  \", \" \", string)\n    \n    return(string)\n}\n\n# Haciendo uso de programación funciona con purrr es muy fácil pasar esta función a cada elemento. y decirle que \n# el resultado es string con map_chr\n\nsubtitulos_proces &lt;- subtitulos %&gt;% \n    mutate(texto = map_chr(value, string_mini_clean)) %&gt;% \n    select(-value)\n\nsubtitulos_proces %&gt;% \n  select(texto)\n#&gt; # A tibble: 671 × 1\n#&gt;    texto                                                                        \n#&gt;    &lt;chr&gt;                                                                        \n#&gt;  1 \"david bisbal lloraré las penas noche de fiesta 2002 \"                       \n#&gt;  2 \"bueno cómo están los máquinas lo primero de todo estáis bien \"              \n#&gt;  3 \"pues eso que hemos venido a bailar a cantar y si hay que llorar las penas d…\n#&gt;  4 \"rayden calle de la llorería benidorm fest 2021 \"                            \n#&gt;  5 \"la verdad es que rayden por no ir a eurovisión con esta canción no lloró mu…\n#&gt;  6 \"anunció que se iba a retirar cuando lo veamos lloramos \"                    \n#&gt;  7 \"fangoria dramas y comedias feliz 2015 \"                                     \n#&gt;  8 \"para algunos el año ha sido más dramático que cómico y viceversa \"          \n#&gt;  9 \"aunque también ha habido muchos como nacho canut a su bola \"                \n#&gt; 10 \"rozalén con estopa vivir territorio estopa 2019 \"                           \n#&gt; # ℹ 661 more rows\n\nDistancia de texto entre rótulos consecutivos\n\n\nsubtitulos_proces &lt;- subtitulos_proces %&gt;% \n    mutate(texto_anterior = lag(texto)) %&gt;% \n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\"))\n    \n \nsubtitulos_proces %&gt;% \n    filter(!is.na(distancia)) %&gt;% \n    select(name,texto,distancia, texto_anterior, everything()) %&gt;% \n    arrange(distancia) %&gt;% \n    DT::datatable(options = list(scrollX=TRUE))\n\n\n\n\n\n\nDecidimos eliminar texto cuya distancia sea menor de 30\n\nsubtitulos_proces &lt;-  subtitulos_proces %&gt;% \n    filter(distancia &gt;= 30) %&gt;% \n    select(-texto_anterior)\n\nsubtitulos_proces %&gt;% \n    select(name,texto, everything()) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\nNo nos hemos quitado todos los duplicados pero sí algunos de ellos.\n\ndim(subtitulos_proces)\n#&gt; [1] 583   5\n\nY ya solo tenemos 583 rótulos\nGuardamos el fichero unido\n\n\nwrite_csv(subtitulos_proces,\n          file = str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\n\nDT::datatable(\n    subtitulos_proces %&gt;%\n        select(-distancia) %&gt;% \n        arrange(n_fichero))\n\n\n\n\n\n\nY aquí os dejo el enlace con los rótulos definitivos"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#plugin",
    "href": "2024/01/cachitos_2023/cachitos_2023_primera_parte.html#plugin",
    "title": "Cachitos 2023. Primera parte",
    "section": "Plugin",
    "text": "Plugin\nCon el plugin de Live Stream instalado en el navegador, lo primero que hacemos es ir a la web de rtveplay, buscar Nochevieja a Cachitos y darle al play. Estando en esa pestaña, le damos al plugin y nos abrirá una ventana emergente dónde se nos ha copiado la info del streaming. Esta info será útil porque es lo que nos va a permitir poder bajar el video usando ffmpeg\n\n\n\nplugin\n\n\nLa info que nos da es algo como esto\nhttps://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4/1704096565613-audio=192755-video=2752986-3331.ts?idasset=7047821\nEn años anteriores me podía bajar directamente el archivo mp4 usando wget, pero este año nuestros amigos de rtve han decidido que no se puede.\nEsto es lo que me sale al intentarlo\n wget https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4\n--2024-01-02 13:15:27--  https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4\nResolviendo rtvehlsvodlote7.rtve.es (rtvehlsvodlote7.rtve.es)... 151.101.134.137, 2a04:4e42:1f::649\nConectando con rtvehlsvodlote7.rtve.es (rtvehlsvodlote7.rtve.es)[151.101.134.137]:443... conectado.\nPetición HTTP enviada, esperando respuesta... 403 Forbidden\n2024-01-02 13:15:27 ERROR 403: Forbidden.\nVolviendo a la info que hemos obtenido usando el plugin, es importante identificar el idasset, en este caso el 7047821. Una vez sabemos eso, justo después de la parte dónde viene indicado el mp4, la cambiamos por esto\nvideo.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=7047821 y ya podemos usar ffmpeg para guardar el archivo.\nSon casi unos 4gb de video.\nffmpeg -i \"https://rtvehlsvodlote7.rtve.es/mediavodv2/resources/TE_SHIECRO/mp4/3/1/1704096565613.mp4/video.m3u8?hls_no_audio_only=true&hls_client_manifest_version=3&idasset=7047821\" -c copy video.mp4"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html",
    "title": "Cachitos 2023. Tercera parte",
    "section": "",
    "text": "Vamos ya con la penúltima entrada del cachitos de este año. Las anteriores, las tenemos en esta y esta otra\nEl csv con el texto de los rótulos para 2023 lo tenemos en este enlace\nVamos al lío"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#librerías",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#librerías",
    "title": "Cachitos 2023. Tercera parte",
    "section": "Librerías",
    "text": "Librerías\n\n\nCode\n\n    suppressPackageStartupMessages({\n        library(tidyverse) \n        library(tidytext)\n        library(quanteda)\n        library(magick)\n        library(RColorBrewer)\n        library(wordcloud)\n        library(topicmodels)\n        \n        }\n    )"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#lectura-de-datos-y-vistazo-datos",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#lectura-de-datos-y-vistazo-datos",
    "title": "Cachitos 2023. Tercera parte",
    "section": "Lectura de datos, y vistazo datos",
    "text": "Lectura de datos, y vistazo datos\n\n\nCode\nroot_directory = \"~/proyecto_cachitos/\"\nanno &lt;- \"2023\"\n\n\nLeemos el csv. Uso DT y así podéis ver todos los datos o buscar cosas, por ejemplo amnistía o crispación\n\n\nCode\nsubtitulos_proces &lt;-  read_csv(str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\nsubtitulos_proces %&gt;% \n  select(texto, n_fichero, n_caracteres) %&gt;% \n  DT::datatable()\n\n\n\n\n\n\n\nPues nos valdría con esto para buscar términos polémicos."
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#algo-de-minería-de-texto",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#algo-de-minería-de-texto",
    "title": "Cachitos 2023. Tercera parte",
    "section": "Algo de minería de texto",
    "text": "Algo de minería de texto\nQuitamos stopwords y tokenizamos de forma que tengamos cada palabra en una fila manteniendo de qué rótulo proviene\n\n\nCode\n\nto_remove &lt;- c(tm::stopwords(\"es\"),\n               \"110\", \"4\",\"1\",\"2\",\"7\",\"10\",\"0\",\"ñ\",\"of\",\n               \"5\",\"á\",\"i\",\"the\",\"3\", \"n\", \"p\",\n               \"ee\",\"uu\",\"mm\",\"ema\", \"zz\",\n               \"wr\",\"wop\",\"wy\",\"x\",\"xi\",\"xl\",\"xt\",\n               \"xte\",\"yí\", \"your\")\n\n\n\nsubtitulos_proces_one_word &lt;- subtitulos_proces %&gt;% \n    unnest_tokens(input = texto,\n                  output = word) %&gt;% \n    filter(! word %in% to_remove) %&gt;% \n    filter(nchar(word) &gt; 1)\n\ndim(subtitulos_proces_one_word)\n#&gt; [1] 4258    5\n\n\n\n\nCode\nDT::datatable(subtitulos_proces_one_word)\n\n\n\n\n\n\n\nContar ocurrencias de cosas es lo más básico.\n\n\nCode\n\npalabras_ordenadas_1 &lt;- subtitulos_proces_one_word %&gt;% \n    group_by(word) %&gt;% \n    summarise(veces = n()) %&gt;% \n    arrange(desc(veces))\n\npalabras_ordenadas_1 %&gt;% \n    slice(1:20) %&gt;% \n    ggplot(aes(x = reorder(word, veces), y = veces)) +\n    geom_col(show.legend = FALSE) +\n    ylab(\"veces\") +\n    xlab(\"\") +\n    coord_flip() +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nY como el año pasado una de las palabras más comunes es “canción” . ¿Y si añadimos las 20 palabras como stopword, junto con algunas como [“tan”, “sólo”,“así”, “aquí”, “hoy”] . La tarea de añadir palabras como stopwords requiere trabajo, tampoco nos vamos a parar tanto.\n\n\nCode\n(add_to_stop_words &lt;- palabras_ordenadas_1 %&gt;% \n    slice(1:25) %&gt;% \n    pull(word) )\n#&gt;  [1] \"año\"     \"canción\" \"si\"      \"vida\"    \"ahora\"   \"menos\"   \"después\"\n#&gt;  [8] \"españa\"  \"aplauso\" \"años\"    \"música\"  \"noche\"   \"tema\"    \"1990\"   \n#&gt; [15] \"feliz\"   \"primera\" \"bien\"    \"siempre\" \"vez\"     \"2023\"    \"aunque\" \n#&gt; [22] \"igual\"   \"señores\" \"tan\"     \"tiempo\"\n\n\nto_remove &lt;- unique(c(to_remove,\n                      add_to_stop_words,\n                      \"tan\", \n                      \"sólo\", \n                      \"así\",\n                      \"aquí\", \n                      \"hoy\",\n                      \"va\"))\n\n\nsubtitulos_proces_one_word &lt;- subtitulos_proces %&gt;% \n    unnest_tokens(input = texto,\n                  output = word) %&gt;% \n    filter(! word %in% to_remove) %&gt;% \n    filter(nchar(word) &gt; 1)\n\n\n\n\nCode\n\npalabras_ordenadas_2 &lt;- subtitulos_proces_one_word %&gt;% \n    group_by(word) %&gt;% \n    summarise(veces = n()) %&gt;% \n    arrange(desc(veces))\n\npalabras_ordenadas_2 %&gt;% \n    slice(1:20) %&gt;% \n    ggplot(aes(x = reorder(word, veces), y = veces)) +\n    geom_col(show.legend = FALSE) +\n    ylab(\"veces\") +\n    xlab(\"\") +\n    coord_flip() +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nTambién podemos ver ahora una nube de palabras\n\n\nCode\n\npal &lt;- brewer.pal(8,\"Dark2\")\n\nsubtitulos_proces_one_word %&gt;% \n    group_by(word) %&gt;% \n    count() %&gt;% \n    with(wordcloud(word, n, random.order = FALSE, max.words = 110, colors=pal))"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#polémicos",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#polémicos",
    "title": "Cachitos 2023. Tercera parte",
    "section": "¿Polémicos?",
    "text": "¿Polémicos?\nCreamos lista de palabras polémicas (se aceptan otras, podéis poner en los comentarios).\n\n\nCode\n\npalabras_polem &lt;-\n    c(\n        \"abascal\",\n        \"almeida\",\n        \"amnistía\",\n        \"ayuso\",\n        \"belarra\",\n        \"bloqueo\",\n        \"brusel\",\n        \"catal\",\n        \"ciudada\",\n        \"comunidad\",\n        \"constitucional\",\n        \"coron\",\n        \"crispación\",\n        \"democr\",\n        \"democracia\",\n        \"derech\",\n        \"díaz\",\n        \"dioni\",\n        \"errejon\",\n        \"extremadura\",\n        \"fach\",\n        \"falcon\",\n        \"fasc\",\n        \"feij\",\n        \"feijóo\",\n        \"gobierno\",\n        \"guardia\",\n        \"guerra\",\n        \"iglesias\",\n        \"izquier\",\n        \"ley\",\n        \"madrid\",\n        \"manipulador\",\n        \"militares\",\n        \"minist\",\n        \"monarca\",\n        \"montero\",\n        \"oposición\",\n        \"page\",\n        \"pandem\",\n        \"polarización\",\n        \"polarizados\",\n        \"pp\",\n        \"principe\",\n        \"prisión\",\n        \"psoe\",\n        \"sumar\",\n        \"puigdemont\",\n        \"republic\",\n        \"rey\",\n        \"rufián\",\n        \"sánchez\",\n        \"sanz\",\n        \"tezanos\",\n        \"toled\",\n        \"transición\",\n        \"ultra\",\n        \"vicepre\",\n        \"vox\",\n        \"yolanda\",\n        \"zarzu\",\n        \"zarzuela\"\n    )\n\n\nY construimos una regex simple\n\n\nCode\n(exp_regx &lt;- paste0(\"^\",paste(palabras_polem, collapse = \"|^\")))\n#&gt; [1] \"^abascal|^almeida|^amnistía|^ayuso|^belarra|^bloqueo|^brusel|^catal|^ciudada|^comunidad|^constitucional|^coron|^crispación|^democr|^democracia|^derech|^díaz|^dioni|^errejon|^extremadura|^fach|^falcon|^fasc|^feij|^feijóo|^gobierno|^guardia|^guerra|^iglesias|^izquier|^ley|^madrid|^manipulador|^militares|^minist|^monarca|^montero|^oposición|^page|^pandem|^polarización|^polarizados|^pp|^principe|^prisión|^psoe|^sumar|^puigdemont|^republic|^rey|^rufián|^sánchez|^sanz|^tezanos|^toled|^transición|^ultra|^vicepre|^vox|^yolanda|^zarzu|^zarzuela\"\n\n\nY nos creamos una variable para identificar si es palabra polémica\n\n\nCode\nsubtitulos_proces_one_word &lt;- subtitulos_proces_one_word %&gt;% \n    mutate(polemica= str_detect(word, exp_regx))\n\n\nsubtitulos_polemicos &lt;- subtitulos_proces_one_word %&gt;% \n    filter(polemica) %&gt;% \n    pull(n_fichero) %&gt;% \n    unique()\n\nsubtitulos_polemicos\n#&gt;  [1] \"00000035.jpg.subtitulo.tif.txt\" \"00000059.jpg.subtitulo.tif.txt\"\n#&gt;  [3] \"00000091.jpg.subtitulo.tif.txt\" \"00000114.jpg.subtitulo.tif.txt\"\n#&gt;  [5] \"00000121.jpg.subtitulo.tif.txt\" \"00000222.jpg.subtitulo.tif.txt\"\n#&gt;  [7] \"00000292.jpg.subtitulo.tif.txt\" \"00000355.jpg.subtitulo.tif.txt\"\n#&gt;  [9] \"00000382.jpg.subtitulo.tif.txt\" \"00000383.jpg.subtitulo.tif.txt\"\n#&gt; [11] \"00000397.jpg.subtitulo.tif.txt\" \"00000411.jpg.subtitulo.tif.txt\"\n#&gt; [13] \"00000427.jpg.subtitulo.tif.txt\" \"00000432.jpg.subtitulo.tif.txt\"\n#&gt; [15] \"00000474.jpg.subtitulo.tif.txt\" \"00000534.jpg.subtitulo.tif.txt\"\n#&gt; [17] \"00000549.jpg.subtitulo.tif.txt\" \"00000595.jpg.subtitulo.tif.txt\"\n#&gt; [19] \"00000640.jpg.subtitulo.tif.txt\" \"00000656.jpg.subtitulo.tif.txt\"\n#&gt; [21] \"00000680.jpg.subtitulo.tif.txt\" \"00000772.jpg.subtitulo.tif.txt\"\n#&gt; [23] \"00000803.jpg.subtitulo.tif.txt\" \"00000810.jpg.subtitulo.tif.txt\"\n#&gt; [25] \"00000825.jpg.subtitulo.tif.txt\" \"00000902.jpg.subtitulo.tif.txt\"\n#&gt; [27] \"00000920.jpg.subtitulo.tif.txt\" \"00000964.jpg.subtitulo.tif.txt\"\n#&gt; [29] \"00000973.jpg.subtitulo.tif.txt\" \"00001103.jpg.subtitulo.tif.txt\"\n#&gt; [31] \"00001129.jpg.subtitulo.tif.txt\" \"00001157.jpg.subtitulo.tif.txt\"\n#&gt; [33] \"00001181.jpg.subtitulo.tif.txt\" \"00001194.jpg.subtitulo.tif.txt\"\n#&gt; [35] \"00001278.jpg.subtitulo.tif.txt\" \"00001330.jpg.subtitulo.tif.txt\"\n#&gt; [37] \"00001343.jpg.subtitulo.tif.txt\"\n\n\nY podemos ver en el texto original antes de tokenizar qué rótulos hemos considerado polémicos y qué texto\n\n\nCode\nsubtitulos_proces %&gt;% \n    filter(n_fichero %in% subtitulos_polemicos) %&gt;% \n    arrange(n_fichero) %&gt;% \n    pull(texto) %&gt;% \n    unique()\n#&gt;  [1] \"ste tema se escribió con cartas de mujeres con cáncer de mama si te ha tocado este año lo de la amnistía igual lo relativizas\"   \n#&gt;  [2] \"si no sabéis quién es quién toñi es la de la izquierda y encorna la que le da el chorro de aire acondicionado\"                   \n#&gt;  [3] \"el de la izquierda del todo ya no está en la coalición para algunos el chunguito verdadero\"                                      \n#&gt;  [4] \"la de la izquierda hizo lo que todo padre sueña al mirar su whatsapp abandonar el grupo\"                                         \n#&gt;  [5] \"quizá por eso se decía que estaba como un tren bueno en extremadura decían como un autobús\"                                      \n#&gt;  [6] \"ana belén conoció a victor manuel al mismo tiempo que a julio iglesias ésta se la cantó a julio\"                                 \n#&gt;  [7] \"la versión que habla de la relación de feijóo con el último resultado electoral se titula nosentera\"                             \n#&gt;  [8] \"a henry como a los ultras de las protestas contra la amnistía le gusta estar cara al sol\"                                        \n#&gt;  [9] \"juan luis guerra 4 40 visa para un sueño pero esto qué es esto es lo que hay 1990\"                                               \n#&gt; [10] \"ojalá todas las guerras que hemos vivido este año se llamaran juan luis\"                                                         \n#&gt; [11] \"vicente fernández y massiel el rey palmarés 1976\"                                                                                \n#&gt; [12] \"con todos los respetos a antonio josé esperábamos que el dúo de rufián fuera con junqueras\"                                      \n#&gt; [13] \"la pandemia creímos que formaban el dúo más estomagante gonzález y guerra han vuelto para recordarnos que no\"                    \n#&gt; [14] \"el pp usó verano azul para su campaña el plan b era fariña o crematorio pero el resultado fue poco yo\"                           \n#&gt; [15] \"el grupo más longevo del pop español años antes de protagonizar su propia guerra de los clones\"                                  \n#&gt; [16] \"en este año de crispación política la propuesta no de ley sobre alud mental logró 340 votos aún así seguimos sin una ley naciona\"\n#&gt; [17] \"los niños protagonistas de esta canción ya tienen mejor currículum que santiago abascal\"                                         \n#&gt; [18] \"el rey de la copla callejera vestía como un agente de tecnocaso y cobía por la puerta chica del imaginarium\"                     \n#&gt; [19] \"alejandro sanz viviendo deprisa ay vida mia 1992\"                                                                                \n#&gt; [20] \"lleva la camisa más abierta que las barreras de sánchez en una negociación de investidura\"                                       \n#&gt; [21] \"si puigdemont la hubiera pedido así la amnistía no sería tan impopular\"                                                          \n#&gt; [22] \"9 de cada 10 españoles son como la cantudo odian planchar la otra es yolanda díaz\"                                               \n#&gt; [23] \"hablando de volcanes los afectados de la palma siguen intentando sacar adelante una ley nacional de volcanes\"                    \n#&gt; [24] \"has sido tú resume el discurso de la oposición desde que llegó pedro sánchez hasta hoy\"                                          \n#&gt; [25] \"danza invisible catalina cajón desastre 1990\"                                                                                    \n#&gt; [26] \"la última gran diva una manchega moderna abierta y rompedora en las antípodas de garcia page\"                                    \n#&gt; [27] \"este programa era como canal red con iglesias y montero siempre vigilantes\"                                                      \n#&gt; [28] \"los reyes del eurodisco alemán también triunfaron en españa y eso que aquí competían con el tigretón y la pantera rosa\"          \n#&gt; [29] \"recordáis la corona del roscón que llevaba bobby farrell pues aqui tenéis el abba\"                                               \n#&gt; [30] \"ha sido la canción más enviada entre pp y psoe estos años aunque la versión definitiva ha sido para irene montero\"               \n#&gt; [31] \"alvaro benito jugaba en el real madrid se lesionó y se dedicó a la música como julio iglesias pero no\"                           \n#&gt; [32] \"a compusieron en pandemia pensando que igual todo acababa no tendríamos que escucharla en bucle durante tres años\"               \n#&gt; [33] \"una mezcla entre un maniquí de una tienda de segunda mano y puigdemont después de sobrevivir a una pelea con junqueras\"          \n#&gt; [34] \"un homenaje a antonio gonzález el pescailla padre de la rumba catalana y de todos los hijos de lola flores\"                      \n#&gt; [35] \"la canción habla de la margen izquierda del nervión el rio en el que desembocan los cinco océanos\"                               \n#&gt; [36] \"un catalán puso de moda los campos de castilla y reivindicó que españa puede avanzar a golpes sin romperse\"                      \n#&gt; [37] \"julio iglesias la vida sigue igual fin de año 1969\"\n\n\nEscribimos en un fichero los subtítulos polémicos , por si hago lo que estoy pensando hacer .\n\n\nCode\nsubtitulos_proces %&gt;% \n    filter(n_fichero %in% subtitulos_polemicos) %&gt;% \n    arrange(n_fichero) %&gt;% \n    write_csv(file = \"~/proyecto_cachitos/2023_solo_polem.csv\")\n\n\nY podemos ver los fotogramas.\n\n\nCode\n# identificamos nombre del archivo jpg con los rótulos polémicos\npolemica_1_fotogramas &lt;- unique(substr(subtitulos_polemicos, 1,9))\n\nhead(polemica_1_fotogramas)\n#&gt; [1] \"00000035.\" \"00000059.\" \"00000091.\" \"00000114.\" \"00000121.\" \"00000222.\"\n\n# creamos la ruta completa donde están\npolemica_1_fotogramas_full &lt;- paste0(str_glue(\"{root_directory}video/{anno}_jpg/\"), polemica_1_fotogramas, \"jpg\")\n\n\n# añadimos sufijo subtitulo.tif para tenr localizado la imagen que tiene solo los rótulos\nsubtitulos_polemicos_1_full &lt;- paste0(polemica_1_fotogramas_full,\".subtitulo.tif\")\n\n\nCon la función image_read del paquete magick leemos las imágenes polémicas y los rótulos\n\n\nCode\nfotogramas_polemicos_img &lt;- map(polemica_1_fotogramas_full, image_read)\nsubtitulos_polemicos_img &lt;- map(subtitulos_polemicos_1_full, image_read)\n\n\n\n\nCode\n\nsubtitulos_polemicos_img[[24]]\n\n\n\n\n\n\n\n\n\n\n\nCode\nfotogramas_polemicos_img[[24]]\n\n\n\n\n\n\n\n\n\nPodemos ver una muestra de algunos de ellos.\nNo es perfecto, esto requiere más trabajo pero por el momento nos podria valer. Vemos una muestra de los rótulos polémicos\n\n\nCode\nset.seed(42)\nindices &lt;- sort(sample(1:length(fotogramas_polemicos_img), 9))\n\nlista_fotogram_polemicos &lt;- lapply(fotogramas_polemicos_img[indices], grid::rasterGrob)\ngridExtra::grid.arrange(grobs=lista_fotogram_polemicos )\n\n\n\n\n\n\n\n\n\nY el recorte de los subtítulos que hicimos enla primera entrega.\n\n\nCode\nlista_subtitulos &lt;-  lapply(subtitulos_polemicos_img[indices], grid::rasterGrob)\ngridExtra::grid.arrange(grobs=lista_subtitulos)"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#tópicos",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#tópicos",
    "title": "Cachitos 2023. Tercera parte",
    "section": "Tópicos",
    "text": "Tópicos\nAquí no me refiero a los tópicos de este país nuestro, sino a identificar si hay temas comunes a varios documentos.\nYa aviso que con tan pocos “documentos”, y siendo tan cortos cada rótulo, es muy probable que no salga mucho..\n\nTópicos usando conteo de palabras.\nContamos palabras con 3 caracteres o más.\nGuardamos la variable name que nos indica en qué rótulo ha aparecido\n\n\nCode\n\nword_counts &lt;- subtitulos_proces_one_word %&gt;% \n    group_by(name, word) %&gt;% \n    count(sort=TRUE) %&gt;% \n    mutate(ncharacters = nchar(word)) %&gt;% \n    filter(\n        ncharacters &gt;= 3) %&gt;% \n    select(-ncharacters) %&gt;% \n    ungroup()\n\n\nlength(unique(word_counts$name))\n#&gt; [1] 583\n\nhead(word_counts, 15)\n#&gt; # A tibble: 15 × 3\n#&gt;     name word            n\n#&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n#&gt;  1    80 paco            3\n#&gt;  2     4 pues            2\n#&gt;  3    11 estopa          2\n#&gt;  4    24 quién           2\n#&gt;  5    59 rollo           2\n#&gt;  6   100 mujer           2\n#&gt;  7   106 pena            2\n#&gt;  8   109 julio           2\n#&gt;  9   111 himno           2\n#&gt; 10   156 dolor           2\n#&gt; 11   257 ley             2\n#&gt; 12   307 aitana          2\n#&gt; 13   322 chapa           2\n#&gt; 14   363 milenarismo     2\n#&gt; 15   396 olé             2\n\n\nAhora convertimos este data.frame a un DocumentTermMatrix\n\n\nCode\n# usamos como peso la TermFrequency de la palabra\nrotulos_dtm &lt;- word_counts %&gt;%\n    cast_dtm(name, word, n, weighting = tm::weightTf)\n\n\nrotulos_dtm\n#&gt; &lt;&lt;DocumentTermMatrix (documents: 583, terms: 2662)&gt;&gt;\n#&gt; Non-/sparse entries: 3780/1548166\n#&gt; Sparsity           : 100%\n#&gt; Maximal term length: 18\n#&gt; Weighting          : term frequency (tf)\n\n\nPodríamos haberlo visto en forma de filas = palabras y columnas = rótulo\n\n\nCode\nword_counts  %&gt;%\n    cast_dfm(word, name, n)\n#&gt; Document-feature matrix of: 2,662 documents, 583 features (99.76% sparse) and 0 docvars.\n#&gt;         features\n#&gt; docs     80 4 11 24 59 100 106 109 111 156\n#&gt;   paco    3 0  0  0  0   0   0   0   0   0\n#&gt;   pues    0 2  0  0  0   0   0   0   0   0\n#&gt;   estopa  0 0  2  0  0   0   0   0   0   0\n#&gt;   quién   0 0  0  2  0   0   0   0   0   0\n#&gt;   rollo   0 0  0  0  2   0   0   0   0   0\n#&gt;   mujer   0 0  0  0  1   2   0   0   0   0\n#&gt; [ reached max_ndoc ... 2,656 more documents, reached max_nfeat ... 573 more features ]\n\n\nVamos a ver si sale algo haciendo un LDA (Latent Dirichlet Allocation)\nConsidero 20 tópicos porque sí. El que quiera elegir con algo más de criterio que se mire esto\n\n\nCode\n\n# Cons\n\nrotulos_lda &lt;- LDA(rotulos_dtm, k = 20, control = list(seed = 1234))\nrotulos_lda\n#&gt; A LDA_VEM topic model with 20 topics.\n\nrotulos_lda_td &lt;- tidy(rotulos_lda)\nrotulos_lda_td\n#&gt; # A tibble: 53,240 × 3\n#&gt;    topic term       beta\n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n#&gt;  1     1 paco  7.59e-224\n#&gt;  2     2 paco  1.29e-223\n#&gt;  3     3 paco  8.20e-224\n#&gt;  4     4 paco  8.90e-224\n#&gt;  5     5 paco  9.20e-224\n#&gt;  6     6 paco  6.36e-224\n#&gt;  7     7 paco  8.77e-224\n#&gt;  8     8 paco  8.41e-224\n#&gt;  9     9 paco  6.13e-224\n#&gt; 10    10 paco  6.17e-  3\n#&gt; # ℹ 53,230 more rows\n\n# se suele ordenar por beta que ahora mismo no recuerdo que era, \n\ntop_terms &lt;- rotulos_lda_td %&gt;%\n    group_by(topic) %&gt;%\n    top_n(3, beta) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -beta)\n\ntop_terms\n#&gt; # A tibble: 136 × 3\n#&gt;    topic term     beta\n#&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt;  1     1 1975   0.0305\n#&gt;  2     1 habla  0.0203\n#&gt;  3     1 himno  0.0152\n#&gt;  4     1 verdad 0.0152\n#&gt;  5     1 sabe   0.0152\n#&gt;  6     2 you    0.0232\n#&gt;  7     2 bravo  0.0174\n#&gt;  8     2 día    0.0116\n#&gt;  9     2 habla  0.0116\n#&gt; 10     2 puerta 0.0116\n#&gt; # ℹ 126 more rows\n\n\ntop_terms %&gt;%\n    mutate(term = reorder_within(term, beta, topic)) %&gt;%\n    ggplot(aes(term, beta)) +\n    geom_bar(stat = \"identity\") +\n    scale_x_reordered() +\n    facet_wrap(~ topic, scales = \"free_x\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nPues la verdad es que yo no veo nada interesante\n\n\nTópicos usando tfidf como peso\nVamos a probar usando tfidf\nComo la función LDA no permite usar un DocumentTermMatrix que se haya construido con cast_dtm y usando como parámetro de weighting el peso tm::weightTfIdf nos construimos los datos de otra forma.\n\n\nCode\n\ntf_idf_data &lt;- subtitulos_proces_one_word %&gt;% \n    filter(nchar(word)&gt;2) %&gt;% \n    group_by(name,word) %&gt;% \n    summarise(veces_palabra = n()) %&gt;% \n    bind_tf_idf(word, name, veces_palabra) %&gt;% \n    ungroup()\n\ntf_idf_data %&gt;% \n    arrange(desc(veces_palabra)) %&gt;%\n    head()\n#&gt; # A tibble: 6 × 6\n#&gt;    name word   veces_palabra    tf   idf tf_idf\n#&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1    80 paco               3 0.375  5.27   1.98\n#&gt; 2     4 pues               2 0.222  4.98   1.11\n#&gt; 3    11 estopa             2 0.333  6.37   2.12\n#&gt; 4    24 quién              2 0.222  4.58   1.02\n#&gt; 5    59 rollo              2 0.222  5.27   1.17\n#&gt; 6   100 mujer              2 0.286  4.98   1.42\n\n\nPara cada palabra tenemos su tf_idf dentro de cada rótulo en el que aparece\n\n\nCode\ntf_idf_data %&gt;% \n    filter(word== \"amnistía\")\n#&gt; # A tibble: 3 × 6\n#&gt;    name word     veces_palabra    tf   idf tf_idf\n#&gt;   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1    13 amnistía             1 0.111  5.27  0.586\n#&gt; 2   175 amnistía             1 0.143  5.27  0.753\n#&gt; 3   334 amnistía             1 0.25   5.27  1.32\n\n\nComo de nuevo LDA solo acepta peso con valores enteros, pues simplemente multiplicamos por 100 el tf_idf y redondeamos\n\n\nCode\ndtm_long &lt;- tf_idf_data %&gt;% \n    mutate(tf_idf_integer = round(100*tf_idf)) %&gt;% \n    cast_dfm(name, word, tf_idf_integer)\n\n\n\n\nCode\nlda_model_long_1 &lt;- LDA(dtm_long, k = 20, control = list(seed = 1234))\n\n\n\n\nCode\nresult &lt;- tidy(lda_model_long_1, 'beta')\n\nresult %&gt;%\n    group_by(topic) %&gt;%\n    top_n(5, beta) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -beta) %&gt;% \n    mutate(term = reorder(term, beta)) %&gt;%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\", ncol = 4) +\n    coord_flip()\n\n\n\n\n\n\n\n\n\nY claramente , yo sigo sin ver nada claro. Aunque me daría pistas para añadir más palabras a las stopwords y para aceptar que para el tamaño de los documentos (unas pocas palabras por rótulo), quizá no valga el LDA.\n\nEsta es la vida del analista de datos, prueba y error y sólo de vez en cuándo algún éxito.\n\n\n\nSólo con los rótulos polémicos\nAsumiendo que parece que no tiene sentido hacer topicmodelling sobre estos datos, me picó la curiosidad de ver qué pasaba si sólo usaba los rótulos polémicos.\n\n\nCode\n\ntf_idf_data_polem &lt;- subtitulos_proces_one_word %&gt;% \n    filter(nchar(word)&gt;2, polemica == TRUE) %&gt;% \n    group_by(name,word) %&gt;% \n    summarise(veces_palabra = n()) %&gt;% \n    bind_tf_idf(word, name, veces_palabra) %&gt;% \n    ungroup() \n\n\ntf_idf_data_polem %&gt;% \n    arrange(desc(veces_palabra)) %&gt;%\n    head()\n#&gt; # A tibble: 6 × 6\n#&gt;    name word        veces_palabra    tf   idf tf_idf\n#&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   257 ley                     2 0.667  2.89   1.93\n#&gt; 2    13 amnistía                1 1      2.48   2.48\n#&gt; 3    24 izquierda               1 1      2.20   2.20\n#&gt; 4    41 izquierda               1 1      2.20   2.20\n#&gt; 5    53 izquierda               1 1      2.20   2.20\n#&gt; 6    57 extremadura             1 1      3.58   3.58\n\n\n\nTopic modelling usando conteo de palabras\n\n\nCode\ndtm_long_polem &lt;- tf_idf_data_polem %&gt;% \n    # filter(tf_idf &gt; 0.00006) %&gt;% \n    # filter(veces_palabra&gt;1) %&gt;%\n    cast_dtm(name, word, veces_palabra)\n\nlda_model_long_polem &lt;- LDA(dtm_long_polem, k = 5, control = list(seed = 1234))\n\nresult_polem &lt;- tidy(lda_model_long_polem, 'beta')\n\n\n\n\nCode\nresult_polem %&gt;%\n    group_by(topic) %&gt;%\n    top_n(5, beta) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -beta) %&gt;% \n    mutate(term = reorder(term, beta)) %&gt;%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\", ncol = 4) +\n    coord_flip()\n\n\n\n\n\n\n\n\n\nY bueno parece que en el tópico 1 vemos palabras como amnistía, puigdemont, rey o sánchez. O en el 2 iglesias, amnistía, catalana.\nSi vemos en qué tópico cae cada documento.\n\n\nCode\nresult_documento_polem &lt;-  tidy(lda_model_long_polem, 'gamma')\n\n\nresult_documento_polem %&gt;%\n    group_by(topic) %&gt;%\n    top_n(7, gamma) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -gamma) %&gt;% \n    mutate(document = reorder(document, gamma)) %&gt;%\n    ggplot(aes(document, gamma, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\", ncol = 4) +\n    coord_flip()\n\n\n\n\n\n\n\n\n\nVeamos algunos subtítulos del tópico 1\n\n\nCode\nsubtitulos_proces %&gt;% \n    filter(name %in% c(175, 385, 334, 598, 321, 13, 408)) %&gt;% \n    pull(texto)\n#&gt; [1] \"ste tema se escribió con cartas de mujeres con cáncer de mama si te ha tocado este año lo de la amnistía igual lo relativizas\"\n#&gt; [2] \"a henry como a los ultras de las protestas contra la amnistía le gusta estar cara al sol\"                                     \n#&gt; [3] \"lleva la camisa más abierta que las barreras de sánchez en una negociación de investidura\"                                    \n#&gt; [4] \"si puigdemont la hubiera pedido así la amnistía no sería tan impopular\"                                                       \n#&gt; [5] \"9 de cada 10 españoles son como la cantudo odian planchar la otra es yolanda díaz\"                                            \n#&gt; [6] \"has sido tú resume el discurso de la oposición desde que llegó pedro sánchez hasta hoy\"                                       \n#&gt; [7] \"una mezcla entre un maniquí de una tienda de segunda mano y puigdemont después de sobrevivir a una pelea con junqueras\"\n\n\nVeamos más documentos relacionados con este tópico\n\n\nCode\n\ntop_10_topic3 &lt;-  result_documento_polem %&gt;%\n    group_by(topic) %&gt;% \n    top_n(12, gamma) %&gt;% \n    filter(topic==1) %&gt;% \n    pull(document)\n\nsubtitulos_proces %&gt;% \n    filter(name %in% top_10_topic3) %&gt;% \n    pull(texto)\n#&gt;  [1] \"ste tema se escribió con cartas de mujeres con cáncer de mama si te ha tocado este año lo de la amnistía igual lo relativizas\"\n#&gt;  [2] \"a henry como a los ultras de las protestas contra la amnistía le gusta estar cara al sol\"                                     \n#&gt;  [3] \"ojalá todas las guerras que hemos vivido este año se llamaran juan luis\"                                                      \n#&gt;  [4] \"vicente fernández y massiel el rey palmarés 1976\"                                                                             \n#&gt;  [5] \"con todos los respetos a antonio josé esperábamos que el dúo de rufián fuera con junqueras\"                                   \n#&gt;  [6] \"el rey de la copla callejera vestía como un agente de tecnocaso y cobía por la puerta chica del imaginarium\"                  \n#&gt;  [7] \"lleva la camisa más abierta que las barreras de sánchez en una negociación de investidura\"                                    \n#&gt;  [8] \"si puigdemont la hubiera pedido así la amnistía no sería tan impopular\"                                                       \n#&gt;  [9] \"9 de cada 10 españoles son como la cantudo odian planchar la otra es yolanda díaz\"                                            \n#&gt; [10] \"has sido tú resume el discurso de la oposición desde que llegó pedro sánchez hasta hoy\"                                       \n#&gt; [11] \"recordáis la corona del roscón que llevaba bobby farrell pues aqui tenéis el abba\"                                            \n#&gt; [12] \"una mezcla entre un maniquí de una tienda de segunda mano y puigdemont después de sobrevivir a una pelea con junqueras\"\n\n\nY bueno si que parece que ha agrupado algunos rótulos relacionados con la amnistía\n\n\nTopic modelling usando tf_idf\n\n\nCode\ndtm_long_polem_tf_idf &lt;- tf_idf_data_polem %&gt;%\n    mutate(tf_idf_integer = round(100 * tf_idf)) %&gt;%\n    cast_dfm(name, word, tf_idf_integer)\n\nlda_model_long_polem_tf_idf &lt;- LDA(dtm_long_polem_tf_idf, k = 5, control = list(seed = 1234))\n\nresult_polem_tf_idf &lt;- tidy(lda_model_long_polem_tf_idf, 'beta')\n\n\n\n\nCode\nresult_polem_tf_idf %&gt;%\n    group_by(topic) %&gt;%\n    top_n(5, beta) %&gt;%\n    ungroup() %&gt;%\n    arrange(topic, -beta) %&gt;% \n    mutate(term = reorder(term, beta)) %&gt;%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\", ncol = 4) +\n    coord_flip()\n\n\n\n\n\n\n\n\n\nY bueno más o menos igual.\n\n\n\nOtras pruebas realizadas\n\nEn vez de considerar que cada rótulo es un documento, consideré los rótulos correspondientes a la primera parte del programa, a la segunda, y así hasta 10. pero no se obtuvo nada consistente\n\nY bueno aquí acaba el análisis del cachitos de este año, salvo que alguien tenga interés en que haga alguna prueba o mejore algo.\n\n\nIA’s\nProbando que me decía ChatGPT 3.5, (no tengo versión de pago). le he pasado la mitad de los rótulos de este año y le digo que me saque los 7 puntos más importantes y me devuelve esto.\nAquí están los 7 puntos más importantes del texto proporcionado:\n\nVariedad de Temas y Artistas: El texto aborda una amplia variedad de temas y artistas de la música, tocando géneros y décadas diferentes.\nAnécdotas y Curiosidades: Se presentan anécdotas y curiosidades sobre algunos artistas, como detalles sobre sus canciones, momentos destacados de sus carreras y datos interesantes.\nComentarios Irónicos y Humor: El texto utiliza un tono irónico y humorístico para comentar sobre diversos aspectos de la música y los artistas, proporcionando un enfoque entretenido.\nReferencias Históricas y Culturales: Se realizan referencias a eventos históricos, políticos y culturales, situando a los artistas y sus obras en un contexto más amplio.\nComentarios Socio-políticos: Se incluyen comentarios que tocan temas sociales y políticos, como la referencia a la amnistía y menciones a personajes políticos.\nObservaciones sobre la Industria Musical: El texto realiza observaciones sobre la industria musical, como la evolución de los formatos de música, la viralidad en las redes sociales y la temática de algunas canciones.\nDiversidad de Géneros Musicales: Se abordan diversos géneros musicales, desde el pop y la música urbana hasta el rock y el flamenco, mostrando la diversidad en la escena musical española.\n\nLuego he probado lo mismo usando Chat Arena que lo acabo de leer en el blog de Carlos Gil Bellosta y le he podido pasar todos los rótulos y el resultado de ChatGpt 4 y demás, me ha decepcionado bastante. Os animo a probarlo\nSi hago una próxima entrada exploraré a ver si puedo conectarme a algún modelo de huggin face y ver qué tal lo hace."
  },
  {
    "objectID": "2024/01/cachitos_2023/test.html",
    "href": "2024/01/cachitos_2023/test.html",
    "title": "test",
    "section": "",
    "text": "Code\nimport plotly.express as px\ndf = px.data.iris()\ndf[\"e\"] = df[\"sepal_width\"]/100\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\",\n                 error_x=\"e\", error_y=\"e\")\nfig.show()"
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_cuarta_parte.html",
    "href": "2024/01/cachitos_2023/cachitos_2023_cuarta_parte.html",
    "title": "Cachitos 2024. Cuarta parte. The python way",
    "section": "",
    "text": "Vamos ya con la última entrada del cachitos de este año. Están de moda los LLM’s y demás, por el momento no estoy pagando ChatGpt ni ningún otro, quizá lo haga en el futuro para ir probando cosas. Lo que si puedo hacer es utilizar modelos libres.\n\nTransformers\nVoy a probar a hacer topic modelling usando BERTopic en python. BERTopic que utiliza cosas como reducción de dimensionalidad con embeddings, hdbscan y los conocidos transformers\nPara eso, he instalado lo necesario en un entorno de miniconda. El script en python es este\n\n\nShow the code\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom spacy.lang.es.stop_words import STOP_WORDS\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n\n\n\n\nShow the code\n## rotulos \nrotulos = pd.read_csv(\"/home/jose/proyecto_cachitos/2023_txt_unido.csv\")\npolemicos = pd.read_csv(\"/home/jose/proyecto_cachitos/2023_solo_polem.csv\")\n\n\nCon BERT se hace embedding de los documentos,luego reducción de la dimensionalidad, clustering utilizando las dimensiones obtenidas y por último un modelo de topic modeling usando LDA con métrica c-tfidf.\nPara este ejemplo concreto todo esto es lo que se suele llamar sobreingeinería pero puede ser útil en análisis de texto mucho más largos y complejos.\nAjustamos el modelo con BERT, dónde por un lado quitamos stopwords y elegimos un modelo de embedding multilingüe .\n\n\nShow the code\n\nlista_stop = list(STOP_WORDS) \n\nvectorizer_model = CountVectorizer(stop_words= lista_stop)\n\ntopic_model = BERTopic(verbose=True, \n    vectorizer_model = vectorizer_model, \n    embedding_model='paraphrase-multilingual-MiniLM-L12-v2',\n    min_topic_size= 5)\n\n## ajustr modelo con el texto de los rotulos\ntopicos, _ = topic_model.fit_transform(rotulos.texto)\n#&gt; 2024-03-02 10:39:17,274 - BERTopic - Embedding - Transforming documents to embeddings.\n#&gt; \nBatches:   0%|          | 0/19 [00:00&lt;?, ?it/s]\nBatches:   5%|5         | 1/19 [00:04&lt;01:27,  4.87s/it]\nBatches:  11%|#         | 2/19 [00:09&lt;01:18,  4.61s/it]\nBatches:  16%|#5        | 3/19 [00:13&lt;01:09,  4.36s/it]\nBatches:  21%|##1       | 4/19 [00:17&lt;01:05,  4.40s/it]\nBatches:  26%|##6       | 5/19 [00:22&lt;01:03,  4.52s/it]\nBatches:  32%|###1      | 6/19 [00:26&lt;00:56,  4.31s/it]\nBatches:  37%|###6      | 7/19 [00:30&lt;00:50,  4.24s/it]\nBatches:  42%|####2     | 8/19 [00:35&lt;00:47,  4.34s/it]\nBatches:  47%|####7     | 9/19 [00:38&lt;00:40,  4.06s/it]\nBatches:  53%|#####2    | 10/19 [00:43&lt;00:38,  4.23s/it]\nBatches:  58%|#####7    | 11/19 [00:47&lt;00:34,  4.28s/it]\nBatches:  63%|######3   | 12/19 [00:50&lt;00:26,  3.84s/it]\nBatches:  68%|######8   | 13/19 [00:53&lt;00:22,  3.73s/it]\nBatches:  74%|#######3  | 14/19 [00:59&lt;00:21,  4.34s/it]\nBatches:  79%|#######8  | 15/19 [01:05&lt;00:19,  4.82s/it]\nBatches:  84%|########4 | 16/19 [01:09&lt;00:13,  4.65s/it]\nBatches:  89%|########9 | 17/19 [01:13&lt;00:08,  4.41s/it]\nBatches:  95%|#########4| 18/19 [01:17&lt;00:04,  4.28s/it]\nBatches: 100%|##########| 19/19 [01:20&lt;00:00,  3.83s/it]\nBatches: 100%|##########| 19/19 [01:20&lt;00:00,  4.23s/it]\n#&gt; 2024-03-02 10:40:41,053 - BERTopic - Embedding - Completed ✓\n#&gt; 2024-03-02 10:40:41,053 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n#&gt; 2024-03-02 10:41:08,076 - BERTopic - Dimensionality - Completed ✓\n#&gt; 2024-03-02 10:41:08,076 - BERTopic - Cluster - Start clustering the reduced embeddings\n#&gt; 2024-03-02 10:41:08,131 - BERTopic - Cluster - Completed ✓\n#&gt; 2024-03-02 10:41:08,137 - BERTopic - Representation - Extracting topics from clusters using representation models.\n#&gt; 2024-03-02 10:41:08,222 - BERTopic - Representation - Completed ✓\n\n\nY no consigo ver este plotly correctamente. Otros plotly desde python si me deja.\n\n\nShow the code\n\nimport plotly.express as px\nfig = topic_model.visualize_barchart(top_n_topics=20)\n##fig.show() # el show no me funciona\n\n\n\n\n\ntopic1\n\n\nAhora solo con los rótulos polémicos, los cuales identificamos en el post anterior.\n\n\nShow the code\n## solo polémicos\n\n\nmodel_polem =  BERTopic(verbose=True, \n    vectorizer_model = vectorizer_model,\n    embedding_model='paraphrase-multilingual-MiniLM-L12-v2',\n    min_topic_size= 2)\n\ntopicos_polem, _ = model_polem.fit_transform(polemicos.texto)\n#&gt; 2024-03-02 10:41:11,263 - BERTopic - Embedding - Transforming documents to embeddings.\n#&gt; \nBatches:   0%|          | 0/2 [00:00&lt;?, ?it/s]\nBatches:  50%|#####     | 1/2 [00:08&lt;00:08,  8.68s/it]\nBatches: 100%|##########| 2/2 [00:16&lt;00:00,  8.37s/it]\nBatches: 100%|##########| 2/2 [00:16&lt;00:00,  8.42s/it]\n#&gt; 2024-03-02 10:41:32,972 - BERTopic - Embedding - Completed ✓\n#&gt; 2024-03-02 10:41:32,972 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n#&gt; 2024-03-02 10:41:48,585 - BERTopic - Dimensionality - Completed ✓\n#&gt; 2024-03-02 10:41:48,585 - BERTopic - Cluster - Start clustering the reduced embeddings\n#&gt; 2024-03-02 10:41:48,599 - BERTopic - Cluster - Completed ✓\n#&gt; 2024-03-02 10:41:48,604 - BERTopic - Representation - Extracting topics from clusters using representation models.\n#&gt; 2024-03-02 10:41:48,628 - BERTopic - Representation - Completed ✓\n\n\nfreq = model_polem.get_topic_info()\nprint(\"Number of topics: {}\".format( len(freq)))\n#&gt; Number of topics: 5\nfreq.head()\n#&gt;    Topic  Count                                       Name  \\\n#&gt; 0     -1      1           -1_red_programa_vigilantes_canal   \n#&gt; 1      0     13                   0_amnistía_año_luis_juan   \n#&gt; 2      1     11               1_julio_iglesias_vida_españa   \n#&gt; 3      2      9  2_abierta_junqueras_imaginarium_recordáis   \n#&gt; 4      3      3          3_canción_desembocan_margen_irene   \n#&gt; \n#&gt;                                                                                           Representation  \\\n#&gt; 0                                          [red, programa, vigilantes, canal, montero, iglesias, , , , ]   \n#&gt; 1                    [amnistía, año, luis, juan, pandemia, resultado, guerra, ley, izquierda, propuesta]   \n#&gt; 2                [julio, iglesias, vida, españa, volcanes, pantera, vicente, viviendo, victor, planchar]   \n#&gt; 3  [abierta, junqueras, imaginarium, recordáis, roscón, respetos, sabéis, rompedora, sobrevivir, rufián]   \n#&gt; 4         [canción, desembocan, margen, irene, enviada, definitiva, currículum, abascal, nervión, niños]   \n#&gt; \n#&gt;                                                                                                                                                                                                                                                                                                                           Representative_Docs  \n#&gt; 0                                                                                                                                                                                                                                                                [este programa era como canal red con iglesias y montero siempre vigilantes]  \n#&gt; 1  [ojalá todas las guerras que hemos vivido este año se llamaran juan luis, ste tema se escribió con cartas de mujeres con cáncer de mama si te ha tocado este año lo de la amnistía igual lo relativizas, en este año de crispación política la propuesta no de ley sobre alud mental logró 340 votos aún así seguimos sin una ley naciona]  \n#&gt; 2                                                                                [julio iglesias la vida sigue igual fin de año 1969, alvaro benito jugaba en el real madrid se lesionó y se dedicó a la música como julio iglesias pero no, ana belén conoció a victor manuel al mismo tiempo que a julio iglesias ésta se la cantó a julio]  \n#&gt; 3         [una mezcla entre un maniquí de una tienda de segunda mano y puigdemont después de sobrevivir a una pelea con junqueras, el rey de la copla callejera vestía como un agente de tecnocaso y cobía por la puerta chica del imaginarium, la última gran diva una manchega moderna abierta y rompedora en las antípodas de garcia page]  \n#&gt; 4                             [los niños protagonistas de esta canción ya tienen mejor currículum que santiago abascal, la canción habla de la margen izquierda del nervión el rio en el que desembocan los cinco océanos, ha sido la canción más enviada entre pp y psoe estos años aunque la versión definitiva ha sido para irene montero]\n\n\n\n\nShow the code\n\nfig_polem = model_polem.visualize_barchart(top_n_topics=20)\n# fig_polem.show() no funciona incrusto el html con el plotly\n\n\n\n\n\ntopci2\n\n\nPara ver los rótulos del topic 0\n\n\nShow the code\ndf = pd.DataFrame({'topic': topicos_polem, 'document': polemicos.texto})\n\ndf.loc[df.topic == 0, 'document']\n#&gt; 0        ste tema se escribió con cartas de mujeres con cáncer de mama si te ha tocado este año lo de la amnistía igual lo relativizas\n#&gt; 2                                           el de la izquierda del todo ya no está en la coalición para algunos el chunguito verdadero\n#&gt; 3                                              la de la izquierda hizo lo que todo padre sueña al mirar su whatsapp abandonar el grupo\n#&gt; 6                                  la versión que habla de la relación de feijóo con el último resultado electoral se titula nosentera\n#&gt; 7                                             a henry como a los ultras de las protestas contra la amnistía le gusta estar cara al sol\n#&gt; 8                                                    juan luis guerra 4 40 visa para un sueño pero esto qué es esto es lo que hay 1990\n#&gt; 9                                                              ojalá todas las guerras que hemos vivido este año se llamaran juan luis\n#&gt; 12                        la pandemia creímos que formaban el dúo más estomagante gonzález y guerra han vuelto para recordarnos que no\n#&gt; 13                               el pp usó verano azul para su campaña el plan b era fariña o crematorio pero el resultado fue poco yo\n#&gt; 15    en este año de crispación política la propuesta no de ley sobre alud mental logró 340 votos aún así seguimos sin una ley naciona\n#&gt; 20                                                              si puigdemont la hubiera pedido así la amnistía no sería tan impopular\n#&gt; 23                                              has sido tú resume el discurso de la oposición desde que llegó pedro sánchez hasta hoy\n#&gt; 31                   a compusieron en pandemia pensando que igual todo acababa no tendríamos que escucharla en bucle durante tres años\n#&gt; Name: document, dtype: object\n\n\nY vemos que BERtopic es una buena herramienta para este tipo de cosas, y que seguramente en análisis de texto más complejos sea muy útil."
  },
  {
    "objectID": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#coda",
    "href": "2024/01/cachitos_2023/cachitos_2023_tercera_parte.html#coda",
    "title": "Cachitos 2023. Tercera parte",
    "section": "Coda",
    "text": "Coda\nEn esta serie de post he dejado reflejado como bajar el video, extraer fotogramas, recortar rótulos, hacer ocrs y un pequeño análisis de los rótulos. Queda para cada cual discernir si los rótulos de este año se meten más con el gobierno, la oposición , los nacionalistas o con otros personajes. Sea como fuere en la serie de post se dan herramientas para que el que quiera realice su propio análisis y se forme una opinión propia.\nSin más, os deseo un Feliz 2024 a todos y espero el año que viene seguir cumpliendo con la ya casi tradición del análisis de Nochevieja a Cachitos"
  },
  {
    "objectID": "2024/01/nvim_quarto.html",
    "href": "2024/01/nvim_quarto.html",
    "title": "Neovim",
    "section": "",
    "text": "Pues me ha dado por aprender cosas de vim, y en particular de Neovim. La verdad es que me está gustando bastante. Os dejo una serie de enlaces\n\nAprende VIM\nMi confi de Neovim\nBlog devlog\nThePrimeagen youtube\nTypecraft youtube\n\nY ahora estoy probando a ver qué tal se edita este post desde Neovim. \nTengo abierto el fichero qmd, a la derecha el repl de R, y debajo un terminal.\nHe configurado Neovim con plugins para tener R como repl Nvim-R autocompletado con cmp e instalado también plugins para LSP(language server protocol) para tener linters en R o en python, o Treesitter para tener coloreado de sintaxis según el lenguaje o y otros plugins diversos.\nTotal, que me está gustando bastante todo el tema de vim, pero la verdadera potencia está en las características propias de vim, como poder asignar atajos de teclado o todo el tema de poder cambiar lo de dentro de un text-object por ejemplo con ci( en modo normal cambias todo lo que hay dentro de un paréntesis que esté en la línea del cursor. O todo el tema de los registros de vim o de la macros. En realidad es todo un mundo.\nPero tal y como dice mi Chat Gpt sarcástico\n\n\n\nvim-vs-emacs\n\n\nPues nada, sigo aprendiendo Neovim y ya iré poniendo posts con curiosidades.\n\n\n\nvim-vs-emacs"
  },
  {
    "objectID": "2024/02/nvim_quarto.html",
    "href": "2024/02/nvim_quarto.html",
    "title": "Guarnición de guerra",
    "section": "",
    "text": "Se conoce como guarnición a las tropas que protegen y están destinadas en un palacio, un castillo, un fuerte, un buque de guerra o una zona o región, por ejemplo.\nEsta semana ha sido interesante en el curro. He vivido lo que en el argot se conoce como “war room”. Básicamente tiene que ver con que hay algo que ha fallado o se ha retrasado - típicamente en producción - y nadie tiene ni puta idea de por qué ha pasado.\nEs entonces cuando se convoca la “war room” , “gabinete de crisis” o “guarnición de guerra”. Consiste en montar reunión durante prácticamente todo el día y durante varios, y se convoca a los equipos implicados. A saber, gente que se engarga de las operaciones en producción, el equipo de desarrollo, e incluso gente de negocio si fuera necesario. Durante estas reuniones se trabaja codo con codo en intentar solucionar el problema y todo lo demás pasa a ser secundario.\nHa sido mi primera “war room” en la empresa dónde curro y a pesar de la situación, que era crítica, ha sido una buena experiencia. Por un lado se ha mandado a tomar por saco todas las tonterías del agile y demás historias, aquí se está para currar y solucionar el problema. La gente en ningún momento ha echado nada en cara al otro equipo , ya sabéis , en plan “tú código es una mierda” y blablalba. En estas war rooms se valora a la gente resolutiva, que ofrece soluciones, se escucha a todo el mundo, porque incluso aunque se digan cosas que a priori pueden ser una locura y"
  },
  {
    "objectID": "2024/02/guarnicion_de_guerra.html",
    "href": "2024/02/guarnicion_de_guerra.html",
    "title": "Guarnición de guerra",
    "section": "",
    "text": "Se conoce como guarnición a las tropas que protegen y están destinadas en un palacio, un castillo, un fuerte, un buque de guerra o una zona o región, por ejemplo.\nEsta semana ha sido interesante en el curro. He vivido lo que en el argot se conoce como “war room”. Básicamente tiene que ver con que hay algo que ha fallado o se ha retrasado - típicamente en producción - y nadie tiene ni puta idea de por qué ha pasado.\nEs entonces cuando se convoca la “war room” , “gabinete de crisis” o “guarnición de guerra”. Consiste en montar reunión durante prácticamente todo el día y durante varios días , y se convoca a los equipos implicados. A saber, gente que se encarga de las operaciones en producción, el equipo de desarrollo, e incluso gente de negocio si fuera necesario. Durante estas reuniones se trabaja codo con codo en intentar solucionar el problema y todo lo demás pasa a ser secundario.\nHa sido mi primera “war room” en la empresa dónde curro y a pesar de la situación, que era crítica, ha sido una buena experiencia. Por un lado se ha mandado a tomar por saco todas las tonterías del agile y demás historias, aquí se está para currar y solucionar el problema. La gente en ningún momento ha echado nada en cara al otro equipo , ya sabéis , en plan “tú código es una mierda” y blablalba. En estas war rooms se valora a la gente resolutiva, que ofrece soluciones, se escucha a todo el mundo, porque incluso aunque se digan cosas que a priori pueden ser una locura, puede que hagan pensar en alguien en la posible solución.\nY tengo que hablar de la gente, al menos en esta “guarnición de guerra” , la gente ha sido lo mejor. La gente de operaciones son muy profesionales, colaboradores, y tienen una paciencia infinita. La gente de mi equipo igual, unos cracks todos. He aprendido muchísimo y sobre todo valoro mucho más a la gente con la que trabajo.\nNo sé si daremos con la tecla del problema, y puede que no sea lo más importante, lo que si sé es que compartir trinchera forja alianzas.\nPor mi parte, puede que de esta “guarnición de guerra” me gane unas cuántas cervezas, pues para cada hipótesis que se me ocurría de dónde pudiera estar el problema ( y una de las hipótesis que defiendo es que ha sido todo fruto del azar) me apostaba un par de cervezas con alguien. ” Me juego 2 cervezas a que esto no falla más en lo que queda de mes” ( ya lleva varios días seguidos sin fallar, la actualización de mi posteriori me da pie a pensar que voy a ganar la apuesta) o cosas así.\nLa sensación es rara, por un lado ha habido problemas con cosas del proyecto y aún no tenemos claro por qué. Por otro lado, el compartir trinchera y trabajar todos a una en intentar solucionarlo me ha gustado mucho, y ha hecho que valore aún más a la gente de operaciones. Gracias a todos y feliz fin de semana.\nSeguimos el lunes"
  },
  {
    "objectID": "2024/02/nomenclatura.html",
    "href": "2024/02/nomenclatura.html",
    "title": "Nomenclatura",
    "section": "",
    "text": "A mis cuarenta y pocos tacos, como decía Sabina. El título de la entrada tiene que ver con algo que me llamó la atención de cuando empecé a currar en cosas de estas del Big Data. Se trata del cambio de denominación de ciertas cosas.\nAlgunos ejemplos.\n\nLlamar modelo multiclase a lo que siempre llamé modelo multinomial.\nModelo multioutput a lo que siempre llamé modelo multivariante (más de una variable dependiente)\nAnálisis supervisado / no supervisado a lo que se conocía como modelos de dependencia y modelos de interdependencia.\nInstancia a lo que siempre se ha denominado observación\nRegresor o clasificador a lo que llamaba modelos de regresión o modelos de respuesta discreta\n\nSin hablar de que la denominación del oficio ha pasado desde “el de los datos que está en el sótano”, hasta “experto en IA, ML , fullstack DS” pasando por los más normales de “Analista de datos”, “Minero de datos” o similar.\nYo simplemente me considero un estadístico que sabe algo de estadística y algo de programación y tecnología.\nNo sé si hablaremos de este tema en próximos episodios del podcast que tenemos Leo y yo.\nFeliz día."
  },
  {
    "objectID": "2024/03/jornadas2024.html",
    "href": "2024/03/jornadas2024.html",
    "title": "Jornadas Hispalenses",
    "section": "",
    "text": "R tiene un color especial\nCada año desde 2009, nos juntamos los entusisastas de R en una ciudad de España para compartir conocimiento, aventuras, desventuras, cervezas y buen ambiente.\nEste año toca en Sevilla, una de mis ciudades favoritas y en la cual viví durante un par de años. Nos juntaremos los días 6, 7 y 8 de Noviembre y prometen ser unas jornadas espectaculares. La gente del grupo de R de Sevilla se lo está currando mucho.\nYa tienen confirmados a 3 conferenciantes para charlas plenarias.\n\nJakub Nowosad Especialista en geografía computacional. Echad un vistazo al enlace. merece la pena.\nRosana Ferreo. Gran divulgadora y docente de todo el R mundo.\nHannah Frick . Ingeniera de software y estadística. Actualmente trabaja en Posit (Rstudio)\n\nYo por mi parte, como vicepresidente de la Comunidad R-Hispano y como asistente a todas las jornadas de R (excepto a las míticas en Mieres), prometo que me lo voy a pasar estupendamente, que voy a aprender un montón y que voy a compartir lo poco o mucho que sé.\nAsí que animaros, inscribiros y mandad comunicaciones. Yo me he metido en un “embolao” y he mandado ya una propuesta de taller.\nPor el momento he puesto esto como topics del taller\n\n\n\ntaller\n\n\nPero seguramente cambie los topics y el orden.\n¡Nos vemos en Sevilla!"
  },
  {
    "objectID": "2024/03/lujuria_e_intervencion.html",
    "href": "2024/03/lujuria_e_intervencion.html",
    "title": "Lujuria e intervención",
    "section": "",
    "text": "Listening"
  },
  {
    "objectID": "2024/03/lujuria_e_intervencion.html#introducción",
    "href": "2024/03/lujuria_e_intervencion.html#introducción",
    "title": "Lujuria e intervención",
    "section": "Introducción",
    "text": "Introducción\nCuenta Richard McElreath en sus vídeos de Statistical Rethinking que la inferencia causal no es más que predecir la intervención. Una de las cosas que más me llamó la atención es lo que él llama “full luxury bayes”. El “full luxury” permite ajustar todo el DAG de un modelo causal, permitiéndonos cosas que los seguidores de Pearl dicen que no se puede hacer, cosas como condicionar por un collider o cosas así.\nUna vez tenemos el DAG entero ajustado conjuntamente, podemos hacer cosas como simular la intervención. Esto no es más que - oye, ¿qué hubiera pasado si todo los individuos hubieran recibido el tratamiento?, ¿y si todos hubieran estado en control?- y de esta forma podemos estimar lo que queremos, que unos lo llaman el efecto causal, o ATE (average treatmen effect) y cosas así."
  },
  {
    "objectID": "2024/03/lujuria_e_intervencion.html#ejemplo",
    "href": "2024/03/lujuria_e_intervencion.html#ejemplo",
    "title": "Lujuria e intervención",
    "section": "Ejemplo",
    "text": "Ejemplo"
  },
  {
    "objectID": "2024/03/lujuria_e_intervencion.html#ejemplo-simulado",
    "href": "2024/03/lujuria_e_intervencion.html#ejemplo-simulado",
    "title": "Lujuria e intervención",
    "section": "Ejemplo simulado",
    "text": "Ejemplo simulado\nSupongamos que tenemos un DAG. Y que el DAG es correcto. Esto que acabo de escribir, de que el DAG es correcto es la principal asunción de toda la inferencia causal. No hay inferencia causal sin una descripción explícita de tu modelo causal (Ciencia antes que estadística). Las técnicas de inferencia causal son sólo herramientas técnicas que nos ayudarán a estimar el efecto. Pero si nuestras asunciones son incorrectas, no hay técnica que nos salve.\n\nDAG\n\n\nShow the code\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\negypt &lt;- MetBrewer::met.brewer(\"Egypt\")\n\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Archivo Narrow\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", size = rel(0.8), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\ntheme_set(\n  theme_nice()\n)\n\n\n\n\nShow the code\n\ndag_coords &lt;-\n  tibble(name = c(\"S\",  \"M\", \"D\"),\n         x = c(0,  1,  2),\n         y = c(0,  0.2,  0))\n\ndag_simple &lt;- dagify(\n       M ~ S,\n       D ~ M,\n       coords = dag_coords\n       )\ndag_simple %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\", size = 10) +\n  geom_dag_point(data = . %&gt;%  filter(name %in% c(\"U\")),\n               shape = 1, stroke = 2, color = \"black\") +\n  geom_dag_edges(edge_color = \"black\", edge_width = 2,\n                 arrow_directed = grid::arrow(length = grid::unit(15, \"pt\"),\n                                              type = \"closed\")) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nSimulamos los datos.\nVamos a simular los datos, de forma que sabremos cuál es el verdadero efecto causal.\n\n\nShow the code\n\nS &lt;- rbinom(100,1, 0.3)\n\nM &lt;- 2 * S + rnorm(100, 0, 1)\n\nD &lt;-  5 + 5 * M + rnorm(100, 0, 1)\n\n\nSabemos que el efecto total de S sobre D es de 10. Por ser M una variable mediadora, y tal como hemos generado los datos se multiplican los coeficientes.\n\n\nUsando dagitty\ndagitty nos permite saber por qué variabbles hemos de condicionar, según los criterios de Pearl, para obtener diferentes efectos.\n\n\nShow the code\n\ndagitty::adjustmentSets(dag_simple, exposure = \"S\", outcome = \"D\")\n#&gt;  {}\n\n\nY vemos que no hay que “controlar” por ninguna variable para obtener el efecto de S sobre D\n\n\nShow the code\ndf &lt;- data.frame(S = S , D = D, M = M)\n\nmod_simple_correcto &lt;- lm(D ~ S, data = df)\nsummary(mod_simple_correcto)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = D ~ S, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.7691  -3.8648  -0.5862   3.6303  14.5658 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   4.6562     0.7031   6.622 1.91e-09 ***\n#&gt; S            11.4955     1.1559   9.945  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.581 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.5023, Adjusted R-squared:  0.4972 \n#&gt; F-statistic:  98.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nY obtenemos el coeficiente correcto."
  },
  {
    "objectID": "2024/03/lujuria_e_intervencion.html#full-luxury-bayes",
    "href": "2024/03/lujuria_e_intervencion.html#full-luxury-bayes",
    "title": "Lujuria e intervención",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nEn modelos sencillos usar dagitty nos permite encontrar el mínimo conjunto de variables por las que controlar, para encontrar el efecto que buscamos. Pero pueden darse situaciones en las que sea necesario condicionar por una variable que en un “path” sea una variable de confusión, mientras que en otro sea un “collider”. En esos casos hay que recurrir al “front door criterio” y a veces ni aún así basta.\nPero tal y como empezaba el post. La inferencia causal no es más que predecir el efecto de la intervención. Y eso vamos a hacer.\nEn análisis bayesiano podemos ajustar el DAG entero y aún así estimar los efectos incluso condicionando por variable que los criterior de Pearl nos dicen que no se pueden. Esto lleva un coste computacional no despreciable si el DAG es complejo, de ahí lo de “luxury”.\nCon las librerías brms y cmdstanr es relativamente sencillo ajustar este tipo de modelos\n\n\nShow the code\n# library(cmdstanr)  # No me funciona cmdstanr al renderizar en quarto\n#set_cmdstan_path(path = \"~/.cmdstan/cmdstan-2.34.1/\")\nlibrary(brms)\nlibrary(ggdist) # pa pintar \nbf1 &lt;- bf(M  ~  S)\nbf2 &lt;- bf(D  ~  M )\nbf_full &lt;- bf1 + bf2 + set_rescor(rescor = FALSE)\n\nmod_full_luxury &lt;- brm(\n                       bf_full, \n                       chains = 4,\n                       cores = 4, \n                       iter = 2000, \n                       data = df) \n                       #backend = \"cmdstanr\")\n\n\nEn el summary del modelo vemos que este modelo ha recuperado los coeficientes correctos.\n\n\nShow the code\n\nsummary(mod_full_luxury)\n#&gt;  Family: MV(gaussian, gaussian) \n#&gt;   Links: mu = identity; sigma = identity\n#&gt;          mu = identity; sigma = identity \n#&gt; Formula: M ~ S \n#&gt;          D ~ M \n#&gt;    Data: df (Number of observations: 100) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; M_Intercept    -0.09      0.14    -0.35     0.18 1.00     6248     3076\n#&gt; D_Intercept     5.05      0.11     4.84     5.25 1.00     6264     2969\n#&gt; M_S             2.31      0.22     1.88     2.75 1.00     6005     3074\n#&gt; D_M             5.02      0.06     4.90     5.15 1.00     5702     2801\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma_M     1.09      0.08     0.95     1.26 1.00     5204     3058\n#&gt; sigma_D     0.97      0.07     0.85     1.11 1.00     5868     3185\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nUna vez tenemos esto , podemos usar las posterior de dos forma. Una sería multiplicando las posteriors del coeficiente de M y el de S, y otra haciendo una “intervención”\n\nMultiplicando posteriors\n\n\nShow the code\n\nposteriors  &lt;- as.data.frame(mod_full_luxury)\n\nhead(posteriors)\n#&gt;   b_M_Intercept b_D_Intercept    b_M_S    b_D_M  sigma_M   sigma_D Intercept_M\n#&gt; 1 -0.2078611456      5.080026 2.369867 4.952089 1.187319 0.9102176   0.6689897\n#&gt; 2 -0.0006802141      5.046711 2.296216 5.092021 1.054625 1.0270438   0.8489196\n#&gt; 3 -0.1377945778      5.036534 2.246501 5.007023 1.132313 0.9869086   0.6934108\n#&gt; 4 -0.0438541017      4.999751 2.367703 4.909060 1.145089 0.9267522   0.8321959\n#&gt; 5 -0.0092326616      4.999116 2.125023 5.206066 1.023490 1.0684353   0.7770260\n#&gt; 6 -0.2239945163      5.021916 2.550931 5.053909 1.017421 1.0031544   0.7198501\n#&gt;   Intercept_D    lprior      lp__\n#&gt; 1    8.884465 -8.663475 -295.7457\n#&gt; 2    8.958653 -8.639493 -294.9437\n#&gt; 3    8.883175 -8.652335 -294.1798\n#&gt; 4    8.771132 -8.654670 -297.0658\n#&gt; 5    8.998672 -8.632764 -299.1002\n#&gt; 6    8.904578 -8.628174 -294.8313\n\n\n\n\nShow the code\n\nefecto_global_S  &lt;- posteriors$b_M_S * posteriors$b_D_M\n\nquantile(efecto_global_S, c(0.025, 0.5, 0.975))\n#&gt;      2.5%       50%     97.5% \n#&gt;  9.458785 11.638945 13.835363\n\n\nPintando la posterior\n\n\nShow the code\n\nefecto_global_S  %&gt;% \n  enframe()  %&gt;%\n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = expression(beta[S1]), y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\nSimulando la intervención\nUna vez tenemos el Dag estimado, podemos recorrerlo haciendo una intervención. Esto no es más que ver como sería el efecto cuando S= 0 y cuando S=1\nEl “truco” es que hay que recorrer el DAG y obteniendo las posteriors tras hacer la intervención.\nPor ejemplo, la posterior del coeficiente de M no vale la que ha sacado el modelo, sino que hay que calcularla utilizando el primer modelo el de M ~ S, pero poniendo que S = 0, y usar esa posterior obtenida en el modelo D ~ M.\n\n\nShow the code\n\n# S == 0\n\nM_post0  &lt;- with(posteriors,  b_M_Intercept + b_M_S * 0 )\nD_post0  &lt;- with(posteriors,  b_D_Intercept + b_D_M * M_post0 )\n\n# S == 1\n\nM_post1  &lt;- with(posteriors,  b_M_Intercept + b_M_S * 1 )\nD_post1  &lt;- with(posteriors,  b_D_Intercept + b_D_M * M_post1 )\n\n\nefecto_global_S_intervencion &lt;- D_post1 - D_post0\n\nefecto_global_S_intervencion  %&gt;% \n  enframe()  %&gt;%\n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = expression(beta[S1]), y = \"Density\")\n\n\n\n\n\n\n\n\n\nEl efecto correcto se recupera ajustando el DAG completo y luego simulando una intervención. Tal y como dice Richard McElreath, la inferencia causal es predecir el efecto de la intervención."
  },
  {
    "objectID": "2024/03/lujuria_e_intervencion.html#conclusión",
    "href": "2024/03/lujuria_e_intervencion.html#conclusión",
    "title": "Lujuria e intervención",
    "section": "Conclusión",
    "text": "Conclusión\nLa inferencia bayesiana nos permite ajustar un DAG completo e incluso ajustar por “colliders” o por variables no observadas. Esto puede ser útil cuando tenemos DAGs en los que no se puede obtener correctamente el efecto buscado , ya sea porque implica condicionar por variables que tengan el doble rol de “confounder” y de “colliders” en diferentes “paths” o por otros motivos. El “full luxury bayes” conlleva coste computacional elevado por lo que la estrategia debería ser la de usarlo sólo en caso necesario. Pero la verdad es que encuentro cierta belleza en ajustar todo el diagrama causal y luego simular la intervención."
  },
  {
    "objectID": "2024/04/tiempos_mejores.html",
    "href": "2024/04/tiempos_mejores.html",
    "title": "Ya vendrán tiempos mejores",
    "section": "",
    "text": "Listening\n\n\n\n\n\n\n\n“Ya vendrán tiempos mejores, dónde no habrá que llorar. Que nos envuelva la risa y brindaremos en un bar”\nAyer se nos fue uno de los grandes del rock y el heavy granadino y un tío genial. Dani era tal cual, sin dobleces, lo que te enseñaba era lo que había. Un tipo de los que hay pocos, su vitalidad y optimismo eran contagiosos.\nNos hizo ( y nos seguirá haciendo) disfrutar con su música durante años, y aunque pesonalmente no hace muchos años que lo conozco, lo he sentido como un amigo. Y aún más importante, hizo feliz a una de las personas que más quiero.\nEn la cafetería del tanatorio no daban crédito ante tamaña multitud de gente que había ido a darle su último adiós. A tomarse un par de birras en su honor. Seguramente se acabaría con las existencias de Alhambra especial y de “milnó”, estoy seguro que sonreía desde alguna parte.\nHasta siempre amigo Dani, nos veremos en el averno. Y como dice una de tus canciones -Que me voy pa la renfe y hazte otro porro si te apetece -"
  },
  {
    "objectID": "2024/04/tiempos_mejores.html#listening",
    "href": "2024/04/tiempos_mejores.html#listening",
    "title": "Ya vendrán tiempos mejores",
    "section": "Listening",
    "text": "Listening\n\n&lt;p&gt;:::&lt;/p&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Muestrear no es pecado&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Muestrear no es pecado&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Blog\"&gt;Blog&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/blog.html\"&gt;/blog.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://github.com/joscani\"&gt;https://github.com/joscani&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://twitter.com/joscani\"&gt;https://twitter.com/joscani&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://fosstodon.org/@joscani\"&gt;https://fosstodon.org/@joscani&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/blog.xml\"&gt;/blog.xml&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Resources\"&gt;Resources&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:R Weekly\"&gt;R Weekly&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://rweekly.org/\"&gt;https://rweekly.org/&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:R Bloggers\"&gt;R Bloggers&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://www.r-bloggers.com/\"&gt;https://www.r-bloggers.com/&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Datanalytics\"&gt;Datanalytics&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://www.datanalytics.com/\"&gt;https://www.datanalytics.com/&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Archivo\"&gt;Archivo&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/archive.html\"&gt;/archive.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Muestrear no es pecado - Ya vendrán tiempos mejores&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Muestrear no es pecado - Ya vendrán tiempos mejores&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Muestrear no es pecado - Ya vendrán tiempos mejores&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Muestrear no es pecado&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Estadística, ciencia de datos, big data, cosas varias&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Estadística, ciencia de datos, big data, cosas varias&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) =&gt; {\n    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';\n    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';\n    let newTheme = '';\n    if(darkModeDefault) {\n      newTheme = isAlternate ? baseTheme : alternateTheme;\n    } else {\n      newTheme = isAlternate ? alternateTheme : baseTheme;\n    }\n    const changeGiscusTheme = () =&gt; {\n      // From: https://github.com/giscus/giscus/issues/336\n      const sendMessage = (message) =&gt; {\n        const iframe = document.querySelector('iframe.giscus-frame');\n        if (!iframe) return;\n        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');\n      }\n      sendMessage({\n        setConfig: {\n          theme: newTheme\n        }\n      });\n    }\n    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;\n    if (isGiscussLoaded) {\n      changeGiscusTheme();\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  const darkModeDefault = false;\n  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n    toggleGiscusIfUsed(toAlternate, darkModeDefault);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n    var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n    var mailtoRegex = new RegExp(/^mailto:/);\n      var filterRegex = new RegExp(\"https:\\/\\/muestrear-no-es-pecado\\.netlify\\.app\\/\");\n    var isInternal = (href) =&gt; {\n        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n    }\n    // Inspect non-navigation links and adorn them if external\n \tvar links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');\n    for (var i=0; i&lt;links.length; i++) {\n      const link = links[i];\n      if (!isInternal(link.href)) {\n        // undo the damage that might have been done by quarto-nav.js in the case of\n        // links that we want to consider external\n        if (link.dataset.originalHref !== undefined) {\n          link.href = link.dataset.originalHref;\n        }\n      }\n    }\n  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n    const config = {\n      allowHTML: true,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start',\n    };\n    if (contentFn) {\n      config.content = contentFn;\n    }\n    if (onTriggerFn) {\n      config.onTrigger = onTriggerFn;\n    }\n    if (onUntriggerFn) {\n      config.onUntrigger = onUntriggerFn;\n    }\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      if (note) {\n        return note.innerHTML;\n      } else {\n        return \"\";\n      }\n    });\n  }\n  const xrefs = window.document.querySelectorAll('a.quarto-xref');\n  const processXRef = (id, note) =&gt; {\n    // Strip column container classes\n    const stripColumnClz = (el) =&gt; {\n      el.classList.remove(\"page-full\", \"page-columns\");\n      if (el.children) {\n        for (const child of el.children) {\n          stripColumnClz(child);\n        }\n      }\n    }\n    stripColumnClz(note)\n    if (id === null || id.startsWith('sec-')) {\n      // Special case sections, only their first couple elements\n      const container = document.createElement(\"div\");\n      if (note.children && note.children.length &gt; 2) {\n        container.appendChild(note.children[0].cloneNode(true));\n        for (let i = 1; i &lt; note.children.length; i++) {\n          const child = note.children[i];\n          if (child.tagName === \"P\" && child.innerText === \"\") {\n            continue;\n          } else {\n            container.appendChild(child.cloneNode(true));\n            break;\n          }\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(container);\n        }\n        return container.innerHTML\n      } else {\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        return note.innerHTML;\n      }\n    } else {\n      // Remove any anchor links if they are present\n      const anchorLink = note.querySelector('a.anchorjs-link');\n      if (anchorLink) {\n        anchorLink.remove();\n      }\n      if (window.Quarto?.typesetMath) {\n        window.Quarto.typesetMath(note);\n      }\n      // TODO in 1.5, we should make sure this works without a callout special case\n      if (note.classList.contains(\"callout\")) {\n        return note.outerHTML;\n      } else {\n        return note.innerHTML;\n      }\n    }\n  }\n  for (var i=0; i&lt;xrefs.length; i++) {\n    const xref = xrefs[i];\n    tippyHover(xref, undefined, function(instance) {\n      instance.disable();\n      let url = xref.getAttribute('href');\n      let hash = undefined; \n      if (url.startsWith('#')) {\n        hash = url;\n      } else {\n        try { hash = new URL(url).hash; } catch {}\n      }\n      if (hash) {\n        const id = hash.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note !== null) {\n          try {\n            const html = processXRef(id, note.cloneNode(true));\n            instance.setContent(html);\n          } finally {\n            instance.enable();\n            instance.show();\n          }\n        } else {\n          // See if we can fetch this\n          fetch(url.split('#')[0])\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.getElementById(id);\n            if (note !== null) {\n              const html = processXRef(id, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      } else {\n        // See if we can fetch a full url (with no hash to target)\n        // This is a special case and we should probably do some content thinning / targeting\n        fetch(url)\n        .then(res =&gt; res.text())\n        .then(html =&gt; {\n          const parser = new DOMParser();\n          const htmlDoc = parser.parseFromString(html, \"text/html\");\n          const note = htmlDoc.querySelector('main.content');\n          if (note !== null) {\n            // This should only happen for chapter cross references\n            // (since there is no id in the URL)\n            // remove the first header\n            if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n              note.children[0].remove();\n            }\n            const html = processXRef(null, note);\n            instance.setContent(html);\n          } \n        }).finally(() =&gt; {\n          instance.enable();\n          instance.show();\n        });\n      }\n    }, function(instance) {\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            div.style.left = 0;\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n        // Handle positioning of the toggle\n    window.addEventListener(\n      \"resize\",\n      throttle(() =&gt; {\n        elRect = undefined;\n        if (selectedAnnoteEl) {\n          selectCodeLines(selectedAnnoteEl);\n        }\n      }, 10)\n    );\n    function throttle(fn, ms) {\n    let throttle = false;\n    let timer;\n      return (...args) =&gt; {\n        if(!throttle) { // first call gets through\n            fn.apply(this, args);\n            throttle = true;\n        } else { // all the others get throttled\n            if(timer) clearTimeout(timer); // cancel #2\n            timer = setTimeout(() =&gt; {\n              fn.apply(this, args);\n              timer = throttle = false;\n            }, ms);\n        }\n      };\n    }\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"joscani/blogComments\"\n        data-repo-id=\"R_kgDOIXA9wA\"\n        data-category=\"General\"\n        data-category-id=\"DIC_kwDOIXA9wM4CSZhs\"\n        data-mapping=\"title\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;\n&lt;input type=\"hidden\" id=\"giscus-base-theme\" value=\"light\"&gt;\n&lt;input type=\"hidden\" id=\"giscus-alt-theme\" value=\"dark\"&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "2024.html",
    "href": "2024.html",
    "title": "2024",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 20, 2024\n\n\nMétricas modelo con quarto y h2o\n\n\n\n\nApr 12, 2024\n\n\nYa vendrán tiempos mejores\n\n\n\n\nMar 31, 2024\n\n\nLujuria e intervención\n\n\n\n\nMar 18, 2024\n\n\nJornadas Hispalenses\n\n\n\n\nFeb 24, 2024\n\n\nNomenclatura\n\n\n\n\nFeb 16, 2024\n\n\nGuarnición de guerra\n\n\n\n\nJan 13, 2024\n\n\nNeovim\n\n\n\n\nJan 9, 2024\n\n\nCachitos 2024. Cuarta parte. The python way\n\n\n\n\nJan 8, 2024\n\n\nCachitos 2023. Tercera parte\n\n\n\n\nJan 4, 2024\n\n\nCachitos 2023. Segunda parte\n\n\n\n\nJan 2, 2024\n\n\nCachitos 2023. Primera parte\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2024/04/metricas_modelo_con_quarto.html",
    "href": "2024/04/metricas_modelo_con_quarto.html",
    "title": "Métricas modelo con quarto y h2o",
    "section": "",
    "text": "Listening\n\n\n\n\n\n\n\nComo muchos sabréis soy bastante fan de usar h2o en modelización. H2O se integra muy bien con R, Python o con Spark. De hecho , gracias a mi insistencia y conocimiento de como usar esta librería he conseguido cambiar la forma de hacer las cosas en más de una empresa, -si, no tengo abuela, pero ya va siendo hora de contar las cosas como han sido-.\nUna vez tienes entrenado un modelo con h2o se puede guardar como un MOJO (Model Object Java Optimization), y este mojo lo puedes usar para predecir usando R, python, java o spark, y es muy fácil de productivizar.\nEn el fichero mojo (una vez lo descomprimes) aparte del modelo también se crea un fichero json en la ruta experimental/modelDetails.json dónde se guarda información sobre el modelo utilizado, métricas de desempeño en train y validación, y un montón de cosas.\nPues parseando ese fichero y tratándolo podemos crearnos un dashboard.\nYo me he creado un fichero quarto que toma como parámetro ese fichero json y genera un dashboard.\n\nFichero qmd\nAl bajar, renombrar quitando el _txt\nDescargar Archivo\nEl contenido es.\n\n#&gt; ---\n#&gt; title: Métricas modelo Churn\n#&gt; author: Squad B2C\n#&gt; format: \n#&gt;   dashboard:\n#&gt;     orientation: columns\n#&gt;     theme: yeti\n#&gt; params:\n#&gt;   fichero_json: \n#&gt;     value: x\n#&gt; ---\n#&gt; \n#&gt; \n#&gt; ```{r setup, include=FALSE}\n#&gt; library(flexdashboard)\n#&gt; library(jsonlite)\n#&gt; library(tidyverse)\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ```{r load_data}\n#&gt; ## '/home/jose/canadasreche@gmail.com/h2o_production/epa_glm/experimental/modelDetails.json'\n#&gt; ## '/home/jose/canadasreche@gmail.com/h2o_production/xgboost_model_target_jazztel_20191120_stable/experimental/modelDetails.json'\n#&gt; df &lt;- fromJSON(params$fichero_json)\n#&gt; # df &lt;- fromJSON(\"churn_iter13mojo/experimental/modelDetails.json\")\n#&gt; \n#&gt; metricas_training &lt;- df$output$training_metrics$thresholds_and_metric_scores$data %&gt;% t() %&gt;% as.data.frame()\n#&gt; \n#&gt; \n#&gt; if(!is.null(df$output$validation_metrics)){\n#&gt;  metricas_validation &lt;- df$output$validation_metrics$thresholds_and_metric_scores$data %&gt;% t() %&gt;% as.data.frame()\n#&gt;  colnames(metricas_validation) &lt;- df$output$validation_metrics$thresholds_and_metric_scores$columns$name\n#&gt; \n#&gt; }\n#&gt; \n#&gt; \n#&gt; colnames(metricas_training) &lt;- df$output$training_metrics$thresholds_and_metric_scores$columns$name\n#&gt; \n#&gt; \n#&gt; colnames(metricas_training) &lt;- df$output$training_metrics$thresholds_and_metric_scores$columns$name\n#&gt; \n#&gt; ```\n#&gt; \n#&gt; # Training Metrics\n#&gt; \n#&gt; ## Column {width=10%}\n#&gt; \n#&gt; ### Row 1\n#&gt; \n#&gt; \n#&gt; ```{r}\n#&gt; #| title: AUC\n#&gt; gauge(df$output$training_metrics$AUC, min = 0.5, max=1, abbreviateDecimals = 2)\n#&gt; ```\n#&gt; \n#&gt; ### Row 2\n#&gt; \n#&gt; ```{r}\n#&gt; #| component: valuebox\n#&gt; #| title: F1\n#&gt; list(\n#&gt;   icon = \"stopwatch\",\n#&gt;   color = \"primary\",\n#&gt;   value = round(metricas_training[which.max(metricas_training$f1), c(\"f1\")], 2)\n#&gt; )\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ## Column {width=40%}\n#&gt; \n#&gt; \n#&gt; ### ROC\n#&gt; \n#&gt; ```{r}\n#&gt; plotly::ggplotly(ggplot(metricas_training, aes(x = fpr, y = tpr)) +\n#&gt;   geom_line(color=\"darkorange\") + geom_abline(intercept = 0, slope = 1, size = rel(0.1)))\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ## Column {width=50%}\n#&gt; \n#&gt; ::: {.panel-tabset}\n#&gt; \n#&gt; ### Gain Lift\n#&gt; \n#&gt; ```{r}\n#&gt; t_gain_lift_data &lt;- as.data.frame(t(df$output$training_metrics$gains_lift_table$data))\n#&gt; colnames(t_gain_lift_data) &lt;- df$output$training_metrics$gains_lift_table$columns$name\n#&gt; \n#&gt; t_gain_lift_data[,] &lt;- sapply(t_gain_lift_data[,], function(x) as.numeric(as.character(x)))\n#&gt; \n#&gt; DT::datatable(round(t_gain_lift_data,2), rownames = FALSE)\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ### Lift Plot\n#&gt; ```{r}\n#&gt; ggplot(t_gain_lift_data, aes(x=cumulative_data_fraction, y = lift)) +\n#&gt;   geom_line(color=\"darkorange\")\n#&gt; ```\n#&gt; \n#&gt; :::\n#&gt; \n#&gt; # Validation Metrics\n#&gt; ## Column {width=10%}\n#&gt; \n#&gt; ### Row 1\n#&gt; \n#&gt; \n#&gt; ```{r}\n#&gt; #| title: AUC\n#&gt; gauge(df$output$validation_metrics$AUC, min = 0.5, max=1, abbreviateDecimals = 2)\n#&gt; ```\n#&gt; \n#&gt; ### Row 2\n#&gt; \n#&gt; ```{r}\n#&gt; #| component: valuebox\n#&gt; #| title: F1\n#&gt; list(\n#&gt;   icon = \"stopwatch\",\n#&gt;   color = \"primary\",\n#&gt;   value = round(metricas_validation[which.max(metricas_validation$f1), c(\"f1\")], 2)\n#&gt; )\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ## Column {width=40%}\n#&gt; \n#&gt; \n#&gt; ### ROC\n#&gt; \n#&gt; ```{r}\n#&gt; plotly::ggplotly(ggplot(metricas_validation, aes(x = fpr, y = tpr)) +\n#&gt;   geom_line(color=\"darkorange\") + geom_abline(intercept = 0, slope = 1, size = rel(0.1)))\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ## Column {width=50%}\n#&gt; \n#&gt; ::: {.panel-tabset}\n#&gt; \n#&gt; ### Gain Lift\n#&gt; \n#&gt; ```{r}\n#&gt; t_gain_lift_data &lt;- as.data.frame(t(df$output$validation_metrics$gains_lift_table$data))\n#&gt; colnames(t_gain_lift_data) &lt;- df$output$validation_metrics$gains_lift_table$columns$name\n#&gt; \n#&gt; t_gain_lift_data[,] &lt;- sapply(t_gain_lift_data[,], function(x) as.numeric(as.character(x)))\n#&gt; \n#&gt; DT::datatable(round(t_gain_lift_data,2), rownames = FALSE)\n#&gt; ```\n#&gt; \n#&gt; \n#&gt; ### Lift Plot\n#&gt; ```{r}\n#&gt; ggplot(t_gain_lift_data, aes(x=cumulative_data_fraction, y = lift)) +\n#&gt;   geom_line(color=\"darkorange\")\n#&gt; ```\n#&gt; \n#&gt; :::\n#&gt; \n#&gt; \n#&gt; # Variable importance\n#&gt; \n#&gt; ## Column\n#&gt; \n#&gt; \n#&gt; ### **Variable Importance**\n#&gt; \n#&gt; ```{r}\n#&gt; var_importance &lt;- df$output$variable_importances$data %&gt;% t() %&gt;% as.data.frame()\n#&gt; colnames(var_importance) &lt;- c(\"variable\", \"importance\",\"rel_importance\", \"otra\")\n#&gt; var_importance &lt;- var_importance %&gt;% \n#&gt;   transmute(\n#&gt;      variable = variable,\n#&gt;      importance = importance %&gt;% as.character() %&gt;% as.numeric,\n#&gt;      rel_importance = rel_importance%&gt;% as.character() %&gt;% as.numeric)\n#&gt; # var_importance$variable &lt;-  as.character(var_importance$variable)\n#&gt; \n#&gt; p &lt;- var_importance %&gt;% \n#&gt;   mutate(variable = fct_reorder(variable, rel_importance)) %&gt;%\n#&gt;   top_n(25, rel_importance)  %&gt;% \n#&gt;   ggplot(\n#&gt;     aes( \n#&gt;     x = variable,\n#&gt;     y = rel_importance \n#&gt;     )) +\n#&gt;   geom_col(fill = \"darkorange\") +\n#&gt;   coord_flip()\n#&gt; \n#&gt; plotly::ggplotly(p)\n#&gt; \n#&gt; write_csv(var_importance, file = \"variables_importantes.csv\")\n#&gt; \n#&gt; \n#&gt; ```\n\n\n\nScript para generar el dashboard\nTengo un script en R , pero podría ser en bash que descomprime el modelo (fichero mojo) en una carpeta temporal y llama al fichero qmd para generar el dashboard\nDescargar generar_metricas_quarto.R\n\n#&gt; #!/usr/bin/env Rscript\n#&gt; args = commandArgs(trailingOnly=TRUE)\n#&gt; \n#&gt; zipfile = args[1]\n#&gt; output_path = paste0(args[2],'.html')\n#&gt; \n#&gt; fichero_qmd &lt;- 'metricas.qmd'\n#&gt; \n#&gt; \n#&gt; # Generar informe automático\n#&gt; \n#&gt; tmp_dir &lt;- tempdir()\n#&gt; tmp &lt;- tempfile()\n#&gt; \n#&gt; unzip(zipfile = zipfile, exdir = tmp_dir )\n#&gt; \n#&gt; fichero_json = paste0(tmp_dir, \"/experimental/modelDetails.json\")\n#&gt; \n#&gt; \n#&gt; quarto::quarto_render(\n#&gt;   input = fichero_qmd, \n#&gt;   execute_params = list(fichero_json = fichero_json), \n#&gt;   output_file = output_path\n#&gt;     \n#&gt; )\n\nY ya sólo quedaría ejecutar esto\nRscript --vanilla generar_metricas_quarto.R modelo_mojo.zip output_file\n\n\nEntrenar un modelo de ejemplo\nTengo unos datos bajados de kaggle\ndatos kaggle\nEntrenamos el modelo con h2o y guardamos el mojo\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(h2o)\n\nh2o.init()\n#&gt; \n#&gt; H2O is not running yet, starting it now...\n#&gt; \n#&gt; Note:  In case of errors look at the following log files:\n#&gt;     /tmp/RtmpwT3Yln/file19a0f3c7ccf4c/h2o_jose_started_from_r.out\n#&gt;     /tmp/RtmpwT3Yln/file19a0f5eb2fdfc/h2o_jose_started_from_r.err\n#&gt; \n#&gt; \n#&gt; Starting H2O JVM and connecting: .. Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         1 seconds 617 milliseconds \n#&gt;     H2O cluster timezone:       Europe/Madrid \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.44.0.3 \n#&gt;     H2O cluster version age:    3 months and 30 days \n#&gt;     H2O cluster name:           H2O_started_from_R_jose_veb551 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   7.80 GB \n#&gt;     H2O cluster total cores:    12 \n#&gt;     H2O cluster allowed cores:  12 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.3.3 (2024-02-29)\n\n# Load data\n\nchurn_df &lt;- read_csv(\"./WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\n# Split data\n\nchurn_hex &lt;-  as.h2o(churn_df)\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nchurn_hex$Churn  &lt;-  as.factor(churn_hex$Churn)\n\nsplits &lt;- h2o.splitFrame(data = churn_hex, ratios = 0.8, seed = 1234)\ntrain &lt;- h2o.assign(splits[[1]], \"train\")\ntest &lt;- h2o.assign(splits[[2]], \"test\")\n\n\n# Train model\n\nx  &lt;-  colnames(churn_hex)[!colnames(churn_hex) %in% c(\"customerID\", \"Churn\")]\nx\n#&gt;  [1] \"gender\"           \"SeniorCitizen\"    \"Partner\"          \"Dependents\"      \n#&gt;  [5] \"tenure\"           \"PhoneService\"     \"MultipleLines\"    \"InternetService\" \n#&gt;  [9] \"OnlineSecurity\"   \"OnlineBackup\"     \"DeviceProtection\" \"TechSupport\"     \n#&gt; [13] \"StreamingTV\"      \"StreamingMovies\"  \"Contract\"         \"PaperlessBilling\"\n#&gt; [17] \"PaymentMethod\"    \"MonthlyCharges\"   \"TotalCharges\"\ny  &lt;- \"Churn\"\ny\n#&gt; [1] \"Churn\"\n\nmodel &lt;-  h2o.xgboost(\n  model_id = \"Churn_model\", \n  x = x,\n  y = y,\n  training_frame = train,\n  validation_frame = test,\n  distribution = \"bernoulli\",\n  nthread = -1,\n  ntrees = 20,\n  max_depth = 3,\n)\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |======================================================================| 100%\n\n\nh2o.save_mojo(model, path = \".\", force = TRUE)\n#&gt; [1] \"/media/hd1/canadasreche@gmail.com/blog_quarto/2024/04/Churn_model.zip\"\n\n\nh2o.shutdown(prompt = FALSE)\n\n\nY ejecutando en la misma carpeta dónde están el modelo y el fichero qmd.\nEvidentemente hay que tener instalado quarto y demás cosas.\n\n\nComando que hay que ejecutar en consola\nRscript --vanilla generar_metricas_quarto.R Churn_model.zip Churn_model_metrics\n\n\nInforme\nY nos genera este bonito informe (descomrpimir el zip)\nInforme generado\n\n\n\ncaptura pantalla\n\n\nY esto es todo, de esta forma es como yo en mi día a día guardo un pequeño dashboard de cada modelo, simplemente leyendo la info que h2o ha guardado en el mojo y así estoy seguro de que esas métricas son justo las que corresponden con los datos usados por el modelo."
  }
]
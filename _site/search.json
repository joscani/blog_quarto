[
  {
    "objectID": "2022.html",
    "href": "2022.html",
    "title": "2022",
    "section": "",
    "text": "Api y docker con R. parte 2\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html",
    "href": "2022/06/21/io-parte-1/index.html",
    "title": "IO Parte 1",
    "section": "",
    "text": "Allá por el año 1997 más o menos andaba yo estudiando Investigación Operativa en la Universidad de Granada. Recuerdo aprender el archiconocido algoritmo del simplex y algo también sobre programación entera (dónde el dominio de las variables está en \\(\\mathcal{Z}\\) ). No se me daba muy bien al principio, pero si recuerdo que luego me acabó gustando y el día que encuentre mis apuntes os pondré una demostración que desarrollé para un teorema que tenía algo que ver con la relación entre espacio primal y el dual.\nBueno, dejando de lado las batallitas de final del siglo pasado y debido a que estuve hace poco en la SEIO 2022 en Granada y coincidí con grandes profesionales de este tema, como por ejemplo el gran Alberto Torrejón Valenzuela, estoy convencido de que la investigación operativa es una de las grandes áreas que aún queda por explotar en las empresas.\nSe lleva haciendo Investigación Operativa desde hace tiempo, véase esto. Por otro lado creo que va a ser el próximo hype por las señales que estoy viendo, la primera de ellas es el cambio de nombre de la materia, ahora estoy empezando escuchar Analítica prescriptiva en vez de Investigación Operativa. Y como es norma en este mundillo, el cambio de nombre precede a la ¿burbuja?.\nPues vamos al grano, en esto de la investigación operativa se ha desarrollado mucho software para resolver este tipo de problemas, a partir de ahora los llamaremos solvers. Dentro de estos podemos destacar solvers comerciales como CPlex y Gurobi, pero también hay software libre como GLPK dentro del proyecto GNU, o sin olvidarnos de la fundación COIN-OR dónde se han desarrollado muchos solvers de manera opensource.\nDentro de nuestros 3 lenguajes favoritos (R, Julia y Python) hay librerías que permiten hacer de API para los diferentes solvers, tanto los de software libre como los comerciales. Paso a enumerar 3 proyectos, uno para cada lenguaje y pongo enlace.\nEn realidad, en principio uno podría decir que se trata sólo de sintaxis y que da igual cual uses pues al final todos utilizan los mismos solvers en el backend . No obstante, hay problemas en los que la misma construcción del mismo para pasárselo al solver puede tardar bastante y, según esto, JuMP parece estar a la altura con respecto a los softwares comerciales como GAMS en la generación del modelo y envío al solver.\nVeamos ahora un ejemplillo tonto de programación lineal y cual es la sintaxis (al fin y al cabo es sólo eso) en cada uno de los lenguajes.\nTenemos el siguiente problema de programación lineal\nY el objetivo es encontrar los valores de x e y que cumpliendo las restricciones minimicen la función 7x + 8y. Para los 3 lenguajes vamos a usar GLPK como solver"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#r",
    "href": "2022/06/21/io-parte-1/index.html#r",
    "title": "IO Parte 1",
    "section": "R",
    "text": "R\nlibrary(ROI)\n## ROI: R Optimization Infrastructure\n## Registered solver plugins: nlminb, alabama, glpk.\n## Default solver: auto.\nlibrary(ROI.plugin.glpk)\nHay que definir el objetivo, las restricciones y los límites de la variable. Para las restricciones en R hay que poner los coeficientes en forma de matriz , de ahí el uso de rbind\nobjetivo          = L_objective(c(7, 8), names = c(\"x\", \"y\"))\nrestricciones     = L_constraint(L = rbind(c(3, 4), c(2, 1)),\n                                   dir = c(\"==\", \">=\"),\n                                   rhs = c(9, 3))\nlimites_variables = V_bound(\n  li = 1:2,\n  ui = 1:2,\n  lb = c(-100, -Inf),\n  ub = c(Inf, 100)\n)\nAhora usamos la función OP que es el constructor del problema, por defecto considera que se trata de un problema de maximización OP(objective, constraints, types, bounds, maximum = FALSE)\nlp  <- OP(\n  objective = objetivo,\n  constraints = restricciones,\n  bounds = limites_variables\n)\nROI_applicable_solvers(lp)\n## [1] \"alabama\" \"glpk\"\nResolvemos con glpk\n(sol <- ROI_solve(lp, solver = \"glpk\"))\n## Optimal solution found.\n## The objective value is: 1.860000e+01\nY vemos cuáles son los valores de x e y que minimizan la función objetivo\nsolution(sol)\n##   x   y \n## 0.6 1.8"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#julia",
    "href": "2022/06/21/io-parte-1/index.html#julia",
    "title": "IO Parte 1",
    "section": "Julia",
    "text": "Julia\nEn Julia tenemos este librito online que va contando estupendamente como usar JuMP y las diferentes formas de utilizarlo.\n\n\nusing JuMP, GLPK\n\nm = Model(GLPK.Optimizer)\n## A JuMP Model\n## Feasibility problem with:\n## Variables: 0\n## Model mode: AUTOMATIC\n## CachingOptimizer state: EMPTY_OPTIMIZER\n## Solver name: GLPK\n\n# Variables y límites\n@variable(m, -100 <= x)\n## x\n@variable(m, y <= 100)\n## y\n\n# Objtivo\n@objective(m, Min, 7x + 8y)\n## 7 x + 8 y\n\n# restricciones\n@constraint(m, constraint1, 3x +  4y == 9)\n## constraint1 : 3 x + 4 y = 9.0\n@constraint(m, constraint2,  2x + 1y  >= 3)\n## constraint2 : 2 x + y ≥ 3.0\n\n# Solving the optimization problem\nJuMP.optimize!(m)\n\n# Printing the optimal solutions obtained\nprintln(\"Optimal Solutions:\")\n## Optimal Solutions:\nprintln(\"x = \", JuMP.value(x))\n## x = 0.5999999999999943\nprintln(\"y = \", JuMP.value(y))\n## y = 1.8000000000000114\nLa verdad que me ha gustado la sintaxis de JuMP , es casi un calco de como lo escribirías a mano. Como curiosidad fijaros en que se puede poner 3x +  4y == 9 , y no hace falta poner 3 * x  + 4 * y\nOtra curiosidad es que desde Julia puedes obtener el modelo en latex\nlatex_formulation(m)\n## $$ \\begin{aligned}\n## \\min\\quad & 7 x + 8 y\\\\\n## \\text{Subject to} \\quad & 3 x + 4 y = 9.0\\\\\n##  & 2 x + y \\geq 3.0\\\\\n##  & x \\geq -100.0\\\\\n##  & y \\leq 100.0\\\\\n## \\end{aligned} $$"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#python",
    "href": "2022/06/21/io-parte-1/index.html#python",
    "title": "IO Parte 1",
    "section": "Python",
    "text": "Python\nEn python usaremos pyomo, el cual también tiene una sintaxis clara.\nfrom pyomo.environ import *\n\nmodel = ConcreteModel()\nmodel.x = Var(domain=NonNegativeReals)\nmodel.y = Var(domain=NonNegativeReals)\n\nmodel.objetivo = Objective(expr = 7*model.x + 8*model.y, sense=minimize)\n\nmodel.constraint1 = Constraint(expr = 3* model.x +  4*model.y == 9)\nmodel.constraint2 = Constraint(expr = 2*model.x + 1*model.y  >= 3)\nresults = SolverFactory('glpk').solve(model)\nif results.solver.status == 'ok':\n    model.pprint()\n## 2 Var Declarations\n##     x : Size=1, Index=None\n##         Key  : Lower : Value : Upper : Fixed : Stale : Domain\n##         None :     0 :   0.6 :  None : False : False : NonNegativeReals\n##     y : Size=1, Index=None\n##         Key  : Lower : Value : Upper : Fixed : Stale : Domain\n##         None :     0 :   1.8 :  None : False : False : NonNegativeReals\n## \n## 1 Objective Declarations\n##     objetivo : Size=1, Index=None, Active=True\n##         Key  : Active : Sense    : Expression\n##         None :   True : minimize : 7*x + 8*y\n## \n## 2 Constraint Declarations\n##     constraint1 : Size=1, Index=None, Active=True\n##         Key  : Lower : Body      : Upper : Active\n##         None :   9.0 : 3*x + 4*y :   9.0 :   True\n##     constraint2 : Size=1, Index=None, Active=True\n##         Key  : Lower : Body    : Upper : Active\n##         None :   3.0 : 2*x + y :  +Inf :   True\n## \n## 5 Declarations: x y objetivo constraint1 constraint2\nprint('Objetivo = ', model.objetivo())\n## Objetivo =  18.6\nprint('\\nDecision Variables')\n## \n## Decision Variables\nprint('x = ', model.x())\n## x =  0.6\nprint('y = ', model.y())\n## y =  1.8"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#miscelánea",
    "href": "2022/06/21/io-parte-1/index.html#miscelánea",
    "title": "IO Parte 1",
    "section": "Miscelánea",
    "text": "Miscelánea\nLa investigación operativa es un campo muy amplio, desde problemas de asignación de turnos, optimizar contenedores en grandes buques que pasan por varios puertos, optimización de gasto en medios para maximizar ventas (Marketing Mix Modelling), problemas de localización (dónde poner una tienda o gasolinera nueva dada una demanda existente y competencia), problemas de grafos, etc.\nHay muchos investigadores que se están dedicando a resolver este tipo de cosas, y dónde no se trata tanto de usar tal o cual solver, sino de formular bien el problema, ya que diferentes formulaciones del mismo problema igualmente válidas dan lugar a tiempos de cómputo muy diferentes. No es de extrañar que se tarden muchas horas en encontrar soluciones a problemas determinados.\nPor otro lado, no puedo dejar de señalar el problema de los incentivos perversos. Me explico, a los investigadores que están en estos temas se les valora por paper publicado en revista de alto impacto, por lo que una vez le han publicado un artículo pasan al siguiente, olvidando la necesaria transferencia entre universidad y empresa. No es culpa suya, el sistema funciona así. Así que animo a todos aquellos que se dedican en serio a estos temas a meter la patita en el mundo de la empresa. Hay mucho trabajo que hacer y dinero que ganar."
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html",
    "title": "Mediator. Full luxury bayes",
    "section": "",
    "text": "Continuando con la serie sobre cosas de inferencia causal y full luxury bayes, antes de que empiece mi amigo Carlos Gil, y dónde seguramente se aprenderá más.\nEste ejemplo viene motivado precisamente por una charla que tuve el otro día con él.\nSea el siguiente diagrama causal\nSe tiene que z es un mediador entre x e y, y la teoría nos dice que si quiero obtener el efecto directo de x sobre y he de condicionar por z , y efectivamente, así nos lo dice el backdoor criterio. Mientras que si quiero el efecto total de x sobre y no he de condicionar por z."
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#datos-simulados",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#datos-simulados",
    "title": "Mediator. Full luxury bayes",
    "section": "Datos simulados",
    "text": "Datos simulados\n\nset.seed(155)\nN <- 1000 \n\nx <- rnorm(N, 2, 1) \nz <- rnorm(N, 4+ 4*x , 2 ) # a z le ponemos más variabilidad, pero daría igual\ny <- rnorm(N, 2+ 3*x + z, 1)\n\nEfecto total de x sobre y \nTal y como hemos simulado los datos, se esperaría que el efecto total de x sobre y fuera\n\\[ \\dfrac{cov(x,y)} {var(x)} \\approx 7 \\]\nY qué el efecto directo fuera de 3\nEfectivamente\nEfecto total\n\n# efecto total\nlm(y~x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      5.881        7.112  \n\n\n\n# efecto directo\nlm(y~x+z)\n\n\nCall:\nlm(formula = y ~ x + z)\n\nCoefficients:\n(Intercept)            x            z  \n     2.0318       3.0339       0.9945"
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#full-luxury-bayes",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#full-luxury-bayes",
    "title": "Mediator. Full luxury bayes",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nHagamos lo mismo pero estimando el dag completo\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\ndat <- list(\n  N = N,\n  x = x,\n  y = y,\n  z = z\n)\nset.seed(1908)\n\nflbi <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a0,\n    z ~ normal( mu , sigma ),\n    \n    mu <- a1 + bx * x,\n   \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx2 * x + bz * z,\n\n    # priors\n    \n    c(a0,a1,a2,bx,bx2, bz) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n\nRunning MCMC with 10 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 6 finished in 10.6 seconds.\nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 8 finished in 11.9 seconds.\nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 9 finished in 12.5 seconds.\nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 13.0 seconds.\nChain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 13.2 seconds.\nChain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 5 finished in 13.2 seconds.\nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 13.9 seconds.\nChain 7 finished in 13.9 seconds.\nChain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 10 finished in 14.4 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 14.6 seconds.\n\nAll 10 chains finished successfully.\nMean chain execution time: 13.1 seconds.\nTotal execution time: 14.8 seconds.\n\n\nY recuperamos los coeficientes y varianzas\n\nprecis(flbi)\n\n           mean         sd      5.5%    94.5%     n_eff     Rhat4\nbz    1.0131931 0.01557219 0.9883466 1.038241  8226.042 1.0005919\nbx2   2.9610334 0.07098661 2.8447278 3.073501  9640.122 1.0004040\nbx    4.1535508 0.06180592 4.0552767 4.252771  8753.661 1.0001456\na2    1.9440850 0.09315785 1.7952495 2.093027 11266.267 1.0001373\na1    3.7089914 0.13578637 3.4916007 3.926010  8709.448 1.0002893\na0    1.9857103 0.03108199 1.9357773 2.035712 16252.310 0.9997523\nk     0.9719412 0.02166791 0.9378896 1.007410 15276.158 0.9999036\ntau   0.9859183 0.02216068 0.9512944 1.021932 15735.140 1.0001540\nsigma 1.9636543 0.04382554 1.8950489 2.034130 15855.098 1.0000898\n\n\n\n# extraemos 10000 muestras de la posteriori \npost <- extract.samples(flbi, n = 10000) \n\nY el efecto directo de x sobre y sería\n\nquantile(post$bx2, probs = c(0.025, 0.5, 0.975))\n\n    2.5%      50%    97.5% \n2.823659 2.962330 3.099855 \n\n\nEn este ejemplo sencillo, podríamos estimar el efecto causal de x sobre y simplemente sumando las posterioris\n\nquantile(post$bx + post$bx2, c(0.025, 0.5, 0.975))\n\n    2.5%      50%    97.5% \n6.928046 7.114180 7.299252 \n\n\nTambién podríamos obtener el efecto causal total de x sobre y simulando una intervención. En este caso, al ser la variable continua, lo que queremos obtener como de diferente es y cuando \\(X = x_i\\) versus cuando \\(X = x_i+1\\).\nSiempre podríamos ajustar otro modelo bayesiano dónde no tuviéramos en cuenta a z y obtendríamos la estimación de ese efecto total de x sobre y, pero siguiendo a Rubin y Gelman, la idea es incluir en nuestro modelo toda la información disponible. Y tal y como dice Richard McElreath , Statistical Rethinking 2022, el efecto causal se puede estimar simulando la intervención.\nAsí que el objetivo es dado nuestro modelo que incluye la variable z, simular la intervención cuando \\(X = x_i\\) y cuando \\(X = x_i+1\\) y una estimación del efecto directo es la resta de ambas distribuciones a posteriori de y.\nCreamos función para calcular el efecto de la intervención y_do_x\n\nget_total_effect <- function(x_value = 0, incremento = 0.5) {\n  n <- length(post$bx)\n  with(post, {\n    # simulate z, y  for x= x_value\n    z_x0 <- rnorm(n, a1 + bx * x_value  , sigma)\n    y_x0 <- rnorm(n, a2  + bz * z_x0 + bx * x_value , tau)\n    \n    # simulate z,y for x= x_value +1 \n    z_x1 <- rnorm(n, a1 + bx * (x_value + incremento)  , sigma)\n    y_x1 <- rnorm(n, a2  + bz * z_x1 + bx2 * (x_value + incremento) , tau)\n    \n    \n    # compute contrast\n    y_do_x <- (y_x1 - y_x0) / incremento\n    return(y_do_x)\n  })\n  \n}\n\nDado un valor de x, podemos calcular el efecto total\n\ny_do_x_0_2 <- get_total_effect(x_value = 0.2) \n\nquantile(y_do_x_0_2)\n\n        0%        25%        50%        75%       100% \n-15.324628   2.395551   6.702379  10.987520  28.909002 \n\n\nPodríamos hacer lo mismo para varios valores de x\n\nx_seq <- seq(-0.5, 0.5, length = 1000)\ny_do_x <-\n  mclapply(x_seq,  function(lambda)\n    get_total_effect(x_value = lambda))\n\nY para cada uno de estos 1000 valores tendría 10000 valores de su efecto total de x sobre y.\n\nlength(y_do_x[[500]])\n\n[1] 10000\n\nhead(y_do_x[[500]])\n\n[1] -1.174794 12.542116 14.663212  6.197720 12.200403 18.384301\n\n\nCalculamos los intervalos de credibilidad del efecto total de x sobre y para cada valor de x.\n\n# lo hacemos simplemente por cuantiles, aunque podríamos calcular el hdi también, \n\nintervalos_credibilidad <-  mclapply( y_do_x, function(x) quantile(x, probs = c(0.025, 0.5, 0.975)))\n\n# Media e intervalor de credibilidad para el valor de x_seq en la posición 500 \nmean(y_do_x[[500]])\n\n[1] 7.051617\n\nintervalos_credibilidad[[500]]\n\n     2.5%       50%     97.5% \n-5.337939  7.081199 19.614823 \n\n\nintervalo de predicción clásico con el lm\nHabría que calcular la predicción de y para un valor de x y para el de x + 1, podemos calcular los intervalos de predicción clásicos parea cada valor de x, pero no para la diferencia ( al menos con la función predict)\n\nmt_lm <- lm(y~x)\npredict(mt_lm, newdata = list(x= x_seq[[500]]), interval  = \"prediction\")\n\n       fit      lwr     upr\n1 5.877439 1.578777 10.1761\n\npredict(mt_lm, newdata = list(x= x_seq[[500]] +1), interval  = \"prediction\")\n\n       fit      lwr     upr\n1 12.98993 8.698051 17.2818\n\n\nPero como sería obtener el intervalo de credibilidad para la media de los efectos totales?\nCalculando para cada valor de x la media de la posteriori del efecto global y juntando todas las medias.\n\nslopes_mean <- lapply(y_do_x, mean)\n\nquantile(unlist(slopes_mean), c(0.025, 0.5, 0.975))\n\n    2.5%      50%    97.5% \n6.038139 7.183291 8.280885 \n\n\nQue tiene mucha menos variabilidad que el efecto global en un valor concreto de x, o si juntamos todas las estimaciones\n\nquantile(unlist(y_do_x),  c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n-5.218073  7.169724 19.560175 \n\n\nEvidentemente, podríamos simplemente no haber tenido en cuenta la variable z en nuestro modelo bayesiano.\n\nflbi_2 <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a1,\n    \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx * x ,\n    \n    # priors\n    \n    c(a1,a2,bx) ~ normal( 0 , 0.5 ),\n    c(tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n\nRunning MCMC with 10 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 finished in 1.2 seconds.\nChain 8 finished in 1.0 seconds.\nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.4 seconds.\nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 finished in 1.5 seconds.\nChain 4 finished in 1.5 seconds.\nChain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 5 finished in 1.6 seconds.\nChain 6 finished in 1.5 seconds.\nChain 9 finished in 1.4 seconds.\nChain 10 finished in 1.5 seconds.\nChain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 finished in 1.7 seconds.\n\nAll 10 chains finished successfully.\nMean chain execution time: 1.4 seconds.\nTotal execution time: 1.9 seconds.\n\n\nY obtenemos directamente el efecto total de x sobre y.\n\nprecis(flbi_2)\n\n         mean         sd      5.5%    94.5%    n_eff     Rhat4\nbx  7.1948083 0.06787458 7.0863389 7.302933 10446.14 1.0001211\na2  5.6085615 0.14948341 5.3685467 5.848723 10598.69 1.0001680\na1  1.9860307 0.03058033 1.9368689 2.034631 14116.13 1.0004122\nk   0.9721054 0.02159773 0.9382407 1.006761 15044.78 0.9995902\ntau 2.1898100 0.04953736 2.1120600 2.269831 14995.33 0.9999084\n\n\n\npost2 <- extract.samples(flbi_2,  n = 10000)\n\nquantile(post2$bx, probs = c(0.025, 0.5, 0.975))\n\n    2.5%      50%    97.5% \n7.061389 7.194525 7.327361"
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#pensamientos-finales",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#pensamientos-finales",
    "title": "Mediator. Full luxury bayes",
    "section": "Pensamientos finales",
    "text": "Pensamientos finales\n\nParece que no es tan mala idea incluir en tu modelo bayesiano toda la información disponible.\nSer pluralista es una buena idea, usando el backdoor criterio dado que nuestro dag sea correcto, nos puede llevar a un modelo más simple y fácil de estimar.\nComo dije en el último post, estimar todo el dag de forma conjunta puede ser útil en varias situaciones.\nYa en 2009 se hablaba de esto aquí"
  },
  {
    "objectID": "2022/02/09/collider-bias/index.html",
    "href": "2022/02/09/collider-bias/index.html",
    "title": "Collider Bias?",
    "section": "",
    "text": "Continuando con temas del post anterior. Dice Pearl, con buen criterio, que si condicionas por un collider abres ese camino causal y creas una relación espuria entre las dos variables “Tratamiento” y “Respuesta” y por lo tanto si condicionas por el collider, aparece un sesgo.\nHablando estilo compadre. Si Tratamiento -> Collider y Respuesta -> Collider, si condiciono en el Collider, es decir, calculo la relación entre Tratamiento y Respuesta para cada valor de C, se introduce un sesgo.\nSi \\[C = 2\\cdot Tratamiento + 3 \\cdot respuesta\\]\nSi sé que C = 3, y que Tratamiento = 4 , ya hay relación entre Tratamiento y respuesta aunque sean independientes, porque ambos son causa de C.\nSimulemos, que es una buena forma de ver qué pasa si condiciono por el collider, siendo el tratamiento y la respuesta independientes.\nSi no ajusto por el collider, obtengo que no hay efecto del tratamiento , correcto\nCondicionando, aparece el sesgo\nRetomando el ejemplo del último post, pero en vez de tener una variable de confusión no observable, tenemos un collider.\nUsando la función adjustmentSets de dagitty nos dice sobre qué variables hay que condicionar si quiero el efecto causal total y directo de M sobre D, siguiendo las reglas de Pearl, ver por ejemplo (J. Pearl (2009), Causality: Models, Reasoning and Inference. Cambridge University Press.)\nSimulo unos datos dónde fuerzo a que no haya efecto causal de M a D.\nEn grafo sería\nY vemos lo de antes,\nVemos si hay efecto causal de M sobre D (uso modelos lineales por simplicidad).\nEl modelo correcto sería sin condicionar por el collider. Y bien, hace lo que debe, no hay efecto de M sobre D, tal y como sabemos que pasa en la realidad\nCondicionando ahora por el collider, tenemos sesgo.\nQueda como curiosidad que si condicionas por B1 en vez de por el collider también hay sesgo, pero si condicionas solo por B2, no hay."
  },
  {
    "objectID": "2022/02/09/collider-bias/index.html#full-luxury-bayes",
    "href": "2022/02/09/collider-bias/index.html#full-luxury-bayes",
    "title": "Collider Bias?",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nNo todos los DAg’s son tan sencillos como el que he puesto, hay veces en los que una misma variable puede ser a la vez un collider y una variable de confusión, porque puede haber varios path causales y tenga diferente rol. En esos casos, condicionar por el collider te abre un path, y si no condicionas te abre otro. Ante esas situaciones, y suponiendo que el dag es correcto, no se podría estimar el efecto causal.\nSin embargo, condicionar en la red bayesiana no significa lo mismo que condicionar en un sólo modelo, sino que significa que introduzco la información que me proporciona el collider en la distribución conjunta y que me obtenga la posteriori.\nAl estimar el DAG completo, usando Stan por ejemplo, se estima tanto el modelo para M, como para D de forma conjunta.\n\nModelo bayesiano sin condicionar por el collider\nFormulamos el modelo usando la librería rethinking y lo ajustamos usando la función ulam que por debajo llama a Stan\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\n\ndat <- list(\n  N = N,\n  M = M,\n  D = D,\n  B1 = B1,\n  B2 = B2, \n  C = C\n)\n\nset.seed(155)\n\nflbi <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 ,\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M ,\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n\n    # priors\n    c(a1,a2,b,m) ~ normal( 0 , 0.5 ),\n    c(k,sigma,tau) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n\nWarning in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 6, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 7, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 39, column 4 to column 29)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 39, column 4 to column 29)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 35, column 4 to column 27)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 finished in 2.2 seconds.\nChain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 finished in 2.3 seconds.\nChain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 finished in 2.4 seconds.\nChain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 finished in 2.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.4 seconds.\nTotal execution time: 2.6 seconds.\n\n\nVemos los parámetros estimados y sus intervalos de credibilidad y extraemos la posteriori\n\nprecis(flbi)\n\n              mean         sd         5.5%      94.5%    n_eff     Rhat4\nm     -0.008692293 0.02258036 -0.044472871 0.02734239 6673.459 1.0005257\nb      1.961391786 0.04356684  1.891207250 2.03241110 5380.781 0.9998034\na2     0.059745778 0.04370034 -0.009909845 0.12931849 5500.657 1.0000543\na1     0.026656585 0.03702030 -0.032118024 0.08601562 6410.710 0.9997482\ntau    0.984889552 0.02213892  0.949803835 1.02046055 7752.122 1.0000469\nsigma  0.967785086 0.02155725  0.933665955 1.00275110 8907.866 1.0001535\nk      0.998568923 1.01036660  0.056543283 2.88207040 9005.060 1.0000657\np      0.482860133 0.01114257  0.465039000 0.50049459 8082.865 1.0000378\n\npost <- extract.samples(flbi)\n\nPintamos la distribución a posteriori del efecto y cómo ya sabíamos, al no condicionar por el collider, se estima sin sesgo que no hay efecto causal de M a D.\n\nplot(bayestestR::hdi(post$m, ci = c( 0.80, 0.95))) +\n  labs(title = \"Estimación sin collider\")\n\n\n\n\n\n\n\n\n\n\nModelo condicionando por el collider\nYa sabemos que no es necesario de hecho condicionar por el collider, más aún, que hacerlo induce un sesgo en la estimación del efecto, ¿pero qué pasa si estimamos el dag al completo?\n\nset.seed(155)\n\nflbi_collider <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 ,\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M ,\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n    \n    # Collider\n    \n     C ~ normal( cmu , csigma ),\n     cmu <- a3 + cm * M  + cd * D,\n\n    # priors\n    c(a1,a2,a3,b,m, cm, cd) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, csigma) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n\nWarning in '/tmp/Rtmpsdysi4/model-8eef4e4ce260.stan', line 3, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/tmp/Rtmpsdysi4/model-8eef4e4ce260.stan', line 4, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 finished in 4.4 seconds.\nChain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 1 finished in 4.5 seconds.\nChain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 finished in 4.5 seconds.\nChain 3 finished in 4.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.5 seconds.\nTotal execution time: 4.6 seconds.\n\n\nViendo la distribución posterior de los parámetros resulta que hemos podido estimar el verdadero efecto causal de M sobre D (que sabemos que es 0), incluso aunque hayamos “condicionado” por el collider.\n\nprecis(flbi_collider)\n\n               mean         sd        5.5%      94.5%     n_eff     Rhat4\ncd      3.968243739 0.04611450  3.89560945 4.04327110  8248.105 1.0004150\ncm      2.934145831 0.04687289  2.85989505 3.00919165  8890.321 1.0001715\nm      -0.009309562 0.02249104 -0.04553421 0.02674973  8503.878 1.0000069\nb       1.962677924 0.04345328  1.89429945 2.03282660  7107.343 0.9999206\na3      0.160077627 0.09126337  0.01252729 0.30475624  7756.140 1.0006626\na2      0.060140941 0.04359355 -0.01003761 0.13036986  7499.209 0.9996584\na1      0.025057624 0.03711509 -0.03519740 0.08448685  7396.713 1.0005212\ncsigma  2.064163930 0.04708394  1.99060945 2.14121220 10185.107 1.0002583\ntau     0.984744502 0.02189981  0.95050602 1.01991110  8571.627 0.9998603\nsigma   0.968314337 0.02180198  0.93405345 1.00380110  9457.782 1.0001085\np       0.482931190 0.01117531  0.46538884 0.50083727 10342.731 0.9998451\n\npost_with_collider <- extract.samples(flbi_collider)\n\n\nquantile(post_with_collider$m)\n\n          0%          25%          50%          75%         100% \n-0.095140500 -0.024429525 -0.009353655  0.005972277  0.072001900 \n\nplot(bayestestR::hdi(post_with_collider$m, ci = c( 0.80, 0.95))) +\n  labs(title = \"Estimación con collider\")\n\n\n\n\n\n\n\n\nAsí, que siendo “pluralista”, estimar el dag completo nos puede ayudar en situaciones dónde el backdoor criterio nos diga que no se puede estimar el efecto causal."
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "",
    "text": "Voy a poner un ejemplo de como utilizar solvers para investigación operativa dentro de R.\nTenemos la siguiente información: * Listado de códigos postales de España con la longitud y latitud del centroide del polígono. * Listado de códigos postales de la ubicación de las sedes de una empresa. * En la empresa hay jefes e indios, no es necesario que haya un jefe por sede.\nSe quiere, para cada provincia de España\n\nAsignar cada código postal de esa provincia a un empleado de la empres (jefe o indio).\nUn mismo código postal no puede estar asignado a más de un empleado.\nEn la medida de lo posible asignar a los empleados los códigos postales más cercanos al lugar de su sede.\nA igualdad de distancia entre un código postal y una sede, se debería asignar ese código postal a un indio.\nNingún indio debe tener asignados menos códigos postales que ningún jefe.\nLos jefes como máximo han de tener 7 códigos postales asignados.\nLos indios como mínimo han de tener 3 códigos postales asignados.\nNo puede haber ningún empleado que esté “desasignado”.\n\nDados estos requisitos debería plantear como es la definición del problema, pero no tengo ganas de ponerme a escribir fórmulas en latex, así que en vez de eso voy a utilizar unos datos simulados y directamente al código.."
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#carga-de-datos-y-crear-datos-ficticios.",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#carga-de-datos-y-crear-datos-ficticios.",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "Carga de datos y crear datos ficticios.",
    "text": "Carga de datos y crear datos ficticios.\n\nCarga códigos postales\nCasualmente, tengo por mi pc un shapefile algo antiguo (de cuando está capa estaba en cartociudad) con la capa de códigos postales de España, la cual si se quiere actualizada vale un dinerillo. correos, 6000 Euros la versión sin actualizaciones.. Bueno, si hacienda y correos somos todos me gustaría al menos poder utilizar esto actualizado sin que me cueste 6k.\nVamos a cargar la capa, obtener los centroides, pasar la geometría a longitud y latitud\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(sf)\n## Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\ncod_postales_raw <- readRDS(here::here(\"data/cp_boundaries.rds\")) %>%\n  select(-cp_num, -cp_2_num)\n\nhead(cod_postales_raw)\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -1536953 ymin: 3373964 xmax: -41802.13 ymax: 5247186\n## Projected CRS: WGS 84 / Pseudo-Mercator\n##      cp cp_2   area_m2                       geometry\n## 1 35560   35 187875455 MULTIPOLYGON (((-1518970 33...\n## 2 27330   27   6659413 MULTIPOLYGON (((-821864.3 5...\n## 3 46680   46  69190773 MULTIPOLYGON (((-51610.46 4...\n## 4 49706   49  90229134 MULTIPOLYGON (((-641488.4 5...\n## 5 21120   21  20068648 MULTIPOLYGON (((-776955.2 4...\n## 6 16623   16 132859998 MULTIPOLYGON (((-256256.7 4...\nPintamos algunos códigos\nplot(st_geometry(cod_postales_raw[1:2000, ]))\n\nPara obtener los centroides, usamos la función st_centroid y pasamos la capa de polígonos a una de puntos\ncod_postales_raw <- st_centroid(cod_postales_raw)\n## Warning in st_centroid.sf(cod_postales_raw): st_centroid assumes attributes are\n## constant over geometries of x\nhead(cod_postales_raw)\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -1525406 ymin: 3382025 xmax: -47782.92 ymax: 5245455\n## Projected CRS: WGS 84 / Pseudo-Mercator\n##      cp cp_2   area_m2                  geometry\n## 1 35560   35 187875455  POINT (-1525406 3382025)\n## 2 27330   27   6659413 POINT (-823274.9 5245455)\n## 3 46680   46  69190773 POINT (-47782.92 4752325)\n## 4 49706   49  90229134 POINT (-637415.5 5057096)\n## 5 21120   21  20068648 POINT (-778872.1 4479315)\n## 6 16623   16 132859998 POINT (-262034.3 4818194)\nplot(st_geometry(cod_postales_raw[1:2000, ]), cex = 0.2)\n\nAhora extraemos de la geometría la longitud y latitud. Para eso hay que transformar la geometría.\ncod_postales_raw <- cod_postales_raw %>%\n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\ncod_postales <- cod_postales_raw %>%\n  mutate(\n    centroide_longitud = unlist(map(geometry, 1)),\n    centroide_latitud = unlist(map(geometry, 2))\n  ) %>%\n  st_drop_geometry() %>% # quitamos la geometría y nos quedamos solo con la longitud y latitud\n  rename(\n    cod_postal = cp,\n    cod_prov = cp_2\n  ) %>%\n  filter(!is.na(centroide_longitud)) # tenía un polígono con NAS\n\nhead(cod_postales)\n##   cod_postal cod_prov   area_m2 centroide_longitud centroide_latitud\n## 1      35560       35 187875455        -13.7029565          29.05011\n## 2      27330       27   6659413         -7.3956047          42.56144\n## 3      46680       46  69190773         -0.4292412          39.21368\n## 4      49706       49  90229134         -5.7260007          41.30272\n## 5      21120       21  20068648         -6.9967272          37.28791\n## 6      16623       16 132859998         -2.3538946          39.67063\nPor otro lado me interesa añadir el literal de provincia, tengo una tabla extraída del INE con la correspondencia entre cod_prov y el literal\nprovincia <- read_csv(here::here(\"data/codprov.csv\"))\n## Rows: 52 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (2): CODIGO, LITERAL\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(provincia)\n## # A tibble: 6 × 2\n##   CODIGO LITERAL         \n##   <chr>  <chr>           \n## 1 02     Albacete        \n## 2 03     Alicante/Alacant\n## 3 04     Almería         \n## 4 01     Araba/Álava     \n## 5 33     Asturias        \n## 6 05     Ávila\nNormalizo a mayúsculas y sin tildes y se lo pego a los códigos postales\nprovincia <- provincia %>%\n  mutate(provincia = toupper(stringi::stri_trans_general(LITERAL, \"Latin-ASCII\")))\n\ncod_postales <- cod_postales %>%\n  left_join(provincia %>%\n    select(\n      CODIGO,\n      provincia\n    ),\n  by = c(\"cod_prov\" = \"CODIGO\")\n  )\n\ndim(cod_postales)\n## [1] 10808     6\nhead(cod_postales %>%\n  select(provincia, cod_prov, everything()))\n##           provincia cod_prov cod_postal   area_m2 centroide_longitud\n## 1       PALMAS, LAS       35      35560 187875455        -13.7029565\n## 2              LUGO       27      27330   6659413         -7.3956047\n## 3 VALENCIA/VALENCIA       46      46680  69190773         -0.4292412\n## 4            ZAMORA       49      49706  90229134         -5.7260007\n## 5            HUELVA       21      21120  20068648         -6.9967272\n## 6            CUENCA       16      16623 132859998         -2.3538946\n##   centroide_latitud\n## 1          29.05011\n## 2          42.56144\n## 3          39.21368\n## 4          41.30272\n## 5          37.28791\n## 6          39.67063\n\n\nDatos ficticios de las sedes de las empresas\nLo que voy a hacer es seleccionar aleatoriamente un número de códigos postales en cada provincia, que serán las sedes de la empresa. En cada provincia pongo al menos a un empleado de tipo = “jefe”. Luego, reparto de forma aleatoria entre los códigos postales que han sido elegidos como sedes otros 120 jefes y 480 indios.\nset.seed(155)\n\n## En cada provincia nos quedamos con  un 6% de códigos postales\n\nsedes_alea <- cod_postales %>%\n  group_by(provincia) %>%\n  slice_sample(prop = 0.06)\n\n\n\n# en cada provincia al menos un jefe y resto de empleados de forma aleatoria, en las diferentes sedes elegidas\n\npersonal <- bind_rows(\n  sedes_alea %>%\n    select(provincia, cod_postal) %>%\n    group_by(provincia) %>%\n    slice_sample(n = 1) %>%\n    ungroup() %>%\n    select(cod_postal) %>%\n    mutate(tipo = \"jefe\"),\n  tibble(\n    tipo = c(rep(\"jefe\", 120), rep(\"indio\", 360)),\n    cod_postal = sample(sedes_alea$cod_postal, size = 480, replace = TRUE)\n  )\n)\nCreamos data set sedes\nsedes <- personal %>%\n  left_join(sedes_alea)\n## Joining, by = \"cod_postal\"\ndim(sedes)\n## [1] 530   7\nsample_n(sedes, 7)\n## # A tibble: 7 × 7\n##   cod_postal tipo  cod_prov    area_m2 centroide_longitud centroide_la…¹ provi…²\n##   <fct>      <chr> <chr>         <dbl>              <dbl>          <dbl> <chr>  \n## 1 37660      indio 37        36448279.              -5.99           40.5 SALAMA…\n## 2 41770      jefe  41       183345907.              -5.55           37.0 SEVILLA\n## 3 08011      jefe  08          968836.               2.16           41.4 BARCEL…\n## 4 34479      indio 34        49891663.              -4.42           42.4 PALENC…\n## 5 34859      indio 34       118812672.              -4.59           42.8 PALENC…\n## 6 09348      indio 09       249695400.              -3.61           42.0 BURGOS \n## 7 29750      indio 29        14389642.              -4.04           36.8 MALAGA \n## # … with abbreviated variable names ¹​centroide_latitud, ²​provincia"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-en-granada",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-en-granada",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "IO al servicio del mal en GRANADA",
    "text": "IO al servicio del mal en GRANADA\nComo ejemplo, vamos a ver como sería para Granada\ncod_postales_granada <- cod_postales %>%\n  filter(provincia == \"GRANADA\") %>%\n  mutate(id = row_number())\n\nsedes_granada <- sedes %>%\n  filter(provincia == \"GRANADA\") %>%\n  arrange(desc(tipo)) %>%\n  mutate(id_sede = row_number())\n\nsedes_granada\n## # A tibble: 11 × 8\n##    cod_postal tipo  cod_prov    area_m2 centroide_long…¹ centr…² provi…³ id_sede\n##    <fct>      <chr> <chr>         <dbl>            <dbl>   <dbl> <chr>     <int>\n##  1 18328      jefe  18        58574459.            -3.87    37.2 GRANADA       1\n##  2 18006      jefe  18         3245912.            -3.61    37.2 GRANADA       2\n##  3 18516      jefe  18       146541813.            -3.24    37.2 GRANADA       3\n##  4 18516      jefe  18       146541813.            -3.24    37.2 GRANADA       4\n##  5 18197      indio 18        10003524.            -3.61    37.2 GRANADA       5\n##  6 18516      indio 18       146541813.            -3.24    37.2 GRANADA       6\n##  7 18414      indio 18        40411565.            -3.34    36.9 GRANADA       7\n##  8 18197      indio 18        10003524.            -3.61    37.2 GRANADA       8\n##  9 18369      indio 18        17670871.            -4.01    37.2 GRANADA       9\n## 10 18611      indio 18        33542783.            -3.60    36.8 GRANADA      10\n## 11 18514      indio 18       110524485.            -3.08    37.2 GRANADA      11\n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nEs importante haber ordenado por tipo , porque vamos a utilizar el mismo índice j para empleados jefe y empleados indios.\nAhora definimos: * m como el número de empleados en las sedes de Granada * n como el número de códigos postales a asignar en Granada * n_sedes como el número de sedes * njefes como el número de jefes * n_indios como el número de indios\nm <- nrow(sedes_granada)\nn <- nrow(cod_postales_granada)\nn_sedes <- length(unique(sedes_granada$cod_postal))\n\nnjefes <- sedes_granada %>%\n  filter(tipo == \"jefe\") %>%\n  count() %>%\n  pull(n)\n\nn_indios <- m - njefes\nNecesitamos definir una función de distancia entre los códigos postales a asignar y las sedes. Para eso usamos la distancia Haversine que está implementada en la librería geosphere. Y aquí ya introducimos uno de los requerimientos. Básicamente aumentamos la distancia un 10% si el empleado es un jefe, de forma que sea peor asignarle ese código postal al jefe en términos de minimizar el total de distancias.\ntransportcost_granada <- function(i, j) {\n  cliente <- cod_postales_granada[i, ]\n  comercial <- sedes_granada[j, ]\n  distancia <-\n    geosphere::distHaversine(\n      c(cliente$centroide_longitud, cliente$centroide_latitud),\n      c(comercial$centroide_longitud, comercial$centroide_latitud)\n    )\n\n  if (comercial[, \"tipo\"] == \"jefe\") {\n    distancia <- distancia * 1.1\n  }\n\n  return(distancia / 1000) # devolvemos la disancia en km\n}\n\n# distancia entre sede 1 y empleado 3\ntransportcost_granada(1, 3)\n## [1] 51.54738\nPintamos los códigos postales y las sedes. Los granadinos reconoceremos la forma de la provincia.\np <-\n  ggplot(\n    cod_postales_granada,\n    aes(centroide_longitud, centroide_latitud)\n  ) +\n  geom_point(size = rel(2), shape = 4) +\n  geom_point(\n    data = sedes_granada,\n    size = rel(3),\n    color = \"darkorange\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank()\n  )\np + ggtitle(\"Sin asignar\")"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#optimización",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#optimización",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "Optimización",
    "text": "Optimización\nPara optimizar el problema vamos a usar la librería ompr que permite plantear el problema de optimización lineal entera de forma sencilla, y se conecta a la librería ROI que es la que al final llama al solver. Como solver vamos a utilizar glpk que es software libre y lo suficientemente bueno para este ejemplo.\nlibrary(ompr)\nlibrary(ompr.roi)\nlibrary(ROI.plugin.glpk)\nlibrary(patchwork) # pa unir los ggplots resultantes\nDefinimos el modelo\nmip_model_granada <- MIPModel() %>%\n  # variable indicadora que indica si una tienda i se asigna a comercial j\n  add_variable(x[i, j], i = 1:n, j = 1:m, type = \"binary\") %>%\n  # Minimizar el objetivo de distancia\n  set_objective(sum_over(transportcost_granada(i, j) * x[i, j], i = 1:n, j = 1:m), \"min\") %>%\n  # cada tienda (código postal) solo debe ir a un comerciial. el comercial puede atender varios\n  add_constraint(sum_over(x[i, j], j = 1:m) == 1, i = 1:n) %>%\n  # todo el mundo tiene que atender al minimo a una tienda\n  add_constraint(sum_over(x[i, j], i = 1:n) >= 1, j = 1:m) %>%\n  #   Los jefes curran menos, como máximo 7 tiendas\n  add_constraint(sum_over(x[i, j], i = 1:n) <= 7, j = 1:njefes) %>%\n  #\n  # # Los indios al menos atienden a 3 tiendas\n  add_constraint(sum_over(x[i, j], i = 1:n) >= 3, j = (njefes + 1):m) %>%\n  # para no sobrecargar mucho a los indios, les pongo un máximo que sea 1.5 veces el núemro de tiendas entre total currantes (jefes + indios)\n  add_constraint(sum_over(x[i, j], i = 1:n) <= round(1.5 * n / m), j = (njefes + 1):m) %>%\n  add_constraint(sum_over(x[i, j], i = 1:n) >= sum_over(x[i, k], i = 1:n), j = (njefes + 1):m, k = 1:njefes)\nAlgunas aclaraciones de la sintaxis anterior.\n\nNuestra variable auxilizar es \\(X_{i,j}\\) dónde la i son los códigos postales y la j cada empleado.\nSe trata de minimizar la suma total de distancias cuando se asigna un código postal a un empleado, para todos los códigos postales y todos los empleados.\nLa restricción add_constraint(sum_over(x[i, j], j = 1:m) == 1  , i = 1:n) si nos fijamos en el sum_over significa sumar en j (empleados) para cada código postal (i) y que esa suma valga 1. Es decir, para cada código postal (i) sólo se permite que sea asignado a un empleado\nadd_constraint(sum_over(x[i, j], i = 1:n) >= 1  , j = 1:m) Que para cada empleado (j) la suma de todos los códigos postales que se le asignen sea mayor o igual que 1. Vamos que no se quede ninguno ocioso.\nadd_constraint(sum_over( x[i,j], i = 1:n)  <= 7, j = 1:njefes) por eso ordeanmos por tipo para que el índice 1:njefes corresponda a los empleados jefes, esta restricción asegura que a un jefe no se le asignen más de 7 códigos postales.\nadd_constraint(sum_over( x[i,j], i = 1:n)  >=  3 , j = (njefes +1):m) Mínimo 3 códigos postales para los indios.\nadd_constraint(sum_over( x[i,j], i = 1:n)  <=  round(1.5 * n/m) , j = (njefes +1):m) Esta restricción intenta equilibrar el número de asignaciones para los indios, de forma que como mucho a un empleado tenga 1.5 veces la media de códigos postales por empleado.\nadd_constraint(sum_over( x[i,j], i = 1:n)  >=  sum_over( x[i,k], i = 1:n) , j = (njefes +1):m, k = 1:njefes) En esta restricción es dónde aseguramos que ningún empleado tenga menos asignaciones que ningún jefe, por eso se ha usado el índice k.\n\nPues el problema tiene 2200 variables (todas binarias) y 257 restricciones.\nmip_model_granada\n## Mixed integer linear optimization problem\n## Variables:\n##   Continuous: 0 \n##   Integer: 0 \n##   Binary: 2200 \n## Model sense: minimize \n## Constraints: 257\nResolvemos con glpk\nresult_granada <- solve_model(mip_model_granada, with_ROI(solver = \"glpk\", verbose = TRUE))\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 257 rows, 2200 columns, 19200 non-zeros\n##       0: obj =   0.000000000e+00 inf =   2.320e+02 (218)\n##     397: obj =   9.427540716e+03 inf =   5.627e-13 (0) 1\n## *   870: obj =   3.723682515e+03 inf =   0.000e+00 (0) 2\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 257 rows, 2200 columns, 19200 non-zeros\n## 2200 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +   870: mip =     not found yet >=              -inf        (1; 0)\n## +   870: >>>>>   3.723682515e+03 >=   3.723682515e+03   0.0% (1; 0)\n## +   870: mip =   3.723682515e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\nresult_granada\n## Status: success\n## Objective value: 3723.683\nY ahora procedemos a ver las asignaciones. Para eso utilizamos la función get_solution que nos va a devolver la solución obtenida para nuestra variable \\(X_{i,j}\\)\nmatching <- result_granada %>%\n  get_solution(x[i, j]) %>%\n  select(i, j, value) %>%\n  filter(value > 0) # nons quedamos con las asignaciones\n\nmatching\n##       i  j value\n## 1    16  1     1\n## 2    27  1     1\n## 3    55  1     1\n## 4    68  1     1\n## 5   119  1     1\n## 6   157  1     1\n## 7   173  1     1\n## 8    13  2     1\n## 9    37  2     1\n## 10   96  2     1\n## 11  113  2     1\n## 12  161  2     1\n## 13  169  2     1\n## 14  178  2     1\n## 15  141  3     1\n## 16   34  4     1\n## 17    1  5     1\n## 18    8  5     1\n## 19   23  5     1\n## 20   30  5     1\n## 21   32  5     1\n## 22   71  5     1\n## 23   98  5     1\n## 24  102  5     1\n## 25  108  5     1\n## 26  112  5     1\n## 27  117  5     1\n## 28  120  5     1\n## 29  122  5     1\n## 30  124  5     1\n## 31  130  5     1\n## 32  132  5     1\n## 33  134  5     1\n## 34  137  5     1\n## 35  138  5     1\n## 36  140  5     1\n## 37  149  5     1\n## 38  170  5     1\n## 39  182  5     1\n## 40  191  5     1\n## 41  192  5     1\n## 42  194  5     1\n## 43  198  5     1\n## 44   10  6     1\n## 45   15  6     1\n## 46   20  6     1\n## 47   65  6     1\n## 48   69  6     1\n## 49   82  6     1\n## 50   83  6     1\n## 51   86  6     1\n## 52   87  6     1\n## 53   92  6     1\n## 54   93  6     1\n## 55  116  6     1\n## 56  128  6     1\n## 57  133  6     1\n## 58  135  6     1\n## 59  144  6     1\n## 60  151  6     1\n## 61  153  6     1\n## 62  163  6     1\n## 63  168  6     1\n## 64  174  6     1\n## 65  177  6     1\n## 66  190  6     1\n## 67  199  6     1\n## 68    2  7     1\n## 69    5  7     1\n## 70    6  7     1\n## 71    7  7     1\n## 72   11  7     1\n## 73   12  7     1\n## 74   17  7     1\n## 75   24  7     1\n## 76   26  7     1\n## 77   28  7     1\n## 78   31  7     1\n## 79   44  7     1\n## 80   48  7     1\n## 81   53  7     1\n## 82   56  7     1\n## 83   72  7     1\n## 84   77  7     1\n## 85   91  7     1\n## 86  104  7     1\n## 87  105  7     1\n## 88  131  7     1\n## 89  147  7     1\n## 90  156  7     1\n## 91  166  7     1\n## 92  171  7     1\n## 93  187  7     1\n## 94  193  7     1\n## 95   14  8     1\n## 96   39  8     1\n## 97   40  8     1\n## 98   47  8     1\n## 99   54  8     1\n## 100  59  8     1\n## 101  60  8     1\n## 102  62  8     1\n## 103  70  8     1\n## 104  73  8     1\n## 105  75  8     1\n## 106  78  8     1\n## 107  79  8     1\n## 108  84  8     1\n## 109  85  8     1\n## 110  90  8     1\n## 111  97  8     1\n## 112  99  8     1\n## 113 101  8     1\n## 114 109  8     1\n## 115 110  8     1\n## 116 118  8     1\n## 117 126  8     1\n## 118 167  8     1\n## 119 185  8     1\n## 120 189  8     1\n## 121 195  8     1\n## 122   9  9     1\n## 123  25  9     1\n## 124  29  9     1\n## 125  33  9     1\n## 126  35  9     1\n## 127  46  9     1\n## 128  50  9     1\n## 129  51  9     1\n## 130  57  9     1\n## 131  63  9     1\n## 132  67  9     1\n## 133  74  9     1\n## 134  80  9     1\n## 135  88  9     1\n## 136 103  9     1\n## 137 107  9     1\n## 138 111  9     1\n## 139 114  9     1\n## 140 115  9     1\n## 141 125  9     1\n## 142 136  9     1\n## 143 162  9     1\n## 144 172  9     1\n## 145 175  9     1\n## 146 179  9     1\n## 147 180  9     1\n## 148 196  9     1\n## 149   3 10     1\n## 150   4 10     1\n## 151  22 10     1\n## 152  36 10     1\n## 153  38 10     1\n## 154  45 10     1\n## 155  49 10     1\n## 156  61 10     1\n## 157  64 10     1\n## 158  76 10     1\n## 159  89 10     1\n## 160 106 10     1\n## 161 127 10     1\n## 162 129 10     1\n## 163 139 10     1\n## 164 143 10     1\n## 165 148 10     1\n## 166 152 10     1\n## 167 154 10     1\n## 168 155 10     1\n## 169 159 10     1\n## 170 176 10     1\n## 171 181 10     1\n## 172 183 10     1\n## 173 186 10     1\n## 174  18 11     1\n## 175  19 11     1\n## 176  21 11     1\n## 177  41 11     1\n## 178  42 11     1\n## 179  43 11     1\n## 180  52 11     1\n## 181  58 11     1\n## 182  66 11     1\n## 183  81 11     1\n## 184  94 11     1\n## 185  95 11     1\n## 186 100 11     1\n## 187 121 11     1\n## 188 123 11     1\n## 189 142 11     1\n## 190 145 11     1\n## 191 146 11     1\n## 192 150 11     1\n## 193 158 11     1\n## 194 160 11     1\n## 195 164 11     1\n## 196 165 11     1\n## 197 184 11     1\n## 198 188 11     1\n## 199 197 11     1\n## 200 200 11     1\nAhora vemos cuántas asignaciones tiene cada empleado y pintamos los resultados\nasignaciones <- matching %>%\n  group_by(j) %>%\n  summarise(asignaciones = sum(value)) %>%\n  arrange(desc(asignaciones)) %>%\n  left_join(sedes_granada, by = c(\"j\" = \"id_sede\"))\n\nasignaciones\n## # A tibble: 11 × 9\n##        j asignaciones cod_postal tipo  cod_prov  area_m2 centr…¹ centr…² provi…³\n##    <int>        <dbl> <fct>      <chr> <chr>       <dbl>   <dbl>   <dbl> <chr>  \n##  1     5           27 18197      indio 18         1.00e7   -3.61    37.2 GRANADA\n##  2     7           27 18414      indio 18         4.04e7   -3.34    36.9 GRANADA\n##  3     8           27 18197      indio 18         1.00e7   -3.61    37.2 GRANADA\n##  4     9           27 18369      indio 18         1.77e7   -4.01    37.2 GRANADA\n##  5    11           27 18514      indio 18         1.11e8   -3.08    37.2 GRANADA\n##  6    10           25 18611      indio 18         3.35e7   -3.60    36.8 GRANADA\n##  7     6           24 18516      indio 18         1.47e8   -3.24    37.2 GRANADA\n##  8     1            7 18328      jefe  18         5.86e7   -3.87    37.2 GRANADA\n##  9     2            7 18006      jefe  18         3.25e6   -3.61    37.2 GRANADA\n## 10     3            1 18516      jefe  18         1.47e8   -3.24    37.2 GRANADA\n## 11     4            1 18516      jefe  18         1.47e8   -3.24    37.2 GRANADA\n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nplot_assignment <- matching %>%\n  inner_join(cod_postales_granada, by = c(\"i\" = \"id\")) %>%\n  inner_join(sedes_granada, by = c(\"j\" = \"id_sede\"), suffix = c(\"_clientes\", \"_comerciales\"))\n\n\n\n\n\np_jefes <- p +\n  geom_segment(\n    data = plot_assignment %>%\n      filter(tipo == \"jefe\"),\n    aes(\n      x = centroide_longitud_comerciales,\n      y = centroide_latitud_comerciales,\n      xend = centroide_longitud_clientes,\n      yend = centroide_latitud_clientes\n    )\n  ) +\n  ggtitle(paste0(\"Asignaciones para los jefes\"))\n\n\np_indios <- p +\n  geom_segment(\n    data = plot_assignment %>%\n      filter(tipo == \"indio\"),\n    aes(\n      x = centroide_longitud_comerciales,\n      y = centroide_latitud_comerciales,\n      xend = centroide_longitud_clientes,\n      yend = centroide_latitud_clientes\n    )\n  ) +\n  ggtitle(paste0(\"Asignaciones para los indios\"))\n\n\np_or <- p +\n  labs(\n    title = \"sin asignar\",\n    subtitle = \"Granada\"\n  )\np_final <- p_or / p_jefes / p_indios\n\np_final"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-eligiendo-provincia",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-eligiendo-provincia",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "IO al servicio del mal eligiendo provincia",
    "text": "IO al servicio del mal eligiendo provincia\nCreo función (francamente mejorable y modularizable) para poder elegir provincia o provincias\nget_asignaciones_x_provincia <- function(cod_postales = cod_postales, sedes = sedes,\n                                         provincia_sel = \"MADRID\", plot = TRUE, ...) {\n  cod_postales_filt <- cod_postales %>%\n    filter(provincia %in% provincia_sel) %>%\n    mutate(id = row_number())\n\n  sedes_filt <- sedes %>%\n    filter(provincia %in% provincia_sel) %>%\n    arrange(desc(tipo)) %>%\n    mutate(id_sede = row_number())\n\n  m <- nrow(sedes_filt)\n  n <- nrow(cod_postales_filt)\n  n_sedes <- length(unique(sedes_filt$cod_postal))\n\n  njefes <- sedes_filt %>%\n    filter(tipo == \"jefe\") %>%\n    count() %>%\n    pull(n)\n\n  n_indios <- m - njefes\n\n  transportcost <- function(i, j) {\n    cliente <- cod_postales_filt[i, ]\n    comercial <- sedes_filt[j, ]\n    distancia <- geosphere::distHaversine(\n      c(cliente$centroide_longitud, cliente$centroide_latitud),\n      c(comercial$centroide_longitud, comercial$centroide_latitud)\n    )\n\n    if (comercial[, \"tipo\"] == \"jefe\") distancia <- distancia * 1.1\n\n    return(distancia / 1000)\n  }\n\n\n  p <- ggplot(cod_postales_filt, aes(centroide_longitud, centroide_latitud)) +\n    geom_point(size = rel(2), shape = 4) +\n    geom_point(data = sedes_filt, size = rel(3), color = \"darkorange\") +\n    # scale_x_continuous(limits = c(0, grid_size+1)) +\n    # scale_y_continuous(limits = c(0, grid_size+1)) +\n    theme(\n      axis.title = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text = element_blank(), panel.grid = element_blank()\n    )\n\n  mip_model <- MIPModel() %>%\n    # variable indicadora que indica si una tienda i se asigna a comercial j\n    add_variable(x[i, j], i = 1:n, j = 1:m, type = \"binary\") %>%\n    # Minimizar el objetivo de distancia\n    set_objective(sum_over(transportcost(i, j) * x[i, j], i = 1:n, j = 1:m), \"min\") %>%\n    # cada tienda (código postal) solo debe ir a un comerciial. el comercial puede atender varios\n    add_constraint(sum_over(x[i, j], j = 1:m) == 1, i = 1:n) %>%\n    # todo el mundo tiene que atender al minimo a una tienda\n    add_constraint(sum_over(x[i, j], i = 1:n) >= 1, j = 1:m) %>%\n    # %>%\n\n    #   Los jefes curran menos, como máximo 7 tiendas\n    add_constraint(sum_over(x[i, j], i = 1:n) <= 7, j = 1:njefes) %>%\n    #\n    # # Los indios al menos atienden a 3 tiendas\n    add_constraint(sum_over(x[i, j], i = 1:n) >= 3, j = (njefes + 1):m) %>%\n    # para no sobrecargar mucho a los indios, les pongo un máximo que sea 1.5 veces el núemro de tiendas entre total currantes (jefes + indios)\n    add_constraint(sum_over(x[i, j], i = 1:n) <= round(1.5 * n / m), j = (njefes + 1):m) %>%\n    add_constraint(sum_over(x[i, j], i = 1:n) >= sum_over(x[i, k], i = 1:n), j = (njefes + 1):m, k = 1:njefes)\n\n\n  result2 <- solve_model(mip_model, with_ROI(solver = \"glpk\", verbose = TRUE))\n\n\n  matching <- result2 %>%\n    get_solution(x[i, j]) %>%\n    select(i, j, value) %>%\n    filter(value > 0)\n\n\n\n  asignaciones <- matching %>%\n    group_by(j) %>%\n    summarise(asignaciones = sum(value)) %>%\n    arrange(desc(asignaciones)) %>%\n    left_join(sedes_filt, by = c(\"j\" = \"id_sede\"))\n\n\n  plot_assignment <- matching %>%\n    inner_join(cod_postales_filt, by = c(\"i\" = \"id\")) %>%\n    inner_join(sedes_filt, by = c(\"j\" = \"id_sede\"), suffix = c(\"_clientes\", \"_comerciales\"))\n\n\n\n  p_jefes <- p +\n    geom_segment(\n      data = plot_assignment %>%\n        filter(tipo == \"jefe\"),\n      aes(\n        x = centroide_longitud_comerciales,\n        y = centroide_latitud_comerciales,\n        xend = centroide_longitud_clientes,\n        yend = centroide_latitud_clientes\n      )\n    ) +\n    ggtitle(paste0(\"Asignaciones para los jefes\"))\n\n\n  p_indios <- p +\n    geom_segment(\n      data = plot_assignment %>%\n        filter(tipo == \"indio\"),\n      aes(\n        x = centroide_longitud_comerciales,\n        y = centroide_latitud_comerciales,\n        xend = centroide_longitud_clientes,\n        yend = centroide_latitud_clientes\n      )\n    ) +\n    ggtitle(paste0(\"Asignaciones para los indios\"))\n\n  subtitulo <- reduce(provincia_sel, function(x, y) paste(x, y, sep = \"-\"))\n  p_or <- p +\n    labs(\n      title = \"sin asignar\",\n      subtitle = subtitulo\n    )\n  p_final <- p_or / p_jefes / p_indios\n\n  if (plot) print(p_final)\n\n  return(list(\n    comerciales = sedes_filt,\n    cod_postales = cod_postales_filt,\n    matching = matching, tot_asignaciones = asignaciones, plot_final = p_final\n  ))\n}\nY veamos algunos ejemplos.\n\nMADRID\nmadrid <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = \"MADRID\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 385 rows, 4425 columns, 45725 non-zeros\n##       0: obj =   0.000000000e+00 inf =   3.400e+02 (320)\n##     498: obj =   1.415569938e+04 inf =   5.690e-14 (0) 1\n## Perturbing LP to avoid stalling [939]...\n## Removing LP perturbation [1341]...\n## *  1341: obj =   5.881701905e+03 inf =   0.000e+00 (0) 4\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 385 rows, 4425 columns, 45725 non-zeros\n## 4425 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1341: mip =     not found yet >=              -inf        (1; 0)\n## +  1341: >>>>>   5.881701905e+03 >=   5.881701905e+03   0.0% (1; 0)\n## +  1341: mip =   5.881701905e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\nPodemos ver cuántos códigos postales le han tocado a cada empleado.\nSe ve que se cumplen las restricciones. Seguramente para ser más equitativo habría que tocar algo a mano, para que a los empleados indios de la misma sede se repartan mejor los códigos postales. pero como primera aproximación no está mal\nmadrid$tot_asignaciones %>% \n  arrange(cod_postal)\n## # A tibble: 15 × 9\n##        j asignaciones cod_postal tipo  cod_prov  area_m2 centr…¹ centr…² provi…³\n##    <int>        <dbl> <fct>      <chr> <chr>       <dbl>   <dbl>   <dbl> <chr>  \n##  1     2            7 28011      jefe  28         3.03e7   -3.75    40.4 MADRID \n##  2     1            7 28015      jefe  28         2.59e6   -3.71    40.4 MADRID \n##  3     5            7 28015      jefe  28         2.59e6   -3.71    40.4 MADRID \n##  4    12           30 28035      indio 28         2.20e7   -3.74    40.5 MADRID \n##  5    14           30 28213      indio 28         8.52e7   -4.19    40.4 MADRID \n##  6     9           30 28521      indio 28         3.53e7   -3.50    40.3 MADRID \n##  7     7           30 28668      indio 28         3.65e6   -3.84    40.4 MADRID \n##  8    13           30 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n##  9     6           25 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 10     8           17 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 11    11            8 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 12     4            7 28817      jefe  28         6.04e7   -3.26    40.5 MADRID \n## 13    15           30 28901      indio 28         1.62e6   -3.73    40.3 MADRID \n## 14    10           30 28931      indio 28         8.78e5   -3.86    40.3 MADRID \n## 15     3            7 28931      jefe  28         8.78e5   -3.86    40.3 MADRID \n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nPodemos ver el detalle, por ejemplo qué códigos postales le toca al empleado j=4\nmadrid_asignaciones <-  madrid$tot_asignaciones  %>% \n  left_join(madrid$matching, by = \"j\") %>% \n  left_join(madrid$cod_postales, by = c(\"i\" = \"id\"), suffix = c(\"\",\"_tienda\")) \n\nmadrid_asignaciones %>% \n  filter(j==4) %>% \n  select(tipo ,j, i, cod_postal, cod_postal_tienda)\n## # A tibble: 7 × 5\n##   tipo      j     i cod_postal cod_postal_tienda\n##   <chr> <int> <int> <fct>      <fct>            \n## 1 jefe      4    61 28817      28810            \n## 2 jefe      4    71 28817      28812            \n## 3 jefe      4    89 28817      28818            \n## 4 jefe      4   121 28817      28515            \n## 5 jefe      4   155 28817      28804            \n## 6 jefe      4   172 28817      28817            \n## 7 jefe      4   219 28817      28811\n\n\nBarcelona\nbarcelona <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel =\"BARCELONA\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 471 rows, 5715 columns, 59055 non-zeros\n##       0: obj =   0.000000000e+00 inf =   4.260e+02 (406)\n##     600: obj =   1.389502410e+04 inf =   9.258e-13 (0) 1\n## Perturbing LP to avoid stalling [1077]...\n## Removing LP perturbation [1716]...\n## *  1716: obj =   7.841913058e+03 inf =   0.000e+00 (0) 5\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 471 rows, 5715 columns, 59055 non-zeros\n## 5715 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1716: mip =     not found yet >=              -inf        (1; 0)\n## +  1716: >>>>>   7.841913058e+03 >=   7.841913058e+03   0.0% (1; 0)\n## +  1716: mip =   7.841913058e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\n\n\nSevilla\nsevilla <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = \"SEVILLA\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 182 rows, 1064 columns, 7448 non-zeros\n##       0: obj =   0.000000000e+00 inf =   1.710e+02 (163)\n##     243: obj =   8.653234667e+03 inf =   5.145e-13 (0)\n## *   572: obj =   3.623165871e+03 inf =   0.000e+00 (0) 1\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 182 rows, 1064 columns, 7448 non-zeros\n## 1064 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +   572: mip =     not found yet >=              -inf        (1; 0)\n## +   572: >>>>>   3.623165871e+03 >=   3.623165871e+03   0.0% (1; 0)\n## +   572: mip =   3.623165871e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\n\n\nGranada y Málaga juntas\ngranada_malaga <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = c(\"GRANADA\",\"MALAGA\"))\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 488 rows, 7160 columns, 80550 non-zeros\n##       0: obj =   0.000000000e+00 inf =   4.230e+02 (393)\n##     515: obj =   2.754380624e+04 inf =   4.807e-13 (0) 1\n## Perturbing LP to avoid stalling [1388]...\n## Removing LP perturbation [1688]...\n## *  1688: obj =   7.728634950e+03 inf =   0.000e+00 (0) 5\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 488 rows, 7160 columns, 80550 non-zeros\n## 7160 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1688: mip =     not found yet >=              -inf        (1; 0)\n## +  1688: >>>>>   7.728634950e+03 >=   7.728634950e+03   0.0% (1; 0)\n## +  1688: mip =   7.728634950e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\nY hasta aquí ha llegado el uso de la IO para el mal. Feliz verano !!"
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "",
    "text": "Tengo una relación extraña con Julia, por un lado me gusta bastante y por otro me parece que aún le falta algo para que lo adopte de forma más seria. Quizá tenga que ver con mi forma de aprender (que seguro que no es óptima), en vez de irme a los tutoriales típicos, me voy directamente a ver cómo se hace algo que me interesa. En este caso hacer modelos bayesianos con Julia usando Turing.\nTuring es una librería escrita en Julia para programación probabilística, podría considerarse como un competidor de Stan, aunque todavía es una librería joven. Turing añade sólo una pequeña capa de programación probabilística, y promete cosas como modelos de redes neuronales dónde los pesos sigan una distribución probabilística\nNo me voy a meter en esos lares, yo soy más prosaico y por el momento sólo quiero ejemplificar con Turing el modelo que cuento en pluralista.\nRecordemos que habías simulado unos datos tal que así.\nEn la simulación se ha forzado que el efecto del número de hijos de la madre (M) sobre el número de hijos de la hija (D) sea cero.\nEl DAG era algo así. En este dag para estimar el efecto de M sobre D, hace falta condicionar por U, pero al ser una variable de confusión no observada, no habría forma de estimarlo de la forma tradicional (a lo Pearl). La solución es estimar el DAG completo."
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#ajuste-en-turing",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#ajuste-en-turing",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Ajuste en Turing",
    "text": "Ajuste en Turing\nRecordemos que nuestra U es una variable que no tenemos, se podría asimilar a una variable con todos sus valores perdidos y cada uno de esos valores perdidos es un parámetro a estimar.\nLibrerías : Aparte de Turing, hace falta ReverseDiff (diferenciación automática) y alguna más.\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nLeo los datos simulados que había guardado en un csv previamente\n\npl = DataFrame(CSV.File(\"data/pluralista.csv\"))\ndescribe(pl)\njulia> describe(pl)\n4×7 DataFrame\n Row │ variable  mean     min       median    max      nmissing  eltype   \n     │ Symbol    Float64  Real      Float64   Real     Int64     DataType \n─────┼────────────────────────────────────────────────────────────────────\n   1 │ D         1.00621  -3.55365  0.986136  6.03293         0  Float64\n   2 │ M         1.00836  -3.91626  0.90395   6.69591         0  Float64\n   3 │ B1        0.473     0        0.0       1               0  Int64\n   4 │ B2        0.487     0        0.0       1               0  Int64\nNos construimos el modelo con Turing.\nAlgunas cosas a comentar.\n\nEl uso de filldist para crear el vector de U y que cada valor siga una Normal(0,1).\n.+ para sumar un escalar como a1 con un vector. El uso del “.operacion” es habitual en julia para hacer broadcast.\nMvNormal al final. Esto lo he leído por ahí para que haga mejor el sampleo.\nAl igual que en Stan se tiene que escribir en cierto orden (y si no no funciona bien) porque Turing no es declarativo.\n\n@model function pluralista(D, M, B1, B2)\n\n    N = Int(length(D))\n\n    # Variable no observada\n    U ~ filldist(Normal(0, 1), N)\n\n\n    # Prior coeficientes\n    a1 ~ Normal(0, 0.5)\n    a2 ~ Normal(0, 0.5)\n    m  ~ Normal(0, 0.5)\n    b  ~ Normal(0, 0.5)\n    p  ~ Beta(2,2)\n    \n    \n    k ~  Exponential(1)\n    σ₁ ~ Exponential(1)\n    σ₂ ~ Exponential(1)\n    \n    B1 ~ Bernoulli(p)\n    B2 ~ Bernoulli(p)\n    \n    #  transformed parameters\n    mu1 = a1 .+ b * B1 + k * U\n    mu2 = a2 .+ b * B2 + m * M + k * U\n    \n    # likelihood\n\n\n    M ~ MvNormal(mu1, σ₁ * I) \n    D ~ MvNormal(mu2, σ₂ * I)\n\nend\nComparando con el código del mismo modelo en Stan (al final del post) se observa que la sintaxis es parecida."
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#muestreo-de-la-posterior-en-turing",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#muestreo-de-la-posterior-en-turing",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Muestreo de la posterior en Turing",
    "text": "Muestreo de la posterior en Turing\nHay que usar reversediff porque si no no acaba nunca.\nRandom.seed!(155)\n\n\nTuring.setadbackend(:reversediff)\nTuring.setrdcache(true)\n\nflbi = sample(\n    pluralista(pl.D, pl.M, pl.B1, pl.B2), \n    NUTS(1000, 0.65),\n    MCMCThreads(),\n    2_000, 4)\njulia> flbi = sample(\n           pluralista(pl.D, pl.M, pl.B1, pl.B2), \n           NUTS(1000, 0.65),\n           MCMCThreads(),\n           2_000, 4)\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.025\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n\nChains MCMC chain (2000×1020×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 136.29 seconds\nCompute duration  = 510.14 seconds\nY ha tardado unos 2 minutos por cadena. Ciertamente no está mal, pero no se acerca a la velocidad de Stan, que lo hace en unos 18 segundos.\nY podemos extraer un resumen de los parámetros que nos interesan con\njulia> summarize(flbi[[:a1, :a2, :b, :m, :σ₁, :σ₂]])\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n          a1    0.0682    0.0538     0.0006    0.0009   3268.9064    1.0007        6.4079\n          a2    0.0326    0.0759     0.0008    0.0024   1015.7923    1.0059        1.9912\n           m    0.0063    0.0430     0.0005    0.0018    554.1348    1.0096        1.0862\n           b    1.9865    0.0593     0.0007    0.0012   2403.5462    1.0008        4.7116\n          σ₁    1.1427    0.1205     0.0013    0.0049    535.2307    1.0086        1.0492\n          σ₂    0.9621    0.0719     0.0008    0.0016   2496.8176    1.0009        4.8944\n          \nY efectivamente, lo ha hecho bien y ha recuperado los verdaderos valores de los parámetros y estimado que el efecto de M sobre D es 0.\nmyplot = plot(flbi[[:a1, :a2, :b, :m, :σ₁, :σ₂]])\n\nsavefig(myplot,\"plurarlista_turing.png\")\n\n\n\nimagen"
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#reflexiones.",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#reflexiones.",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Reflexiones.",
    "text": "Reflexiones.\n\nMe ha parecido fácil escribir un modelo bayesiano como este en Turing\nNo he conseguido ver como hacer que me funcione un predict sobre nuevos datos que tengan B1 y B2, pero no M y D. Cuestión de empezar más poco a poco con los tutoriales que hay por ahí.\nPor el momento parece que Stan sigue siendo el estado del arte en estas cosas, aunque lo de integrar Turing con Flux por ejemplo, promete.\n\nMismo modelo en Stan.\n\ndata{\n    int N;\n    vector[N] D;\n    vector[N] M;\n    int B1[N];\n    int B2[N];\n}\n\n\nparameters{\n    vector[N] U;\n    real m;\n    real b;\n    real a2;\n    real a1;\n    real<lower=0> tau;\n    real<lower=0> sigma;\n    real<lower=0> k;\n    real<lower=0,upper=1> p;\n}\n\ntransformed parameters {\n  vector[N] nu;\n  vector[N] mu;\n\n\n  for ( i in 1:N ) {\n        nu[i] = a2 + b * B2[i] + m * M[i] + k * U[i];\n    }\n    \n  for ( i in 1:N ) {\n        mu[i] = a1 + b * B1[i] + k * U[i];\n    }\n\n\n}\n\nmodel{\n    \n    U ~ normal( 0 , 1 );\n    \n    a1 ~ normal( 0 , 0.5 );\n    a2 ~ normal( 0 , 0.5 );\n    m  ~ normal( 0 , 0.5 );\n    b  ~ normal( 0 , 0.5 );\n    p  ~ beta( 2 , 2 );\n    \n    k ~ exponential( 1 );\n    sigma ~ exponential( 1 );\n    tau   ~ exponential( 1 );\n    B2    ~ bernoulli( p );\n    B1    ~ bernoulli( p );\n\n    D ~ normal( nu , tau );\n    M ~ normal( mu , sigma );\n}\n\n// genero point_loglikelihood, util para evaluar modelo con psis loo\ngenerated quantities {\n vector[N] log_lik_D;\n vector[N] log_lik_M;\n\n  for (i in 1:N)\n    log_lik_D[i] = normal_lpdf(D[i] | nu[i], tau);\n\n  for (i in 1:N)\n    log_lik_M[i] = normal_lpdf(M[i] | mu[i], sigma);\n\n\n  }"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html",
    "href": "2022/09/18/veloooosidad/index.html",
    "title": "Veeelooosidad",
    "section": "",
    "text": "No, este post no va sobre la canción de Medina Azahara sino de comparar un par de librerías para lectura y procesamiento de datos. A saber, polars escrita en Rust y con api en python versus vroom en combinación con librerías como data.table o collapse en R. Estas últimas usan por debajo C++, así que tanto por el lado de python como por el de R el principal mérito se debe a usar Rust y C++."
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#datos-hardware-y-entornos",
    "href": "2022/09/18/veloooosidad/index.html#datos-hardware-y-entornos",
    "title": "Veeelooosidad",
    "section": "Datos, hardware y entornos",
    "text": "Datos, hardware y entornos\nPara hacer la comparación vamos a usar un dataset de 100 millones de filas y 9 columnas, el mismo que se usa en h2o.ai db-benchmark.\nLo voy a probar en mi pc, que es un slimbook de justo antes de la pandemia, con 1gb de ssd, 32Gb de RAM y procesador Intel i7-9750H (12) @ 4.500GHz con 6 núcleos (12 hilos) y corriendo Linux Mint 20.\n\nR\nPara R voy a chequear vroom y data.table para leer los datos y data.table, tidytable y collapse para el procesamiento\nR: Uso R version 4.2.1 (2022-06-23) – “Funny-Looking Kid” vroom: 1.5.7 data.table: 1.14.2 tidytable: 0.8.1.9 collapse: 1.8.8\n\n\nPython\nUso un entorno de conda con python 3.6.12 polars: ‘0.12.5’"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#scripts",
    "href": "2022/09/18/veloooosidad/index.html#scripts",
    "title": "Veeelooosidad",
    "section": "Scripts",
    "text": "Scripts\n\nR\nEn R voy a usar microbenchmark para realizar varias ejecuciones\nFichero: tests.R\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vroom)\nlibrary(tidytable)\n\nlibrary(microbenchmark)\n\n# Check lectu\n\nsetDTthreads(0L)\n\nlectura <- microbenchmark(\n    vroom  = vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE), \n    data.table = data.table::fread(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\"),\n    times = 3L\n)\n\nprint(lectura)\n\n# group by sum\n\nx <- vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE)\n\n# x= sample_frac(x, size = 0.1)\nx_dt <- qDT(x)\n\ngroup_by_performance <- microbenchmark(\n    data.table = x_dt[, lapply(.SD, mean, na.rm = TRUE), keyby = id1, .SDcols = 7:9],\n    # dplyr      = x %>%\n    #     group_by(id1, id2) %>%\n    #     summarise(v1 = sum(v1, na.rm = TRUE)) %>% \n    #     ungroup(),\n    tidytable = x_dt %>%\n        summarize.(v1 = sum(v1),\n                   v2 = sum(v2),\n                   v3 = sum(v3),\n                   .by = c(id1, id2)),\n    # base_R = tapply(x$v1, list(x$id1, x$id2), sum, na.rm = TRUE),\n\n    collapse= x_dt %>%\n        fgroup_by(id1, id2) %>%\n        fsummarise(v1 = fsum(v1),\n                   v2 = fsum(v2),\n                   v3 = fsum(v3)),\n\n    collapse_pure = {\n        g <- GRP(x, ~ id1 +id2)\n        fsum(x$v1, g)\n        fsum(x$v2, g)\n    },\n    times = 5L\n)\n\nprint(group_by_performance)\n\n\nPython\nFichero: tests.py\nimport polars as pl\nimport time\n\nstart = time.time()\ndf = pl.read_csv(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\")\nend = time.time()\n\nprint(end -start)\n\nstart = time.time()\n\n(\n    df\n.lazy()\n    .groupby(['id1','id2'])\n    .agg(\n        [\n            pl.col(\"v1\").sum().alias('v1_sum'),\n            pl.col(\"v2\").sum().alias('v2_sum'),\n            pl.col(\"v3\").sum().alias('v3_sum')\n        ]\n    )\n.collect()\n)\nend = time.time()\nprint(end - start)"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#resultados",
    "href": "2022/09/18/veloooosidad/index.html#resultados",
    "title": "Veeelooosidad",
    "section": "Resultados",
    "text": "Resultados\nPara comparar, ejecuto los scripts desde consola y teniendo cerrado navegadores, ides y demás.\n\nR\nRscript tests.R\nLectura en R\nUnit: seconds\n       expr       min        lq      mean    median        uq       max neval\n      vroom  7.783958  7.953598  8.185716  8.123239  8.386596  8.649953     3\n data.table 41.914928 42.809751 45.213309 43.704575 46.862499 50.020424     3\n\nGroup by y sum en R.\nUnit: seconds\n          expr      min       lq     mean   median       uq       max neval cld\n    data.table 1.469617 1.476545 1.550360 1.486647 1.633409  1.685581     5   a\n     tidytable 1.182273 1.189111 1.291734 1.279313 1.314287  1.493686     5   a\n      collapse 1.799175 1.813744 6.255215 1.891603 2.076616 23.694936     5   a\n collapse_pure 1.553002 1.555598 1.570758 1.566454 1.571605  1.607132     5   a\n\nPor lo que más o menos, usar vroom para leer y tidytable, data.table o collapse para hacer el cálculo sale por unos 10 segundos o un poco menos.\n\n\nPython\npython tests.py \n7.755492448806763\n1.8228027820587158\n\nY vemos que con polars tenemos más o menos los mismos tiempos."
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#conclusiones",
    "href": "2022/09/18/veloooosidad/index.html#conclusiones",
    "title": "Veeelooosidad",
    "section": "Conclusiones",
    "text": "Conclusiones\nTanto en R como en Python tenemos librerías muy rápidas que , si tenemos suficiente RAM podemos trabajar con conjunto de datos bastante tochos y hacer cosas en tiempos más que razonables.\nPolars es una librería muy nueva y muy bien hecha, ojalá hagan api para R. No obstante, data.table lleva tiempo en R y su desempeño es consistente en múltiples situaciones. Mi consejo es echarle un ojo al fastverse."
  },
  {
    "objectID": "2022/04/10/transparente/2022-04-10-transparente.html",
    "href": "2022/04/10/transparente/2022-04-10-transparente.html",
    "title": "Transparente",
    "section": "",
    "text": "El otro día le decía a mis compañeros que hay cosas que no entiendo de la jerga del mundillo en el que nos movemos, (para echar unas risas ver el video de Pantomima Full) .\nYa lo de “tengo una call”, o lo de “estamos alineados” me toca bastante los … pero bueno. Pero hay varias expresiones que me repatean profundamente, y voy a enumerarlas por orden decreciente de odio.\n\n“Reglas de negocio”. Esta frase te la sueltan cuándo no quieren decirte que lo que se hace son 3 “wheres” que alguien decidió en su día y que ni se evaluó su eficacia entonces, ni ahora. Es como un mantra bajo el cual cabe todo, cuándo no te quieren decir alguna cosa absurda que se hace, se dice “son reglas de negocio”, y ya es como que no puedes preguntar de qué se trata, es eso y fin.\n“Hacer foco”. ¿Cómo que hacer foco? ¿quíén ha empezado a maltratar el idioma de tal manera? Uno se enfoca, se pone el foco, etc, pero no se hace foco. Con lo bonito que sería decir “énfasis” o “hincapié”. Supongo que esto lo dijo alguien con poder en el pasado, y ya nadie se atrevió a corregir, y no sólo eso, sino que se adoptó y ahora es ley.\n“Transparente”. Igual que con reglas de negocio, alguien dice, “para vosotros esto será transparente” y te quedas igual, no sabes si significa que te va a afectar lo que sea que se haga, si no te afecta, si te afecta pero poco, o vete tu a saber el qué. Con lo que fácil que sería un “sujeto , verbo, predicado”.\n\nEn fin, buena semana santa, no hagan muchos “quick win” , beban algo “transparente” y si no saben como explicar algún desastre digan que “son reglas de negocio” y que “hay que hacer foco” en “el roadmap” para que estemos todos “alineados” y llegar a los “Okr’s” del próximo “Q”.\nAdvertencia: Huyan de todo áquel que use estas expresiones más de 2 veces cada media hora. Su productividad aumentará"
  },
  {
    "objectID": "2022/05/29/no-mentir-s/index.html",
    "href": "2022/05/29/no-mentir-s/index.html",
    "title": "No mentirás",
    "section": "",
    "text": "Hay veces que uno se deja llevar por la emoción cuando hace algo y a veces se exagera un poco con lo que hace tu criatura.\nTal es el caso de la librería Nanyml, la cual tiene buena pinta pero exagera en al menos dos partes. La primera y más evidente es cuándo dice que puede estimar el desempeño futuro de un modelo sin comparar con lo que realmente pase, así promete el Estimating Performance without Targets\nOs juro que me he leído la documentación varias veces e incluso he visto el código y en ningún lado he visto que haga eso que promete.\nEn realidad lo que hace no es más que basarse en dos asunciones que, si se leen en primer lugar, hace que la afirmación presuntuosa de estimar el desempeño de un modelo sin ver el target se caiga por su propio peso. A saber, las dos asunciones son.\n\nEl modelo retorna probabilidades bien calibradas siempre.\nLa relación de \\(P[y | X]\\) no cambia .\n\nEstas dos asunciones por si solas lo que nos dicen es que vas a medir el desempeño de un modelo (sin ver el verdadero valor del target) asumiendo de partida que el modelo es tan bueno como lo era cuando lo entrenaste.\nLa segunda parte es en lo que denomina CBPE algorithm que si se lee con atención no es otra cosa que simplemente utilizar el modelo para obtener predicciones sobre un nuevo conjunto de datos.\nAsí, para calcular el AUC estimado, lo que hace es asumir que el modelo es bueno, y obtener las diferentes matrices de confusión que se derivan de escoger los posibles puntos de corte y, aquí viene el tema, considerar que el valor predicho por el modelo, es el verdadero valor.\nCon estas asunciones , cualquier cambio en la métrica del AUC se debería sólo y exclusivamente a cambios en la estructura de la población y no a que el modelo haya dejado de ser bueno (lo cual es imposible puesto que es una de las asunciones)..\nEjemplo. Si tenemos 3 grupos distintos dónde tenemos un evento binario. Supongamos que el primero de ellos viene de una población con proporción igual a 0.25, el segundo grupo viene de una población con proporción de 0.8 y el tercero de una población con proporción de 0.032. Si tomamos 1000, 300 y 600 observaciones de cada población respectivamente podemos simular tener un score que cumpla la condición de estar bien calibrado\n\nps1 <- rbeta(1000, 1, 3)\nps2 <- rbeta(300, 4, 1)\nps3 <- rbeta(600, 2, 60)\n\nps <- c(ps1, ps2, ps3)\n\nmean(ps1) ;  mean(ps2); mean(ps3)\n\n[1] 0.2460785\n\n\n[1] 0.7967239\n\n\n[1] 0.03217107\n\n\nLa distribución de los “scores” sería\n\n\n\n\n\n\n\n\n\nPues el CBPE no sería otra cosa que calcular el auc del modelo ¡¡asumiendo que las probabilidades estimadas son correctas!! . Es como intentar demostrar algo teniendo como asunción que es cierto. Pero vayamos al cálculo.\nSiguiendo lo descrito por la documentación y comprobando con el código de la librería se tendría que\n\ntpr_fpr <- function(threshold, ps) {\n  yj <- ifelse(ps >= threshold, 1, 0) \n  p_false = abs(yj - ps)\n  p_true = 1- p_false\n  n <- length(yj)\n  tp <- sum(p_true[yj == 1])\n  fp <- sum(p_false[yj==1])\n  tn <- sum(p_true[yj==0] )\n  fn <- sum(p_false[yj==0] )\n  tpr <- tp / (tp + fn)\n  fpr <- fp /(fp + tn)\n  return(data.frame(tpr = tpr, fpr = fpr))\n}\n\n\npscortes = sort(unique(ps), decreasing = TRUE)\n\ndfs <-  lapply(pscortes, function(x) tpr_fpr(x, ps))\n\nvalores <- do.call(rbind, dfs)\n\n\nplot(valores$fpr, valores$tpr, type = \"l\")\n\n\n\n\n\n\n\nsimple_auc <- function(TPR, FPR){\n  # inputs already sorted, best scores first \n  dFPR <- c(diff(FPR), 0)\n  dTPR <- c(diff(TPR), 0)\n  sum(TPR * dFPR) + sum(dTPR * dFPR)/2\n}\n\nwith(valores, simple_auc(tpr, fpr))\n\n[1] 0.8922705\n\n\nCómo se ve, para calcular el auc sólo se tiene en cuenta las probabilidades estimadas, por lo que pierde todo el sentido para obtener un desempeño de cómo de bien lo hace el modelo.\nDe hecho, si hubiera simulado para cada observación una bernoulli tomando como probabilidad de éxito el score tendría lo siguiente, y tomo esa simulación como el valor real , obtengo el mismo auc que con CBPE.\n\nlabels <- rbinom(length(ps), 1, ps)\n(res <- pROC::auc(labels, ps))\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\nArea under the curve: 0.8991\n\n\nEs decir, en la misma definición de lo que es una matriz de confusión y las métricas asociadas va implícita la idea de comparar la realidad con la estimación, si sustituyes la realidad por la estimación , entonces pierde el sentido.\nPero veamos para qué si puede servir esta cosa. Pues nos puede servir para detectar cambios de distribuciones conjuntas entre dos conjuntos de datos. Me explico, supongamos que quiero predecir sobre un conjunto de datos que en vez de tener 1000 observaciones de la primera población hay 200, y que de la segunda hay 100 y 10000 de la tercera. Pues en este caso, el cambio en el auc se debe solo a eso, al cambio de la estructura de la población global.\n\nps1_new <- rbeta(200, 1, 3)\nps2_new <- rbeta(100, 4, 1)\nps3_new <- rbeta(10000, 2, 60)\n\nps_new <- c(ps1_new, ps2_new, ps3_new)\n\n\nlabels <- rbinom(length(ps_new), 1, ps_new)\n(res <- pROC::auc(labels, ps_new))\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls < cases\n\n\nArea under the curve: 0.7695\n\n\nLa bajada del “auc estimado” solo se debe a cambios en la estructura de la nueva población que tiene muchas más observaciones de la población 3.\nPor lo tanto, lo que nannyml hace y no está mal, ojo, es simplemente ver cuál serían métricas agregadas (como el auc) cuando cambia la estructura pero no la probabilidad condicionada de y con respecto a las variables independientes.\nLo que no me parece bien es poner en la documentación que calcula el desempeño de un modelo sin ver el target, puesto que confunde y ya ha dado lugar a algún post en “towards data science” (gente, formaros primero con libros antes de leer post de estos sitios) con más humo que madera.\nY como se suele decir “No mentirás”."
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "",
    "text": "En Palabras para Julia parte 3 hablaba de modelos bayesianos con Turing.jl, y me quedé con una espinita clavada, que era la de poder predecir de forma relativamente fácil con Turing, o incluso guardar de alguna forma la “posterior samples” y poder usar mi modelo en otra sesión de Julia.\nEmpiezo una serie de entradas cuyo objetivo es ver si puedo llegar a la lógica para poner “en producción” un modelo bayesiando con Turing, pero llegando incluso a crear un binario en linux que me permita predecir con un modelo y desplegarlo incluso en entornos dónde no está instalado Julia. La verdad, que no sé si lo conseguiré, pero al menos aprendo algo por el camino.\nSi, ya sé que existen los dockers y todo eso, pero no está de más saber que existen alternativas que quizá sean mejores. Ya en el pasado he tratado temas de cómo productivizar modelos de h2o sobre spark aquí o con Julia aquí. El objetivo final será llegar a tener un binario en linux que tome como argumento la ruta dónde se haya guardado las posterior samples de un modelo bayesiano y la ruta con especificación de dicho modelo en texto (para que Turing sepa como usar esas posterior samples) y que nos genere la posterior predictive para nuevos datos.\nAsí que vamos al lío. Empezamos por ver como entrenamos un modelo bayesiano con Turing y como se puede guardar y utilizar posteriormente."
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html#entrenamiento-con-julia",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html#entrenamiento-con-julia",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "Entrenamiento con Julia",
    "text": "Entrenamiento con Julia\nVamos a hacer un ejemplo sencillo, entrenando una regresión lineal múltiple de forma bayesiana. El dataset forma parte del material del libro Introduction to Statistical Learning. Advertising\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\n\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\nmm = DataFrame(CSV.File(\"data/Advertising.csv\"))\ndescribe(mm)\n\n\n200×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │       1    230.1     37.8       69.2     22.1\n   2 │       2     44.5     39.3       45.1     10.4\n   3 │       3     17.2     45.9       69.3      9.3\n  ⋮  │    ⋮        ⋮        ⋮         ⋮         ⋮\n 198 │     198    177.0      9.3        6.4     12.8\n 199 │     199    283.6     42.0       66.2     25.5\n 200 │     200    232.1      8.6        8.7     13.4\n                                     194 rows omitted\n\njulia> describe(mm)\n5×7 DataFrame\n Row │ variable   mean      min   median   max    nmissing  eltype   \n     │ Symbol     Float64   Real  Float64  Real   Int64     DataType \n─────┼───────────────────────────────────────────────────────────────\n   1 │ Column1    100.5      1     100.5   200           0  Int64\n   2 │ TV         147.043    0.7   149.75  296.4         0  Float64\n   3 │ radio       23.264    0.0    22.9    49.6         0  Float64\n   4 │ newspaper   30.554    0.3    25.75  114.0         0  Float64\n   5 │ sales       14.0225   1.6    12.9    27.0         0  Float64\nEspecificamos el modelo, y aquí tengo que comentar un par de cosas. Una que julia gracias a que implementa eficazmente el Multiple dispatch, podemos tener una misma función que devuelva cosas diferentes dependiendo de que le pasemos, así una función puede tener diferentes métodos. El otro aspecto es el uso del condition en Turing (alias |) se puede especificar el modelo sin pasar como argumento la variable dependiente y usarla solo para obtener la posterior, lo cual nos va a permitir hacer algo como predict( modelo(Xs), cadena_mcmc), y no tener que pasar la y como un valor perdido.\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n    \n    radio::AbstractVector{<:Real},\n    newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\nAhora tenemos el mismo modelo, que me a servir tanto para pasarle como argumentos escalares como vectores, nótese que la función Normal tomo como argumento la desviación típica, mientrar que MvNormal toma una matriz de varianzas/covarianzas. Se aconseja el uso de MvNormal en Turing pues mejora el tiempo de cálculo de la posteriori.\nObtenemos la posteriori de los parámetros, pasándole como datos el dataset de Advertising. Es importante que la columna de la variable dependiente se pase como NamedTuple, esto se puede hacer en julia usando (; vector_y) .\n\n# utilizamos 4 cadenas con n_samples = 2000  para cada una\n\n# usamos | para pasarle los datos de Y que no habiamos pasado en la especificacion del modelo\n\nchain = sample(mm_model_sin_sales(mm.TV, mm.radio, mm.newspaper) | (; mm.sales),\n    NUTS(0.65),MCMCThreads(),\n    2_000, 4)\n    \nY en unos 18 segundos tenemos nuestra MCMC Chain.\nChains MCMC chain (2000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 18.06 seconds\nCompute duration  = 71.54 seconds\nparameters        = a, tv_coef, radio_coef, newspaper_coef, σ₁\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n      parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n          Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n               a    2.0952    0.2712     0.0030    0.0038   5123.1176    0.9999       71.6139\n         tv_coef    0.0481    0.0013     0.0000    0.0000   7529.0954    0.9998      105.2461\n      radio_coef    0.1983    0.0087     0.0001    0.0001   5230.9995    1.0000       73.1220\n  newspaper_coef    0.0040    0.0059     0.0001    0.0001   6203.9490    1.0002       86.7224\n              σ₁    1.7205    0.0874     0.0010    0.0011   5441.9631    1.0000       76.0709\n\nQuantiles\n      parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n          Symbol   Float64   Float64   Float64   Float64   Float64 \n\n               a    1.5635    1.9104    2.0982    2.2788    2.6182\n         tv_coef    0.0455    0.0472    0.0481    0.0489    0.0508\n      radio_coef    0.1814    0.1923    0.1983    0.2042    0.2155\n  newspaper_coef   -0.0077    0.0001    0.0040    0.0078    0.0157\n              σ₁    1.5585    1.6607    1.7169    1.7781    1.8997\nVale, estupendo,en chain tenemos las 8000 samples para cada uno de los 5 parámetros , y también las de temas del ajuste interno por HMC, de ahí lo de (2000×17×4 Array{Float64, 3}).\nPero ¿cómo podemos predecir para nuevos datos?\nPues podemos pasarle simplemente 3 escalares correspondientes a las variables TV, radio y newspaper.\nEs necesario pasarle a la función predict la llamada al modelo con los nuevos datos mm_model_sin_sales(tv_valor, radio_valor,newspaper_valor) y las posterioris (la cadena MCMC) de los parámetros.\n\njulia> predict(mm_model_sin_sales(2, 5, 7), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales    3.2203    1.7435     0.0195    0.0176   8053.9924    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales   -0.1863    2.0441    3.2553    4.4030    6.6547\nTambién podemos pasarle más valores\n\njulia> mm_last = last(mm, 3)\n3×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │     198    177.0      9.3        6.4     12.8\n   2 │     199    283.6     42.0       66.2     25.5\n   3 │     200    232.1      8.6        8.7     13.4\n\n\njulia> predicciones = predict(mm_model_sin_sales(mm_last.TV, mm_last.radio, mm_last.newspaper), chain)\nChains MCMC chain (2000×3×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales[1], sales[2], sales[3]\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n    sales[1]   12.5192    1.7427     0.0195    0.0170   8270.6268    1.0000\n    sales[2]   24.3266    1.7560     0.0196    0.0222   7720.4172    1.0001\n    sales[3]   14.9901    1.7327     0.0194    0.0188   8039.4940    0.9999\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n    sales[1]    9.0888   11.3344   12.5241   13.6990   15.9571\n    sales[2]   20.8369   23.1519   24.3414   25.4967   27.7429\n    sales[3]   11.6549   13.8304   14.9617   16.1471   18.3733\nPodría quedarme con las predicciones para sales[1] y calcular el intervalo de credibilidad el 80%\njulia> quantile(reshape(Array(predicciones[\"sales[1]\"]), 8000), [0.1, 0.5, 0.9])\n3-element Vector{Float64}:\n 10.28185973755853\n 12.524091380928425\n 14.74877121738519"
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html#guardar-cadena-y-predecir",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html#guardar-cadena-y-predecir",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "Guardar cadena y predecir",
    "text": "Guardar cadena y predecir\nAhora viene la parte que nos interesa a los que nos dedicamos a esto y queremos usar un modelo entrenado hace 6 meses sobre datos de hoy. Guardar lo que hicimos y predecir sin necesidad de reentrenar.\nGuardamos la posteriori\n\n\nwrite( \"cadena.jls\", chain)\nY ahora, cerramos julia y abrimos de nuevo.\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\n# posteriori guardada\nchain = read(\"cadena.jls\", Chains)\n\n# Especificación del modelo (esto puede ir en otro fichero .jl)\n\n# Si tengo en un fichero jl el código de @model, lo puedo incluir ahí. \n\n\n# ruta = \"especificacion_modelo.jl\"\n# include(ruta)\n\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n     radio::AbstractVector{<:Real},\n      newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\n\n\n\nY aqui viene la parte importante. En la que utilizamos el modelo guardado, que no es más que las posterioris de los parámetros que hemos salvado en disco previamente.\n\n## predecimos la misma observación , fila 198 del dataset\n\npredict(mm_model_sin_sales(177, 9.3, 6.4 ), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales   12.4723    1.7285     0.0193    0.0186   8326.7650    0.9998\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales    9.0844   11.3106   12.4727   13.6334   15.7902\n       \nY voilá. Sabiendo que se puede guardar la posteriori y usarla luego , veo bastante factible poder llegar al objetivo de crear un “motor de predicción” de modelos bayesianos con Turing, que sea un ejecutable y que tome como argumentos la posteriori guardada de un modelo ajustado y en texto (con extensión jl ) y escriba el resultado en disco. Y lo dicho, que pueda desplegar este ejecutable en cualquier sistema linux, sin tener que instalar docker ni nada, solo hacer un unzip"
  },
  {
    "objectID": "2022/10/26/sigo-trasteando-con-julia/index.html",
    "href": "2022/10/26/sigo-trasteando-con-julia/index.html",
    "title": "Sigo trasteando con julia",
    "section": "",
    "text": "Siguiendo con lo que contaba aquí me he construido un binario para predecir usando un modelo de xgboost con Julia. La ventaja es que tengo un tar.gz que puedo descomprimir en cualquier linux (por ejemplo un entorno de producción sin acceso a internet y que no tenga ni vaya a tener julia instalado, ni docker ni nada de nada), descomprimir y poder hacer un miapp_para_predecir mi_modelo_entrenado.jls csv_to_predict.csv resultado.csv y que funcione y vaya como un tiro.\nPongo aquí los ficheros relevantes.\nPor ejemplo mi fichero para entrenar un modelo y salvarlo .\nFichero train_ boston.jl\n# Training model julia\nusing  CSV,CategoricalArrays, DataFrames, MLJ, MLJXGBoostInterface\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\ndf1[:, :target] .= ifelse.(df1[!, :medv_20].== \"NG20\", 1, 0)\nconst target = CategoricalArray(df1[:, :target])\n\nconst X = df1[:, Not([:medv_20, :target])]\n\nTree = @load XGBoostClassifier pkg=XGBoost\ntree_model = Tree(objective=\"binary:logistic\", max_depth = 6, num_round = 800)\nmach = machine(tree_model, X, target)\n\nThreads.nthreads()\nevaluate(tree_model, X, target, resampling=CV(shuffle=true),measure=log_loss, verbosity=0)\nevaluate(tree_model, X, target,\n                resampling=CV(shuffle=true), measure=bac, operation=predict_mode, verbosity=0)\n\n\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\nyhat = predict(mach, X[test,:])\n\nevaluate(tree_model, X[test,:], target[test], measure=auc, operation=predict_mode, verbosity=0)\n\nniveles = levels.(yhat)[1]\nniveles[1]\n\nlog_loss(yhat, target[test]) |> mean\n\nres = pdf(yhat, niveles)\nres_df = DataFrame(res,:auto)\n\nMLJ.save(\"models/boston_xg.jls\", mach)\nY luego los ficheros que uso para construirme la app binaria .. Recordemos del post que mencionaba que lo que necesito es el código del programa principal (el main) y un fichero de precompilación que sirve para que al crear la app se compilen las funciones que voy a usar.\nfichero precomp.jl,\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"models/mimodelo_xg_binario.jls\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, [\"target_0\", \"target_1\"])\n\nCSV.write(\"data/predicciones.csv\", res_df)\nfichero xgboost_predict_binomial.jl , aquí es dónde está el main\nmodule xgboost_predict_binomial\n\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, ntasks= Sys.CPU_THREADS)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, [\"target_0\", \"target_1\"])          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\ny si todo está correcto y siguiendo las instrucciones del post anterior, se compilaría haciendo por ejemplo esto\nusing PackageCompiler\ncreate_app(\"../xgboost_predict_binomial\", \"../xg_binomial_inference\",\n precompile_execution_file=\"../xgboost_predict_binomial/src/precomp_file.jl\", force=true, filter_stdlibs = true, cpu_target = \"x86_64\")\nY esto me crea una estructura de directorios dónde está mi app y todo lo necesario para ejecutar julia en cualqueir linux.\n\n╰─ $ ▶ tree -L 2 xg_binomial_inference\nxg_binomial_inference\n├── bin\n│   ├── julia\n│   └── xgboost_predict_binomial\n├── lib\n│   ├── julia\n│   ├── libjulia.so -> libjulia.so.1.8\n│   ├── libjulia.so.1 -> libjulia.so.1.8\n│   └── libjulia.so.1.8\n└── share\n    └── julia\ny poner por ejemplo en el .bashrc el siguiente alias.\nalias motor_xgboost=/home/jose/Julia_projects/xgboost_model/xg_binomial_inference/bin/xgboost_predict_binomial\ny ya está listo.\nAhora tengo un dataset a predecir de 5 millones de filas\n\n╰─ $ ▶ wc -l data/test.csv \n5060001 data/test.csv\n\n head -n4 data/test.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\ny bueno, tardo unos 11 segundos en obtener las predicciones y escribir el resultado\n╰─ $ ▶ time motor_xgboost models/boston_xg.jls data/test.csv pred.csv\n\nreal    0m11,091s\nuser    0m53,293s\nsys 0m2,321s\n\ny comprobamos que lo ha hecho bien\n\n╰─ $ ▶ wc -l  pred.csv \n5060001 pred.csv\n\n\n╰─ $ ▶ head -n 5 pred.csv \ntarget_0,target_1\n0.9999237,7.63197e-5\n0.99120975,0.008790266\n0.99989164,0.00010834133\n0.99970543,0.00029458306\nY nada, pues esto puede servir para subir modelos a producción en entornos poco amigables (sin python3, sin R, sin julia, sin spark, sin docker, sin internet). Es un poco old style que me diría mi arquenazi favorito Rubén, pero\nOs dejo el tar.gz para que probéis, también os dejo el Project.tomly el Manifest.toml y el fichero con el que he entrenado los datos. para que uséis el mismo entorno de julia que he usado yo.\nenlace_drive"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html",
    "title": "Api y docker con R. parte 1",
    "section": "",
    "text": "Todo el mundo anda haciendo apis para poner modelos en producción, y oye, está bien. Si además lo complementas con dockerizarlo para tener un entorno controlado y que te valga para ponerlo en cualquier sitio dónde esté docker instalado pues mejor.\nAquí voy a contar un ejemplo de como se puede hacer con R usando plumber y docker, en siguentes post contaré como hacerlo con vetiver que es una librería que está para R y Python que tiene algún extra, como versionado de modelos y demás.\nLo primero de todo es trabajar en un proyecto nuevo y usar renv. renv es para gestionar entornos de R, ojo que también funciona bien si tienes que mezclar R y python. Tiene cosas interesantes como descubrir las librerías que usas en tu proyecto y aún mejor, si estas librerías ya las tienes instaladas pues te crea enlaces simbólicos a dónde están y te permite ahorrar un montón de espacio, que al menos yo, no he conseguido ver cómo hacer eso con conda."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#objetivo",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#objetivo",
    "title": "Api y docker con R. parte 1",
    "section": "Objetivo",
    "text": "Objetivo\nMi objetivo es ver cómo pondría un modelo bayesiano ajustado con brms para que me devuelva predicciones puntuales y las posterioris en un entorno de producción."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#entrenando-modelo",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#entrenando-modelo",
    "title": "Api y docker con R. parte 1",
    "section": "Entrenando modelo",
    "text": "Entrenando modelo\nPara eso voy a usar datos de un antiguo post.\nUna vez que estemos en ese nuevo proyecto, ajustamos y guardamos un modelo .\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n## ✔ tibble  3.1.8      ✔ dplyr   1.0.10\n## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n## ✔ readr   2.1.3      ✔ forcats 0.5.2 \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(brms)\n## Loading required package: Rcpp\n## Loading 'brms' package (version 2.18.0). Useful instructions\n## can be found by typing help('brms'). A more detailed introduction\n## to the package is available through vignette('brms_overview').\n## \n## Attaching package: 'brms'\n## \n## The following object is masked from 'package:stats':\n## \n##     ar\nlibrary(cmdstanr)\n## Warning: package 'cmdstanr' was built under R version 4.3.0\n## This is cmdstanr version 0.5.2\n## - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n## - Use set_cmdstan_path() to set the path to CmdStan\n## - Use install_cmdstan() to install CmdStan\n## Using all cores. 12 in my machine, y que haga las cadenas en paralelo\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n## CmdStan path set to: /home/jose/cmdstan\ntrain <- read_csv(here::here(\"data/train_local.csv\"))\n## Rows: 662 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): segmento, tipo, edad_cat\n## dbl (2): valor_cliente, n\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# guiña a librería antigua\ncar::some(train)\n## # A tibble: 10 × 5\n##    segmento tipo  valor_cliente edad_cat     n\n##    <chr>    <chr>         <dbl> <chr>    <dbl>\n##  1 Rec      SM                2 21- 40       4\n##  2 Best     SM                1 41-50      475\n##  3 Best     C                 4 >60       2807\n##  4 No_way   C                 1 41-50      356\n##  5 No_way   B                 5 40-60      221\n##  6 Rec      SF                2 >60        152\n##  7 Rec      B                 4 40-60      194\n##  8 Best     C                 5 41-50     4934\n##  9 No_way   B                 3 41-50     1064\n## 10 No_way   SF                8 41-50       29\nAjustamos un modelo bayesiano con efectos aleatorios y usando la columna n como pesos de las filas. (leer el post dónde usé estos datos para saber más)\ntrain <- train %>% \n    mutate(target1 = as_factor(ifelse(segmento == \"Best\", \"Best\", \"Other\")))\n\n\nformula <- brmsformula(\n    target1| resp_weights(n)  ~ (1 | edad_cat) + (1 | valor_cliente) + (1 | tipo)\n    )\n\nmod <- brm(\n    formula,\n     family = \"bernoulli\", data = train, \n    iter = 4000, warmup = 1000, cores = 4, chains = 4,\n    seed = 10,\n    backend = \"cmdstanr\", \n     refresh = 0) # refresh 0 qu eno quiero que se me llene el post de los output de las cadenas mcm\n\nsaveRDS(mod, here::here(\"brms_model.rds\"))"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#comprobamos-que-nuestro-modelo-funciona",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#comprobamos-que-nuestro-modelo-funciona",
    "title": "Api y docker con R. parte 1",
    "section": "Comprobamos que nuestro modelo funciona",
    "text": "Comprobamos que nuestro modelo funciona\nlibrary(tidybayes)\n## \n## Attaching package: 'tidybayes'\n## The following objects are masked from 'package:brms':\n## \n##     dstudent_t, pstudent_t, qstudent_t, rstudent_t\nmod_reload <- readRDS(here::here(\"brms_model.rds\"))\n \n# \n\ntest <-  read_csv(here::here(\"data/test_local.csv\"))\n## Rows: 656 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): segmento, tipo, edad_cat\n## dbl (2): valor_cliente, n\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# estimacion puntual\npredict(mod_reload, head(test))\n##        Estimate Est.Error Q2.5 Q97.5\n## [1,] 0.23216667 0.4222324    0     1\n## [2,] 0.13233333 0.3388669    0     1\n## [3,] 0.16075000 0.3673155    0     1\n## [4,] 0.13825000 0.3451766    0     1\n## [5,] 0.12716667 0.3331735    0     1\n## [6,] 0.07333333 0.2606937    0     1\n# full posterior\n# para 6 filas guarda los valores obtenidos en las 3000 iteraciones de cada cadena\n# 3000 * 4 * 6 = 72000 valores \n\nposterior_pred <- add_epred_draws(head(test), mod_reload) \n\nhead(posterior_pred )\n## # A tibble: 6 × 10\n## # Groups:   segmento, tipo, valor_cliente, edad_cat, n, .row [1]\n##   segmento tipo  valor_cliente edad_cat     n  .row .chain .itera…¹ .draw .epred\n##   <chr>    <chr>         <dbl> <chr>    <dbl> <int>  <int>    <int> <int>  <dbl>\n## 1 Rec      C                 0 21- 40     132     1     NA       NA     1  0.230\n## 2 Rec      C                 0 21- 40     132     1     NA       NA     2  0.234\n## 3 Rec      C                 0 21- 40     132     1     NA       NA     3  0.233\n## 4 Rec      C                 0 21- 40     132     1     NA       NA     4  0.230\n## 5 Rec      C                 0 21- 40     132     1     NA       NA     5  0.232\n## 6 Rec      C                 0 21- 40     132     1     NA       NA     6  0.226\n## # … with abbreviated variable name ¹​.iteration\ndim(posterior_pred)\n## [1] 72000    10\nPara la primer fila podemos tener la distribución a posteriori\nposterior_pred %>% \n  filter(.row == 1) %>% \n  ggplot(aes(x=.epred)) +\n  geom_density() \n\nPues listo, ya tenemos el modelo entrenado y guardado, ahora sólo queda escribir el código para la api y el Dockerfile.."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creando-el-plumber.r",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creando-el-plumber.r",
    "title": "Api y docker con R. parte 1",
    "section": "Creando el plumber.R",
    "text": "Creando el plumber.R\nUna cosa importante, si hemos usado renv es escribir el fichero con las dependencias que usamos. Eso se hace con renv::snapshot() y se crea un fichero dónde están descritas las dependencias versionadas de nuestro proyecto.\nPero quizá para el docker no necesitemos todas, en este caso, partiendo del fichero anterior nos creamos otro con sólo las dependencias necesarias. Yo lo he llamado vetiver_renv.lock porque empecé trasteando con vetiver y soy demasiado vago como para cambiar ahora el nombre. El contenido del fichero es\nvetiver_renv.lock\n{\n  \"R\": {\n    \"Version\": \"4.2.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"binarios\",\n        \"URL\": \"https://packagemanager.rstudio.com/all/latest\"\n      },\n      {\n        \"Name\": \"ropenspain\",\n        \"URL\": \"https://ropenspain.r-universe.dev\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"plumber\": {\n      \"Package\": \"plumber\",\n      \"Version\": \"1.2.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"8b65a7a00ef8edc5ddc6fabf0aff1194\",\n      \"Requirements\": [\n        \"R6\",\n        \"crayon\",\n        \"ellipsis\",\n        \"httpuv\",\n        \"jsonlite\",\n        \"lifecycle\",\n        \"magrittr\",\n        \"mime\",\n        \"promises\",\n        \"rlang\",\n        \"sodium\",\n        \"stringi\",\n        \"swagger\",\n        \"webutils\"\n      ]\n    },\n    \"brms\": {\n      \"Package\": \"brms\",\n      \"Version\": \"2.18.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"afcb0d871e1759b68b29eb6affd37a10\",\n      \"Requirements\": [\n        \"Matrix\",\n        \"Rcpp\",\n        \"abind\",\n        \"backports\",\n        \"bayesplot\",\n        \"bridgesampling\",\n        \"coda\",\n        \"future\",\n        \"ggplot2\",\n        \"glue\",\n        \"loo\",\n        \"matrixStats\",\n        \"mgcv\",\n        \"nleqslv\",\n        \"nlme\",\n        \"posterior\",\n        \"rstan\",\n        \"rstantools\",\n        \"shinystan\"\n      ]\n    },\n    \"tidybayes\": {\n      \"Package\": \"tidybayes\",\n      \"Version\": \"3.0.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"d501501261b724f35ec9f2b80f4421b5\",\n      \"Requirements\": [\n        \"arrayhelpers\",\n        \"coda\",\n        \"dplyr\",\n        \"ggdist\",\n        \"ggplot2\",\n        \"magrittr\",\n        \"posterior\",\n        \"rlang\",\n        \"tibble\",\n        \"tidyr\",\n        \"tidyselect\",\n        \"vctrs\",\n        \"withr\"\n      ]\n    }\n  }\n}\n\nCómo veis también he añadido la librería tidybayes, porque me va a resultar útil para sacar la posteriori de las predicciones de los nuevos datos.\nCreamos el fichero plumber.R que no es más que decir cómo se va a predecir y crear un par de endpoints que permiten tanto obtener estimaciones puntuales como la full posterior. Con la librería plumber se hace fácil sin más que usar decoradores.\nFichero plumber.R\nlibrary(brms)\nlibrary(plumber)\nlibrary(tidybayes)\n\nbrms_model <- readRDS(\"brms_model.rds\")\n\n\n#* @apiTitle brms predict Api\n#* @apiDescription Endpoints for working with brms model\n\n## ---- filter-logger\n#* Log some information about the incoming request\n#* @filter logger\nfunction(req){\n    cat(as.character(Sys.time()), \"-\",\n        req$REQUEST_METHOD, req$PATH_INFO, \"-\",\n        req$HTTP_USER_AGENT, \"@\", req$REMOTE_ADDR, \"\\n\")\n    forward()\n}\n\n## ---- post-data\n#* Submit data and get a prediction in return\n#* @post /predict\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    predict(brms_model, data) |>\n        as.data.frame()\n}\n\n\n#* @post /full_posterior\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    add_epred_draws(data, brms_model) \n        \n}\nNo tiene mucho misterio, los endpoint se crean usando\n#* @post  /nombre_endpoing\ny creando una función que va a tomar los datos que le pasemos en formato json a la api, los pasa a data.frame y usa el modelo previamente cargado para obtener las estimaciones puntuales en un caso y la full posterior (con add_epred_draws) en el otro."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creamos-el-docker",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creamos-el-docker",
    "title": "Api y docker con R. parte 1",
    "section": "Creamos el docker",
    "text": "Creamos el docker\nIba a contar lo que es docker, pero mejor lo miráis en su web. Sólo quedarnos con la idea que es como tener una máquina virtual que puedo usar en otro sitio, pero es mucho más ligera y puede usar cosas del sistema anfitrión e interactuar con él.\nPara crear nuestra imagen docker tenemos que crear un fichero que se llame Dockerfile dónde vamos a ir diciéndole como cree nuestra máquina virtual.\nEs importante que estén los ficheros anteriores, el modelo salvado , el plumber.R y el fichero .lock en las rutas correctas dónde los busca el Dockerfile, en mi caso, lo he puesto todo en el mismo sitio.\nContendido del Dockerfile\n# Docker file para modelo brms\n\nFROM rocker/r-ver:4.2.1\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  default-jdk \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  make \\\n  zlib1g-dev \\\n  libxml2-dev \\\n  libglpk-dev \\\n  && apt-get clean\n\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\n\n## Copio el modelo y el fichero de la api\nCOPY brms_model.rds /opt/ml/brms_model.rds\nCOPY plumber.R /opt/ml/plumber.R\n\nEXPOSE 8081\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8081)\"]\n\n\nImportante que el puerto que se exponga con EXPOSE sea el mismo que usa el plumber, en este caso el 8081.\nAhora para construir la imagen docker y ejecutarla\ndocker build -t mi_modelo_brms .\n\nY despues de un rato podemos ejecutarlo mapeando el puerto\nnohup docker container run --rm -p 8081:8081 mi_modelo_brms &"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#funciona",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#funciona",
    "title": "Api y docker con R. parte 1",
    "section": "¿Funciona?",
    "text": "¿Funciona?\nPodemos usar curl, python, php o cualquier otra cosa para mandar peticiones a la api y que nos devuelva predicciones, con R sería algo así.\ntest %>% \n    head(2) \n## # A tibble: 2 × 5\n##   segmento tipo  valor_cliente edad_cat     n\n##   <chr>    <chr>         <dbl> <chr>    <dbl>\n## 1 Rec      C                 0 21- 40     132\n## 2 Best     B                 0 41-50       19\nbase_url <- \"http://0.0.0.0:8081\"\n\napi_res <- httr::POST(url = paste0(base_url, \"/predict\"),\n                      body = head(test),\n                      encode = \"json\")\npredicted_values <- httr::content(api_res, as = \"text\", encoding = \"UTF-8\")\n\njsonlite::fromJSON(predicted_values)\n##   Estimate Est.Error Q2.5 Q97.5\n## 1   0.2283    0.4198    0     1\n## 2   0.1356    0.3424    0     1\n## 3   0.1604    0.3670    0     1\n## 4   0.1320    0.3385    0     1\n## 5   0.1215    0.3267    0     1\n## 6   0.0737    0.2612    0     1\napi_res2 <- httr::POST(url = paste0(base_url, \"/full_posterior\"),\n                      body = head(test,1),\n                      encode = \"json\")\nposterior_values <- httr::content(api_res2, as = \"text\", encoding = \"UTF-8\")\n\n\njsonlite::fromJSON(posterior_values)  %>% \n  head(100)\n##     segmento tipo valor_cliente edad_cat   n .row .draw .epred\n## 1        Rec    C             0   21- 40 132    1     1 0.2297\n## 2        Rec    C             0   21- 40 132    1     2 0.2341\n## 3        Rec    C             0   21- 40 132    1     3 0.2330\n## 4        Rec    C             0   21- 40 132    1     4 0.2296\n## 5        Rec    C             0   21- 40 132    1     5 0.2321\n## 6        Rec    C             0   21- 40 132    1     6 0.2256\n## 7        Rec    C             0   21- 40 132    1     7 0.2211\n## 8        Rec    C             0   21- 40 132    1     8 0.2215\n## 9        Rec    C             0   21- 40 132    1     9 0.2259\n## 10       Rec    C             0   21- 40 132    1    10 0.2245\n## 11       Rec    C             0   21- 40 132    1    11 0.2330\n## 12       Rec    C             0   21- 40 132    1    12 0.2263\n## 13       Rec    C             0   21- 40 132    1    13 0.2262\n## 14       Rec    C             0   21- 40 132    1    14 0.2426\n## 15       Rec    C             0   21- 40 132    1    15 0.2307\n## 16       Rec    C             0   21- 40 132    1    16 0.2348\n## 17       Rec    C             0   21- 40 132    1    17 0.2293\n## 18       Rec    C             0   21- 40 132    1    18 0.2281\n## 19       Rec    C             0   21- 40 132    1    19 0.2304\n## 20       Rec    C             0   21- 40 132    1    20 0.2277\n## 21       Rec    C             0   21- 40 132    1    21 0.2283\n## 22       Rec    C             0   21- 40 132    1    22 0.2355\n## 23       Rec    C             0   21- 40 132    1    23 0.2297\n## 24       Rec    C             0   21- 40 132    1    24 0.2257\n## 25       Rec    C             0   21- 40 132    1    25 0.2191\n## 26       Rec    C             0   21- 40 132    1    26 0.2275\n## 27       Rec    C             0   21- 40 132    1    27 0.2328\n## 28       Rec    C             0   21- 40 132    1    28 0.2312\n## 29       Rec    C             0   21- 40 132    1    29 0.2190\n## 30       Rec    C             0   21- 40 132    1    30 0.2370\n## 31       Rec    C             0   21- 40 132    1    31 0.2303\n## 32       Rec    C             0   21- 40 132    1    32 0.2252\n## 33       Rec    C             0   21- 40 132    1    33 0.2190\n## 34       Rec    C             0   21- 40 132    1    34 0.2269\n## 35       Rec    C             0   21- 40 132    1    35 0.2311\n## 36       Rec    C             0   21- 40 132    1    36 0.2309\n## 37       Rec    C             0   21- 40 132    1    37 0.2313\n## 38       Rec    C             0   21- 40 132    1    38 0.2361\n## 39       Rec    C             0   21- 40 132    1    39 0.2335\n## 40       Rec    C             0   21- 40 132    1    40 0.2414\n## 41       Rec    C             0   21- 40 132    1    41 0.2333\n## 42       Rec    C             0   21- 40 132    1    42 0.2283\n## 43       Rec    C             0   21- 40 132    1    43 0.2354\n## 44       Rec    C             0   21- 40 132    1    44 0.2314\n## 45       Rec    C             0   21- 40 132    1    45 0.2357\n## 46       Rec    C             0   21- 40 132    1    46 0.2240\n## 47       Rec    C             0   21- 40 132    1    47 0.2241\n## 48       Rec    C             0   21- 40 132    1    48 0.2355\n## 49       Rec    C             0   21- 40 132    1    49 0.2260\n## 50       Rec    C             0   21- 40 132    1    50 0.2268\n## 51       Rec    C             0   21- 40 132    1    51 0.2278\n## 52       Rec    C             0   21- 40 132    1    52 0.2213\n## 53       Rec    C             0   21- 40 132    1    53 0.2246\n## 54       Rec    C             0   21- 40 132    1    54 0.2316\n## 55       Rec    C             0   21- 40 132    1    55 0.2313\n## 56       Rec    C             0   21- 40 132    1    56 0.2209\n## 57       Rec    C             0   21- 40 132    1    57 0.2269\n## 58       Rec    C             0   21- 40 132    1    58 0.2323\n## 59       Rec    C             0   21- 40 132    1    59 0.2280\n## 60       Rec    C             0   21- 40 132    1    60 0.2357\n## 61       Rec    C             0   21- 40 132    1    61 0.2275\n## 62       Rec    C             0   21- 40 132    1    62 0.2387\n## 63       Rec    C             0   21- 40 132    1    63 0.2387\n## 64       Rec    C             0   21- 40 132    1    64 0.2231\n## 65       Rec    C             0   21- 40 132    1    65 0.2370\n## 66       Rec    C             0   21- 40 132    1    66 0.2313\n## 67       Rec    C             0   21- 40 132    1    67 0.2243\n## 68       Rec    C             0   21- 40 132    1    68 0.2335\n## 69       Rec    C             0   21- 40 132    1    69 0.2275\n## 70       Rec    C             0   21- 40 132    1    70 0.2340\n## 71       Rec    C             0   21- 40 132    1    71 0.2250\n## 72       Rec    C             0   21- 40 132    1    72 0.2373\n## 73       Rec    C             0   21- 40 132    1    73 0.2259\n## 74       Rec    C             0   21- 40 132    1    74 0.2405\n## 75       Rec    C             0   21- 40 132    1    75 0.2227\n## 76       Rec    C             0   21- 40 132    1    76 0.2210\n## 77       Rec    C             0   21- 40 132    1    77 0.2337\n## 78       Rec    C             0   21- 40 132    1    78 0.2306\n## 79       Rec    C             0   21- 40 132    1    79 0.2242\n## 80       Rec    C             0   21- 40 132    1    80 0.2235\n## 81       Rec    C             0   21- 40 132    1    81 0.2247\n## 82       Rec    C             0   21- 40 132    1    82 0.2188\n## 83       Rec    C             0   21- 40 132    1    83 0.2129\n## 84       Rec    C             0   21- 40 132    1    84 0.2415\n## 85       Rec    C             0   21- 40 132    1    85 0.2293\n## 86       Rec    C             0   21- 40 132    1    86 0.2312\n## 87       Rec    C             0   21- 40 132    1    87 0.2189\n## 88       Rec    C             0   21- 40 132    1    88 0.2236\n## 89       Rec    C             0   21- 40 132    1    89 0.2262\n## 90       Rec    C             0   21- 40 132    1    90 0.2317\n## 91       Rec    C             0   21- 40 132    1    91 0.2316\n## 92       Rec    C             0   21- 40 132    1    92 0.2288\n## 93       Rec    C             0   21- 40 132    1    93 0.2299\n## 94       Rec    C             0   21- 40 132    1    94 0.2288\n## 95       Rec    C             0   21- 40 132    1    95 0.2311\n## 96       Rec    C             0   21- 40 132    1    96 0.2264\n## 97       Rec    C             0   21- 40 132    1    97 0.2269\n## 98       Rec    C             0   21- 40 132    1    98 0.2287\n## 99       Rec    C             0   21- 40 132    1    99 0.2283\n## 100      Rec    C             0   21- 40 132    1   100 0.2191\nSeguramente usar una api para obtener la posteriori que tiene tantos valores para cada dato no sea lo más eficiente, porque lo devuelve en formato json y luego hay que convertirlo a data.frame, pero funciona."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#salvar-docker-en-un-tar.gz",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#salvar-docker-en-un-tar.gz",
    "title": "Api y docker con R. parte 1",
    "section": "Salvar docker en un tar.gz",
    "text": "Salvar docker en un tar.gz\nSi no tenemos un sitio estilo docker hub dónde registrar nuestros docker o por cualquier otra causa, podemos usar docker save para generar un fichero comprimido y docker load para importarlo.\nSería algo así como\n\ndocker save mi_modelo_brms | gzip > mi_modelo_brms_docker.tar.gz\nCopiar ese tar.gz a dónde toque\ndocker load < mi_modelo_brms_docker.tar.gz"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#adelanto-con-vetiver",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#adelanto-con-vetiver",
    "title": "Api y docker con R. parte 1",
    "section": "Adelanto con vetiver",
    "text": "Adelanto con vetiver\nCon la librería vetiver se simplifica todo este proceso, puesto que crea por ti el plumber.R y el dockerfile y tiene movidas para guardar la monitorización del modelo y demás. Está tanto para R como para python. En R soporta los modelos que estén en tidymodels y en python soporta scikit-learn, statmodels, xgboost y creo que también pytorch"
  },
  {
    "objectID": "2022/10/30/api-y-docker-con-r-parte-2.html",
    "href": "2022/10/30/api-y-docker-con-r-parte-2.html",
    "title": "Api y docker con R. parte 2",
    "section": "",
    "text": "En la entrada de api y docker con R parte I veíamos que es muy fácil construir una api y dockerizarla para tener un modelo bayesiano en producción. Pero hay un pequeño incoveniente, el docker que hemos creado se base en rocker/verse que se basan en ubuntu. Y ubuntu ocupa mucho. Pero gracias a gente como Gabor Csardi (autor entre otras librerías de igraph), tenemos r-hub/minimal, que permiten tener una imagen de docker con R basadas en alpine, de hecho una imagen de docker con R y dplyr son unos 50 mb.\nLo primero de todo es ver cuánto ocupa el docker creado en el primer post.\nPues son unos cuántos gigas, mayoritariamente al estar basado en ubuntu y al que los docker de rocker/verse instalan todo el software de R recomendado, los ficheros de ayuda, las capacidades gráficas, etc..\nPero con r-hub/minimal podemos dejar bastante limpio el tema. Leyendo el Readme del repo vemos que han configurado una utilidad a la que llaman installr que permite instalar librerías del sistema o de R, instalando los compiladores de C, fortran etc que haga falta y eliminarlos una vez están compiladas la librerías.\nSin más, cambiamos el Dockerfile del otro día por este otro .\nY haciendo docker build -t mi_modelo_brms_rminimal . pasado un rato puesto que ha de compilar las librerías tenemos nuestra api dockerizada con la misma funcionalidad que el otro día.\nY con un tamaño mucho más contenido\nque se va a unos 655 mb, de los cuales unos 300 MB se deben a stan y rstan. Pero vamos, no está mal, pasar de 3.4 Gb a 665MB."
  },
  {
    "objectID": "2022/10/30/api-y-docker-con-r-parte-2.html#actualización-usando-renv",
    "href": "2022/10/30/api-y-docker-con-r-parte-2.html#actualización-usando-renv",
    "title": "Api y docker con R. parte 2",
    "section": "Actualización, usando renv",
    "text": "Actualización, usando renv\nPor temas de buenas prácticas es recomendable usar renv para crear el archivo renv.lock dónde se guarda qué versión de las librerías estamos usando, y además porque usa por defecto un repo con las librerías compiladas.\nLo primero que hago es crearme un nuevo proyecto dónde pongo el modelo entrenado que queremos usar brms_model.rds que entrené en el primer post y el fichero plumber.R y ningún fichero más.\nFichero plumber.R\n\n#\n# This is a Plumber API. In RStudio 1.2 or newer you can run the API by\n# clicking the 'Run API' button above.\n#\n# In RStudio 1.1 or older, see the Plumber documentation for details\n# on running the API.\n#\n# Find out more about building APIs with Plumber here:\n#\n#    https://www.rplumber.io/\n#\n# save as bos_rf_score.R\n\nlibrary(brms)\nlibrary(plumber)\nlibrary(tidybayes)\n\nbrms_model <- readRDS(\"brms_model.rds\")\n\n\n#* @apiTitle brms predict Api\n#* @apiDescription Endpoints for working with brms model\n## ---- filter-logger\n#* Log some information about the incoming request\n#* @filter logger\nfunction(req){\n    cat(as.character(Sys.time()), \"-\",\n        req$REQUEST_METHOD, req$PATH_INFO, \"-\",\n        req$HTTP_USER_AGENT, \"@\", req$REMOTE_ADDR, \"\\n\")\n    forward()\n}\n\n## ---- post-data\n#* Submit data and get a prediction in return\n#* @post /predict\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    predict(brms_model, data) |>\n        as.data.frame()\n}\n\n\n#* @post /full_posterior\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    add_epred_draws(data, brms_model) \n    \n}\n\nA continuación activo renv en el proyecto\n\n renv::activate()\n* Project '~/Rstudio_projects/r-api-minimal' loaded. [renv 0.16.0]\n\nUna vez que está activado y el fichero plumber.R está creado en el directorio uso hydrate para que encuentre qué librerías se usan en el proyecto\n\n> renv::hydrate()\n* Discovering package dependencies ... Done!\n* Copying packages into the cache ... Done!\n\ny ya podemos crear el fichero renv::snapshot(), donde pone todas las librerías que se van a instalar y si vienen de CRAN , de GitHub o de RSPM(rstudio package manager)\n\nrenv::snapshot()\nThe following package(s) will be updated in the lockfile:\n\n# CRAN ===============================\n- Matrix           [* -> 1.5-1]\n- R6               [* -> 2.5.1]\n- RColorBrewer     [* -> 1.1-3]\n- Rcpp             [* -> 1.0.9]\n- base64enc        [* -> 0.1-3]\n- bslib            [* -> 0.4.0]\n- cachem           [* -> 1.0.6]\n- codetools        [* -> 0.2-18]\n- colorspace       [* -> 2.0-3]\n- ellipsis         [* -> 0.3.2]\n- fansi            [* -> 1.0.3]\n- farver           [* -> 2.1.1]\n- fastmap          [* -> 1.1.0]\n- generics         [* -> 0.1.3]\n- ggplot2          [* -> 3.3.6]\n- htmltools        [* -> 0.5.3]\n- jquerylib        [* -> 0.1.4]\n- labeling         [* -> 0.4.2]\n- lattice          [* -> 0.20-45]\n- lifecycle        [* -> 1.0.3]\n- magrittr         [* -> 2.0.3]\n- memoise          [* -> 2.0.1]\n- mgcv             [* -> 1.8-40]\n- mime             [* -> 0.12]\n- munsell          [* -> 0.5.0]\n- pkgconfig        [* -> 2.0.3]\n- prettyunits      [* -> 1.1.1]\n- processx         [* -> 3.7.0]\n- ps               [* -> 1.7.1]\n- rappdirs         [* -> 0.3.3]\n- rprojroot        [* -> 2.0.3]\n- sass             [* -> 0.4.2]\n- stringi          [* -> 1.7.8]\n- tibble           [* -> 3.1.8]\n- utf8             [* -> 1.2.2]\n- withr            [* -> 2.5.0]\n\n# GitHub =============================\n- glue             [* -> jimhester/fstrings@HEAD]\n\n# RSPM ===============================\n- BH               [* -> 1.78.0-0]\n- Brobdingnag      [* -> 1.2-9]\n- DT               [* -> 0.26]\n- HDInterval       [* -> 0.2.2]\n- MASS             [* -> 7.3-58.1]\n- RcppEigen        [* -> 0.3.3.9.2]\n- RcppParallel     [* -> 5.1.5]\n- StanHeaders      [* -> 2.21.0-7]\n- abind            [* -> 1.4-5]\n- arrayhelpers     [* -> 1.1-0]\n- backports        [* -> 1.4.1]\n- bayesplot        [* -> 1.9.0]\n- bridgesampling   [* -> 1.1-2]\n- brms             [* -> 2.18.0]\n- callr            [* -> 3.7.2]\n- checkmate        [* -> 2.1.0]\n- cli              [* -> 3.4.1]\n- coda             [* -> 0.19-4]\n- colourpicker     [* -> 1.1.1]\n- commonmark       [* -> 1.8.1]\n- cpp11            [* -> 0.4.3]\n- crayon           [* -> 1.5.2]\n- crosstalk        [* -> 1.2.0]\n- curl             [* -> 4.3.3]\n- desc             [* -> 1.4.2]\n- digest           [* -> 0.6.30]\n- distributional   [* -> 0.3.1]\n- dplyr            [* -> 1.0.10]\n- dygraphs         [* -> 1.1.1.6]\n- fontawesome      [* -> 0.3.0]\n- fs               [* -> 1.5.2]\n- future           [* -> 1.28.0]\n- ggdist           [* -> 3.2.0]\n- ggridges         [* -> 0.5.4]\n- globals          [* -> 0.16.1]\n- gridExtra        [* -> 2.3]\n- gtable           [* -> 0.3.1]\n- gtools           [* -> 3.9.3]\n- htmlwidgets      [* -> 1.5.4]\n- httpuv           [* -> 1.6.6]\n- igraph           [* -> 1.3.5]\n- inline           [* -> 0.3.19]\n- isoband          [* -> 0.2.6]\n- jsonlite         [* -> 1.8.2]\n- later            [* -> 1.3.0]\n- lazyeval         [* -> 0.2.2]\n- listenv          [* -> 0.8.0]\n- loo              [* -> 2.5.1]\n- markdown         [* -> 1.2]\n- matrixStats      [* -> 0.62.0]\n- miniUI           [* -> 0.1.1.1]\n- mvtnorm          [* -> 1.1-3]\n- nleqslv          [* -> 3.3.3]\n- nlme             [* -> 3.1-160]\n- numDeriv         [* -> 2016.8-1.1]\n- parallelly       [* -> 1.32.1]\n- pillar           [* -> 1.8.1]\n- pkgbuild         [* -> 1.3.1]\n- plumber          [* -> 1.2.1]\n- plyr             [* -> 1.8.7]\n- posterior        [* -> 1.3.1]\n- promises         [* -> 1.2.0.1]\n- purrr            [* -> 0.3.5]\n- renv             [* -> 0.16.0]\n- reshape2         [* -> 1.4.4]\n- rlang            [* -> 1.0.6]\n- rstan            [* -> 2.21.7]\n- rstantools       [* -> 2.2.0]\n- scales           [* -> 1.2.1]\n- shiny            [* -> 1.7.2]\n- shinyjs          [* -> 2.1.0]\n- shinystan        [* -> 2.6.0]\n- shinythemes      [* -> 1.2.0]\n- sodium           [* -> 1.2.1]\n- sourcetools      [* -> 0.1.7]\n- stringr          [* -> 1.4.1]\n- svUnit           [* -> 1.0.6]\n- swagger          [* -> 3.33.1]\n- tensorA          [* -> 0.36.2]\n- threejs          [* -> 0.3.3]\n- tidybayes        [* -> 3.0.2]\n- tidyr            [* -> 1.2.1]\n- tidyselect       [* -> 1.2.0]\n- vctrs            [* -> 0.4.2]\n- viridisLite      [* -> 0.4.1]\n- webutils         [* -> 1.1]\n- xfun             [* -> 0.34]\n- xtable           [* -> 1.8-4]\n- xts              [* -> 0.12.2]\n- yaml             [* -> 2.3.6]\n- zoo              [* -> 1.8-11]\n\nThe version of R recorded in the lockfile will be updated:\n- R                [*] -> [4.2.1]\n\nDo you want to proceed? [y/N]: y\n* Lockfile written to '~/Rstudio_projects/r-api-minimal/renv.lock'.\n\nY ya sólo queda crear el Dockerfile usando como base r-hub/minimal\nDockerfile\n# Docker file para modelo brms\n\nFROM rhub/r-minimal:4.2.1\n\n# copio fichero de las librerías\nCOPY renv.lock renv.lock\n\n# uso -c para que se queden instaladas los compiladores de c y fortran\n\nRUN installr -c -a \"curl-dev linux-headers gfortran libcurl libxml2 libsodium-dev libsodium automake autoconf\"\n\n#instalo renv\nRUN installr -c renv\n\n# uso renv para instlar la versión de las librerías que hay en renv.lock\nRUN Rscript -e \"renv::restore()\"\n\n## Copio el modelo y el fichero de la api\nCOPY brms_model.rds /opt/ml/brms_model.rds\nCOPY plumber.R /opt/ml/plumber.R\n\n# exponemos el puerto\nEXPOSE 8081\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8081)\"]\ny como antes construimos el docker image\ndocker build -t mi_modelo_brms_rminimal_renv .\nEl docker usando renv es sustancialmente más pesado, ocupa 1.29 Gb\nSeguramente se puede optimizar más si no usara brms, puesto que importa shinystan, bayesplot y otras librerías que no son estrictamente necesarias para nuestro propósito. Habrá que esperar a que Virgilio haga la función predict de INLA para darle una vuelta a esto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Aquí estoy de nuevo",
    "section": "",
    "text": "Estoy cambiando el blog de blogdown a quarto Welcome!"
  },
  {
    "objectID": "posts/leaflet_ejemplo/index.html",
    "href": "posts/leaflet_ejemplo/index.html",
    "title": "Leaflet example",
    "section": "",
    "text": "library(leaflet)\n\nm <- leaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=174.768, lat=-36.852, popup=\"The birthplace of R\")\nm  # Print the map"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Series\n\nArchive\nEjemplo de crear una serie, ver esto\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\njulia\n\n\nproduccion\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n3 min\n\n\njulia,linux,produccion\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\ndocker\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n36 min\n\n\napi,docker,R\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n4 min\n\n\nR,python\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\nestadística\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n47 min\n\n\nInvestigación operativa\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\nbayesian\n\n\nJulia\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n16 min\n\n\nJulia,julia,análisis bayesiano\n\n\n\n\n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\nestadística\n\n\nR python\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n7 min\n\n\nestadística,Investigación operativa,R,Julia,julia\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n4 min\n\n\nestadística,R python\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n1 min\n\n\nagile\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n9 min\n\n\nJulia,análisis bayesiano\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\nbayesian\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n6 min\n\n\nanálisis bayesiano,causal inference,R\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\nbayesian\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n6 min\n\n\ncausal inference,análisis bayesiano,estadística\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Leaflet example\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n\n\n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Hola a todos\nSoy José Luis Cañadas Reche, científico de datos (o estadístico que sabe algo de programación )\nMe puedes encontrar en Twitter o GitHub y en Fosstodon.\n\n\n\nBlogs antiguo y nuevo\n\nBlog antiguo\n\n\nBlog nuevo\n\n\n\nInvestigación operativa\n\nInvestigación operativa\nEjemplo simple de como usar Julia, R y python para investigación operativa"
  }
]
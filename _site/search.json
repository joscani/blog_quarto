[
  {
    "objectID": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "href": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "title": "EPA, muestreo y partial pooling",
    "section": "",
    "text": "Sale la EPA a a finales de Abril, con datos de Enero a Marzo. Es proverbial el retraso en la publicación de resultados por parte de las administraciones públicas. En intercambio de tweets con Carlos Gil, comentaba la posibilidad de ir actualizando datos poco a poco, en plan como las elecciones: - Al 20% del escrutinio de la EPA, el número de parados y ocupados en España es de X y cosas así.\nGracias a que la EPA tiene un buen diseño muestral no sería tan difícil hacerlo, e incluso realizar buensa estimaciones con poco escrutado, al fin y al cabo la epa es un panel,(los sujetos permanecen en la EPA varias oleadas) y es de suponer que hay cierta relación entre la variable latente “estar en paro” y que se trate del mismo individuo, y relación de esa variable con los de determinado grupo de edad al que pertenece, y que la estimación en otros grupos de edad ayude a estimar la tasa de paro en otro grupo, etc…. En fin, que me lío.\nPues de toda esa estructura e información compartida es de lo que van, grosso modo, los modelos mixtos y el partial pooling. En este mismo blog los he comentado alguna vez y he puesto algún ejemplo. Con la EPA hice hace unos años un ejercicio para ver precisamente cómo, con poca muestra, se pueden tener buenas estimaciones. Aquí os lo dejo, al final hay algunas referencias, justo las que usé (no me gusta poner referencias de cosas que no he leído solo por rellenar).\nPues nada, buen confinamiento, yo voy a ver si instalo la nueva versión de R en mi linux, dudo entre arriesgarme y hacerlo a pelo o usar un docker."
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "",
    "text": "Y seguimos dando vueltas a los datos de post anteriores. Siempre hay quien dice que el bayesiano no sirve para big data y qué se acaba el universo antes de que termine de ajustar tu modelo (esto último creo que se lo he dicho yo alguna vez a Carlos).\nPero ya hemos visto en los dos post anteriores que podemos condensar los datos en menos filas sin perder información, así que , ¿por qué no utilizar un modelo bayesiano?\nDel post anterior\nY tenemos nuestros conjuntos de train y de test en local"
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "Modelo bayesiano.",
    "text": "Modelo bayesiano.\nPues ahora vamos a probar a hacer un modelo bayesiano jerárquico, podríamos hacer el equivalente a glmer usando la librería rstanarm y ajustar varias regresiones logísticas independientes, pero en vez de eso vamos a ver como ajustar directamente la distribución multinomial usando brms.\nLos modelos serían algo así como\n\\[\n\\begin{equation} ans \\sim Multinomial(\\boldsymbol{\\theta}) \\end{equation}\n\\]\nDónde\n\\[\n\\begin{equation}\n\\boldsymbol{\\theta} = \\{\\theta_{Rec}, \\theta_{Best}, \\theta_{Neut}, \\theta_{\\text{No_way}}\\}\n\\end{equation}\n\\]\nLo bueno de stan y de brms es que se puede modelar directamente la Multinomial, es decir, el número de “éxitos” en cada categoría dado un número de intentos. En brms podemos usar trials para especificarlo. Sería el equivalente al weights en glmer. De esta forma podemos trabajar con los datos agregados en vez de tenerlos individuales. Si tengo, 1000 clientes con edad < 21 y valor_cliente = 8, en vez de poner 1000 filas, pongo una columna de frecuencias, que es lo que hemos hecho.\n\nLibrerías\nYo uso cmdstan como backend para brms en vez de rstan, está más actualizado y tarda menos en muestrear.\n\n# Core libraries\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(cmdstanr)\n\n# For beauty plots\nlibrary(ggridges)\n\n## Using all cores. 12 in my machine\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n\n\n\nAdecuando los datos\nPara poder ajustar el modelo de regresión multinomial se necesita tener los datos de una determinada forma, básicamente tener una columna de tipo matriz. Para eso vamos a pivotar los datos y usar cbind\nPivotamos\n\ntrain_wider <-   train_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)), \n    total = Rec + Neut + Best + No_way\n  ) \n\ntest_wider <- test_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)),\n    total = Rec + Neut + Best + No_way\n  )\n\n\nDT::datatable(train_wider)\n\n\n\n\n\n\nY ahora unimos las columnas que indican el conteo en cada perfil de Rec, Best, Neut y NoWay en un columna que es una matriz\n\n# lo hacemos solo para el train, para el test no hace falta\n\ntrain_wider$cell_counts <- with(train_wider, cbind(Rec, Best, Neut, No_way))\nclass(train_wider$cell_counts)\n#> [1] \"matrix\" \"array\"\n\n\nDT::datatable( train_wider %>% \n                 select(tipo, valor_cliente,\n                        cell_counts, everything()\n))\n\n\n\n\n\n\nPues ya podemos ajustar el modelo. Brms tiene una función get_prior para poner las priors por defecto.\nVoy a usar un modelo con efectos aleatorios que tarda unos pocos minutos, pero si usamos cell_counts | trials(total) ~ edad_cat + valor_cliente el modelo se ajusta en menos de 60 segundos. Bueno, vamos a verlo\n\n\nAjuste de los modelos\nModelo efectos fijos\n\nformula_efectos_fijos <- brmsformula(\n  cell_counts | trials(total) ~ edad_cat + valor_cliente\n)\n\n# get priors\npriors <- get_prior(formula_efectos_fijos, train_wider, family = multinomial())\n\ntictoc::tic(\"Modelo efectos fijos\")\nmodel_multinomial1 <- brm(formula_efectos_fijos, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\",\n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 2 finished in 47.9 seconds.\n#> Chain 3 finished in 48.8 seconds.\n#> Chain 1 finished in 49.5 seconds.\n#> Chain 4 finished in 53.0 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 49.8 seconds.\n#> Total execution time: 53.1 seconds.\ntictoc::toc()\n#> Modelo efectos fijos: 70.554 sec elapsed\n\nModelo con efectos aleatorios\nY tarda unos 9 minutos o así\n\nformula <- brmsformula(\n  cell_counts | trials(total) ~ (1|edad_cat) + (1|valor_cliente\n))\n\n# get priors\npriors <- get_prior(formula, train_wider, family = multinomial())\n\nPodemos ver las priors que ha considerado por defecto. Y vemos las priors que ha tomado para modelar la distribución de las \\(\\sigma\\) asociadas a edad_cat y valor_cliente\n\npriors\n#>                 prior     class      coef         group resp    dpar nlpar lb\n#>                (flat) Intercept                                              \n#>  student_t(3, 0, 2.5) Intercept                               muBest         \n#>  student_t(3, 0, 2.5)        sd                               muBest        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5) Intercept                               muNeut         \n#>  student_t(3, 0, 2.5)        sd                               muNeut        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5) Intercept                              muNoway         \n#>  student_t(3, 0, 2.5)        sd                              muNoway        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente      muNoway        0\n#>  ub       source\n#>          default\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n\n\ntictoc::tic(\"modelo mixto\")\nmodel_multinomial2 <- brm(formula, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\", \n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 715.6 seconds.\n#> Chain 4 finished in 728.4 seconds.\n#> Chain 2 finished in 728.8 seconds.\n#> Chain 3 finished in 732.7 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 726.4 seconds.\n#> Total execution time: 732.8 seconds.\ntictoc::toc()\n#> modelo mixto: 755.055 sec elapsed\n\nPodemos ver el modelo con\n\nsummary(model_multinomial2)\n#>  Family: multinomial \n#>   Links: muBest = logit; muNeut = logit; muNoway = logit \n#> Formula: cell_counts | trials(total) ~ (1 | edad_cat) + (1 | valor_cliente) \n#>    Data: train_wider (Number of observations: 184) \n#>   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 12000\n#> \n#> Group-Level Effects: \n#> ~edad_cat (Number of levels: 5) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.95      0.45     0.43     2.14 1.00     2460\n#> sd(muNeut_Intercept)      0.55      0.31     0.23     1.40 1.00     2566\n#> sd(muNoway_Intercept)     0.55      0.29     0.24     1.38 1.00     2237\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      4370\n#> sd(muNeut_Intercept)      4061\n#> sd(muNoway_Intercept)     3810\n#> \n#> ~valor_cliente (Number of levels: 10) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.98      0.30     0.58     1.74 1.00     1792\n#> sd(muNeut_Intercept)      0.53      0.16     0.31     0.94 1.00     1841\n#> sd(muNoway_Intercept)     1.71      0.44     1.07     2.78 1.00     1681\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      3351\n#> sd(muNeut_Intercept)      3528\n#> sd(muNoway_Intercept)     2940\n#> \n#> Population-Level Effects: \n#>                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> muBest_Intercept     -0.05      0.55    -1.15     1.01 1.00     1209     2238\n#> muNeut_Intercept      1.02      0.33     0.35     1.67 1.01     1036     2136\n#> muNoway_Intercept     0.68      0.60    -0.55     1.85 1.00      907     1612\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPintarlo\n\nplot(model_multinomial2, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE incluso ver el modelo en stan\n\nmodel_multinomial2$model\n#> // generated with brms 2.18.0\n#> functions {\n#>   /* multinomial-logit log-PMF\n#>    * Args:\n#>    *   y: array of integer response values\n#>    *   mu: vector of category logit probabilities\n#>    * Returns:\n#>    *   a scalar to be added to the log posterior\n#>    */\n#>   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#>     return multinomial_lpmf(y | softmax(mu));\n#>   }\n#> }\n#> data {\n#>   int<lower=1> N; // total number of observations\n#>   int<lower=2> ncat; // number of categories\n#>   array[N, ncat] int Y; // response array\n#>   array[N] int trials; // number of trials\n#>   // data for group-level effects of ID 1\n#>   int<lower=1> N_1; // number of grouping levels\n#>   int<lower=1> M_1; // number of coefficients per level\n#>   array[N] int<lower=1> J_1; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_1_muBest_1;\n#>   // data for group-level effects of ID 2\n#>   int<lower=1> N_2; // number of grouping levels\n#>   int<lower=1> M_2; // number of coefficients per level\n#>   array[N] int<lower=1> J_2; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_2_muBest_1;\n#>   // data for group-level effects of ID 3\n#>   int<lower=1> N_3; // number of grouping levels\n#>   int<lower=1> M_3; // number of coefficients per level\n#>   array[N] int<lower=1> J_3; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_3_muNeut_1;\n#>   // data for group-level effects of ID 4\n#>   int<lower=1> N_4; // number of grouping levels\n#>   int<lower=1> M_4; // number of coefficients per level\n#>   array[N] int<lower=1> J_4; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_4_muNeut_1;\n#>   // data for group-level effects of ID 5\n#>   int<lower=1> N_5; // number of grouping levels\n#>   int<lower=1> M_5; // number of coefficients per level\n#>   array[N] int<lower=1> J_5; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_5_muNoway_1;\n#>   // data for group-level effects of ID 6\n#>   int<lower=1> N_6; // number of grouping levels\n#>   int<lower=1> M_6; // number of coefficients per level\n#>   array[N] int<lower=1> J_6; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_6_muNoway_1;\n#>   int prior_only; // should the likelihood be ignored?\n#> }\n#> transformed data {\n#>   \n#> }\n#> parameters {\n#>   real Intercept_muBest; // temporary intercept for centered predictors\n#>   real Intercept_muNeut; // temporary intercept for centered predictors\n#>   real Intercept_muNoway; // temporary intercept for centered predictors\n#>   vector<lower=0>[M_1] sd_1; // group-level standard deviations\n#>   array[M_1] vector[N_1] z_1; // standardized group-level effects\n#>   vector<lower=0>[M_2] sd_2; // group-level standard deviations\n#>   array[M_2] vector[N_2] z_2; // standardized group-level effects\n#>   vector<lower=0>[M_3] sd_3; // group-level standard deviations\n#>   array[M_3] vector[N_3] z_3; // standardized group-level effects\n#>   vector<lower=0>[M_4] sd_4; // group-level standard deviations\n#>   array[M_4] vector[N_4] z_4; // standardized group-level effects\n#>   vector<lower=0>[M_5] sd_5; // group-level standard deviations\n#>   array[M_5] vector[N_5] z_5; // standardized group-level effects\n#>   vector<lower=0>[M_6] sd_6; // group-level standard deviations\n#>   array[M_6] vector[N_6] z_6; // standardized group-level effects\n#> }\n#> transformed parameters {\n#>   vector[N_1] r_1_muBest_1; // actual group-level effects\n#>   vector[N_2] r_2_muBest_1; // actual group-level effects\n#>   vector[N_3] r_3_muNeut_1; // actual group-level effects\n#>   vector[N_4] r_4_muNeut_1; // actual group-level effects\n#>   vector[N_5] r_5_muNoway_1; // actual group-level effects\n#>   vector[N_6] r_6_muNoway_1; // actual group-level effects\n#>   real lprior = 0; // prior contributions to the log posterior\n#>   r_1_muBest_1 = sd_1[1] * z_1[1];\n#>   r_2_muBest_1 = sd_2[1] * z_2[1];\n#>   r_3_muNeut_1 = sd_3[1] * z_3[1];\n#>   r_4_muNeut_1 = sd_4[1] * z_4[1];\n#>   r_5_muNoway_1 = sd_5[1] * z_5[1];\n#>   r_6_muNoway_1 = sd_6[1] * z_6[1];\n#>   lprior += student_t_lpdf(Intercept_muBest | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNeut | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNoway | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_5 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_6 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] muBest = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNeut = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNoway = rep_vector(0.0, N);\n#>     // linear predictor matrix\n#>     array[N] vector[ncat] mu;\n#>     muBest += Intercept_muBest;\n#>     muNeut += Intercept_muNeut;\n#>     muNoway += Intercept_muNoway;\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n]\n#>                    + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNeut[n] += r_3_muNeut_1[J_3[n]] * Z_3_muNeut_1[n]\n#>                    + r_4_muNeut_1[J_4[n]] * Z_4_muNeut_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNoway[n] += r_5_muNoway_1[J_5[n]] * Z_5_muNoway_1[n]\n#>                     + r_6_muNoway_1[J_6[n]] * Z_6_muNoway_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       mu[n] = transpose([0, muBest[n], muNeut[n], muNoway[n]]);\n#>     }\n#>     for (n in 1 : N) {\n#>       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#>     }\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#>   target += std_normal_lpdf(z_1[1]);\n#>   target += std_normal_lpdf(z_2[1]);\n#>   target += std_normal_lpdf(z_3[1]);\n#>   target += std_normal_lpdf(z_4[1]);\n#>   target += std_normal_lpdf(z_5[1]);\n#>   target += std_normal_lpdf(z_6[1]);\n#> }\n#> generated quantities {\n#>   // actual population-level intercept\n#>   real b_muBest_Intercept = Intercept_muBest;\n#>   // actual population-level intercept\n#>   real b_muNeut_Intercept = Intercept_muNeut;\n#>   // actual population-level intercept\n#>   real b_muNoway_Intercept = Intercept_muNoway;\n#> }\n\nViendo el código en stan que genera brms utiliza parametrización con multinomial_lpmf que toma el log de la probabilidad de la multinomial y usa softmax sobre el predictor lineal. multivariate_discrete_stan\nEn la parte de functions tiene\nreal multinomial_logit2_lpmf(int[] y, vector mu) {\n      return multinomial_lpmf(y | softmax(mu));\n  }\nY en la de model\n   for (n in 1:N) {\n      target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n     }\nY en la parte del predictor lineal mu[n] es dónde ha ido añadiendo los group levels effects.\nPor ejemplo la parte de la edad_cat para la categoría Best está en la parte de transformed parameters dónde z_1[1] se modela como normal y sd_1 como una t de student\nr_1_muBest_1 = (sd_1[1] * (z_1[1]));\nY en la parte de model va añadiendo términos al muBest que es al final el que entra en la parte de la verosimilitud.\nmuBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n] + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\nAquí añade el efecto de la edad r_1_muBest_1[J_1[n]] lo multiplica por Z_1_mubest_1[n] que es el indicador en los datos de la matriz Z para los efectos aleatorios (todo igual a 1) y luego añade el efecto de la variable valor_cliente.\nLa verdad es que eel bloque model que genera brms es un poco complicado. Imagino que genera código optimizado. Para los que quieran verlo todo con stan directamente este libro tiene un ejemplo básico\nEn brms tenemos la función make_standata que nos genera los datos tal y como se los pasa a Stan.\n\ndatos_stan <- make_standata(formula, data = train_wider, \n              family = multinomial(),\n              prior =  priors)\n\n\nnames(datos_stan)\n#>  [1] \"N\"             \"Y\"             \"trials\"        \"ncat\"         \n#>  [5] \"K_muBest\"      \"X_muBest\"      \"Z_1_muBest_1\"  \"Z_2_muBest_1\" \n#>  [9] \"K_muNeut\"      \"X_muNeut\"      \"Z_3_muNeut_1\"  \"Z_4_muNeut_1\" \n#> [13] \"K_muNoway\"     \"X_muNoway\"     \"Z_5_muNoway_1\" \"Z_6_muNoway_1\"\n#> [17] \"J_1\"           \"J_2\"           \"J_3\"           \"J_4\"          \n#> [21] \"J_5\"           \"J_6\"           \"N_1\"           \"M_1\"          \n#> [25] \"NC_1\"          \"N_2\"           \"M_2\"           \"NC_2\"         \n#> [29] \"N_3\"           \"M_3\"           \"NC_3\"          \"N_4\"          \n#> [33] \"M_4\"           \"NC_4\"          \"N_5\"           \"M_5\"          \n#> [37] \"NC_5\"          \"N_6\"           \"M_6\"           \"NC_6\"         \n#> [41] \"prior_only\"\n\n\n# datos\ndatos_stan$N\n#> [1] 184\n\n# numero de niveles edad\ndatos_stan$N_1\n#> [1] 5\n\n# numero niveles valor_cliente\ndatos_stan$N_2\n#> [1] 10\n\nEn los J_1, J_2, está codificado a que nivel de edad y valor_cliente perteneces esa fila. J_3 y J_4 es igual a J_1 y J_2. Lo repite para cada categoría de respuesta.\n\ndatos_stan$J_1\n#>   [1] 1 2 2 3 4 2 4 5 1 5 3 3 4 2 2 2 3 4 1 3 4 4 5 1 2 2 3 4 5 1 1 2 3 3 4 1 3\n#>  [38] 4 4 1 1 1 2 5 1 2 4 4 5 1 1 2 4 4 5 5 2 3 4 5 2 2 4 4 4 5 5 3 4 1 4 1 3 4\n#>  [75] 1 1 2 2 3 4 5 5 5 4 5 1 3 3 4 5 1 1 3 4 5 1 1 3 5 1 2 3 3 1 4 5 3 1 1 3 1\n#> [112] 1 2 3 3 1 2 3 1 2 2 5 3 3 2 3 5 1 4 5 3 5 5 2 4 5 1 2 2 5 1 3 3 2 1 2 4 2\n#> [149] 3 1 3 4 4 1 3 4 5 5 3 4 5 2 4 5 3 4 2 5 1 4 1 2 2 1 2 3 5 2 2 4 3 2 5 3\n\n\ndatos_stan$J_2\n#>   [1]  1  1  1  1  1  2  2  3  3  3  3  3  3  4  5  5  5  5  6  6  6  6  7  7  7\n#>  [26]  7  7  7  8  8  8  8  8  8  8  9  9  9  9 10 10  1  1  2  2  2  2  2  4  3\n#>  [51]  3  3  3  3  4  4  4  4  4  5  5  5  5  5  5  6  6  6  6  7  7  8  8  8  9\n#>  [76]  9  9  9  9  9  1  1  1  1  2  2  2  2  3  4  4  4  4  4  5  5  5  5  6  6\n#> [101]  6  6  6  7  7  8 10  1  1  1  2  2  2  2  3  3  3  3  5  6  6  7  7  7  8\n#> [126]  8  9  9  9  1  1  2  2  2  2  3  4  4  4  5  5  5  5  6  8  8  8  9  9 10\n#> [151]  1  1  1  4  4  4  6  7  7  7  8  8  8  9  2  4  3  5  6  6  7  7  1  6  9\n#> [176]  9  3  3  7 10  4 10  8 10\n\nPero yo estoy interesado en ver 2 cosas, como de bien predice sobre test y cuál es la probabilidad de cada clase condicionada a cada perfil\nPredicción\nPodemos obtener o bien todas las estimaciones o resumirlas\n\npredicciones_test <-  posterior_predict(model_multinomial2, newdata = test_wider)\n\nAquí lo que tenemos es un array de dimensiones 12000, 180, 4 . Que se corresponde a tener las 12000 estimaciones ( 4 cadenas x 3000 muestras efectivas) , para las 180 filas del conjunto de test\n\ndim(predicciones_test)\n#> [1] 12000   181     4\n\nPor ejemplo para la fila 35 de test que sería\n\ntest_wider[1,]\n#> # A tibble: 1 × 8\n#>   tipo  valor_cliente edad_cat   Rec  Best  Neut No_way total\n#>   <fct>         <dbl> <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl>\n#> 1 C                 0 21- 40     141   110   336     80   667\n\nY las predicciones (de la 1 a la 20) de las 1200\n\npredicciones_test[1:20, 1, ]\n#>       Rec Best Neut No_way\n#>  [1,]  75  153  255    184\n#>  [2,]  74  151  259    183\n#>  [3,]  78  126  239    224\n#>  [4,]  88  157  224    198\n#>  [5,]  79  148  260    180\n#>  [6,]  82  134  257    194\n#>  [7,]  61  145  250    211\n#>  [8,]  60  152  257    198\n#>  [9,]  80  132  242    213\n#> [10,]  62  136  263    206\n#> [11,]  72  151  272    172\n#> [12,]  75  133  259    200\n#> [13,]  78  150  240    199\n#> [14,]  78  129  244    216\n#> [15,]  78  137  249    203\n#> [16,]  73  136  265    193\n#> [17,]  68  142  251    206\n#> [18,]  68  157  234    208\n#> [19,]  67  143  248    209\n#> [20,]  62  180  227    198\n\nComo ahora todo es tidy voy a usar tidybayespara tener esa predicción.\n\npredicciones_tidy <- test_wider %>% \n  add_epred_draws(model_multinomial2) \n\nY se nos ha quedado un dataset muy muy grande\n\ndim(predicciones_tidy)\n#> [1] 8688000      14\n\n\nDT::datatable(predicciones_tidy %>% \n                ungroup() %>% \n                sample_n(30) %>% \n                select(edad_cat, valor_cliente,.category, .epred))\n\n\n\n\n\n\nPero si quisiéramos pintar las probabilidades estimadas tendríamos que dividir el valor predicho de cada categoría por el total de clientes en cada fila del conjunto de datos de test. Hay una forma más sencilla construyendo un conjunto de datos que tenga todas las combinaciones de edad_cat y valor_cliente y añadiendo columna totalcon valor 1.\n\n\nfake_data <- test_wider %>% \n  tidyr::expand(edad_cat, valor_cliente) %>% \n  mutate(total = 1)\n\n\ndf_pintar <-  fake_data %>% \n  add_epred_draws(model_multinomial2) %>% \n  mutate(valor_cliente = as_factor(valor_cliente))\n\nDe esta forma, al tener total = 1, el modelo devuelve la probabilidad de cada clase, si total = 13, hubiera devuelto “el reparto” de esos 13 individuos en los 4 grupos\n\nDT::datatable(df_pintar %>% \n  sample_n(30) %>% \n  select(edad_cat, valor_cliente, .category, .epred))\n\n\n\n\n\n\nAñadir las 12000 predicciones por fila ya “sólo” nos deja unos 2 millones de filas\n\ndim(df_pintar)\n#> [1] 2400000       9\n\nPintemos\nPor ejemplo si queremos ver las estimaciones que le da según la edad podemos ver la distribución posteriori de la probabilidad de cada segmento condicionada a cada grupo de edad. Salen distribuciones con varias modas debido a la variable valor_cliente que no estamos representando\n\ndf_pintar %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n             ) +\n  geom_density_ridges(scale = 0.8, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() \n\n\n\n\n\n\n\n\nSi vemos la posteriori para los clientes de mayor valor. Se ve claramente que a menor edad mayor probabilidad de pertenecer al segmento “Best” , mientras que a mayor edad es mucho más probabilidad del segmento “No_way”.\n\ndf_pintar %>%  \n  filter(valor_cliente == 0) %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Cliente valor: 0\")\n\n\n\n\n\n\n\n\nTeniendo toda la distribución podemos ver los resultados desde otro punto de vista. Por ejemplo, ver las probabilidades para los menores de 21.\n\ndf_pintar %>%  \n  filter(edad_cat %in% c(\"<21\")) %>% \n  ggplot(aes(x=.epred, y = valor_cliente, fill = .category) \n  ) +\n  geom_density_ridges(scale = 3, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Clientes menores de 21\\n Probabilidades estimadas\")\n\n\n\n\n\n\n\n\nEn fin, que se puede hacer estadística bayesiana aún con grandes volúmenes de datos, si te conviertes en lo que mi amigo Antonio llama un “artesano del dato”.\nFeliz semana"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "Series\n\nCachitos nochevieja\nPost relacionados con extracción de imágenes y análisis de subtítulos de “cachitos nochevieja”\n\n\nJulia\nPost relacionados Julia\n\n\n\nTodos los posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMeta-análisis. Agregando encuestas\n\n\n\n\n\n\n\nmuestreo\n\n\n2023\n\n\nencuestas electorales\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nCategóricas a lo catboost. Pensamientos\n\n\n\n\n\n\n\nEstadística\n\n\ncategóricas\n\n\nR\n\n\ncatboost\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nMapeando\n\n\n\n\n\n\n\nEstadística\n\n\ngis\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\n\n\n\nEstadística\n\n\nmachine learning\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nConformal prediction. Estilo compadre\n\n\n\n\n\n\n\nEstadística\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nArrow y S3\n\n\n\n\n\n\n\nbig data\n\n\nR\n\n\nC++\n\n\nS3\n\n\nAWS\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nExplicatividad no usual\n\n\n\n\n\n\n\nestadística\n\n\nranger\n\n\nExplicatividad\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\n\n\n\nestadística\n\n\nbrms\n\n\nanálisis bayesiano\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nConsejos para dejar spss\n\n\n\n\n\n\n\nestadística\n\n\nsociología\n\n\n2022\n\n\nspss\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\nJulia\n\n\nproduccion\n\n\nlinux\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\ndocker\n\n\nR\n\n\napi\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n37 min\n\n\n\n\n\n\n  \n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\nestadística\n\n\nInvestigación operativa\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n48 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\nJulia\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\nestadística\n\n\npython\n\n\nR\n\n\nInvestigación operativa\n\n\nJulia\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTransparente\n\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nPluralista\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\ntextmining\n\n\nocr\n\n\n2022\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nCocinando\n\n\n\n\n\n\n\nmuestreo\n\n\n2022\n\n\nencuestas electorales\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\n\n\n\nestadística\n\n\nspark\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nLecturas para el finde\n\n\n\n\n\n\n\nestadística\n\n\nanálisis bayesiano\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n¿A dónde va Vicente?\n\n\n\n\n\n\n\nárboles\n\n\nciencia de datos\n\n\nh2o\n\n\nestadísticca\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\ncorrespondencias\n\n\nfactorización\n\n\nfactorial\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n¿A/B qué?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\n\n\n\nR\n\n\npython\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2021\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\nsoftware\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nImputando datos. La estructura importa\n\n\n\n\n\n\n\nestadística\n\n\nimputación\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\n\n\n\nestadística\n\n\nbig data\n\n\nanálisis bayesiano\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\n\n\n\n2021\n\n\nR\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\n\n\n\nciencia de datos\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2021\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nUna colina\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\nlinux\n\n\npolémica\n\n\nocr\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTendencias\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n¿Y si … ? Parte II\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2020\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n¿Y si … ? Parte I\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nEjemplillo con NMF\n\n\n\n\n\n\n\nestadística\n\n\ncorrespondencias\n\n\nfactorización\n\n\nnmf\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\n\n\n\nestadística\n\n\nempresas\n\n\nbig data\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nR 4.0.2 en amazon linux\n\n\n\n\n\n\n\nR\n\n\nlinux\n\n\naws\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nPredicción, Estimación y Atribución\n\n\n\n\n\n\n\nestadística\n\n\nciencia de datos\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2020\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nFactoriales….\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nEPA, muestreo y partial pooling\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nEncuesta\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmuestreo\n\n\nencuestas\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nEstimación muy burda del número de contagios.\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2020\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nEl virus\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLecciones aprendidas instalando paquetes de R\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCosas de pandas\n\n\n\n\n\n\n\nR python\n\n\npython\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2020\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nFinde de cacharreo\n\n\n\n\n\n\n\nsoftware\n\n\nh2o\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLa fatal arrogancia\n\n\n\n\n\n\n\nciencia de datos\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2019\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nCosas que deben cambiar\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2019\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nCodificación parcial y python\n\n\n\n\n\n\n\n2019\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2019\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nMalditas proporciones pequeñas III\n\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nEl randomforest no nos deja ver el árbol\n\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\nciencia de datos\n\n\ntiempos modernos\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2019\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nMalditas proporciones pequeñas II\n\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2019\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nMalditas proporciones pequeñas I\n\n\n\n\n\n\n\n2019\n\n\nestadística\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2019\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 23, 2023\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\nMar 26, 2023\n\n\nConformal prediction. Estilo compadre\n\n\n\n\nFeb 19, 2023\n\n\nArrow y S3\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\nDec 4, 2022\n\n\nConsejos para dejar spss\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 29, 2022\n\n\nLeaflet example\n\n\n\n\nOct 27, 2022\n\n\nAquí estoy de nuevo\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\nDec 30, 2020\n\n\n¿Y si … ? Parte II\n\n\n\n\nNov 15, 2020\n\n\n¿Y si … ? Parte I\n\n\n\n\nOct 21, 2020\n\n\nEjemplillo con NMF\n\n\n\n\nOct 18, 2020\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\nOct 15, 2020\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\nAug 20, 2020\n\n\nR 4.0.2 en amazon linux\n\n\n\n\nJun 11, 2020\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\nJun 7, 2020\n\n\nPredicción, Estimación y Atribución\n\n\n\n\nJun 4, 2020\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\nJun 2, 2020\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\nMay 24, 2020\n\n\nFactoriales….\n\n\n\n\nApr 28, 2020\n\n\nEPA, muestreo y partial pooling\n\n\n\n\nApr 8, 2020\n\n\nEncuesta\n\n\n\n\nMar 29, 2020\n\n\nEstimación muy burda del número de contagios.\n\n\n\n\nMar 10, 2020\n\n\nEl virus\n\n\n\n\nMar 1, 2020\n\n\nLecciones aprendidas instalando paquetes de R\n\n\n\n\nFeb 17, 2020\n\n\nCosas de pandas\n\n\n\n\nFeb 8, 2020\n\n\nFinde de cacharreo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2021.html",
    "href": "2021.html",
    "title": "2021",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2023.html",
    "href": "2023.html",
    "title": "2023",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 23, 2023\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\nMar 26, 2023\n\n\nConformal prediction. Estilo compadre\n\n\n\n\nFeb 19, 2023\n\n\nArrow y S3\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2020/04/08/encuesta/index.html",
    "href": "2020/04/08/encuesta/index.html",
    "title": "Encuesta",
    "section": "",
    "text": "Estudio serológico covid19\nEl muestreo, ese gran olvidado. Se trata de una herramienta muy útil cuando se quiere saber una o varias características de una población pero, por lo que sea, no es factible abordar a toda la población o porque se necesita tener una idea aproximada de dicha característica de forma rápida.\nBueno, pues si queremos saber el porcentaje y el número de personas que han tenido el coronavirus por provincia la herramienta adecuada es el muestreo. Se trata de elegir una muestra representativa a nivel provincial y hacerles test serológicos a todos los incluidos en esa muestra y así poder extrapolar los resultados al conjunto de la provincia. Por fin, el Instituto de Salud Carlos III junto con el INE van a realizar dicho estudio.\nEn todo muestreo hay una fase crucial, que es la del diseño muestral, tengo que decir que después de leer el documento técnico que me parece un muy buen diseño. Se trata de un diseño bietápico estratificado con un tamaño mínimo de 600 personas por provincia y dónde por ejemplo se van a hacer 5000 encuestas en Madrid.\nEl tamaño muestral total elegido, 30 mil hogares (unos 62 mil personas), y la implicación del INE garantizan la rigurosidad y la representatividad de la encuesta. Yo por mi parte, solo comentar que una vez se tengan los microdatos, existen formas de mejorar algo las estimaciones en áreas pequeñas, entendiendo áreas pequeñas a combinaciones de variables con poca representación en la muestra, por ejemplo, si quisieramos saber la proporción de mujeres contagiadas en Cádiz cuya edad esté entre 20 y 25 años. En ese caso, es probable que una estimación directa\n\\[ \\hat{Prop} = \\dfrac{\\text{Positivos en ese grupo}}{\\text{Total personas encuestadas en ese grupo}} \\] sea poco precisa debido a que haya caído poca muestra en ese grupo.\nPara estos casos puede ser útil la utilización de estimaciones con partial pooling, entrada blog. A colación de esto, hice un estudio hace unos años sobre como incluso con poca muestra las estimaciones de este tipo suelen arrojar mejores estimaciones, aquí"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html",
    "href": "2023/02/19/Arrow y S3/index.html",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#ejemplo-1",
    "href": "2023/02/19/Arrow y S3/index.html#ejemplo-1",
    "title": "Arrow y S3",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\nMostrar / ocultar código\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket <- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#>  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#>  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#>  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#> [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#> [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n\n\n\n\nMostrar / ocultar código\ntic()\nbucket <- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi <- open_dataset(bucket) \nremote_taxi\n#> FileSystemDataset with 158 Parquet files\n#> vendor_name: string\n#> pickup_datetime: timestamp[ms]\n#> dropoff_datetime: timestamp[ms]\n#> passenger_count: int64\n#> trip_distance: double\n#> pickup_longitude: double\n#> pickup_latitude: double\n#> rate_code: string\n#> store_and_fwd: string\n#> dropoff_longitude: double\n#> dropoff_latitude: double\n#> payment_type: string\n#> fare_amount: double\n#> extra: double\n#> mta_tax: double\n#> tip_amount: double\n#> tolls_amount: double\n#> total_amount: double\n#> improvement_surcharge: double\n#> congestion_surcharge: double\n#> pickup_location_id: int64\n#> dropoff_location_id: int64\n#> year: int32\n#> month: int32\ntoc()\n#> 6.287 sec elapsed\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero. (Viendo el htop de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\nMostrar / ocultar código\ntic()\nresult <- remote_taxi |> \n    filter(year == 2019, month == 1) |>\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n    ) |>\n    mutate(pct_shared = shared_trips / all_trips * 100) |>\n    collect()\n\nresult |> \n    print(n = 200)\n#> # A tibble: 1 × 3\n#>   all_trips shared_trips pct_shared\n#>       <int>        <int>      <dbl>\n#> 1   7667255      2094091       27.3\ntoc()\n#> 12.59 sec elapsed\n\n\nNo está mal , ¿verdad?"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#ejemplo-2",
    "href": "2023/02/19/Arrow y S3/index.html#ejemplo-2",
    "title": "Arrow y S3",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\n\nMostrar / ocultar código\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun <- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds <- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n\n\nCuántos datos\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <int>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 24.383 sec elapsed\n\n\nY usando duckdb como engine de consulta .\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    to_duckdb() %>%\n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <dbl>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 33.624 sec elapsed\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente."
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#nota",
    "href": "2023/02/19/Arrow y S3/index.html#nota",
    "title": "Arrow y S3",
    "section": "Nota",
    "text": "Nota\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta documentación\nFeliz domingo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hola a todos",
    "section": "",
    "text": "Hola a todos\nSoy José Luis Cañadas Reche, científico de datos (o estadístico que sabe algo de programación )\nMe puedes encontrar en Twitter o GitHub y en Fosstodon.\n\n\n\nBlogs: Últimos post y enlaces a blog antiguo y nuevo\n\nMeta-análisis. Agregando encuestas\n\n\nCategóricas a lo catboost\n\n\nMapeando\n\n\nRegresión cuantil con catboost\n\n\nConformal prediction. Estilo compadre\n\n\nArrow y S3\n\n\nExplicatividad no usual\n\n\nBlog nuevo\n\n\nBlog antiguo\n\n\n\nInvestigación operativa\n\nEjemplo simple\nEjemplo simple de como usar Julia, R y python para investigación operativa\n\n\nLocalización: Indios y jefes\nAsignar localizaciones según demanda y diferentes restricciones. Un problema usual"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html",
    "href": "2023/02/19/Arrow_y_s3/index.html",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#ejemplo-1",
    "href": "2023/02/19/Arrow_y_s3/index.html#ejemplo-1",
    "title": "Arrow y S3",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\nCode\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket &lt;- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#&gt;  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#&gt;  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#&gt;  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#&gt; [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#&gt; [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n\n\n\n\nCode\ntic()\nbucket &lt;- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi &lt;- open_dataset(bucket) \nremote_taxi\n#&gt; FileSystemDataset with 158 Parquet files\n#&gt; vendor_name: string\n#&gt; pickup_datetime: timestamp[ms]\n#&gt; dropoff_datetime: timestamp[ms]\n#&gt; passenger_count: int64\n#&gt; trip_distance: double\n#&gt; pickup_longitude: double\n#&gt; pickup_latitude: double\n#&gt; rate_code: string\n#&gt; store_and_fwd: string\n#&gt; dropoff_longitude: double\n#&gt; dropoff_latitude: double\n#&gt; payment_type: string\n#&gt; fare_amount: double\n#&gt; extra: double\n#&gt; mta_tax: double\n#&gt; tip_amount: double\n#&gt; tolls_amount: double\n#&gt; total_amount: double\n#&gt; improvement_surcharge: double\n#&gt; congestion_surcharge: double\n#&gt; pickup_location_id: int64\n#&gt; dropoff_location_id: int64\n#&gt; year: int32\n#&gt; month: int32\ntoc()\n#&gt; 8.916 sec elapsed\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero. (Viendo el htop de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\nCode\ntic()\nresult &lt;- remote_taxi |&gt; \n    filter(year == 2019, month == 1) |&gt;\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count &gt; 1, na.rm = TRUE)\n    ) |&gt;\n    mutate(pct_shared = shared_trips / all_trips * 100) |&gt;\n    collect()\n\nresult |&gt; \n    print(n = 200)\n#&gt; # A tibble: 1 × 3\n#&gt;   all_trips shared_trips pct_shared\n#&gt;       &lt;int&gt;        &lt;int&gt;      &lt;dbl&gt;\n#&gt; 1   7667255      2094091       27.3\ntoc()\n#&gt; 12.982 sec elapsed\n\n\nNo está mal , ¿verdad?"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#ejemplo-2",
    "href": "2023/02/19/Arrow_y_s3/index.html#ejemplo-2",
    "title": "Arrow y S3",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\n\nCode\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun &lt;- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds &lt;- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n\n\nCuántos datos\n\n\nCode\n\ntic()\nres &lt;- ds %&gt;% \n    filter(year == 2021 & month == 3)  |&gt; \n    select(year, month, day ) |&gt; \n    group_by(day) |&gt; \n    summarise(\n        n_filas = n()\n    ) |&gt; \n    collect()\n\nres |&gt; \n    arrange(day) |&gt; \n    print(n  = 10)\n#&gt; # A tibble: 30 × 2\n#&gt;      day n_filas\n#&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1     1 6260454\n#&gt;  2     2 6245312\n#&gt;  3     3 6243455\n#&gt;  4     4 6242304\n#&gt;  5     5 6241816\n#&gt;  6     6 6241089\n#&gt;  7     7 6241633\n#&gt;  8     8 6241651\n#&gt;  9    10 6240299\n#&gt; 10    11 6239817\n#&gt; # … with 20 more rows\ntoc()\n#&gt; 33.974 sec elapsed\n\n\nY usando duckdb como engine de consulta .\n\n\nCode\n\ntic()\nres &lt;- ds %&gt;% \n    filter(year == 2021 & month == 3)  |&gt; \n    select(year, month, day ) |&gt; \n    to_duckdb() %&gt;%\n    group_by(day) |&gt; \n    summarise(\n        n_filas = n()\n    ) |&gt; \n    collect()\n\nres |&gt; \n    arrange(day) |&gt; \n    print(n  = 10)\n#&gt; # A tibble: 30 × 2\n#&gt;      day n_filas\n#&gt;    &lt;int&gt;   &lt;dbl&gt;\n#&gt;  1     1 6260454\n#&gt;  2     2 6245312\n#&gt;  3     3 6243455\n#&gt;  4     4 6242304\n#&gt;  5     5 6241816\n#&gt;  6     6 6241089\n#&gt;  7     7 6241633\n#&gt;  8     8 6241651\n#&gt;  9    10 6240299\n#&gt; 10    11 6239817\n#&gt; # … with 20 more rows\ntoc()\n#&gt; 38.924 sec elapsed\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente."
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#nota",
    "href": "2023/02/19/Arrow_y_s3/index.html#nota",
    "title": "Arrow y S3",
    "section": "Nota",
    "text": "Nota\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta documentación\nFeliz domingo"
  },
  {
    "objectID": "2020/03/29/estimación-muy-burda-del-número-de-contagios/index.html",
    "href": "2020/03/29/estimación-muy-burda-del-número-de-contagios/index.html",
    "title": "Estimación muy burda del número de contagios.",
    "section": "",
    "text": "Leo por ahí estimaciones de que hay en España más de 1 millón de contagiados y la verdad es que no tengo ni idea. Pero no se me ocurre ir poniendo ese dato por ahí como verdad absoluta, como hacen algunos .\n\n\nHagamos un ejercicio simple y muy burdo, lo reconozco. Supongamos que el número de fallecidos por coronavirus está bien recogido, lo miro en mi dashboard que para eso lo hice y me dice que hoy 29 de Marzo hay un total acumulado de 6528 fallecidos.\n\n\nLos de las estimaciones de más de un millón de contagiados me dicen que usan una tasa de letalidad global del 2% ergo, le saldrían que esos 6528 se corresponderían con una población de contagiados de 326400, bastante lejos del más de un millón..\n\n\nQue si, que ya sé que desde que te contagias hasta que te recuperas pueden pasar de 6 a 22 días, pero aún así me parece que no se puede dar esa cifra tan alegremente.\n\n\nOtras estimaciones algo mas serias, también burdas, pero al menos se reconoce y se intenta medir la incertidumbre dan una cifra (si se ejecuta el modelo) entre 150 mil y 300 mil contagiados, aquí y el github con el código\n\n\nEn fin, yo voy a hacer el ejercicio de aplicar unas tasas de letalidad por edad, por ver qué sale. Ya aviso, que esto tiene la validez que tiene , aunque no creo que mucha menos que la de la empresa del millón.\n\n\nLeemos fallecidos oficiales por edad y sexo del repo de datadista (muchas gracias )\n\nlibrary(tidyverse)\n## ── Attaching packages ───────────────────────────────────────────────────────────── tidyverse 1.3.0.9000 ──\n## ✓ ggplot2 3.3.0     ✓ purrr   0.3.3\n## ✓ tibble  2.1.3     ✓ dplyr   0.8.5\n## ✓ tidyr   1.0.2     ✓ stringr 1.4.0\n## ✓ readr   1.3.1     ✓ forcats 0.5.0\n## ── Conflicts ───────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\ndat1 <- read_csv(\"https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/nacional_covid19_rango_edad.csv\")\n## Parsed with column specification:\n## cols(\n##   fecha = col_date(format = \"\"),\n##   rango_edad = col_character(),\n##   sexo = col_character(),\n##   casos_confirmados = col_double(),\n##   hospitalizados = col_double(),\n##   ingresos_uci = col_double(),\n##   fallecidos = col_double()\n## )\n\nLas tasas de letalidad que vamos a utilizar vienen de un estudio en China, estudio\n\n\n\n\n\n\nAge\n\n\n(deaths/cases)\n\n\nCFR (95% CI)\n\n\n\n\n\n\n≤ 9 years\n\n\n(0/416)\n\n\n0%\n\n\n\n\n10 to 19 years\n\n\n(1/549)\n\n\n0.18% (0.03 to 1.02%)\n\n\n\n\n20 to 49 years\n\n\n(63/19790)\n\n\n0.32% (0.25% to 0.41%)\n\n\n\n\n50 to 59 years\n\n\n(130/10,008)\n\n\n1.3% (1.1% to 1.5%)\n\n\n\n\n60 to 69. years\n\n\n(309/8583)\n\n\n3.6% (3.2% to 4.0%)\n\n\n\n\n70 to 79 years\n\n\n(312/3918)\n\n\n8.0% (7.2% to 8.9%)\n\n\n\n\n≥80 years\n\n\n(208/1408)\n\n\n14.8% (13.0% to 16.7%)\n\n\n\n\n\ntmp <- dat1 %>%\n    filter(rango_edad != \"Total\" & sexo == \"ambos\")  %>%\n    mutate(rango_edad_2 =\n               fct_collapse(rango_edad,\n                   \"20-49\" = c(\"20-29\",\"30-39\",\"40-49\"),\n                   \">= 80\"  = c(\"80 y +\",\"80-89\",\"90 y +\")\n               )\n           ) %>% \n    group_by(rango_edad_2) %>% \n    summarise(fallecidos = sum(fallecidos))\n\ntmp$tasa_letalidad <-  c(0, 0.18, 0.32, 1.3, 3.6,8, 14.8)/100\n\nY nos saldría nuestra estimación estilo compadre de esta forma\n\n(tmp <-  tmp %>% \n    mutate(contagiados_estim = fallecidos / tasa_letalidad) )\n## # A tibble: 7 x 4\n##   rango_edad_2 fallecidos tasa_letalidad contagiados_estim\n##   <fct>             <dbl>          <dbl>             <dbl>\n## 1 0-9                   0         0                   NaN \n## 2 10-19                 7         0.0018             3889.\n## 3 20-49               194         0.0032            60625 \n## 4 50-59               245         0.013             18846.\n## 5 60-69               797         0.036             22139.\n## 6 70-79              2253         0.08              28162.\n## 7 >= 80              5966         0.148             40311.\n\nY un total de contagiados de\n\nsum(tmp$contagiados_estim, na.rm=T)\n## [1] 173972.2\n\nEn fin, y oigan, si con el número de fallecidos el número de contagiados es el que dice esa empresa, no sería tan mala noticia, significaría que la letalidad es más baja de lo que dicen ciertos estudios."
  },
  {
    "objectID": "2020/03/10/el-virus/index.html",
    "href": "2020/03/10/el-virus/index.html",
    "title": "El virus",
    "section": "",
    "text": "En estos tiempos tan asépticos ya no estamos acostumbrados (en algunos países), a tratar con agentes patógenos altamente contagiosos como el que llena los titulares de periódicos y televisiones estos días.\nSin más, vamos a comparar los datos de España e Italia, plagiando con total descaro a mi amigo Carlos Gil que puso este post de ayer y en este de hoy.\nYo me acabo de enterar de que estoy en cuarentena preventiva por un posible contagio de la mujer de un compañero, así que en casita a teletrabajar unos días.\nEl código\nA pintar.\nSi quitamos 9 días a la fecha de España vemos que estamso alineados a como estaba Itala en ese día.\nY bueno, sigo plagiando a Carlos."
  },
  {
    "objectID": "2020/03/10/el-virus/index.html#mapa-en-leaflet",
    "href": "2020/03/10/el-virus/index.html#mapa-en-leaflet",
    "title": "El virus",
    "section": "\nMapa en leaflet\n",
    "text": "Mapa en leaflet\n\n\nY ahora un mapita con los datos del último día, del 9 de Marzo en el momento de escribir estas líneas\n\ncvirus_map_data <- cvirus_longer %>%\n  filter(fecha == max(fecha))\n\n\npal <- colorNumeric(\n  palette = \"Reds\",\n  domain = c(-1, log(max(cvirus_map_data$casos + 1)))\n)\n\n\nleaflet(cvirus_map_data) %>%\n  # addProviderTiles('CartoDB.Positron') %>%\n  addProviderTiles(\"Stamen.Toner\") %>%\n  addCircleMarkers(\n    lng = ~Long,\n    lat = ~Lat,\n    label = ~ paste0( país, \": \", casos ),\n    radius = ~ 3 * log( casos + 1 ) ,\n    color = ~ pal(log( casos + 1 ) )\n  ) \n\n\n\n\n\nY me he quedado con ganas de hacer un mapa con mapview del estilo de estos, mapview, pero la verdad es que entre unas cosas y otras hoy ando bastante cansado. Mañana lo hago."
  },
  {
    "objectID": "2020/03/01/lecciones-aprendidas-instalando-paquetes-de-r/index.html",
    "href": "2020/03/01/lecciones-aprendidas-instalando-paquetes-de-r/index.html",
    "title": "Lecciones aprendidas instalando paquetes de R",
    "section": "",
    "text": "Ay, la nube.. que bien suena ¿verdad? Si, hasta que te toca pelearte con amazonlinux y versiones viejunas de R. Total, que me ha tocado lidiar un poco con la versión de R 3.4.1 de hace ya 3 años y tener que compilar en mi máquina un montón de librerías para amazon linux (que viene siendo un centos 7 modificado por aws)\nAsí que lo primero es montarse un Dockerfile dónde id diciendo qué librerías de sistemas hay que añadir, y alguna ñapa por problemas con el compilador de C.\nFROM amazonlinux:2018.03-with-sources\nMAINTAINER canadasreche@gmail.com \n\n# Update yum\nRUN yum -y update \n\n# set locales\nRUN echo \"LANG=en_US.utf8\" >> /etc/locale.conf\n#RUN localedef -c -f UTF-8 -i en_US en_US.UTF-8\nRUN export LC_ALL=en_US.UTF-8\n\n\n# Install system libraries\n# El make -j 8 es para que al compilar en c use 9 jobs\nRUN export MAKE='make -j 8'\nRUN yum install -y xorg-x11-xauth.x86_64 xorg-x11-server-utils.x86_64 xterm libXt libX11-devel \\\nlibXt-devel libcurl-devel git compat-gmp4 compat-libffi5 libxml2-devel libjpeg-devel openssl-devel \\\nboost boost-devel autoconf flex bison libssh2-devel java-1.8.0-openjdk java-1.8.0-openjdk-devel \\\nfontconfig-devel cairo-devel\n\n# Development tools \nRUN yum groupinstall 'Development Tools' -y\n\n# Install and update R\nRUN yum install -y R-core R-base R-core-devel R-devel\nRUN yum update -y R-core R-base R-core-devel R-devel\n\n\n# ENV JAVA_HOME /usr/java/latest\n\n# Fix problem with c compiler\nRUN mkdir ~/.R\nRUN echo \"CC=gcc64\" >> ~/.R/Makevars\n\nCMD [\"bash\"] \nY ahora una vez que nos ponemos en el directorio dónde tenemos el dockerfile, lo construimos con\ndocker build -t amazon-linux-r .\nSi todo ha ido bien, ya tenemos nuestra imagen de docker de amazon linux con R 3.4.1 instalado.\nCreamos y entramos en un container de esa imagen dónde adjuntamos un volumen (carpeta que se va a compartir entre mi máquina y el docker)\n docker run --rm -it -v ~/Descargas/libcentosR-3.4.1:/libR amazon-linux-r /bin/bash\nY listo ya estamos preparados para instalar paquetes\nEntramos en R y lo primero que hacemos es cambiar el .libPaths , para que todo lo que instalemos se quede en la carpeta que compartimos\n\n\nMostrar / ocultar código\n.libPaths(\"/libR\")\n\n\nComo me acabo de comprar un portátil con 6 cores, establezco la variable de entorno MAKE para que el código de C se compile usando 6 jobs. Esto hará que la instalación de la mayoría de librerías vaya mucho más rápida.\n\n\nMostrar / ocultar código\nSys.setenv(MAKE = \"make -j 6\")\n\n\nComo la versión de R que hay en amazon linux es viejuna (junio de 2017) y como hubo un cambio drástico en la versión 3.5 necesitamos hacer una vuelta al pasado para tener los repos de CRAN que habia en ese momento. Para eso, en primer lugar instalamos la librería checkpoint que nos va a facilitar el trabajo. Con esta librería podemos apuntar a los repos de CRAN que había en una fecha determinada. En realidad apuntamos a un repo de microsoft que hace mirror diarios del CRAN.\n\n\nMostrar / ocultar código\ninstall.packages(\"checkpoint\")\nlibrary(checkpoint)\n\n# apuntamos justo al repo que había antes de la verión  de R 3.5\nsetSnapshot(\"2018-03-31\")\n\n\nY ahora ya podemos instalar las librerías, por ejemplo estas.\n\n\nMostrar / ocultar código\n\nlist.of.packages <- c(\n  \"BayesFactor\", \"C50\", \"car\", \"caret\", \"catboost\",\n  \"coin\", \"cowplot\", \"DALEX\", \"DALEXtra\", \"DataExplorer\", \"dqrng\",\n  \"drifter\", \"EIX\", \"emmeans\", \"factoextra\", \"FactoMineR\", \"FFTrees\",\n  \"flextable\", \"forecast\", \"gdtools\", \"ggforce\", \"ggiraph\", \"ggiraphExtra\",\n  \"ggpubr\", \"glmnet\", \"highcharter\", \"iBreakDown\", \"igraph\", \"imbalance\",\n  \"iml\", \"ingredients\", \"inum\", \"KernelKnn\", \"libcoin\", \"lime\",\n  \"lme4\", \"minqa\", \"ModelMetrics\", \"multcomp\", \"mvtnorm\", \"networkD3\",\n  \"party\", \"partykit\", \"pbkrtest\", \"plotrix\", \"prediction\", \"randomForestExplainer\",\n  \"ranger\", \"RcppArmadillo\", \"RcppEigen\", \"RMySQL\", \"RSpectra\",\n  \"sitmo\", \"sjPlot\", \"sjstats\", \"smotefamily\",\n  \"survey\", \"systemfonts\", \"threejs\", \"uwot\", \"xgb2sql\",\n  \"xgboost\", \"yarrr\", \"ztable\", \"tcltk\"\n)\n\nnew.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, \"Package\"])]\nif (length(new.packages)) install.packages(new.packages)\n\n\nY una vez que estén instadas podemos hacer una vuelta al futuro y actualizar las que podamos.\n\n\nMostrar / ocultar código\n\nsetSnapshot(\"2018-03-31\")\nupdate.packages(ask=FALSE)\n\n\nY esto es todo, parece sencillo, pero me ha quitado bastante tiempo todas estas pequeñas vicisitudes.."
  },
  {
    "objectID": "2020/02/17/cosas-de-pandas/index.html",
    "href": "2020/02/17/cosas-de-pandas/index.html",
    "title": "Cosas de pandas",
    "section": "",
    "text": "Estoy usando Rmarkdown así que primero defino que versión o entorno de python quiero\n\nSys.setenv(RETICULATE_PYTHON = \"/usr/bin/python3\")\n\n\nimport pandas as pd\ndf = pd.DataFrame({'A' : [1, 2, 3, 4], 'B' : [4, 3, 2, 1]})\ndf\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n\n\nEl index es autonumérico\n\ndf.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nPues ahora viene lo raro, al menos para mí. Si hacemos iloc O:3 me devuelve las 3 primeras filas (como debe ser)\n\ndf.iloc[0:3,]\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n\n\nPero si hacemos loc poniendo 0:3 me devuelve 4 filas !!!\n\ndf.loc[0:3, ]\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n\n\n¿Algún pythoniso que me pueda aclarar el porqué de este comportamiento?"
  },
  {
    "objectID": "2020/02/08/finde-de-cacharreo/index.html",
    "href": "2020/02/08/finde-de-cacharreo/index.html",
    "title": "Finde de cacharreo",
    "section": "",
    "text": "Bueno, pues he cambiado de portátil. Me he dado un capricho y me he pillado un slimbook prox 15 con 6 cores (12 hilos) , 32 Gb de RAM y una tarjeta gráfica nvidia de las normalitas.\nPues después de algunos (muchos) problemas con los drivers de la tarjeta nvidia en linux, más específicamente en linux mint xfce, he conseguido que todo vaya bien, y hasta he podido probar computación con gpu en R.\nHe probado h2o4gpu y también keras tirando de tensorflow, tirando de la docu de rstudio y de este pequeño tutorial para ver que al menos funcionaba.\nPues visto que ya va todo suave me dispongo a ver El gran carnaval y disfruta del sábado con el gran Kirk Douglas.\nSalud y no os flipéis mucho con el Deep Lenin, que tampoco es para tanto."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html",
    "href": "2023/01/29/explicatividad_no_usual/index.html",
    "title": "Explicatividad no usual",
    "section": "",
    "text": "Buscando en el portátil sobre otras cosas me he encontrado un pequeño ejercicio para implementar la idea que se comenta aquí\nLa idea es muy sencilla, tal y como comenta Carlos. Si tengo un modelo que sea tipo randomForest\n\nDe cada observación a predecir se anota en qué nodo terminal ha caído en cada árbol\nPara cada nodo terminal en cada árbol se recorre el camino hacia “arriba” para saber qué variables están implicadas en ese nodo\nSe cuenta para cada nodo terminal el número de veces que aparece una variable en cada árbol (o se da más importancia a las qeu estén más altos en el árbol)\nSe agrega de alguna manera para cada observación qué variables y cómo de importantes están en los caminos de los nodos terminales en los que han caído.\nEsa info sería la importancia de las variables a nivel individual\nSe podría clusterizar observaciones con similares variables importantes\n\nAntes de nada, sí, ya sé que existen cosas como los shap values y que a partir de ellos se puede hacer algo parecido. Pero no está de más de vez en cuando buscarse uno las habichueleas de forma más artesanal.."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-1",
    "href": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-1",
    "title": "Explicatividad no usual",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nPues ale, vamos a hacerlo con iris, tan denostado hoy en día. Pobre Fisher o Anderson si levantaran la cabeza.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ranger)\n\n# ya veremos para que usamos furrr y FactoMineR\nlibrary(furrr) \nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\n\n\nCode\n\n# 5 arbolitos tiene mi.. \nset.seed(47)\nrg_iris &lt;-  ranger(Species ~ . , data = iris, num.trees = 5)\n\n\nInfo del árbol 3\n\n\nCode\n(arbol3 &lt;- treeInfo(rg_iris, tree = 3))\n#&gt;    nodeID leftChild rightChild splitvarID splitvarName splitval terminal\n#&gt; 1       0         1          2          0 Sepal.Length     5.55    FALSE\n#&gt; 2       1         3          4          2 Petal.Length     2.45    FALSE\n#&gt; 3       2         5          6          3  Petal.Width     1.70    FALSE\n#&gt; 4       3        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 5       4         7          8          2 Petal.Length     4.25    FALSE\n#&gt; 6       5         9         10          1  Sepal.Width     3.60    FALSE\n#&gt; 7       6        11         12          0 Sepal.Length     6.00    FALSE\n#&gt; 8       7        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 9       8        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 10      9        13         14          2 Petal.Length     4.95    FALSE\n#&gt; 11     10        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 12     11        15         16          2 Petal.Length     4.85    FALSE\n#&gt; 13     12        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 14     13        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 15     14        17         18          3  Petal.Width     1.55    FALSE\n#&gt; 16     15        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 17     16        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 18     17        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 19     18        19         20          2 Petal.Length     5.45    FALSE\n#&gt; 20     19        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt; 21     20        NA         NA         NA         &lt;NA&gt;       NA     TRUE\n#&gt;    prediction\n#&gt; 1        &lt;NA&gt;\n#&gt; 2        &lt;NA&gt;\n#&gt; 3        &lt;NA&gt;\n#&gt; 4      setosa\n#&gt; 5        &lt;NA&gt;\n#&gt; 6        &lt;NA&gt;\n#&gt; 7        &lt;NA&gt;\n#&gt; 8  versicolor\n#&gt; 9   virginica\n#&gt; 10       &lt;NA&gt;\n#&gt; 11     setosa\n#&gt; 12       &lt;NA&gt;\n#&gt; 13  virginica\n#&gt; 14 versicolor\n#&gt; 15       &lt;NA&gt;\n#&gt; 16 versicolor\n#&gt; 17  virginica\n#&gt; 18  virginica\n#&gt; 19       &lt;NA&gt;\n#&gt; 20 versicolor\n#&gt; 21  virginica\n\n\nAnalizando un poco, vemos que el nodo raíz (0) se parte por la variable Sepal.Length. Luego el nodo 1 se bifurca a la izquierda hacia el 3 y a la derecha hacia el 4, siendo Petal.Length la variable que decide esa partición.\nLa idea sería recorrer el árbol partiendo de un nodo terminal y ver qué camino ha seguido. Para eso hacemos el siguiente código\nHacemos un ejemplo, partiendo del nodo terminal 8\nCamino del Nodo 8\n\n\nCode\nnodo_terminal &lt;-  8\n\nnodos &lt;- nodo_terminal # vamos a ir sobreescribiendo la variable nodos hasta llegar al nodo raíz 0 \nvariables &lt;- vector() # guardamos el nombre de las variables de split\n\n  while (!0 %in% nodos) {\n    tmp &lt;- arbol3 %&gt;%\n      filter(leftChild %in% nodos |\n        rightChild %in% nodos)\n    \n    print(str_glue(\"Nodo hijo: {nodos}\"))\n    \n    nodos &lt;- unique(tmp$nodeID)\n    print(str_glue(\"Nodo padre: {nodos}\"))\n    \n    \n    print(str_glue(\"variable de split en nodo padre: {tmp$splitvarName}\"))\n    \n    variables &lt;- c(variables, unique(tmp$splitvarName)) # la última variable de este vector es la que está más arriba en el árbol\n\n  }\n#&gt; Nodo hijo: 8\n#&gt; Nodo padre: 4\n#&gt; variable de split en nodo padre: Petal.Length\n#&gt; Nodo hijo: 4\n#&gt; Nodo padre: 1\n#&gt; variable de split en nodo padre: Petal.Length\n#&gt; Nodo hijo: 1\n#&gt; Nodo padre: 0\n#&gt; variable de split en nodo padre: Sepal.Length\n\n\nY vemos que para llegar al nodo terminal 8 ha utilizado dos veces la variable Petal.Length y una la variable Sepal.Length\nNos creamos una funcioncita para esto, donde al final construyo un data.frame donde guardo eel nodo terminal que estamos investigando, las variables que se han usado para llegar a ese nodo y una variable peso que se calcula asignando un peso igual 1 a la variable que está más alta en el árbol y menos a las demás. Si hay 4 variables se crea un vector c(4,3,2,1) en orden de más alta en el árbol a más baja y se divide por el número de variables. así se tendrían estos pesos 1, 0.75, 0.5, 0.25\n\n\nCode\n\nextraerVariables_nodos &lt;- function(nodo_terminal, info_arbol) {\n  nodos &lt;- nodo_terminal\n  variables &lt;- vector()\n\n  while (!0 %in% nodos) {\n    tmp &lt;- info_arbol %&gt;%\n      filter(leftChild %in% nodos |\n        rightChild %in% nodos)\n\n    variables &lt;- c(variables, unique(tmp$splitvarName))\n\n    nodos &lt;- unique(tmp$nodeID)\n  }\n\n  return(\n      data.frame(\n          nodo_terminal = nodo_terminal,\n          variables = variables,\n          peso = seq_along(variables) / sum(length(variables))\n      )\n  )\n}\n\n\nComprobamos\n\n\nCode\n\nextraerVariables_nodos(nodo_terminal = 8, info_arbol = arbol3)\n#&gt;   nodo_terminal    variables      peso\n#&gt; 1             8 Petal.Length 0.3333333\n#&gt; 2             8 Petal.Length 0.6666667\n#&gt; 3             8 Sepal.Length 1.0000000\n\n\nOk. Lo suyo sería extraer la misma info pero para cada nodo terminal del árbol que estamos considerando. Pues nos creamos la funcioncita, que dado un modelo y un número de árbol, saque la info anterior para todos los nodos terminales\n\n\nCode\n\nextraerVariablePorArbol &lt;- function(modelo, arbol, verbose = FALSE) {\n  \n  info_arbol &lt;- treeInfo(modelo, arbol)\n  nodos_terminales &lt;- treeInfo(modelo, arbol) %&gt;%\n    filter(terminal == TRUE) %&gt;%\n    pull(nodeID) %&gt;%\n    unique()\n if(verbose) print(nodos_terminales)\n  \n  variables_por_arbol &lt;- map_df(\n    nodos_terminales,\n    function(nodos) {\n      extraerVariables_nodos(nodos, info_arbol)\n    }\n  )\n\n  variables_por_arbol$arbol &lt;- arbol\n  variables_por_arbol\n}\n\n\nComprobemos\n\n\nCode\n# arbol 3 \n(importancia_individual_arbol3 &lt;- extraerVariablePorArbol(rg_iris, 3))\n#&gt;    nodo_terminal    variables      peso arbol\n#&gt; 1              3 Petal.Length 0.5000000     3\n#&gt; 2              3 Sepal.Length 1.0000000     3\n#&gt; 3              7 Petal.Length 0.3333333     3\n#&gt; 4              7 Petal.Length 0.6666667     3\n#&gt; 5              7 Sepal.Length 1.0000000     3\n#&gt; 6              8 Petal.Length 0.3333333     3\n#&gt; 7              8 Petal.Length 0.6666667     3\n#&gt; 8              8 Sepal.Length 1.0000000     3\n#&gt; 9             10  Sepal.Width 0.3333333     3\n#&gt; 10            10  Petal.Width 0.6666667     3\n#&gt; 11            10 Sepal.Length 1.0000000     3\n#&gt; 12            12 Sepal.Length 0.3333333     3\n#&gt; 13            12  Petal.Width 0.6666667     3\n#&gt; 14            12 Sepal.Length 1.0000000     3\n#&gt; 15            13 Petal.Length 0.2500000     3\n#&gt; 16            13  Sepal.Width 0.5000000     3\n#&gt; 17            13  Petal.Width 0.7500000     3\n#&gt; 18            13 Sepal.Length 1.0000000     3\n#&gt; 19            15 Petal.Length 0.2500000     3\n#&gt; 20            15 Sepal.Length 0.5000000     3\n#&gt; 21            15  Petal.Width 0.7500000     3\n#&gt; 22            15 Sepal.Length 1.0000000     3\n#&gt; 23            16 Petal.Length 0.2500000     3\n#&gt; 24            16 Sepal.Length 0.5000000     3\n#&gt; 25            16  Petal.Width 0.7500000     3\n#&gt; 26            16 Sepal.Length 1.0000000     3\n#&gt; 27            17  Petal.Width 0.2000000     3\n#&gt; 28            17 Petal.Length 0.4000000     3\n#&gt; 29            17  Sepal.Width 0.6000000     3\n#&gt; 30            17  Petal.Width 0.8000000     3\n#&gt; 31            17 Sepal.Length 1.0000000     3\n#&gt; 32            19 Petal.Length 0.1666667     3\n#&gt; 33            19  Petal.Width 0.3333333     3\n#&gt; 34            19 Petal.Length 0.5000000     3\n#&gt; 35            19  Sepal.Width 0.6666667     3\n#&gt; 36            19  Petal.Width 0.8333333     3\n#&gt; 37            19 Sepal.Length 1.0000000     3\n#&gt; 38            20 Petal.Length 0.1666667     3\n#&gt; 39            20  Petal.Width 0.3333333     3\n#&gt; 40            20 Petal.Length 0.5000000     3\n#&gt; 41            20  Sepal.Width 0.6666667     3\n#&gt; 42            20  Petal.Width 0.8333333     3\n#&gt; 43            20 Sepal.Length 1.0000000     3\n\n\nSolo queda extraer lo mismo pero para cada arbolito\n\n\nCode\nextraerVariablesPorModelo &lt;- function(modelo, parallel = TRUE) {\n  \n  \n  arboles &lt;- modelo$num.trees\n\n  if (parallel) {\n    # Si hay muchos árboles usamos procesamiento en paralelo\n    future::plan(multisession)\n    \n    furrr::future_map_dfr(\n      seq_len(arboles),\n      function(arbol) {\n        extraerVariablePorArbol(modelo, arbol = arbol)\n      }\n    )\n  } else{\n      map_df(\n          seq_len(arboles),\n          function(arbol) {\n              extraerVariablePorArbol(modelo, arbol = arbol)\n          }\n      )  \n  }\n}\n\n\n\n\nCode\n(importancia_individual_todos_arboles &lt;-  extraerVariablesPorModelo(rg_iris, parallel = FALSE))\n#&gt;     nodo_terminal    variables      peso arbol\n#&gt; 1               1  Petal.Width 1.0000000     1\n#&gt; 2               5 Petal.Length 0.3333333     1\n#&gt; 3               5  Petal.Width 0.6666667     1\n#&gt; 4               5  Petal.Width 1.0000000     1\n#&gt; 5               8  Petal.Width 0.3333333     1\n#&gt; 6               8  Petal.Width 0.6666667     1\n#&gt; 7               8  Petal.Width 1.0000000     1\n#&gt; 8               9  Petal.Width 0.2500000     1\n#&gt; 9               9 Petal.Length 0.5000000     1\n#&gt; 10              9  Petal.Width 0.7500000     1\n#&gt; 11              9  Petal.Width 1.0000000     1\n#&gt; 12             10  Petal.Width 0.2500000     1\n#&gt; 13             10 Petal.Length 0.5000000     1\n#&gt; 14             10  Petal.Width 0.7500000     1\n#&gt; 15             10  Petal.Width 1.0000000     1\n#&gt; 16             11 Petal.Length 0.2500000     1\n#&gt; 17             11  Petal.Width 0.5000000     1\n#&gt; 18             11  Petal.Width 0.7500000     1\n#&gt; 19             11  Petal.Width 1.0000000     1\n#&gt; 20             12 Petal.Length 0.2500000     1\n#&gt; 21             12  Petal.Width 0.5000000     1\n#&gt; 22             12  Petal.Width 0.7500000     1\n#&gt; 23             12  Petal.Width 1.0000000     1\n#&gt; 24              3  Petal.Width 0.5000000     2\n#&gt; 25              3 Sepal.Length 1.0000000     2\n#&gt; 26              4  Petal.Width 0.5000000     2\n#&gt; 27              4 Sepal.Length 1.0000000     2\n#&gt; 28              8  Sepal.Width 0.3333333     2\n#&gt; 29              8  Petal.Width 0.6666667     2\n#&gt; 30              8 Sepal.Length 1.0000000     2\n#&gt; 31             10 Sepal.Length 0.3333333     2\n#&gt; 32             10  Petal.Width 0.6666667     2\n#&gt; 33             10 Sepal.Length 1.0000000     2\n#&gt; 34             11 Petal.Length 0.2500000     2\n#&gt; 35             11  Sepal.Width 0.5000000     2\n#&gt; 36             11  Petal.Width 0.7500000     2\n#&gt; 37             11 Sepal.Length 1.0000000     2\n#&gt; 38             13  Sepal.Width 0.2500000     2\n#&gt; 39             13 Sepal.Length 0.5000000     2\n#&gt; 40             13  Petal.Width 0.7500000     2\n#&gt; 41             13 Sepal.Length 1.0000000     2\n#&gt; 42             14  Sepal.Width 0.2500000     2\n#&gt; 43             14 Sepal.Length 0.5000000     2\n#&gt; 44             14  Petal.Width 0.7500000     2\n#&gt; 45             14 Sepal.Length 1.0000000     2\n#&gt; 46             15  Sepal.Width 0.2000000     2\n#&gt; 47             15 Petal.Length 0.4000000     2\n#&gt; 48             15  Sepal.Width 0.6000000     2\n#&gt; 49             15  Petal.Width 0.8000000     2\n#&gt; 50             15 Sepal.Length 1.0000000     2\n#&gt; 51             17  Sepal.Width 0.1666667     2\n#&gt; 52             17  Sepal.Width 0.3333333     2\n#&gt; 53             17 Petal.Length 0.5000000     2\n#&gt; 54             17  Sepal.Width 0.6666667     2\n#&gt; 55             17  Petal.Width 0.8333333     2\n#&gt; 56             17 Sepal.Length 1.0000000     2\n#&gt; 57             18  Sepal.Width 0.1666667     2\n#&gt; 58             18  Sepal.Width 0.3333333     2\n#&gt; 59             18 Petal.Length 0.5000000     2\n#&gt; 60             18  Sepal.Width 0.6666667     2\n#&gt; 61             18  Petal.Width 0.8333333     2\n#&gt; 62             18 Sepal.Length 1.0000000     2\n#&gt; 63              3 Petal.Length 0.5000000     3\n#&gt; 64              3 Sepal.Length 1.0000000     3\n#&gt; 65              7 Petal.Length 0.3333333     3\n#&gt; 66              7 Petal.Length 0.6666667     3\n#&gt; 67              7 Sepal.Length 1.0000000     3\n#&gt; 68              8 Petal.Length 0.3333333     3\n#&gt; 69              8 Petal.Length 0.6666667     3\n#&gt; 70              8 Sepal.Length 1.0000000     3\n#&gt; 71             10  Sepal.Width 0.3333333     3\n#&gt; 72             10  Petal.Width 0.6666667     3\n#&gt; 73             10 Sepal.Length 1.0000000     3\n#&gt; 74             12 Sepal.Length 0.3333333     3\n#&gt; 75             12  Petal.Width 0.6666667     3\n#&gt; 76             12 Sepal.Length 1.0000000     3\n#&gt; 77             13 Petal.Length 0.2500000     3\n#&gt; 78             13  Sepal.Width 0.5000000     3\n#&gt; 79             13  Petal.Width 0.7500000     3\n#&gt; 80             13 Sepal.Length 1.0000000     3\n#&gt; 81             15 Petal.Length 0.2500000     3\n#&gt; 82             15 Sepal.Length 0.5000000     3\n#&gt; 83             15  Petal.Width 0.7500000     3\n#&gt; 84             15 Sepal.Length 1.0000000     3\n#&gt; 85             16 Petal.Length 0.2500000     3\n#&gt; 86             16 Sepal.Length 0.5000000     3\n#&gt; 87             16  Petal.Width 0.7500000     3\n#&gt; 88             16 Sepal.Length 1.0000000     3\n#&gt; 89             17  Petal.Width 0.2000000     3\n#&gt; 90             17 Petal.Length 0.4000000     3\n#&gt; 91             17  Sepal.Width 0.6000000     3\n#&gt; 92             17  Petal.Width 0.8000000     3\n#&gt; 93             17 Sepal.Length 1.0000000     3\n#&gt; 94             19 Petal.Length 0.1666667     3\n#&gt; 95             19  Petal.Width 0.3333333     3\n#&gt; 96             19 Petal.Length 0.5000000     3\n#&gt; 97             19  Sepal.Width 0.6666667     3\n#&gt; 98             19  Petal.Width 0.8333333     3\n#&gt; 99             19 Sepal.Length 1.0000000     3\n#&gt; 100            20 Petal.Length 0.1666667     3\n#&gt; 101            20  Petal.Width 0.3333333     3\n#&gt; 102            20 Petal.Length 0.5000000     3\n#&gt; 103            20  Sepal.Width 0.6666667     3\n#&gt; 104            20  Petal.Width 0.8333333     3\n#&gt; 105            20 Sepal.Length 1.0000000     3\n#&gt; 106             1  Petal.Width 1.0000000     4\n#&gt; 107             5  Petal.Width 0.3333333     4\n#&gt; 108             5 Petal.Length 0.6666667     4\n#&gt; 109             5  Petal.Width 1.0000000     4\n#&gt; 110             6  Petal.Width 0.3333333     4\n#&gt; 111             6 Petal.Length 0.6666667     4\n#&gt; 112             6  Petal.Width 1.0000000     4\n#&gt; 113             9 Sepal.Length 0.2500000     4\n#&gt; 114             9 Petal.Length 0.5000000     4\n#&gt; 115             9 Petal.Length 0.7500000     4\n#&gt; 116             9  Petal.Width 1.0000000     4\n#&gt; 117            10 Sepal.Length 0.2500000     4\n#&gt; 118            10 Petal.Length 0.5000000     4\n#&gt; 119            10 Petal.Length 0.7500000     4\n#&gt; 120            10  Petal.Width 1.0000000     4\n#&gt; 121            12  Petal.Width 0.2500000     4\n#&gt; 122            12 Petal.Length 0.5000000     4\n#&gt; 123            12 Petal.Length 0.7500000     4\n#&gt; 124            12  Petal.Width 1.0000000     4\n#&gt; 125            13 Sepal.Length 0.2000000     4\n#&gt; 126            13  Petal.Width 0.4000000     4\n#&gt; 127            13 Petal.Length 0.6000000     4\n#&gt; 128            13 Petal.Length 0.8000000     4\n#&gt; 129            13  Petal.Width 1.0000000     4\n#&gt; 130            14 Sepal.Length 0.2000000     4\n#&gt; 131            14  Petal.Width 0.4000000     4\n#&gt; 132            14 Petal.Length 0.6000000     4\n#&gt; 133            14 Petal.Length 0.8000000     4\n#&gt; 134            14  Petal.Width 1.0000000     4\n#&gt; 135             1 Petal.Length 1.0000000     5\n#&gt; 136             6  Sepal.Width 0.3333333     5\n#&gt; 137             6  Petal.Width 0.6666667     5\n#&gt; 138             6 Petal.Length 1.0000000     5\n#&gt; 139             8 Petal.Length 0.3333333     5\n#&gt; 140             8  Petal.Width 0.6666667     5\n#&gt; 141             8 Petal.Length 1.0000000     5\n#&gt; 142            10 Petal.Length 0.2500000     5\n#&gt; 143            10  Sepal.Width 0.5000000     5\n#&gt; 144            10  Petal.Width 0.7500000     5\n#&gt; 145            10 Petal.Length 1.0000000     5\n#&gt; 146            11 Sepal.Length 0.2500000     5\n#&gt; 147            11 Petal.Length 0.5000000     5\n#&gt; 148            11  Petal.Width 0.7500000     5\n#&gt; 149            11 Petal.Length 1.0000000     5\n#&gt; 150            12 Sepal.Length 0.2500000     5\n#&gt; 151            12 Petal.Length 0.5000000     5\n#&gt; 152            12  Petal.Width 0.7500000     5\n#&gt; 153            12 Petal.Length 1.0000000     5\n#&gt; 154            14 Sepal.Length 0.2000000     5\n#&gt; 155            14 Petal.Length 0.4000000     5\n#&gt; 156            14  Sepal.Width 0.6000000     5\n#&gt; 157            14  Petal.Width 0.8000000     5\n#&gt; 158            14 Petal.Length 1.0000000     5\n#&gt; 159            15 Petal.Length 0.1666667     5\n#&gt; 160            15 Sepal.Length 0.3333333     5\n#&gt; 161            15 Petal.Length 0.5000000     5\n#&gt; 162            15  Sepal.Width 0.6666667     5\n#&gt; 163            15  Petal.Width 0.8333333     5\n#&gt; 164            15 Petal.Length 1.0000000     5\n#&gt; 165            16 Petal.Length 0.1666667     5\n#&gt; 166            16 Sepal.Length 0.3333333     5\n#&gt; 167            16 Petal.Length 0.5000000     5\n#&gt; 168            16  Sepal.Width 0.6666667     5\n#&gt; 169            16  Petal.Width 0.8333333     5\n#&gt; 170            16 Petal.Length 1.0000000     5\n\n\nAhora ya tenemos qué variables llevan a cada nodo terminal en cada árbol e incluso un peso que vale 1 si la variable es la primera en el “camino” hacia el nodo\nPero lo que nosotros queremos es para cada observación que predecimos, ver su nodo terminal en cada árbol y pegarle las variables importantes en cada nodo.\nSería algo así.\n\n\nCode\n# lo hacems de momento con todo iris, en la realidad serían los datos de test o el conjunto de datos a predecir. \n\n\nnodos_terminales &lt;- predict(rg_iris, iris, type = \"terminalNodes\")$predictions\n\n# cada fila corresponde a una observación y cada columna al nodo terminal en cada árbol\nhead(nodos_terminales, 10 )\n#&gt;       [,1] [,2] [,3] [,4] [,5]\n#&gt;  [1,]    1    3    3    1    1\n#&gt;  [2,]    1    3    3    1    1\n#&gt;  [3,]    1    3    3    1    1\n#&gt;  [4,]    1    3    3    1    1\n#&gt;  [5,]    1    3    3    1    1\n#&gt;  [6,]    1    3    3    1    1\n#&gt;  [7,]    1    3    3    1    1\n#&gt;  [8,]    1    3    3    1    1\n#&gt;  [9,]    1    3    3    1    1\n#&gt; [10,]    1    3    3    1    1\n\n\nLo ponemos de otra forma.\n\n\nCode\n# añadimos el id de la fila\nnodos_terminales_df &lt;- nodos_terminales %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(var = \"id\")\n  \n  \ncolnames(nodos_terminales_df)[-1] &lt;- 1:(ncol(nodos_terminales_df)-1)\n\nhead(nodos_terminales_df)\n#&gt;   id 1 2 3 4 5\n#&gt; 1  1 1 3 3 1 1\n#&gt; 2  2 1 3 3 1 1\n#&gt; 3  3 1 3 3 1 1\n#&gt; 4  4 1 3 3 1 1\n#&gt; 5  5 1 3 3 1 1\n#&gt; 6  6 1 3 3 1 1\n\n\nPivotamos para facilitar luego las agregaciones por observaciones\n\n\nCode\n nodos_terminales_df &lt;- nodos_terminales_df %&gt;% \n    tidyr::pivot_longer( colnames(nodos_terminales_df)[-1], names_to = \"arbol\", values_to = \"nodo_terminal\")\n\nhead(nodos_terminales_df)\n#&gt; # A tibble: 6 × 3\n#&gt;   id    arbol nodo_terminal\n#&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 1     1                 1\n#&gt; 2 1     2                 3\n#&gt; 3 1     3                 3\n#&gt; 4 1     4                 1\n#&gt; 5 1     5                 1\n#&gt; 6 2     1                 1\n\n\na la importancia en todos los árboles lo llamo info_modelo\n\n\nCode\n  \ninfo_modelo &lt;-  importancia_individual_todos_arboles\ninfo_modelo$arbol &lt;- as.character(info_modelo$arbol)\n\nhead(info_modelo)\n#&gt;   nodo_terminal    variables      peso arbol\n#&gt; 1             1  Petal.Width 1.0000000     1\n#&gt; 2             5 Petal.Length 0.3333333     1\n#&gt; 3             5  Petal.Width 0.6666667     1\n#&gt; 4             5  Petal.Width 1.0000000     1\n#&gt; 5             8  Petal.Width 0.3333333     1\n#&gt; 6             8  Petal.Width 0.6666667     1\n\n\nHacemos el join con la info de cada nodo terminal para cada observación con las variables que llevan a cada nodo terminal (en cada árbol)\n\n\nCode\nfinal &lt;- nodos_terminales_df %&gt;% \n    left_join(info_modelo, by = c(\"nodo_terminal\", \"arbol\"))\n\n# para el individuo 30\n\nfinal %&gt;% \n    filter(id == 30)\n#&gt; # A tibble: 7 × 5\n#&gt;   id    arbol nodo_terminal variables     peso\n#&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 30    1                 1 Petal.Width    1  \n#&gt; 2 30    2                 3 Petal.Width    0.5\n#&gt; 3 30    2                 3 Sepal.Length   1  \n#&gt; 4 30    3                 3 Petal.Length   0.5\n#&gt; 5 30    3                 3 Sepal.Length   1  \n#&gt; 6 30    4                 1 Petal.Width    1  \n#&gt; 7 30    5                 1 Petal.Length   1\n\n\nAgregamos la info para cada individuo, de forma que contemos cuántas veces aparece cada variable, sumamos los pesos y ordenamos\n\n\nCode\nres &lt;- final %&gt;%\n    group_by(id, variables) %&gt;%\n    summarise(\n      total = n(),\n      ponderado = sum(peso)) %&gt;%\n    group_by(id) %&gt;%\n    mutate(\n      importancia_caso = total / sum(total),\n      importancia_ponderada = ponderado / sum(ponderado)\n    ) %&gt;% \n    top_n(10, importancia_ponderada) %&gt;% \n    ungroup() %&gt;% \n    arrange(as.numeric(id), desc(importancia_ponderada))\n\n\n\n\nCode\nres %&gt;% \n    filter(id == 30)\n#&gt; # A tibble: 3 × 6\n#&gt;   id    variables    total ponderado importancia_caso importancia_ponderada\n#&gt;   &lt;chr&gt; &lt;chr&gt;        &lt;int&gt;     &lt;dbl&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n#&gt; 1 30    Petal.Width      3       2.5            0.429                 0.417\n#&gt; 2 30    Sepal.Length     2       2              0.286                 0.333\n#&gt; 3 30    Petal.Length     2       1.5            0.286                 0.25\n\n\nY esa sería la importancia de las variables específica para la observación 30"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#todo-junto.",
    "href": "2023/01/29/explicatividad_no_usual/index.html#todo-junto.",
    "title": "Explicatividad no usual",
    "section": "Todo junto.",
    "text": "Todo junto.\nNos podemos crear una funcioncita que lo haga todo.\n\n\nCode\ngetIndividualImportance &lt;-  function(modelo, data, top = modelo$num.independent.variables, ...){\n    \n params_ellipsis &lt;- list(...)\n  \n  # get terminalNodes\n  nodos_terminales &lt;- predict(modelo, data, type = \"terminalNodes\")$predictions\n  \n  nodos_terminales_df &lt;- nodos_terminales %&gt;% \n    as.data.frame() \n  nodos_terminales_df$id &lt;- rownames(data)\n  nodos_terminales_df &lt;-  nodos_terminales_df %&gt;%\n      dplyr::select(id, everything())\n      \n  \n  \n  colnames(nodos_terminales_df)[-1] &lt;- 1:(ncol(nodos_terminales_df)-1)\n  \n  nodos_terminales_df &lt;- nodos_terminales_df %&gt;% \n    tidyr::pivot_longer( colnames(nodos_terminales_df)[-1], names_to = \"arbol\", values_to = \"nodo_terminal\")\n  \n  # get variables_path for each tree and terminal node\n  info_modelo &lt;-  extraerVariablesPorModelo(modelo, parallel = params_ellipsis$parallel)\n  info_modelo$arbol &lt;- as.character(info_modelo$arbol)\n  \n  # join both\n  \n  final &lt;- nodos_terminales_df %&gt;% \n    left_join(info_modelo, by = c(\"nodo_terminal\", \"arbol\"))\n  \n  res &lt;- final %&gt;%\n    group_by(id, variables) %&gt;%\n    summarise(\n      total = n(),\n      ponderado = sum(peso)) %&gt;%\n    group_by(id) %&gt;%\n    # para poder comparar luego observaciones, para cadda individuo, divido las veces qeu \n     # aparece una variable por el total de veces que han aparecido todas sus variables\n    mutate(\n      importancia_caso = total / sum(total),\n      importancia_ponderada = ponderado / sum(ponderado)\n    ) %&gt;% \n    top_n(top, importancia_ponderada) %&gt;% \n    ungroup() %&gt;% \n    arrange(as.numeric(id), desc(importancia_ponderada))\n  \n}\n\n\nY comprobamos\n\n\nCode\nexplicatividad_iris &lt;-  getIndividualImportance(rg_iris, iris, parallel = TRUE)\n\n\n\n\nCode\nDT::datatable(explicatividad_iris)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#agrupando-observaciones-con-similar-importancia-de-variables",
    "href": "2023/01/29/explicatividad_no_usual/index.html#agrupando-observaciones-con-similar-importancia-de-variables",
    "title": "Explicatividad no usual",
    "section": "Agrupando observaciones con similar importancia de variables",
    "text": "Agrupando observaciones con similar importancia de variables\nPodríamos hacer ahora un PCA pero yo voy a utilizar un CA usando la importancia_ponderada\n\n\nCode\n\n  tabla_para_diagonalizar &lt;- xtabs(ponderado ~ id+ variables, data= explicatividad_iris)\n  tabla_para_diagonalizar\n#&gt;      variables\n#&gt; id    Petal.Length Petal.Width Sepal.Length Sepal.Width\n#&gt;   1      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   10     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   100    2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   101    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   102    2.8333333   5.4166667    3.0000000   0.2500000\n#&gt;   103    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   104    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   105    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   106    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   107    3.6666667   4.3333333    2.3333333   0.6666667\n#&gt;   108    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   109    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   11     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   110    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   111    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   112    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   113    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   114    2.8333333   5.1666667    3.2500000   0.2500000\n#&gt;   115    2.8333333   5.4166667    3.0000000   0.2500000\n#&gt;   116    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   117    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   118    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   119    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   12     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   120    3.8000000   5.5500000    2.2500000   1.9000000\n#&gt;   121    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   122    2.2500000   5.5000000    3.0000000   0.2500000\n#&gt;   123    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   124    2.2500000   5.5833333    2.6666667   0.0000000\n#&gt;   125    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   126    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   127    2.4166667   5.6666667    2.9166667   0.0000000\n#&gt;   128    2.2500000   5.5833333    2.6666667   0.0000000\n#&gt;   129    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   13     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   130    4.0666667   6.0666667    2.2000000   2.1666667\n#&gt;   131    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   132    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   133    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   134    3.8000000   5.9000000    2.2000000   2.1000000\n#&gt;   135    3.9500000   5.9500000    2.2000000   1.9000000\n#&gt;   136    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   137    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   138    2.8333333   5.5000000    2.6666667   0.0000000\n#&gt;   139    2.6666667   5.7500000    3.0833333   0.0000000\n#&gt;   14     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   140    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   141    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   142    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   143    2.8333333   5.4166667    3.0000000   0.2500000\n#&gt;   144    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   145    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   146    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   147    2.5833333   5.0000000    2.9166667   0.0000000\n#&gt;   148    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   149    2.5833333   5.2500000    2.6666667   0.0000000\n#&gt;   15     1.0000000   3.3333333    2.0000000   0.6666667\n#&gt;   150    3.0833333   5.6666667    3.0000000   0.2500000\n#&gt;   16     1.0000000   3.3333333    2.0000000   0.6666667\n#&gt;   17     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   18     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   19     1.0000000   3.3333333    2.0000000   0.6666667\n#&gt;   2      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   20     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   21     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   22     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   23     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   24     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   25     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   26     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   27     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   28     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   29     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   3      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   30     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   31     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   32     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   33     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   34     1.5000000   2.6666667    2.0000000   0.3333333\n#&gt;   35     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   36     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   37     1.5000000   2.6666667    2.0000000   0.3333333\n#&gt;   38     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   39     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   4      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   40     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   41     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   42     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   43     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   44     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   45     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   46     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   47     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   48     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   49     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   5      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   50     1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   51     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   52     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   53     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   54     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   55     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   56     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   57     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   58     3.6666667   4.3333333    2.3333333   0.6666667\n#&gt;   59     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   6      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   60     3.0000000   4.1666667    2.0000000   0.3333333\n#&gt;   61     3.4000000   4.3000000    2.2000000   0.6000000\n#&gt;   62     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   63     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   64     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   65     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   66     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   67     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   68     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   69     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   7      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   70     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   71     2.6666667   5.8333333    3.2500000   0.2500000\n#&gt;   72     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   73     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   74     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   75     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   76     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   77     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   78     3.9166667   5.6666667    2.2500000   2.1666667\n#&gt;   79     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   8      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   80     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   81     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   82     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   83     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   84     4.0666667   6.0666667    2.2000000   2.1666667\n#&gt;   85     3.0000000   4.1666667    2.0000000   0.3333333\n#&gt;   86     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   87     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   88     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   89     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   9      1.5000000   2.5000000    2.0000000   0.0000000\n#&gt;   90     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   91     3.6500000   4.5500000    2.2000000   1.1000000\n#&gt;   92     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   93     2.9000000   5.3000000    2.2000000   1.6000000\n#&gt;   94     3.4000000   4.3000000    2.2000000   0.6000000\n#&gt;   95     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   96     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   97     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   98     2.5000000   5.1666667    2.0000000   1.3333333\n#&gt;   99     3.4000000   4.3000000    2.2000000   0.6000000\n\n\nY al hacer un CA podemos ver qué individuos están asociados con las variables pero por la importancia ponderada.\n\n\nCode\n\nres_ca &lt;- FactoMineR::CA(tabla_para_diagonalizar, graph = FALSE)\n\nfviz_ca(res_ca)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-2",
    "href": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-2",
    "title": "Explicatividad no usual",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\nUtilicemos esto para los datos de Boston Housing\n\n\nCode\nboston_df &lt;-  MASS::Boston\n\n\n\nHousing Values in Suburbs of Boston Description The Boston data frame has 506 rows and 14 columns.\n\n\nUsage Boston Format This data frame contains the following columns:\n\n\ncrim per capita crime rate by town.\n\n\nzn proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus proportion of non-retail business acres per town.\n\n\nchas Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox nitrogen oxides concentration (parts per 10 million).\n\n\nrm average number of rooms per dwelling.\n\n\nage proportion of owner-occupied units built prior to 1940.\n\n\ndis weighted mean of distances to five Boston employment centres.\n\n\nrad index of accessibility to radial highways.\n\n\ntax full-value property-tax rate per $10,000.\n\n\nptratio pupil-teacher ratio by town.\n\n\nblack 1000(Bk−0.63)^2 where BkBk is the proportion of blacks by town.\n\n\nlstat lower status of the population (percent).\n\n\nmedv median value of owner-occupied homes in $1000s.\n\n\n\nCode\nset.seed(47)\n\nidx &lt;-  sample(1:nrow(boston_df),300)\ntrain_df &lt;- boston_df[idx,]\n\ntest_df &lt;- boston_df[-idx, ]\n\n\n\nModelo con ranger\n\n\nCode\nrg_boston &lt;-  ranger(medv ~ ., data = train_df, num.trees = 50)\n\n\nVariables importantes a nivel individual\nPor simplificar, voy a seleccionar solo las 5 variables más importantes para cada observación\n\n\nCode\n\nimportancia_individual &lt;- getIndividualImportance(rg_boston, test_df,top = 5, parallel = TRUE)\n\n\n\n\nCode\ndim(importancia_individual)\n#&gt; [1] 1030    6\n\n\n\n\nCode\nDT::datatable(importancia_individual)\n\n\n\n\n\n\n\n\n\nAgrupando\n\n\nCode\ntabla_diag_boston &lt;- xtabs(ponderado ~ id+ variables, data= importancia_individual)\nhead(tabla_diag_boston)\n#&gt;      variables\n#&gt; id         age     crim      dis    indus    lstat      nox  ptratio       rm\n#&gt;   100 18.34266  0.00000  0.00000 21.95556 37.81429 17.20397  0.00000 38.40952\n#&gt;   108  0.00000  0.00000 28.51039 27.88223 50.73328 34.23095  0.00000 41.07002\n#&gt;   110 27.78868  0.00000 29.29312  0.00000 51.24393 33.02176  0.00000 42.08247\n#&gt;   111  0.00000  0.00000 27.27855 33.53332 49.47596 26.90144  0.00000 43.21389\n#&gt;   112  0.00000  0.00000 24.53920 25.53335 54.25059 30.61195  0.00000 39.80125\n#&gt;   119  0.00000  0.00000 26.33785 28.28968 53.85364 35.92384  0.00000 39.11696\n#&gt;      variables\n#&gt; id         tax\n#&gt;   100  0.00000\n#&gt;   108  0.00000\n#&gt;   110  0.00000\n#&gt;   111  0.00000\n#&gt;   112  0.00000\n#&gt;   119  0.00000\n\n\n\n\nCode\nres_ca &lt;- FactoMineR::CA(tabla_diag_boston, graph = FALSE)\n\nfviz_ca(res_ca)\n\n\n\n\n\n\n\n\n\nPodemos hacer un HCPC usando las dimensiones obtenidas. Lo que hace es un cluster jerárquico usando las dimensiones obtenidas en la estructura factorial.\n\n\nCode\nres_hcpc &lt;- HCPC(res_ca, graph = FALSE)\n\nfviz_cluster(res_hcpc,\n             repel = TRUE,            # Avoid label overlapping\n             show.clust.cent = TRUE, # Show cluster centers\n             palette = \"jco\",         # Color palette see ?ggpubr::ggpar\n             ggtheme = theme_minimal(),\n             main = \"Factor map\"\n             )\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(res_hcpc, choice = \"3D.map\")\n\n\n\n\n\n\n\n\n\nUna utilidad interesante es la descripción de las variables de los clusters. Dónde nos dice cuales son la variables más importantes para cada uno.\nCuando en un cluster su Intern % para una variable se desvíe mucho de glob % quiere decir que en esa variable la distribución es distinta de en la población general y por tanto es una variables que caracteriza al cluster.\nEn este caso estaremos encontrando grupos de individuos con mismas variables importantes en el randomForest.\nClaramente se ven grupos dónde es muy importante la variable de criminalidad o la edad\n\n\nCode\nres_hcpc$desc.var\n#&gt; $`1`\n#&gt;          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; crim    15.592962  2.127929    642.8840   657.2566  0.000000e+00        Inf\n#&gt; nox     20.157813 12.416539    831.0888  3835.1142  2.807733e-52  15.215104\n#&gt; dis     11.939900  9.329212    492.2715  2881.5271  2.572108e-09   5.956809\n#&gt; rm      20.191263 25.487484    832.4679  7872.3559  1.117021e-17  -8.561196\n#&gt; age      4.411035  7.568723    181.8631  2337.7625  1.230524e-18  -8.811891\n#&gt; tax      0.000000  1.001897      0.0000   309.4575  9.327733e-20  -9.096514\n#&gt; ptratio  0.000000  3.118072      0.0000   963.0835  2.250504e-61 -16.529487\n#&gt; indus    0.000000 10.860935      0.0000  3354.6328 2.069270e-222 -31.835808\n#&gt; \n#&gt; $`2`\n#&gt;         Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; age     14.32559  7.568723    803.0250  2337.7625  7.150050e-85  19.521920\n#&gt; nox     19.56112 12.416539   1096.5037  3835.1142  9.886451e-65  16.989118\n#&gt; dis     15.61069  9.329212    875.0616  2881.5271  3.449026e-63  16.779470\n#&gt; rm      21.85203 25.487484   1224.9213  7872.3559  2.382720e-12  -7.010027\n#&gt; tax      0.00000  1.001897      0.0000   309.4575  1.880511e-27 -10.855366\n#&gt; crim     0.00000  2.127929      0.0000   657.2566  2.961603e-58 -16.090764\n#&gt; ptratio  0.00000  3.118072      0.0000   963.0835  1.146579e-85 -19.615212\n#&gt; indus    0.00000 10.860935      0.0000  3354.6328 2.480702e-311 -37.718398\n#&gt; \n#&gt; $`3`\n#&gt;         Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; dis     14.41601  9.329212    885.4575  2881.5271  7.965757e-48  14.528753\n#&gt; indus   15.72645 10.860935    965.9468  3354.6328  4.377749e-39  13.078362\n#&gt; nox     17.05504 12.416539   1047.5513  3835.1142  1.891207e-32  11.860817\n#&gt; rm      24.28797 25.487484   1491.8110  7872.3559  1.531797e-02  -2.424773\n#&gt; tax      0.00000  1.001897      0.0000   309.4575  2.402280e-30 -11.448146\n#&gt; crim     0.00000  2.127929      0.0000   657.2566  1.885153e-64 -16.951216\n#&gt; ptratio  0.00000  3.118072      0.0000   963.0835  8.318594e-95 -20.657729\n#&gt; age      0.00000  7.568723      0.0000  2337.7625 1.165375e-235 -32.779199\n#&gt; \n#&gt; $`4`\n#&gt;          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; age     13.713854  7.568723    994.4487  2337.7625 7.010364e-100  21.214544\n#&gt; indus   15.980898 10.860935   1158.8415  3354.6328  3.363370e-53  15.353354\n#&gt; rm      27.297454 25.487484   1979.4522  7872.3559  6.332137e-05   4.000079\n#&gt; nox     11.031380 12.416539    799.9313  3835.1142  3.337066e-05  -4.149154\n#&gt; tax      0.000000  1.001897      0.0000   309.4575  1.541120e-36 -12.624810\n#&gt; crim     0.000000  2.127929      0.0000   657.2566  1.026729e-77 -18.661061\n#&gt; dis      3.479402  9.329212    252.3059  2881.5271 1.613296e-102 -21.498348\n#&gt; ptratio  0.000000  3.118072      0.0000   963.0835 2.246449e-114 -22.730329\n#&gt; \n#&gt; $`5`\n#&gt;           Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; ptratio 13.4631336  3.118072   731.46909   963.0835  0.000000e+00        Inf\n#&gt; indus   16.0021844 10.860935   869.41894  3354.6328  2.134777e-37  12.779485\n#&gt; rm      30.0450942 25.487484  1632.38800  7872.3559  6.922077e-17   8.348354\n#&gt; age      5.9723379  7.568723   324.48468  2337.7625  4.791243e-07  -5.034489\n#&gt; dis      5.8922388  9.329212   320.13279  2881.5271  6.783780e-24 -10.079838\n#&gt; tax      0.0000000  1.001897     0.00000   309.4575  1.567007e-26 -10.659939\n#&gt; crim     0.2645368  2.127929    14.37262   657.2566  4.931237e-37 -12.714201\n#&gt; nox      1.1050567 12.416539    60.03913  3835.1142 3.406088e-250 -33.783844\n#&gt; \n#&gt; $`6`\n#&gt;          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#&gt; tax     13.270152  1.001897   309.45748   309.4575  0.000000e+00        Inf\n#&gt; ptratio  9.932087  3.118072   231.61443   963.0835  3.328525e-58  16.083531\n#&gt; indus   15.455762 10.860935   360.42551  3354.6328  2.442145e-12   7.006580\n#&gt; rm      30.502626 25.487484   711.31557  7872.3559  1.684450e-08   5.641633\n#&gt; crim     0.000000  2.127929     0.00000   657.2566  4.470886e-23  -9.892856\n#&gt; dis      2.414161  9.329212    56.29780  2881.5271  1.144880e-43 -13.857568\n#&gt; age      1.455459  7.568723    33.94103  2337.7625  5.050710e-44 -13.916197\n#&gt; nox      0.000000 12.416539     0.00000  3835.1142 2.015379e-140 -25.227201\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"descfreq\" \"list\"\n\n\nY por supuesto tenemos los datos con el cluster asignado y los valores de cada variable (no son los valores originales de las variables , sino la importancia ponderada que tenían con el procedimiento descrito para cada observación )\n\n\nCode\nres_hcpc$data.clust %&gt;% \n         dplyr::select(clust, everything()) %&gt;% \n    slice_sample(prop = 0.3) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\n\nSi unimos con el dataset original\n\n\nCode\ntest_df_with_cluster &lt;-  res_hcpc$data.clust %&gt;% \n    rownames_to_column(var = \"id\") %&gt;% \n    dplyr::select(id, clust)\n\nunido &lt;- test_df %&gt;% \n    rownames_to_column(var = \"id\") %&gt;% \n    inner_join(test_df_with_cluster, by = \"id\")\n\n\nY efectivamente vemos que el cluster 1 tiene mucho más ratio de criminalidad, y además es la variable más importante para ese grupo en relación con la variable dependiente medv. No causa sorpresa ver que es justo en ese cluster dónde el precio de la propiedad es más bajo\n\n\nCode\nunido %&gt;% \n    group_by(clust) %&gt;% \n    summarise(across(c(lstat,crim, age, black, medv), list(mean = mean, median = median), .names = \"{.col}_{.fn}\" )) %&gt;% \n    DT::datatable()\n\n\n\n\n\n\n\nCarlos en el post que inspira este, comenta que este tipo de procedimientos sería útil para aquellas de las observaciones con un mayor score predicho. En este ejemplo se podría aplicar para clusterizar las observaciones con un mayor valor predicho del valor de la propiedad."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#nota",
    "href": "2023/01/29/explicatividad_no_usual/index.html#nota",
    "title": "Explicatividad no usual",
    "section": "Nota",
    "text": "Nota\n\nHice el código deprisa y corriendo, es claramente mejorable y podría ir mucho más rápido. El objetivo era mostrar como se puede obtener variables importantes a nivel de observación en este tipo de modelos, simplemente recorriendo por qué camino ha ido cada observación en cada árbol\n\n\nEstaría chulo representar espacialmente la distribución de los clusters obtenidos"
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html",
    "title": "Conformal prediction. Estilo compadre",
    "section": "",
    "text": "El jueves pasado asistí al más que recomendable meetup de PyData Madrid, que cuenta entre sus organizadores con el gran Juan Luis Cano Rodríguez, antiguo compañero mío de curro y tocayo de iniciales.\nEl caso es que en una de las charlas, Ignacio Peletier, mencionó de pasada lo del “Conformal Prediction”. Y siendo que Ignacio es un gran científico de datos, y que hacía unos meses que había tenido varias charlas con Carlos sobre el particular, pues he decidido ver un poco más en detalle de qué iba el asunto ."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#recursos",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#recursos",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Recursos",
    "text": "Recursos\nUn excelente sitio para empezar a bichear con este tema es el Readme de este repo, dónde han ido recopilando enlaces a libros, posts, papers y código relativo a lo de conformal prediction.\nEn particular, uno de los recursos que me ha gustado es este minicurso de Christoph Molnar.\nOtro recurso útil es este post de Carlos, dónde se esboza un poco en qué consiste esto de la predicción conforme y en por qué no es algo tan novedoso como se cree."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#experimentando",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#experimentando",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Experimentando",
    "text": "Experimentando\nLa predicción conforme se puede aplicar tanto a modelos de regresión de clasificación. Su objetivo es simplemente medir la incertidumbre de una predicción dada.\nEn el caso de regresión no tiene mucho misterio:\n\nSe entrena un modelo usando un conjunto de datos de entrenamiento.\nSe mide el error en un conjunto de datos de validación, calibración, utilizando la norma L1, es decir \\(\\mid y - \\hat{y}\\mid\\)\nSe elige una medida de dispersión del error, por ejemplo el cuantil \\((1- \\alpha) = 0.95\\) de los errores anteriores.\nPara una nueva predicción se da su intervalo como \\((\\hat{y} - q_{1-\\alpha}, \\hat{y} + q_{1-\\alpha})\\)\n\nEn el caso de clasificación la cosa es más divertida. Puesto que lo que se quiere obtener es un conjunto de etiquetas probables. Tipo {A} {A, B} {B, C}\nEn este caso según he leído aquí el algoritmo sería\n\nSe entrena un modelo usando un conjunto de datos de entrenamiento.\nSe mide el error en un conjunto de datos de validación, calibración, viendo para cada observación el valor que el modelo le ha dado para la predicción de la clase verdadera. Es un conjunto de validación , sabemos cuál es la verdad. Y se calcula el error como \\(1- p_{i}\\) siendo \\(p_i\\), la probabilidad predicha para la clase verdadera\nSe calcula el cuantil de orden \\(1-\\alpha\\) de esos errores y se guarda. Se entiende que el modelo está bien calibrado y que el conjunto de validación y que los scores que da el modelo se pueden asumir como probabilidades\nPara una nueva predicción se tendrá una \\(p_i\\) para cada clase. Se calcula \\(1-p_i\\) para cada clase y se considera que esa clase forma parte del prediction set si ese valor es menor o igual que el valor del cuantil anterior.\n\nPues vamos a ver como se haría con R en estilo compadre, y puede que con alguna pequeña modificación por mi parte.\n\nEjemplo\n\n\nCode\nlibrary(tidyverse)\nlibrary(MASS)\n\n\nVamos a usar el conjunto de datos housing \n\n\nCode\n\nskimr::skim(housing)\n\n\n\nData summary\n\n\nName\nhousing\n\n\nNumber of rows\n72\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSat\n0\n1\nTRUE\n3\nLow: 24, Med: 24, Hig: 24\n\n\nInfl\n0\n1\nFALSE\n3\nLow: 24, Med: 24, Hig: 24\n\n\nType\n0\n1\nFALSE\n4\nTow: 18, Apa: 18, Atr: 18, Ter: 18\n\n\nCont\n0\n1\nFALSE\n2\nLow: 36, Hig: 36\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nFreq\n0\n1\n23.35\n17.67\n3\n10\n19.5\n31.75\n86\n▇▅▂▁▁\n\n\n\n\n\nY vamos a justar un modelito tonto usando regresión logística ordinal, sobre los 40 primeros datos\n\n\nCode\n\nhouse.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing[1:40,])\n\nhead(predict(house.plr, type = \"probs\"))\n#&gt;         Low    Medium      High\n#&gt; 1 0.3639420 0.2575531 0.3785049\n#&gt; 2 0.3639420 0.2575531 0.3785049\n#&gt; 3 0.3639420 0.2575531 0.3785049\n#&gt; 4 0.3263764 0.2552793 0.4183443\n#&gt; 5 0.3263764 0.2552793 0.4183443\n#&gt; 6 0.3263764 0.2552793 0.4183443\n\n\nGuardamos las predicciones para el conjunto de validación , que va a ser las filas de la 41 a la 55, junto con el valor de Sat verdadero\n\n\nCode\n\npredictions &lt;- as.data.frame(predict(house.plr, type = \"probs\", newdata = housing[41:55,]))\n\ntt &lt;- cbind(predictions, True_class=housing$Sat[41:55])\n\ntt\n#&gt;          Low    Medium      High True_class\n#&gt; 41 0.3055662 0.2524868 0.4419471     Medium\n#&gt; 42 0.3055662 0.2524868 0.4419471       High\n#&gt; 43 0.1595073 0.1930758 0.6474169        Low\n#&gt; 44 0.1595073 0.1930758 0.6474169     Medium\n#&gt; 45 0.1595073 0.1930758 0.6474169       High\n#&gt; 46 0.4671694 0.2484190 0.2844116        Low\n#&gt; 47 0.4671694 0.2484190 0.2844116     Medium\n#&gt; 48 0.4671694 0.2484190 0.2844116       High\n#&gt; 49 0.4260865 0.2544760 0.3194375        Low\n#&gt; 50 0.4260865 0.2544760 0.3194375     Medium\n#&gt; 51 0.4260865 0.2544760 0.3194375       High\n#&gt; 52 0.2425400 0.2363201 0.5211399        Low\n#&gt; 53 0.2425400 0.2363201 0.5211399     Medium\n#&gt; 54 0.2425400 0.2363201 0.5211399       High\n#&gt; 55 0.4014928 0.2566312 0.3418760        Low\n\n\nAhora, para la primera fila sería hacer (1-0.2524), puesto que la clase real es “Medium” y para la segunda sería (1-0.44), puesto que la clase real es “High”. No estoy muy inspirado hoy y no he conseguido una forma elegante de hacerlo en R, y ChatGpt no me ha servido de mucha ayuda, seguramente porque aún no soy muy ducho preguntándole.\nAsí que he tirado iterando para cada fila con un map y quedándode con el valor predicho de la columna cuyo nombre coincida con el valor en True_class\n\n\nCode\n\ntt$prob_true_class &lt;- map_dbl(1:nrow(tt), .f = function(i) \n    tt[i, colnames(tt) == tt$True_class[i]])\n\ntt$resid &lt;- 1-tt$prob_true_class\n\nhead(tt)\n#&gt;          Low    Medium      High True_class prob_true_class     resid\n#&gt; 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#&gt; 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#&gt; 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#&gt; 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#&gt; 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#&gt; 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n\n\nDefinimos un \\(\\alpha = 0.3\\) y calculamos el cuantil 70 .\n\n\nCode\n(qhat = quantile(tt$resid, 0.7))\n#&gt;       70% \n#&gt; 0.7507675\n\n\nY ya estamos listos para hacer la predicción conforme para nuevos datos.\n\n\nCode\n# predecimos de la fila 51 a la 70 \npredicciones &lt;- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\n\nhead(predicciones)\n#&gt;          Low    Medium      High\n#&gt; 51 0.4260865 0.2544760 0.3194375\n#&gt; 52 0.2425400 0.2363201 0.5211399\n#&gt; 53 0.2425400 0.2363201 0.5211399\n#&gt; 54 0.2425400 0.2363201 0.5211399\n#&gt; 55 0.4014928 0.2566312 0.3418760\n#&gt; 56 0.4014928 0.2566312 0.3418760\n\n\nNos creamos un data.frame que indique si el valor de 1 - predicciones es menor o igual que el cuantil elegido\n\n\nCode\nset &lt;- as.data.frame(1 - predicciones &lt;= qhat)\n\nhead(set)\n#&gt;      Low Medium High\n#&gt; 51  TRUE   TRUE TRUE\n#&gt; 52 FALSE  FALSE TRUE\n#&gt; 53 FALSE  FALSE TRUE\n#&gt; 54 FALSE  FALSE TRUE\n#&gt; 55  TRUE   TRUE TRUE\n#&gt; 56  TRUE   TRUE TRUE\n\n\nAl igual que antes, utilizo un map para obtener el conjunto de etiquetas, para la primera fila serían todas, para la segunda sería {“Medium”, “High”}\n\n\nCode\n\nset$conformal &lt;-  map_chr(1:nrow(set), .f= function(i) {\n     set_list = colnames(set)[unlist(set[i,])]\n     paste0(set_list, collapse = \",\")\n     })\n\nhead(set)\n#&gt;      Low Medium High       conformal\n#&gt; 51  TRUE   TRUE TRUE Low,Medium,High\n#&gt; 52 FALSE  FALSE TRUE            High\n#&gt; 53 FALSE  FALSE TRUE            High\n#&gt; 54 FALSE  FALSE TRUE            High\n#&gt; 55  TRUE   TRUE TRUE Low,Medium,High\n#&gt; 56  TRUE   TRUE TRUE Low,Medium,High\n\n\nSe lo pego al dataset original de test (filas 51 a 70), junto con las predicciones y la clase verdadera.\n\n\nCode\n\nset_fin &lt;-  cbind( True_class = housing$Sat[51:70], as.data.frame(predicciones),\n                  set_conformal =set$conformal)\n\nhead(set_fin)\n#&gt;    True_class       Low    Medium      High   set_conformal\n#&gt; 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399            High\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399            High\n#&gt; 54       High 0.2425400 0.2363201 0.5211399            High\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High\n\n\nY ya estaría.\nUna cosa que se suele calcular es la cobertura de cada clase, es decir, la proporción de veces que cada clase está dentro del conjunto.\n\n\nCode\nset_fin &lt;- set_fin |&gt; \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                               .y = set_conformal , \n                               ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nhead(set_fin)\n#&gt;    True_class       Low    Medium      High   set_conformal class_in_set\n#&gt; 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High         TRUE\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n\n\n\n\nCode\nset_fin |&gt; \n    group_by(True_class) |&gt; \n    summarise(cov = mean(class_in_set))\n#&gt; # A tibble: 3 × 2\n#&gt;   True_class   cov\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;\n#&gt; 1 Low        0.571\n#&gt; 2 Medium     0.5  \n#&gt; 3 High       1\n\n\n\n\nModificación 1.\nNo me convence lo de tener un sólo cuantil, común a todas las clases, ¿no sería mejor tener una medida de cómo se distribuyen los errores para cada una de las clases?\nUsamos el conjunto de validación dónde tenemos el \\(1-p_i\\) que nos dice en cuánto se ha equivocado el modelo en predecir la clase real\n\n\nCode\nhead(tt)\n#&gt;          Low    Medium      High True_class prob_true_class     resid\n#&gt; 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#&gt; 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#&gt; 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#&gt; 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#&gt; 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#&gt; 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n\n\nCalculamos el quantil 70 para cada clase, y así vemos que varía por clase\n\n\nCode\n(qhat_by_class &lt;- tt |&gt; \n    group_by(True_class) |&gt; \n    summarise(qhat = quantile(resid, 0.7)) |&gt; \n        pivot_wider(names_from = True_class, values_from = qhat))\n#&gt; # A tibble: 1 × 3\n#&gt;     Low Medium  High\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.726  0.761 0.656\n\n\n\n\nCode\npredicciones &lt;- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\ncomplementarios &lt;- 1-predicciones\nhead(complementarios)\n#&gt;          Low    Medium      High\n#&gt; 51 0.5739135 0.7455240 0.6805625\n#&gt; 52 0.7574600 0.7636799 0.4788601\n#&gt; 53 0.7574600 0.7636799 0.4788601\n#&gt; 54 0.7574600 0.7636799 0.4788601\n#&gt; 55 0.5985072 0.7433688 0.6581240\n#&gt; 56 0.5985072 0.7433688 0.6581240\n\n\nY vemos si cada \\(1-p_i\\) es menor o igual que el cuantil correspondiente de cada clase\n\n\nCode\nset_adjust &lt;- data.frame(Low = complementarios[,1] &lt;= qhat_by_class$Low,\n                        Medium = complementarios[,2] &lt;= qhat_by_class$Medium,\n                         High = complementarios[,3] &lt;= qhat_by_class$High )\n\n\nhead(set_adjust)\n#&gt;      Low Medium  High\n#&gt; 51  TRUE   TRUE FALSE\n#&gt; 52 FALSE  FALSE  TRUE\n#&gt; 53 FALSE  FALSE  TRUE\n#&gt; 54 FALSE  FALSE  TRUE\n#&gt; 55  TRUE   TRUE FALSE\n#&gt; 56  TRUE   TRUE FALSE\n\n\n\n\nCode\nset_adjust$conformal &lt;-  map_chr(1:nrow(set_adjust), .f= function(i) {\n    set_list = colnames(set_adjust)[unlist(set_adjust[i,])]\n    paste0(set_list, collapse = \",\")\n})\n\nhead(set_adjust)\n#&gt;      Low Medium  High  conformal\n#&gt; 51  TRUE   TRUE FALSE Low,Medium\n#&gt; 52 FALSE  FALSE  TRUE       High\n#&gt; 53 FALSE  FALSE  TRUE       High\n#&gt; 54 FALSE  FALSE  TRUE       High\n#&gt; 55  TRUE   TRUE FALSE Low,Medium\n#&gt; 56  TRUE   TRUE FALSE Low,Medium\n\n\nComo antes, nos quedamos con la clase de verdad, la predicción en probabilidad de cada clase y la predicción conforme\n\n\nCode\nset_adjust_fin &lt;-  cbind( True_class = housing$Sat[51:70], as.data.frame(predict(house.plr, newdata = housing[51:70,],type=\"probs\")),\n                   set_conformal =set_adjust$conformal)\n\nhead(set_adjust_fin)\n#&gt;    True_class       Low    Medium      High set_conformal\n#&gt; 51       High 0.4260865 0.2544760 0.3194375    Low,Medium\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399          High\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399          High\n#&gt; 54       High 0.2425400 0.2363201 0.5211399          High\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760    Low,Medium\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760    Low,Medium\n\n\n\n\nCode\nset_adjust_fin &lt;- set_adjust_fin |&gt; \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                                .y = set_conformal , \n                                ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nset_adjust_fin\n#&gt;    True_class       Low    Medium      High   set_conformal class_in_set\n#&gt; 51       High 0.4260865 0.2544760 0.3194375      Low,Medium        FALSE\n#&gt; 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#&gt; 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#&gt; 55        Low 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#&gt; 56     Medium 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#&gt; 57       High 0.4014928 0.2566312 0.3418760      Low,Medium        FALSE\n#&gt; 58        Low 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#&gt; 59     Medium 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#&gt; 60       High 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#&gt; 61        Low 0.1967801 0.2160332 0.5871868            High        FALSE\n#&gt; 62     Medium 0.1967801 0.2160332 0.5871868            High        FALSE\n#&gt; 63       High 0.1967801 0.2160332 0.5871868            High         TRUE\n#&gt; 64        Low 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#&gt; 65     Medium 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#&gt; 66       High 0.4732214 0.2472855 0.2794931      Low,Medium        FALSE\n#&gt; 67        Low 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#&gt; 68     Medium 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#&gt; 69       High 0.4320378 0.2537829 0.3141793      Low,Medium        FALSE\n#&gt; 70        Low 0.2470311 0.2378946 0.5150743            High        FALSE\n\n\nY aquí ya vemos que la cobertura es distinta y que la clase “High” ya no está en el 100% de los prediction sets\n\n\nCode\nset_adjust_fin |&gt; \n    group_by(True_class) |&gt; \n    summarise(cov = mean(class_in_set))\n#&gt; # A tibble: 3 × 2\n#&gt;   True_class   cov\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;\n#&gt; 1 Low        0.571\n#&gt; 2 Medium     0.667\n#&gt; 3 High       0.429\n\n\nDe hecho si tabulamos ambas predicciones conformes , vemos que de las 10 predicciones que el primer método ponía como {Low, Medium, High} , el segundo pone 7 como {Low, Medium } y 3 como {Low, Medium, High}\n\n\nCode\n\ntable(set_fin$set_conformal, set_adjust_fin$set_conformal)\n#&gt;                  \n#&gt;                   High Low,Medium Low,Medium,High\n#&gt;   High               7          0               0\n#&gt;   Low,High           0          3               0\n#&gt;   Low,Medium,High    0          7               3\n\n\n\n\nModificación 2.\nVale, todo esto está muy bien, pero ¿y si simplemente para cada observación ordeno de forma decreciente su probabilidad predicha y me quedo con las clases que lleguen al 60% de probabilidad, por ejemplo?\n\n\nCode\n\n(predicciones_df &lt;-  as.data.frame(predicciones ))\n#&gt;          Low    Medium      High\n#&gt; 51 0.4260865 0.2544760 0.3194375\n#&gt; 52 0.2425400 0.2363201 0.5211399\n#&gt; 53 0.2425400 0.2363201 0.5211399\n#&gt; 54 0.2425400 0.2363201 0.5211399\n#&gt; 55 0.4014928 0.2566312 0.3418760\n#&gt; 56 0.4014928 0.2566312 0.3418760\n#&gt; 57 0.4014928 0.2566312 0.3418760\n#&gt; 58 0.3622588 0.2575226 0.3802186\n#&gt; 59 0.3622588 0.2575226 0.3802186\n#&gt; 60 0.3622588 0.2575226 0.3802186\n#&gt; 61 0.1967801 0.2160332 0.5871868\n#&gt; 62 0.1967801 0.2160332 0.5871868\n#&gt; 63 0.1967801 0.2160332 0.5871868\n#&gt; 64 0.4732214 0.2472855 0.2794931\n#&gt; 65 0.4732214 0.2472855 0.2794931\n#&gt; 66 0.4732214 0.2472855 0.2794931\n#&gt; 67 0.4320378 0.2537829 0.3141793\n#&gt; 68 0.4320378 0.2537829 0.3141793\n#&gt; 69 0.4320378 0.2537829 0.3141793\n#&gt; 70 0.2470311 0.2378946 0.5150743\n\n\n\n\nCode\nmodificacion_2 &lt;- predicciones_df |&gt; \n     rownames_to_column(var = \"individuo\") |&gt; \n    pivot_longer(cols = Low:High) |&gt; \n    group_by(individuo) |&gt; \n    arrange( desc(value)) |&gt; \n    mutate(suma_acumulada = cumsum(value)) |&gt; \n    arrange(individuo)\n\nhead(modificacion_2, 10)\n#&gt; # A tibble: 10 × 4\n#&gt; # Groups:   individuo [4]\n#&gt;    individuo name   value suma_acumulada\n#&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1 51        Low    0.426          0.426\n#&gt;  2 51        High   0.319          0.746\n#&gt;  3 51        Medium 0.254          1    \n#&gt;  4 52        High   0.521          0.521\n#&gt;  5 52        Low    0.243          0.764\n#&gt;  6 52        Medium 0.236          1    \n#&gt;  7 53        High   0.521          0.521\n#&gt;  8 53        Low    0.243          0.764\n#&gt;  9 53        Medium 0.236          1    \n#&gt; 10 54        High   0.521          0.521\n\n\nUhmm, pero no me acaba de convencer ordenar de forma descendente por la probabilidad predicha de cada clase. Por ejemplo para el individuo 51, si tomo Low +High llegaría a 0.74, pero si tomo Low + Medium llego al 67% . Si quisiera el menor conjunto de etiquetas que lleguen como mínimo al 60% la opción buena sería Low + Medium para ese individuo.\nNo me veo con ganas de implementar todas las posibles sumas de probabilidades estimadas y elegir el conjunto que cumpla la restricción de llegar al menos al 60% y si hay varios para mismo individuos que se quede con el conjunto más pequeño."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#conclusión.",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#conclusión.",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Conclusión.",
    "text": "Conclusión.\n\nLo de la predicción conforme para el caso de regresión me parece bastante sencillo, no es más que sumar y restar una medida de dispersión de los residuos a la predicción para nuevos datos.\nPara clasificación es un poco más interesante, sobre todo para casos en los que el usuario quiere una etiqueta o etiquetas y no se conforma con las probabilidades predichas de cada clase.\nSubyace la hipótesis de que los scores del modelo están bien calibrados y reflejan la verdadera probabilidad.\n\nPues nada más, tengan un feliz día."
  },
  {
    "objectID": "2019/11/30/la-fatal-arrogancia/index.html",
    "href": "2019/11/30/la-fatal-arrogancia/index.html",
    "title": "La fatal arrogancia",
    "section": "",
    "text": "Important\n\n\n\nNo, no voy a hablar de liberalismo ni de Hayek. Solo quería hacer una pequeña reflexión sobre las nuevas generaciones de científicos de datos o como se les quiera llamar.\n\n\nVengo observando hace cosa de 3 años, que las nuevas generaciones creen que es fácil utilizar modelos estadísticos (o de Machín Lenin como dice algún amigo mío) para predecir cosas como la bolsa, o acertar ,cual demiurgo, si se va a sufrir un cáncer y cosas por el estilo.\nLa fatal arrogancia a la que me refiero es aquella que hace que hasta los que trabajamos en estos temas, nos dejemos llevar por el “hype” (no sé bien como traducir esto) que se vive actualmente y nos creamos la cantidad de mentiras que se dicen. Es fácil entusiasmarse por cosas como el deep learning, reinforcement learning o el último algoritmo de boosting, pero ninguna de estas cosas puede hacer magia y separar totalmente la señal del ruido.\nTambién noto lo de la fatal arrogancia entre aquellos que dicen que para qué necesitan saber estadística, ¿a cuántos conocéis que den intervalos de confianza o de credibilidad en vez de solo estimaciones puntuales? Seguramente a muy pocos.\nEn fin, de lo que estoy seguro es que si hay alguno que ha conseguido a predecir con éxito notable los movimientos de la bolsa, lo que menos hace es alardear de su éxito dando charlas en meetups.\nNos vemos en los bares."
  },
  {
    "objectID": "2023/04/23/quantile-catboost/index.html",
    "href": "2023/04/23/quantile-catboost/index.html",
    "title": "Regresión cuantil a lo machín lenin con catboost",
    "section": "",
    "text": "Hay veces, más de las que se cree, en que nos interesa estimar un cuantil en vez de la media. Si tenemos una variable dependinte \\(y\\) y una o varias independientes \\(X\\), lo que se suele hacer es una regresión cuantil.\nSi visitamos uno de los papers originales de dicha técnica Computing Regression Quantiles vemos que trata de minimizar la siguiente expresión.\n\\[\n\\arg\\min_b R_{\\theta}(b)  = \\sum_{i = 1}^{n}\\rho_\\theta \\left( y_i - x_ib\\right)\n\\] Con \\(\\theta \\in (0,1)\\) y\n\\[\n\\begin{equation}\n    \\rho_\\theta(u) =\n        \\begin{cases}\n        \\theta u  &  u \\geq 0\\\\\n        (\\theta -1) & u  &lt; 0 \\\\\n        \\end{cases}\n\\end{equation}\n\\]\nLo cual es simplemente “penalizar” por \\(\\theta\\) cuando el residuo sea mayor o igual que 0, es decir, cuando nos equivocamos por arriba y por \\((\\theta -1)\\) si nos equivocamos por abajo.\nEjemplo, si \\(y_i = 40\\) y \\(f(x) = 50\\) y queremos estimar el cuantil 0.95. Entonces como el residuo es menor que 0, se pondera por 0.05\n\\[\\rho_(40 - 50) = (0.95 -1) (40 - 50) = 0.5 \\] Si en cambio \\(f(x) = 30\\), es decir, nos equivocamos por abajo, pero a la misma distancia del valor real entonces\n\\[\\rho(40-30) = 0.95 (40-30) = 9.5 \\]\nY por tanto la función a minimizar \\(\\arg\\min_b R_{\\theta}(b)\\) cuando \\(\\theta &gt; 0.5\\) va a tener un valor mucho mayor cuando nos “equivocamos” por abajo que por arriba. Y debido a cómo está definido \\(\\rho_\\theta(u)\\) se consigue la regresión cuantil con cuantil igual a \\(\\theta\\). En el paper (de 1987) viene mejor explicado y el algoritmo para resolverlo en el caso de que \\(f(x)\\) sea lineal.\nFuera coñas, el caso es que la gente de yandex en su librería catboost han utilizado esto para hacer la regresión cuantil, simplemente utilizando la expresión anterior como función de pérdida. Aquí se puede ver las diferentes funciones de pérdida que usan según el caso.\nPara la regresión cuantil usan\n\\[L(t, a, \\alpha) = \\dfrac{\\sum_{i}^{N} \\omega_i(\\alpha - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}\\] Dónde\nComo vemos, es lo mismo que se cuenta en el paper de 1987. Pero al meterlo como función de pérdida en el algoritmo sirve para el algoritmo de boosting que se utiliza en la librería.\nLa gente de catboost, atinadamente ha dicho, y ¿por qué no construimos un función de pérdida que minimice globalmente varios cuantiles? Lo cual es algo así como “encuéntrame la distribución de los parámetros que mejor se ajusta a estos datos en vez de un sólo parámetro”.\nPero esto son arbolitos y boosting, no hay lo que se dice un parámetro de la función propiamente dicho, por lo que al final lo que se “aprende” debe ser la configuración de árboles que minimiza globalmente los cuantiles indicados.\nBueno, la función de pérdida “multi-quantile” es una modificación simple de la anterior.\n\\[L(t, a, \\alpha_q) = \\dfrac{\\sum_{i}^{N} \\omega_i \\sum_{q=1}^{Q}(\\alpha_q - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}\\]"
  },
  {
    "objectID": "2023/04/23/quantile-catboost/index.html#ejemplo",
    "href": "2023/04/23/quantile-catboost/index.html#ejemplo",
    "title": "Regresión cuantil a lo machín lenin con catboost",
    "section": "Ejemplo",
    "text": "Ejemplo\nEl ejemplo no es mío, lo he visto por algún sitio que no me acuerdo.\n\n\n\n\n\n\nTip\n\n\n\ncatboost se puede utilizar en R y python.\n\n\n\n\nCode\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\nsns.set()\n\nn = 800\n\n# X aleatorias\nx_train = np.random.rand(n)\nx_test = np.random.rand(n)\n\n# un poquito de ruido gaussiano\n\nnoise_train = np.random.normal(0, 0.3, n)\nnoise_test = np.random.normal(0, 0.3, n)\n\n# Simulamos y_train e y _x como y = 2 + 3 * x + ruido\na, b = 2, 3\n\n# al lio\ny_train = a * x_train + b + noise_train\ny_test = a * x_test + b + noise_test\n\n\nPintamos\n\n\nCode\nsns.scatterplot(x = x_train, y = y_train).set(title = \"Ejemplillo\")\n\n\n\n\n\n\n\n\n\nVaos a predecir 10 cuantiles\n\n\nCode\nquantiles = [q/10 for q in range(1, 10)]\n\n# se ponen en string separados por commas\nquantile_str = str(quantiles).replace('[','').replace(']','')\n\nprint(quantile_str)\n#&gt; 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n\n\nModelito\n\n\nCode\nmodel = CatBoostRegressor(iterations=100,\n                          loss_function=f'MultiQuantile:alpha={quantile_str}')\n\nmodel.fit(x_train.reshape(-1,1), y_train)\n#&gt; 0:   learn: 0.1977907    total: 49.8ms   remaining: 4.93s\n#&gt; 1:   learn: 0.1934878    total: 54.5ms   remaining: 2.67s\n#&gt; 2:   learn: 0.1891389    total: 61.2ms   remaining: 1.98s\n#&gt; 3:   learn: 0.1849242    total: 64.9ms   remaining: 1.56s\n#&gt; 4:   learn: 0.1808818    total: 68.4ms   remaining: 1.3s\n#&gt; 5:   learn: 0.1769520    total: 71.7ms   remaining: 1.12s\n#&gt; 6:   learn: 0.1732251    total: 74.7ms   remaining: 993ms\n#&gt; 7:   learn: 0.1696774    total: 77.9ms   remaining: 896ms\n#&gt; 8:   learn: 0.1661482    total: 80.9ms   remaining: 818ms\n#&gt; 9:   learn: 0.1628541    total: 84.5ms   remaining: 760ms\n#&gt; 10:  learn: 0.1596378    total: 87.6ms   remaining: 709ms\n#&gt; 11:  learn: 0.1565479    total: 90.7ms   remaining: 665ms\n#&gt; 12:  learn: 0.1536016    total: 93.8ms   remaining: 628ms\n#&gt; 13:  learn: 0.1507692    total: 96.9ms   remaining: 595ms\n#&gt; 14:  learn: 0.1480845    total: 100ms    remaining: 567ms\n#&gt; 15:  learn: 0.1454840    total: 103ms    remaining: 541ms\n#&gt; 16:  learn: 0.1429863    total: 106ms    remaining: 518ms\n#&gt; 17:  learn: 0.1407059    total: 109ms    remaining: 497ms\n#&gt; 18:  learn: 0.1383697    total: 112ms    remaining: 477ms\n#&gt; 19:  learn: 0.1361779    total: 115ms    remaining: 460ms\n#&gt; 20:  learn: 0.1340707    total: 118ms    remaining: 444ms\n#&gt; 21:  learn: 0.1320342    total: 122ms    remaining: 432ms\n#&gt; 22:  learn: 0.1300775    total: 125ms    remaining: 418ms\n#&gt; 23:  learn: 0.1282447    total: 129ms    remaining: 408ms\n#&gt; 24:  learn: 0.1264588    total: 133ms    remaining: 399ms\n#&gt; 25:  learn: 0.1247594    total: 137ms    remaining: 390ms\n#&gt; 26:  learn: 0.1232137    total: 141ms    remaining: 381ms\n#&gt; 27:  learn: 0.1216563    total: 145ms    remaining: 373ms\n#&gt; 28:  learn: 0.1202034    total: 149ms    remaining: 365ms\n#&gt; 29:  learn: 0.1187735    total: 153ms    remaining: 357ms\n#&gt; 30:  learn: 0.1174263    total: 157ms    remaining: 350ms\n#&gt; 31:  learn: 0.1161250    total: 162ms    remaining: 344ms\n#&gt; 32:  learn: 0.1148583    total: 172ms    remaining: 350ms\n#&gt; 33:  learn: 0.1136688    total: 176ms    remaining: 342ms\n#&gt; 34:  learn: 0.1125292    total: 180ms    remaining: 335ms\n#&gt; 35:  learn: 0.1114357    total: 184ms    remaining: 327ms\n#&gt; 36:  learn: 0.1104088    total: 187ms    remaining: 318ms\n#&gt; 37:  learn: 0.1094203    total: 190ms    remaining: 309ms\n#&gt; 38:  learn: 0.1085302    total: 193ms    remaining: 301ms\n#&gt; 39:  learn: 0.1076702    total: 196ms    remaining: 294ms\n#&gt; 40:  learn: 0.1068348    total: 200ms    remaining: 288ms\n#&gt; 41:  learn: 0.1060263    total: 204ms    remaining: 281ms\n#&gt; 42:  learn: 0.1052530    total: 207ms    remaining: 274ms\n#&gt; 43:  learn: 0.1045115    total: 212ms    remaining: 269ms\n#&gt; 44:  learn: 0.1037861    total: 216ms    remaining: 264ms\n#&gt; 45:  learn: 0.1031053    total: 220ms    remaining: 259ms\n#&gt; 46:  learn: 0.1024645    total: 225ms    remaining: 254ms\n#&gt; 47:  learn: 0.1018457    total: 229ms    remaining: 248ms\n#&gt; 48:  learn: 0.1012497    total: 232ms    remaining: 242ms\n#&gt; 49:  learn: 0.1006996    total: 235ms    remaining: 235ms\n#&gt; 50:  learn: 0.1001835    total: 239ms    remaining: 229ms\n#&gt; 51:  learn: 0.0996695    total: 242ms    remaining: 223ms\n#&gt; 52:  learn: 0.0991990    total: 246ms    remaining: 218ms\n#&gt; 53:  learn: 0.0987716    total: 249ms    remaining: 212ms\n#&gt; 54:  learn: 0.0983443    total: 252ms    remaining: 206ms\n#&gt; 55:  learn: 0.0979411    total: 255ms    remaining: 200ms\n#&gt; 56:  learn: 0.0975621    total: 258ms    remaining: 195ms\n#&gt; 57:  learn: 0.0972083    total: 261ms    remaining: 189ms\n#&gt; 58:  learn: 0.0968639    total: 265ms    remaining: 184ms\n#&gt; 59:  learn: 0.0965243    total: 269ms    remaining: 179ms\n#&gt; 60:  learn: 0.0961923    total: 272ms    remaining: 174ms\n#&gt; 61:  learn: 0.0958829    total: 275ms    remaining: 168ms\n#&gt; 62:  learn: 0.0956101    total: 278ms    remaining: 163ms\n#&gt; 63:  learn: 0.0953473    total: 281ms    remaining: 158ms\n#&gt; 64:  learn: 0.0950908    total: 284ms    remaining: 153ms\n#&gt; 65:  learn: 0.0948327    total: 287ms    remaining: 148ms\n#&gt; 66:  learn: 0.0946033    total: 290ms    remaining: 143ms\n#&gt; 67:  learn: 0.0943820    total: 293ms    remaining: 138ms\n#&gt; 68:  learn: 0.0941723    total: 297ms    remaining: 133ms\n#&gt; 69:  learn: 0.0939663    total: 300ms    remaining: 129ms\n#&gt; 70:  learn: 0.0937677    total: 304ms    remaining: 124ms\n#&gt; 71:  learn: 0.0935819    total: 307ms    remaining: 119ms\n#&gt; 72:  learn: 0.0933969    total: 310ms    remaining: 115ms\n#&gt; 73:  learn: 0.0932181    total: 313ms    remaining: 110ms\n#&gt; 74:  learn: 0.0930514    total: 316ms    remaining: 105ms\n#&gt; 75:  learn: 0.0928960    total: 320ms    remaining: 101ms\n#&gt; 76:  learn: 0.0927433    total: 323ms    remaining: 96.4ms\n#&gt; 77:  learn: 0.0925871    total: 326ms    remaining: 91.9ms\n#&gt; 78:  learn: 0.0924635    total: 329ms    remaining: 87.6ms\n#&gt; 79:  learn: 0.0923467    total: 333ms    remaining: 83.1ms\n#&gt; 80:  learn: 0.0922292    total: 336ms    remaining: 78.7ms\n#&gt; 81:  learn: 0.0921089    total: 339ms    remaining: 74.3ms\n#&gt; 82:  learn: 0.0919860    total: 342ms    remaining: 70.1ms\n#&gt; 83:  learn: 0.0918862    total: 346ms    remaining: 65.9ms\n#&gt; 84:  learn: 0.0917925    total: 349ms    remaining: 61.6ms\n#&gt; 85:  learn: 0.0916927    total: 353ms    remaining: 57.4ms\n#&gt; 86:  learn: 0.0916089    total: 356ms    remaining: 53.1ms\n#&gt; 87:  learn: 0.0915482    total: 356ms    remaining: 48.6ms\n#&gt; 88:  learn: 0.0914624    total: 362ms    remaining: 44.7ms\n#&gt; 89:  learn: 0.0913841    total: 365ms    remaining: 40.5ms\n#&gt; 90:  learn: 0.0912981    total: 368ms    remaining: 36.4ms\n#&gt; 91:  learn: 0.0912323    total: 371ms    remaining: 32.3ms\n#&gt; 92:  learn: 0.0911620    total: 374ms    remaining: 28.2ms\n#&gt; 93:  learn: 0.0910968    total: 378ms    remaining: 24.1ms\n#&gt; 94:  learn: 0.0910390    total: 381ms    remaining: 20ms\n#&gt; 95:  learn: 0.0909742    total: 384ms    remaining: 16ms\n#&gt; 96:  learn: 0.0909243    total: 387ms    remaining: 12ms\n#&gt; 97:  learn: 0.0908512    total: 390ms    remaining: 7.95ms\n#&gt; 98:  learn: 0.0907883    total: 393ms    remaining: 3.97ms\n#&gt; 99:  learn: 0.0907341    total: 396ms    remaining: 0us\n#&gt; &lt;catboost.core.CatBoostRegressor object at 0x7f75bdded3c0&gt;\n\n\nPredecimos\n\n\nCode\n\n# Make predictions on the test set\npreds = model.predict(x_test.reshape(-1, 1))\npreds = pd.DataFrame(preds, columns=[f'pred_{q}' for q in quantiles])\n\npreds.head(6)\n#&gt;    pred_0.1  pred_0.2  pred_0.3  ...  pred_0.7  pred_0.8  pred_0.9\n#&gt; 0  3.401601  3.542894  3.619259  ...  3.938859  4.041568  4.165809\n#&gt; 1  4.150397  4.269908  4.355643  ...  4.701792  4.816983  4.958197\n#&gt; 2  4.350918  4.476999  4.595515  ...  4.933372  5.017778  5.147716\n#&gt; 3  2.829240  2.936415  3.034001  ...  3.365471  3.446338  3.577815\n#&gt; 4  3.456021  3.574613  3.658350  ...  3.966363  4.047775  4.166815\n#&gt; 5  3.356988  3.505757  3.580486  ...  3.858433  3.961821  4.084200\n#&gt; \n#&gt; [6 rows x 9 columns]\n\n\nPintamos\n\n\nCode\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x_test, y_test)\n\nfor col in ['pred_0.1', 'pred_0.5', 'pred_0.9']:\n    ax.scatter(x_test.reshape(-1,1), preds[col], alpha=0.50, label=col)\n\nax.legend()\n\n\n\n\n\n\n\n\n\nY ya estaría, no parece mala alternativa si uno tiene que hacer este tipo de cosas.\n\n\n\n\n\n\nTip\n\n\n\nOjalá le sirva a mi amigo Kenet para una cosa que estaba bicheando.\n\n\nPues poco más. Feliz domingo\nCon R también se puede, como no.\n\n\nCode\nlibrary(reticulate) # para comunicar R y python y poder convertir datos y funciones de uno a otro bidireccionalmente\nlibrary(catboost)\n\nX_train &lt;- as.matrix(py$x_train) # catboost en R espera  una matriz\nY_train &lt;-  as.matrix(py$y_train)\n\n\nX_test &lt;- as.matrix(py$x_test) \nY_test &lt;-  as.matrix(py$y_test)\n\nhead(X_train) ; head(Y_train)\n#&gt;           [,1]\n#&gt; [1,] 0.6499924\n#&gt; [2,] 0.9125034\n#&gt; [3,] 0.4820761\n#&gt; [4,] 0.2060426\n#&gt; [5,] 0.5411845\n#&gt; [6,] 0.1779657\n#&gt;          [,1]\n#&gt; [1,] 4.165367\n#&gt; [2,] 5.018479\n#&gt; [3,] 4.146041\n#&gt; [4,] 2.942043\n#&gt; [5,] 4.287877\n#&gt; [6,] 3.589826\n\n(quantiles_str &lt;-  py$quantile_str)\n#&gt; [1] \"0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\"\n\n\n\n\nCode\ntrain_pool &lt;- catboost.load_pool(data = X_train, label = Y_train)\ntest_pool &lt;- catboost.load_pool(data = X_test)\nloss_function &lt;-  paste0(\"MultiQuantile:alpha=\", quantiles_str)\n\nfit_params &lt;-  list(\n    iterations = 100,\n    loss_function= loss_function\n    )\n\n\n\n\nCode\n\nmodel &lt;- catboost.train(train_pool, params=fit_params)\n#&gt; 0:   learn: 0.1977907    total: 50.6ms   remaining: 5s\n#&gt; 1:   learn: 0.1934878    total: 56.9ms   remaining: 2.79s\n#&gt; 2:   learn: 0.1891389    total: 61.3ms   remaining: 1.98s\n#&gt; 3:   learn: 0.1849242    total: 66.1ms   remaining: 1.58s\n#&gt; 4:   learn: 0.1808818    total: 71ms remaining: 1.35s\n#&gt; 5:   learn: 0.1769520    total: 74.5ms   remaining: 1.17s\n#&gt; 6:   learn: 0.1732251    total: 78.2ms   remaining: 1.04s\n#&gt; 7:   learn: 0.1696774    total: 83ms remaining: 955ms\n#&gt; 8:   learn: 0.1661482    total: 86.6ms   remaining: 876ms\n#&gt; 9:   learn: 0.1628541    total: 89.7ms   remaining: 807ms\n#&gt; 10:  learn: 0.1596378    total: 93.1ms   remaining: 753ms\n#&gt; 11:  learn: 0.1565479    total: 97.3ms   remaining: 714ms\n#&gt; 12:  learn: 0.1536016    total: 102ms    remaining: 683ms\n#&gt; 13:  learn: 0.1507692    total: 105ms    remaining: 647ms\n#&gt; 14:  learn: 0.1480845    total: 109ms    remaining: 615ms\n#&gt; 15:  learn: 0.1454840    total: 113ms    remaining: 592ms\n#&gt; 16:  learn: 0.1429863    total: 118ms    remaining: 576ms\n#&gt; 17:  learn: 0.1407059    total: 122ms    remaining: 557ms\n#&gt; 18:  learn: 0.1383697    total: 126ms    remaining: 536ms\n#&gt; 19:  learn: 0.1361779    total: 129ms    remaining: 518ms\n#&gt; 20:  learn: 0.1340707    total: 133ms    remaining: 500ms\n#&gt; 21:  learn: 0.1320342    total: 137ms    remaining: 485ms\n#&gt; 22:  learn: 0.1300775    total: 140ms    remaining: 468ms\n#&gt; 23:  learn: 0.1282447    total: 144ms    remaining: 455ms\n#&gt; 24:  learn: 0.1264588    total: 147ms    remaining: 441ms\n#&gt; 25:  learn: 0.1247594    total: 151ms    remaining: 429ms\n#&gt; 26:  learn: 0.1232137    total: 154ms    remaining: 416ms\n#&gt; 27:  learn: 0.1216563    total: 157ms    remaining: 404ms\n#&gt; 28:  learn: 0.1202034    total: 161ms    remaining: 394ms\n#&gt; 29:  learn: 0.1187735    total: 164ms    remaining: 383ms\n#&gt; 30:  learn: 0.1174263    total: 169ms    remaining: 375ms\n#&gt; 31:  learn: 0.1161250    total: 172ms    remaining: 365ms\n#&gt; 32:  learn: 0.1148583    total: 175ms    remaining: 356ms\n#&gt; 33:  learn: 0.1136688    total: 180ms    remaining: 349ms\n#&gt; 34:  learn: 0.1125292    total: 185ms    remaining: 343ms\n#&gt; 35:  learn: 0.1114357    total: 188ms    remaining: 334ms\n#&gt; 36:  learn: 0.1104088    total: 195ms    remaining: 332ms\n#&gt; 37:  learn: 0.1094203    total: 199ms    remaining: 325ms\n#&gt; 38:  learn: 0.1085302    total: 202ms    remaining: 316ms\n#&gt; 39:  learn: 0.1076702    total: 205ms    remaining: 308ms\n#&gt; 40:  learn: 0.1068348    total: 212ms    remaining: 305ms\n#&gt; 41:  learn: 0.1060263    total: 216ms    remaining: 298ms\n#&gt; 42:  learn: 0.1052530    total: 219ms    remaining: 290ms\n#&gt; 43:  learn: 0.1045115    total: 223ms    remaining: 284ms\n#&gt; 44:  learn: 0.1037861    total: 228ms    remaining: 278ms\n#&gt; 45:  learn: 0.1031053    total: 232ms    remaining: 272ms\n#&gt; 46:  learn: 0.1024645    total: 235ms    remaining: 265ms\n#&gt; 47:  learn: 0.1018457    total: 242ms    remaining: 262ms\n#&gt; 48:  learn: 0.1012497    total: 246ms    remaining: 256ms\n#&gt; 49:  learn: 0.1006996    total: 250ms    remaining: 250ms\n#&gt; 50:  learn: 0.1001835    total: 253ms    remaining: 243ms\n#&gt; 51:  learn: 0.0996695    total: 257ms    remaining: 237ms\n#&gt; 52:  learn: 0.0991990    total: 261ms    remaining: 232ms\n#&gt; 53:  learn: 0.0987716    total: 265ms    remaining: 225ms\n#&gt; 54:  learn: 0.0983443    total: 268ms    remaining: 219ms\n#&gt; 55:  learn: 0.0979411    total: 272ms    remaining: 214ms\n#&gt; 56:  learn: 0.0975621    total: 275ms    remaining: 208ms\n#&gt; 57:  learn: 0.0972083    total: 279ms    remaining: 202ms\n#&gt; 58:  learn: 0.0968639    total: 283ms    remaining: 197ms\n#&gt; 59:  learn: 0.0965243    total: 287ms    remaining: 191ms\n#&gt; 60:  learn: 0.0961923    total: 292ms    remaining: 187ms\n#&gt; 61:  learn: 0.0958829    total: 298ms    remaining: 182ms\n#&gt; 62:  learn: 0.0956101    total: 304ms    remaining: 178ms\n#&gt; 63:  learn: 0.0953473    total: 308ms    remaining: 173ms\n#&gt; 64:  learn: 0.0950908    total: 313ms    remaining: 168ms\n#&gt; 65:  learn: 0.0948327    total: 317ms    remaining: 163ms\n#&gt; 66:  learn: 0.0946033    total: 324ms    remaining: 160ms\n#&gt; 67:  learn: 0.0943820    total: 329ms    remaining: 155ms\n#&gt; 68:  learn: 0.0941723    total: 332ms    remaining: 149ms\n#&gt; 69:  learn: 0.0939663    total: 338ms    remaining: 145ms\n#&gt; 70:  learn: 0.0937677    total: 342ms    remaining: 140ms\n#&gt; 71:  learn: 0.0935819    total: 346ms    remaining: 134ms\n#&gt; 72:  learn: 0.0933969    total: 349ms    remaining: 129ms\n#&gt; 73:  learn: 0.0932181    total: 353ms    remaining: 124ms\n#&gt; 74:  learn: 0.0930514    total: 358ms    remaining: 119ms\n#&gt; 75:  learn: 0.0928960    total: 361ms    remaining: 114ms\n#&gt; 76:  learn: 0.0927433    total: 364ms    remaining: 109ms\n#&gt; 77:  learn: 0.0925871    total: 369ms    remaining: 104ms\n#&gt; 78:  learn: 0.0924635    total: 374ms    remaining: 99.3ms\n#&gt; 79:  learn: 0.0923467    total: 377ms    remaining: 94.3ms\n#&gt; 80:  learn: 0.0922292    total: 380ms    remaining: 89.2ms\n#&gt; 81:  learn: 0.0921089    total: 386ms    remaining: 84.8ms\n#&gt; 82:  learn: 0.0919860    total: 390ms    remaining: 79.9ms\n#&gt; 83:  learn: 0.0918862    total: 394ms    remaining: 75ms\n#&gt; 84:  learn: 0.0917925    total: 398ms    remaining: 70.2ms\n#&gt; 85:  learn: 0.0916927    total: 404ms    remaining: 65.8ms\n#&gt; 86:  learn: 0.0916089    total: 408ms    remaining: 61ms\n#&gt; 87:  learn: 0.0915482    total: 409ms    remaining: 55.8ms\n#&gt; 88:  learn: 0.0914624    total: 412ms    remaining: 50.9ms\n#&gt; 89:  learn: 0.0913841    total: 416ms    remaining: 46.2ms\n#&gt; 90:  learn: 0.0912981    total: 420ms    remaining: 41.6ms\n#&gt; 91:  learn: 0.0912323    total: 424ms    remaining: 36.9ms\n#&gt; 92:  learn: 0.0911620    total: 427ms    remaining: 32.2ms\n#&gt; 93:  learn: 0.0910968    total: 431ms    remaining: 27.5ms\n#&gt; 94:  learn: 0.0910390    total: 437ms    remaining: 23ms\n#&gt; 95:  learn: 0.0909742    total: 441ms    remaining: 18.4ms\n#&gt; 96:  learn: 0.0909243    total: 444ms    remaining: 13.7ms\n#&gt; 97:  learn: 0.0908512    total: 449ms    remaining: 9.17ms\n#&gt; 98:  learn: 0.0907883    total: 454ms    remaining: 4.58ms\n#&gt; 99:  learn: 0.0907341    total: 457ms    remaining: 0us\n\n\n\n\nCode\npredicciones &lt;- catboost.predict(model, pool = test_pool)\n\n\n\n\nCode\ncolnames(predicciones) &lt;- paste0(\"quantile_\", 1:9) \n\nhead(predicciones)\n#&gt;      quantile_1 quantile_2 quantile_3 quantile_4 quantile_5 quantile_6\n#&gt; [1,]   3.401601   3.542894   3.619259   3.716297   3.794767   3.868639\n#&gt; [2,]   4.150397   4.269908   4.355643   4.431774   4.502443   4.586961\n#&gt; [3,]   4.350918   4.476999   4.595515   4.651673   4.713752   4.815297\n#&gt; [4,]   2.829240   2.936415   3.034001   3.123039   3.208654   3.292293\n#&gt; [5,]   3.456021   3.574613   3.658350   3.741498   3.824226   3.897069\n#&gt; [6,]   3.356988   3.505757   3.580486   3.651319   3.720015   3.791630\n#&gt;      quantile_7 quantile_8 quantile_9\n#&gt; [1,]   3.938859   4.041568   4.165809\n#&gt; [2,]   4.701792   4.816983   4.958197\n#&gt; [3,]   4.933372   5.017778   5.147716\n#&gt; [4,]   3.365471   3.446338   3.577815\n#&gt; [5,]   3.966363   4.047775   4.166815\n#&gt; [6,]   3.858433   3.961821   4.084200"
  },
  {
    "objectID": "test_quarto_things.html",
    "href": "test_quarto_things.html",
    "title": "to_test",
    "section": "",
    "text": "Note\n\n\n\ndfdf\n\n\n\ntab1tab2tab3\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nplot(mtcars$mpg, mtcars$cyl)\n\n\n\n\n\n\n\n\n\n\nparece que mola\n\n\na ver\n\n\n\n\n😨\n\n\n\n\n\n\nTip\n\n\n\nmi tip"
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "",
    "text": "El título de la entrada, sobre todo lo de la parte de “estilo compadre” viene de mis tiempos en consultoría, y tiene que ver con la necesidad de dar soluciones subóptimas a problemas acuciantes. Otra de mis frases, de la que puede que se acuerden Boris, Laura y Lourdes fue la de “si me das madera te hago un troncomóvil, no un ferrari”, lo cual es el equivalente a GIGO de toda la vida, pero a mi estilo.\nVamos al lío, últimamente ando estudianddo estadística bayesiana con el excelente material que pone a disposición de todo el mundo, y gratis, Aki Vehtari en este sitio Curso BDA3. Aki Vehtari es uno de los autores junto con Gelman y otros del libro Bayesian Data Analysis.\nEn la página 48 y siguientes tienen un ejemplo de como realizar inferencia bayesiana para el ratio muertes por cáncer usando un modelo básico Poisson-Gamma. Pero lo interesante es que comentan como construir una priori a partir de los datos, y que la forma en que lo hacen en este ejemplo puede considerarse una aproximación a como se construye en los modelos jerárquicos.\nTotal, que dado que en mi pueblo han aumentado, por desgracia, los casos y nos han confinado perimetralmente, voy a hacer el ejercicio de utilizar los datos del área sanitaria granada nordeste y adaptar el ejemplo.\nAviso que mi conocimiento de estadística bayesiana es limitado y muy probablemente puede que haga algo mal. Estoy aprendiendo, jejej."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#datos",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#datos",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "Datos",
    "text": "Datos\nEn primer lugar los datos por municipios vienen aqui. Y bueno, estaría bien que estuvieran un poco mejor organizados, puesto que solo puedes bajarte los últimos datos actualizados, no hay serie histórica por municipio, o al menos yo no la he encontrado.\nDespués de bajarme el excel al final me quedo solo con los datos de los municipios del área sanitaria Granada Nordeste.\n\nlibrary(tidyverse)\ng_nordeste <- read_csv(here::here(\"data/g_nordeste_20210326.csv\"))\n\ng_nordeste\n#> # A tibble: 46 × 3\n#>    lugar_de_residencia poblacion_miles confirmados_pdia_14_dias\n#>    <chr>                         <dbl>                    <dbl>\n#>  1 Alamedilla                    0.569                        0\n#>  2 Albuñán                       0.409                        0\n#>  3 Aldeire                       0.63                         0\n#>  4 Alicún de Ortega              0.471                        0\n#>  5 Alquife                       0.58                         0\n#>  6 Baza                         20.4                         27\n#>  7 Beas de Guadix                0.329                        0\n#>  8 Benalúa                       3.31                        11\n#>  9 Benamaurel                    2.26                         4\n#> 10 Calahorra (La)                0.668                        0\n#> # ℹ 36 more rows\n\nY podríamos contruir las tasas brutas por cada mil habitantes.\n\ng_nordeste <- g_nordeste %>% \n  mutate(tasa_bruta = confirmados_pdia_14_dias / poblacion_miles)\n\ng_nordeste %>% \n  arrange(-tasa_bruta)\n#> # A tibble: 46 × 4\n#>    lugar_de_residencia poblacion_miles confirmados_pdia_14_dias tasa_bruta\n#>    <chr>                         <dbl>                    <dbl>      <dbl>\n#>  1 Cogollos de Guadix            0.642                       17      26.5 \n#>  2 Purullena                     2.31                        29      12.6 \n#>  3 Cortes de Baza                1.84                        22      11.9 \n#>  4 Peza (La)                     1.17                         8       6.86\n#>  5 Dólar                         0.628                        4       6.37\n#>  6 Zújar                         2.54                        15       5.90\n#>  7 Cúllar                        4.09                        23       5.62\n#>  8 Cuevas del Campo              1.74                         6       3.44\n#>  9 Benalúa                       3.31                        11       3.32\n#> 10 Huéneja                       1.17                         3       2.56\n#> # ℹ 36 more rows\n\nBueno, y vemos que mi pueblo, está el tercero con mayor tasa con 12.58 por 1000 habitantes o 1258 por cada 100 mil (si revisan la situación el próximo martes posiblemente cierren la activad esencial)."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#inferencia",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#inferencia",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "Inferencia",
    "text": "Inferencia\nBueno, pues podríamos considerar que los casos en un municipio \\(y_i\\) la verosimilitud sería de la forma\n\\[y_i \\sim Poisson(X_i\\cdot\\theta_i)\\] dónde \\(X_i\\) sería la población en miles y \\(\\theta_i\\) la tasa por cada 1000 habitantes.\nAhora el tema para hacer inferencia bayesiana es especificar la prior, y como comentan en el libro podríamos construir la prior utilizando los datos. El número de casos sigue una “predictive distribution” binomial negativa y con un poco de álgebra llegan a igualar la media y varianza de las tasas brutas y obtener los parámetros de esa binomial negativa. Aquí es cuándo llega lo de “estilo compadre”, en vez de considerar la binomial negativa, yo voy a ajustar una Gamma a las tasas brutas y calculo el \\(\\alpha\\) y \\(\\beta\\) de la gamma por el método de los momentos.\nElegir una Gamma como Prior es en parte porque es distribución conjugada de la Poisson y la posterior se obtiene directamente.\nSin más, sería resolver estas ecuaciones\n\\[E[\\text{tasas brutas}] = \\dfrac{\\alpha_{prior}}{\\beta_{prior}}\\] \\[Var[\\text{tasas brutas}] = \\dfrac{\\alpha_{prior}}{\\beta_{prior}^{2}}\\]\nDespejando\n\nmedia = mean(g_nordeste$tasa_bruta)\ncuasivarianza = var(g_nordeste$tasa_bruta)\n(beta = media/cuasivarianza)\n#> [1] 0.1028402\n(alpha = media* beta)\n#> [1] 0.2312364\n\nPor lo que usaremos como prior una \\(Gamma(0.10, 0.23)\\), que parece un poco débil, seguramente porque las tasas brutas son muy diferentes entre los municipios.\nComparando la densidad de las tasas brutas con la densidad de la priori no parece mala elección\n\n# repito 10 veces cada tasa para tner suficients puntos para ver la densidad  estimada \ndf <- data.frame(raw_thetas = rep(g_nordeste$tasa_bruta,10),   \n                 simulados = rgamma(nrow(g_nordeste)*10, alpha, beta) )\n\ndf %>% \n    ggplot(aes(x=raw_thetas)) + \n    geom_density(size = 1) +\n    geom_density(aes(x=simulados), col = \"darkred\", linetype=2, size = 1)\n\n\n\n\n\n\n\n\nY ya sólo quedaría calcular la posterior para cada municipio. Que sería de esta forma\n\\[P\\left( \\theta_i\\mid data \\right) \\sim \\text{Gamma}(\\alpha_{prior} + y_i, \\beta_{prior} + x_i)\\] Dónde \\(y_i\\) es el número de casos en los últimos 14 días en cada municipio y \\(x_i\\) los expuestos, es decir, la población (en miles) en cada municipio. Con esto ya podemos calcular, y añadimos también los intervalos de credibilidad\n\ng_nordeste <- g_nordeste %>% \n    mutate(\n        posterior_a = alpha + confirmados_pdia_14_dias,\n        posterior_b = beta + poblacion_miles,\n        posterior_mean = posterior_a/posterior_b, \n        lb = qgamma(.025, posterior_a, posterior_b), \n        ub = qgamma(.025, posterior_a, posterior_b, lower.tail = FALSE)\n    ) \n\nY si vemos los datos de mi pueblo y de alguno más.\n\ng_nordeste %>%\n    filter(lugar_de_residencia %in% c(\"Cortes de Baza\",\"Castilléjar\", \"Baza\",\"Castril\", \"Benamaurel\", \"Zújar\")) %>%\n    select(lugar_de_residencia, poblacion_miles,\n           confirmados_pdia_14_dias,\n           tasa_bruta,\n           posterior_mean) %>% \n  arrange(-posterior_mean)\n#> # A tibble: 6 × 5\n#>   lugar_de_residencia poblacion_miles confirmados_pdia_14_dias tasa_bruta\n#>   <chr>                         <dbl>                    <dbl>      <dbl>\n#> 1 Cortes de Baza                 1.84                       22     11.9  \n#> 2 Zújar                          2.54                       15      5.90 \n#> 3 Benamaurel                     2.26                        4      1.77 \n#> 4 Baza                          20.4                        27      1.32 \n#> 5 Castilléjar                    1.32                        1      0.757\n#> 6 Castril                        2.02                        0      0    \n#> # ℹ 1 more variable: posterior_mean <dbl>\n\nPues no varía mucho la posterior con respecto a a la bruta. Puede deberse a dos motivos, uno, que al tener gran variabilidad las tasas brutas en los municipios considerados la información que comparten es poca comparada con la información específica que aporta cada municipio y la verosimilitud se impone a la prior y por otro lado, al no haber hecho full bayesian para estimar la prior , hemos utilizado los datos de los muncipios dos veces, una para obtener los parámetros de la prior y otra para la posterior, lo que puede llevar a sobreajuste. En los modelos jerárquicos bien estimados (y no al estilo compadre), si se estima bien, pero esto es una aproximación para entender un poco la lógica que hay debajo.\nY ya solo falta pintar .\n\ng_nordeste %>% \n    ggplot(aes(x=reorder(lugar_de_residencia, posterior_mean), y = posterior_mean)) +\n    geom_point(color = \"darkred\", size = rel(2)) +\n    geom_errorbar(aes(ymin = lb , ymax = ub)) +\n    coord_flip() +\n    labs(x = \"municipio\", y = \"Tasa x 1000 habitantes\")\n\n\n\n\n\n\n\n\nLos intervalos de credibilidad más pequeños se corresponden con los municipios con mayor población. A la vista de estos datos, se deberían usar este tipo de estimadores (bien hechos) sobre todo para estimar en municipios con una población menor, y no tomar decisiones basadas en una estimación puntual.\nCoda. Utilizando un glmer con family poisson (o con binomial si se modela la tasa directamente) con efecto aleatorio el lugar de residencia se obtienen prácticamente los mismos resultados"
  },
  {
    "objectID": "2019/09/10/index.html",
    "href": "2019/09/10/index.html",
    "title": "Cosas que deben cambiar",
    "section": "",
    "text": "La semana pasada estuve en la bella ciudad de Alcoy en el congreso de Estadística e Investigación Operativa gracias a que nos invitaron a dar una sesión invitada presentando la Comunidad R-hispano.\nComo estoy en el mundo de la empresa mi percepción fue que, salvo en las charlas de investigación operativa, la distancia entre lo que se hace y se enseña en la universidad y lo que se utiliza en la empresa es bestial.\nLo peor de todo es que después de hablar con varios profesores allí presentes me invadió el pesimismo, los planes de estudios están obsoletos, se enseña mucha teoría y apenas hay algo que se parezca a un ejercicio que tenga el más mínimo parecido con lo que luego se van a encontrar esos alumnos en el mundo real.\nEn esta era de palabras grandilocuentes como big data, machine learning, reinforcement learning o real time analytics los informáticos están ganando la batalla a los matemáticos y estadísticos, son ellos los que están adaptando sus planes de estudio, son ellos los que están creando los masters de ciencia de datos, y los pocos profesores de matemáticas o estadística que quieren colaborar con los informáticos en crear grados y másters adecuados se encuentran a menudo con las reticencias cuando no negativa de sus propios departamentos.\nAfortunadamente en otros lugares del planeta hay gente que está poniendo las bases matemáticas formales para todas estas técnicas, entre los que cabe destacar a Friedman, Tibshirani, Hastie o Efron.\nOs dejo las referencias a un par de libros, que todo analista de datos debería tener en su colección.\n\nElements of Statistical Learning\nComputer Age Statistical Inference: Algorithms, Evidence and Data Science\n\nAsí que, aún queda esperanza.."
  },
  {
    "objectID": "2019/09/10/cosas-que-deben-cambiar/index.html",
    "href": "2019/09/10/cosas-que-deben-cambiar/index.html",
    "title": "Cosas que deben cambiar",
    "section": "",
    "text": "La semana pasada estuve en la bella ciudad de Alcoy en el congreso de Estadística e Investigación Operativa gracias a que nos invitaron a dar una sesión invitada presentando la Comunidad R-hispano.\nComo estoy en el mundo de la empresa mi percepción fue que, salvo en las charlas de investigación operativa, la distancia entre lo que se hace y se enseña en la universidad y lo que se utiliza en la empresa es bestial.\nLo peor de todo es que después de hablar con varios profesores allí presentes me invadió el pesimismo, los planes de estudios están obsoletos, se enseña mucha teoría y apenas hay algo que se parezca a un ejercicio que tenga el más mínimo parecido con lo que luego se van a encontrar esos alumnos en el mundo real.\nEn esta era de palabras grandilocuentes como big data, machine learning, reinforcement learning o real time analytics los informáticos están ganando la batalla a los matemáticos y estadísticos, son ellos los que están adaptando sus planes de estudio, son ellos los que están creando los masters de ciencia de datos, y los pocos profesores de matemáticas o estadística que quieren colaborar con los informáticos en crear grados y másters adecuados se encuentran a menudo con las reticencias cuando no negativa de sus propios departamentos.\nAfortunadamente en otros lugares del planeta hay gente que está poniendo las bases matemáticas formales para todas estas técnicas, entre los que cabe destacar a Friedman, Tibshirani, Hastie o Efron.\nOs dejo las referencias a un par de libros, que todo analista de datos debería tener en su colección.\n\nElements of Statistical Learning\nComputer Age Statistical Inference: Algorithms, Evidence and Data Science\n\nAsí que, aún queda esperanza.."
  },
  {
    "objectID": "2019/07/15/codificación-parcial/index.html",
    "href": "2019/07/15/codificación-parcial/index.html",
    "title": "Codificación parcial y python",
    "section": "",
    "text": "O como se conoce en estos tiempos modernos one hot encoding. En realidad se trata simplemente de cómo codificar una variable categórica en un conjunto de números que un algoritmo pueda utilizar.\nYa hablé de esto mismo en el post codificación de variables categóricas I\nBásicamente, la codificación parcial lo que hace es crearse tantas variables indicadoras como niveles tengo en mi variable menos 1.\nEjemplo. Construimos un conjunto de datos simple, con 3 variables\n\nset.seed(155)\n\nx1 <- rnorm(n = 100, mean = 4, sd = 1.5 )\nx2_cat <- factor(rep(c(\"a\",\"b\",\"c\",\"d\"),  25 ))\ny <- numeric(length = 100)\n\n# Construimos artificialmente relación entre x e y\ny <- 2 +  4 * x1 + rnorm(25, 0, 1)\n\n# cambiamos \"el intercept\" según la variable categórica\n\ny[x2_cat == \"a\"] <- y[x2_cat == \"a\"] + 8\ny[x2_cat == \"b\"] <- y[x2_cat == \"b\"] - 5\ny[x2_cat == \"c\"] <- y[x2_cat == \"c\"] + 3\ny[x2_cat == \"d\"] <- y[x2_cat == \"d\"] - 3\n\ndat <- data.frame(y, x1, x2_cat)\nhead(dat)\n#>          y       x1 x2_cat\n#> 1 31.00555 5.200100      a\n#> 2 19.13571 5.061407      b\n#> 3 20.49049 3.888438      c\n#> 4 17.93157 4.978832      d\n#> 5 25.23501 3.989047      a\n#> 6 21.86234 6.272138      b\n\nEn R al definir x2_cat como un factor él ya sabe que para ciertos métodos (por ejemplo una regresión) hay que codificar esa variable y por defecto utiliza la codificación parcial. Con la función contrasts vemos como lo hace.\n\ncontrasts(dat$x2_cat)\n#>   b c d\n#> a 0 0 0\n#> b 1 0 0\n#> c 0 1 0\n#> d 0 0 1\n\nY en las columnas tenemos las 3 variables indicadoras que ha construido. ¿Por qué 3? pues muy fácil, puesto que la “a” se puede codificar con el valor 0 en las tres variables indicadoras. Esto que parece una obviedad evita problemas de colinealidad en los algoritmos de regresión por ejemplo.\n\nfit1 <-  lm(y ~ x1 + x2_cat, data = dat)\nsummary(fit1)\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2_cat, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.55549 -0.67355 -0.00943  0.59814  1.99480 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.12924    0.28393   35.67   <2e-16 ***\n#> x1            3.94536    0.06034   65.39   <2e-16 ***\n#> x2_catb     -12.95740    0.26148  -49.55   <2e-16 ***\n#> x2_catc      -4.97712    0.25845  -19.26   <2e-16 ***\n#> x2_catd     -10.98359    0.25785  -42.60   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9094 on 95 degrees of freedom\n#> Multiple R-squared:  0.9855, Adjusted R-squared:  0.9849 \n#> F-statistic:  1616 on 4 and 95 DF,  p-value: < 2.2e-16\n\n¿Qué hubiera pasado si hubiéramos tratado con 4 variables indicadoras?\n\ndat2 <- dat\ndat2$ind1 <- ifelse(dat$x2_cat == \"a\", 1, 0)\ndat2$ind2 <- ifelse(dat$x2_cat == \"b\", 1, 0)\ndat2$ind3 <- ifelse(dat$x2_cat == \"c\", 1, 0)\ndat2$ind4 <- ifelse(dat$x2_cat == \"d\", 1, 0)\n\n\nhead(dat2[dat2$x2_cat==\"d\", ],3)\n#>           y       x1 x2_cat ind1 ind2 ind3 ind4\n#> 4  17.93157 4.978832      d    0    0    0    1\n#> 8   9.30838 2.736958      d    0    0    0    1\n#> 12 12.31765 2.943479      d    0    0    0    1\n\nSi metemos ahora esas variables en el modelo\n\nfit2 <-  lm(y ~ x1 +  ind2 + ind3 + ind4 + ind1, data = dat2)\nsummary(fit2)\n#> \n#> Call:\n#> lm(formula = y ~ x1 + ind2 + ind3 + ind4 + ind1, data = dat2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.55549 -0.67355 -0.00943  0.59814  1.99480 \n#> \n#> Coefficients: (1 not defined because of singularities)\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.12924    0.28393   35.67   <2e-16 ***\n#> x1            3.94536    0.06034   65.39   <2e-16 ***\n#> ind2        -12.95740    0.26148  -49.55   <2e-16 ***\n#> ind3         -4.97712    0.25845  -19.26   <2e-16 ***\n#> ind4        -10.98359    0.25785  -42.60   <2e-16 ***\n#> ind1               NA         NA      NA       NA    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9094 on 95 degrees of freedom\n#> Multiple R-squared:  0.9855, Adjusted R-squared:  0.9849 \n#> F-statistic:  1616 on 4 and 95 DF,  p-value: < 2.2e-16\n\nY vemos que como hay colinealidad R no estima el coeficiente de una de las variables indicadoras y hasta nos avisa con el mensaje Coefficients: (1 not defined because of singularities)\nPues la verdad es que mola que R sepa como tratar las categóricas si las has definido como factor pero también hace que la gente se olvide de que lo que en realidad hace es la codificación parcial.\nHablando de esto con un colega salió a colación que en python hay que explicitar la codificación y que quizá eso sea bueno porque así se sabe lo que se está haciendo y no hay lugar a dudas. Hasta aquí todo correcto, salvo que leyendo la documentación de pandas get_dummies resulta que por defecto construye tantas variables indicadoras como categorías y sólo tiene como opcional lo de quitar la primera con el parámetro drop_first, total me dije, no pasa nada, veamos como lo hace scikit learn y nada, resulta que por defecto también deja todas OneHotEncoder.\nReflexionando me dije, bueno, pues entonces cuando haga una regresión lineal con sklearn si uso las opciones por defecto de codificar las categóricas pues me debe saltar lo mismo que en R, es decir que hay un coeficiente que no puede estimar, pero resulta que sklearn hace un pelín de trampa y no salta el error, y no salta porque en sklearn la regresión lineal no ajusta una regresión lineal clásica, sino que por defecto y sin que tú lo pidas te hace una regresión regularizada y entonces no salta ese problema.\nPues la verdad , ¿qué puedo decir? no me hace gracia que por defecto no me quite la variable indicadora que sobra ni que haga regresión con regularización sin yo decirle nada.\nEn fin, veamos el ejemplo con python, aprovecho que escribo en un rmarkdown y puedo pasar objetos de R a python entre chunks sin muchos problemas.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndat_py = r.dat\ndat_py.describe()\n#>                 y          x1\n#> count  100.000000  100.000000\n#> mean    18.633855    3.988010\n#> std      7.402175    1.540500\n#> min     -1.163152    0.315987\n#> 25%     14.283915    2.815231\n#> 50%     19.325282    3.885684\n#> 75%     22.438970    5.062777\n#> max     43.075931    8.304381\ndat_py.x2_cat.value_counts()\n#> a    25\n#> b    25\n#> c    25\n#> d    25\n#> Name: x2_cat, dtype: int64\n\nconvertimos a dummies con pandas por ejemplo\n\ndat_py = pd.get_dummies(data=dat_py)\nprint(dat_py.head())\n#>            y        x1  x2_cat_a  x2_cat_b  x2_cat_c  x2_cat_d\n#> 0  31.005546  5.200100         1         0         0         0\n#> 1  19.135715  5.061407         0         1         0         0\n#> 2  20.490494  3.888438         0         0         1         0\n#> 3  17.931571  4.978832         0         0         0         1\n#> 4  25.235006  3.989047         1         0         0         0\n\n\nx_variables = ['x1', 'x2_cat_a', 'x2_cat_b','x2_cat_c','x2_cat_d']\n# Selecciono y convierto a numpy array\nX = dat_py[x_variables].values  \ny = dat_py['y'].values\nX[0:3]\n#> array([[5.20010037, 1.        , 0.        , 0.        , 0.        ],\n#>        [5.06140731, 0.        , 1.        , 0.        , 0.        ],\n#>        [3.88843797, 0.        , 0.        , 1.        , 0.        ]])\ny[0:3]\n#> array([31.00554596, 19.1357146 , 20.49049385])\n\n\n\nlm = LinearRegression()\nfit_python = lm.fit(X,y)\nprint('Intercept: ',fit_python.intercept_)\n#> Intercept:  2.899716060541241\nprint('Coef: ',fit_python.coef_)\n#> Coef:  [ 3.94536095  7.22952708 -5.72787734  2.25241099 -3.75406074]\n\nY vemos que si estima todos los coeficientes cuando no debería haber podido, esto tiene que ver como he dicho antes con que LinearRegression de sklearn no es la regresión lineal al uso sino que mete regularización.\nOtro día veremos la librería statmodels de python cuya salida nos da una información más rica de los modelos y bastante parecida a lo que estamos acostumbrados con R.\nNota: Leyendo la docu de LinearRegression en ningún sitio dice que use regularización así que no alcanzo a entender por qué ha podido estimar todos los coeficientes. A ver si alguno de mis amigos pythonisos me lo aclara."
  },
  {
    "objectID": "2019/07/03/malditas-proporciones-pequeñas-iii/index.html",
    "href": "2019/07/03/malditas-proporciones-pequeñas-iii/index.html",
    "title": "Malditas proporciones pequeñas III",
    "section": "",
    "text": "Volviendo al ejemplo de lo de las proporciones pequeñas, se trataba básicamente de que se tenía una población con una prevalencia de cierto evento del 4 x 1000 más o menos y en post anteriores veíamos cómo calcular tamaños de muestra y tal para ver cómo detectar un incremento de un 15% en esa proporción.\nAhora vamos a suponer que tenemos una población de 1.5 millones, pero que hay 5 grupos diferenciados, con prevalencias del 6, 5, 4, 3 y 2 por mil respectivamente y todos del mismo tamaño. Simulemos dicha población\n\nset.seed(155)\n\ngr1 <- rbinom(n = 3E5, size = 1, prob = 0.006)\ngr2 <- rbinom(n = 3E5, size = 1, prob = 0.005)\ngr3 <- rbinom(n = 3E5, size = 1, prob = 0.004)\ngr4 <- rbinom(n = 3E5, size = 1, prob = 0.003)\ngr5 <- rbinom(n = 3E5, size = 1, prob = 0.002)\n\npop <-  data.frame(grupo = rep(letters[1:5],each= 3E5),\n                   evento = c(gr1,gr2,gr3,gr4,gr5))\n\nVeamos 30 casos al azar.\n\npop[sample(1:nrow(pop), 30),]\n#>         grupo evento\n#> 92075       a      0\n#> 424873      b      0\n#> 1201923     e      0\n#> 1159523     d      0\n#> 830570      c      0\n#> 546477      b      0\n#> 1120381     d      0\n#> 613315      c      0\n#> 485130      b      0\n#> 52029       a      0\n#> 619858      c      0\n#> 590223      b      0\n#> 1034676     d      0\n#> 1153071     d      0\n#> 1266210     e      0\n#> 502866      b      0\n#> 99782       a      0\n#> 1388671     e      0\n#> 26049       a      0\n#> 971047      d      0\n#> 709908      c      0\n#> 376850      b      0\n#> 487569      b      0\n#> 365383      b      0\n#> 376533      b      0\n#> 1094390     d      0\n#> 873846      c      0\n#> 514258      b      0\n#> 1423814     e      0\n#> 730321      c      0\n\nComprobamos la prevalencia en la población total y por grupos\n\nmean(pop$evento)\n#> [1] 0.003958\n\n\nwith(pop,tapply(evento, grupo, mean))\n#>           a           b           c           d           e \n#> 0.005836667 0.004963333 0.004213333 0.002846667 0.001930000\n\nSupongamos ahora que encuentro un grupo por ahí del mismo tamaño (300k) con una prevalencia igual al mejor grupo que tengo, es decir, del 6 x 1000 y que la gente de marketing me cree y me deja que quite 300K con peor prevalencia y que los sustituya con mi grupo. ¿Cuánto sería la mejora de la prevalencia en esa nueva población?\nSimulemos\n\npop_new <- pop\npop_new$evento[pop_new$grupo==\"e\"] <- rbinom(n = 3E5, size = 1, prob = 0.006)\n\nmean(pop_new$evento)\n#> [1] 0.004748667\nmean(pop_new$evento)/mean(pop$evento)\n#> [1] 1.199764\n\nLa mejora sería del 19.9764191, % pero, ¿es porque he tenido suerte?. Si todos los meses encontrara un grupo así de majo, ¿en qué valores de mejora me estaría moviendo?\nSimulemos 100 realizaciones de este ejercicio\n\nres <- replicate(100, {\n  pop$evento[pop$grupo==\"e\"] <- rbinom(n = 3E5, size = 1, prob = 0.006)\n  return(100 * (mean(pop$evento) / 0.004 - 1))\n})\n\nY si dibujamos la función de densidad tenemos\n\nplot(density(res), main = \"% de mejora\", lwd = 2, col = \"darkblue\")\n\n\n\n\n\n\n\n\nVaya, pues parece que aún encontrando ese grupo tan molón y quitando el grupo malo mi mejora se va a quedar en torno al 19%.. Uhmm, ¿qué le digo a mi jefe cuándo me pida una mejora del 30%?"
  },
  {
    "objectID": "2023/05/06/mapeando/index.html",
    "href": "2023/05/06/mapeando/index.html",
    "title": "Mapeando",
    "section": "",
    "text": "Siempre me ha gustado el tema de los Sistemas de información geográfica y derivados. Ya cuando trabajaba en el IESA fui a un curso en Vigo sobre gvSIG y luego aprendí cosas con QGIS , el cual me sigue pareciendo un software más que excelente.\nHoy en día se pueden hacer muchísimas cosas con Python y con R, incluso tienen conectores con librerías de javascript para hacer cosas resultonas y con interactividad, veáse esto por ejemplo. En realidad tanto python como R tiran de las mismas librerías de bajo nivel, a saber, gdal, libproj, libgeos y similares, las cuales, para ser honestos, pueden meterte en un infierno de dependencias en según qué sistemas unix. Eso sí, una vez sales de ahí es una gozada.\nTotal, el caso es que el pasado jueves me llega una duda del gran Leonardo Hansa, que reproduzco a continuación.\nY bueno, como no tenía mucho más que hacer esa tarde y soy un picado de la vida, pues me dije.\nY me puse manos a la obra a investigar a ver cómo se podría hacer eso de encontrar polígonos adyacentes a uno dado usando Rstats. El caso es que llegué a varios hilos dónde se comentaban diferentes formas, una era usando la función poly2nb de la librería spdep y también cómo hacerlo usando la función st_intersects de la librería sf . En este issue incluso comentaban los grandes de estas cosas, Edzer Pebesma , Roger Bivand y nuestro conocico Virgilio Gómez Rubio.\nBueno, vamos al código, que de eso se trata.\nCode\nlibrary(tidyverse)\nlibrary(sf)\n\nmapa &lt;- st_read(here::here(\"data/shapefiles/cod_postales/cp_19_dis.shp\"))\n#&gt; Reading layer `cp_19_dis' from data source \n#&gt;   `/media/hd1/canadasreche@gmail.com/blog_quarto/data/shapefiles/cod_postales/cp_19_dis.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 10809 features and 4 fields (with 1 geometry empty)\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2021660 ymin: 3203377 xmax: 481778.5 ymax: 5433002\n#&gt; Projected CRS: WGS 84 / Pseudo-Mercator\n\nhead(mapa)\n#&gt; Simple feature collection with 6 features and 4 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -1536953 ymin: 3373964 xmax: -41802.13 ymax: 5247186\n#&gt; Projected CRS: WGS 84 / Pseudo-Mercator\n#&gt;      cp cp_num cp_2 cp_2_num                       geometry\n#&gt; 1 35560  35560   35       35 MULTIPOLYGON (((-1518970 33...\n#&gt; 2 27330  27330   27       27 MULTIPOLYGON (((-821864.3 5...\n#&gt; 3 46680  46680   46       46 MULTIPOLYGON (((-51610.46 4...\n#&gt; 4 49706  49706   49       49 MULTIPOLYGON (((-641488.4 5...\n#&gt; 5 21120  21120   21       21 MULTIPOLYGON (((-776955.2 4...\n#&gt; 6 16623  16623   16       16 MULTIPOLYGON (((-256256.7 4...\nPinto códigos postales de la provincia de Madrid por ejemplo, coloreando por área de cada polígono.\nCode\nmapa |&gt;\n    \n    # calculo area usando st_area\n    mutate(area_m2 = st_area(mapa) |&gt; \n               as.numeric()) |&gt;\n    filter(cp_2 == 28) |&gt;\n    ggplot() +\n    geom_sf(aes(fill = area_m2)) +\n    scale_fill_viridis_c()\nPara encontrar los polígonos adyacentes uso st_intersects que permite por ejemplo saber qué puntos están dentro de un polígono y cosas así. Al aplicarlo sobre una geometría de tipo polígono lo que encuentra son los polígonos adyacentes.\nCode\n# mido a ver cuánto tarda. \ntictoc::tic()\nlista_adyacentes &lt;- st_intersects(mapa)\ntictoc::toc()\n#&gt; 2.531 sec elapsed\nAhora si quisiera saber qué polígonos son adyacentes a uno dado, es simplemente seleccionar en lista adyacentes, por ejemplo\nCode\nlista_adyacentes[1]\n#&gt; [[1]]\n#&gt; [1]    1 3684 4621 6345 7017 7996\nPara ver los adyacentes a mi código postal en Madrid.\nCode\n(fila_mi_cp &lt;-  mapa |&gt; \n    rownames_to_column() |&gt; \n    filter(cp == \"28043\") |&gt; \n    pull(rowname))\n#&gt; [1] \"9138\"\n\n\n(mis_vecinos_fila &lt;- lista_adyacentes[as.numeric(fila_mi_cp)])\n#&gt; [[1]]\n#&gt; [1]  493 4831 4998 5127 6941 7662 9138 9427\nPero me devuelve el número de fila, para ver el cp sería\nCode\nmi_cp &lt;-  \"28043\"\n(mis_vecinos &lt;- mapa$cp[mis_vecinos_fila[[1]]])\n#&gt; [1] \"28028\" \"28042\" \"28002\" \"28022\" \"28033\" \"28016\" \"28043\" \"28027\"\n\n(adyacentes &lt;-  setdiff(mis_vecinos, mi_cp))\n#&gt; [1] \"28028\" \"28042\" \"28002\" \"28022\" \"28033\" \"28016\" \"28027\"\nPintamos\nCode\n  mapa |&gt; \n    filter(cp_2 == 28) |&gt;\n        mutate(\n            tipo = case_when(\n                cp == mi_cp ~ \"mi_cp\", \n                cp %in% adyacentes ~ \"mis_copostales_vecinos\", \n                TRUE ~ \"resto de codpostales\"\n            )\n        ) |&gt; \n        ggplot() +\n        geom_sf(aes(fill = tipo)) +\n        scale_fill_viridis_d()\ny listo.. Lo ponemos en unas funcioncitas todo\nCode\nget_adyacentes &lt;- function(mapa = mapa, id_col = \"cp_num\") {\n  \n  # quiza sacar el st_intersects de la función sea mejor\n  nb &lt;-  st_intersects(mapa)\n  \n  get_nb &lt;-  function(x){\n      res &lt;- mapa[[id_col]][x]\n      res\n  }\n  \n  adjacency_list &lt;-  lapply(nb, get_nb)\n  adjacency_list_names &lt;-  mapa[[id_col]][1:length(adjacency_list)]\n  names(adjacency_list) &lt;- adjacency_list_names\n  adjacency_list\n}\n\n\n\n\nget_mis_vecinos &lt;-  function(mi_cp, cps_adyacentes){\n    cp_simbol &lt;-  as.symbol(mi_cp) # a simbolo para poder llamara cps_adyacentes[[`18814`]]\n    mis_vecinos &lt;-  cps_adyacentes[[cp_simbol]]\n}\n\n\n\n# mapa seleccionando solo tu provincia \n\n\nplot_cp_vecinos &lt;-  function(mi_cp, cps_adyacentes, mapa){\n    cp_simbol &lt;-  as.symbol(mi_cp)\n    mis_vecinos &lt;-  cps_adyacentes[[cp_simbol]]\n    mi_prop &lt;-  stringr::str_sub(mi_cp, 1, 2) |&gt; as.numeric()\n    adyacentes &lt;-  setdiff(mis_vecinos, mi_cp)\n    mapa |&gt; \n        filter(cp_2_num == mi_prop) |&gt; \n        mutate(\n            tipo = case_when(\n                cp_num == mi_cp ~ \"mi_cp\", \n                cp_num %in% adyacentes ~ \"mis_copostales_vecinos\", \n                TRUE ~ \"resto de codpostales\"\n            )\n        ) |&gt; \n        ggplot() +\n        geom_sf(aes(fill = tipo)) +\n        scale_fill_viridis_d()\n}\nY ya podemos usarlo\nCode\ncps_adyacentes &lt;-  get_adyacentes(mapa, id_col = \"cp_num\")\n\n(mis_cps_vecinos_pueblo &lt;- get_mis_vecinos(18814, cps_adyacentes))\n#&gt; [1] 18815 18800 18817 18811 18814 18818 18816\nPintamos\nCode\n\nplot_cp_vecinos(18814, cps_adyacentes, mapa) +\n    labs(title = \"CP núcleo principal Cortes de Baza\")\nCode\n# plaza cascorro\n\nplot_cp_vecinos(28005, cps_adyacentes, mapa) +\n    labs(title = \"cp Plaza Cascorro\")\nCode\n# Pongo Carmona por ser el de mayor área \n\n(get_mis_vecinos(41410, cps_adyacentes))\n#&gt;  [1] 41620 41300 41449 41339 41420 41359 41610 41600 41320 41309 41520 41429\n#&gt; [13] 41410 41510 41310 41016 41500 41440\n\nplot_cp_vecinos(41410, cps_adyacentes, mapa) +\n    labs(title= \"Carmona\")"
  },
  {
    "objectID": "2023/05/06/mapeando/index.html#notas",
    "href": "2023/05/06/mapeando/index.html#notas",
    "title": "Mapeando",
    "section": "Notas",
    "text": "Notas\n\nLos códigos postales que uso están desactualizados. La capa de shapefile es un producto que vende Correos a precio no barato. Antes se podían descargar de CartoCiudad, pero ya no. Algún enlace interesante:\n\npost de Raúl Vaquerizo\npost de nosolosig\n\nLibros online sobre GIS en R:\n\nr-spatial\ngeocomputation with R\n\nApplied Spatial Data Analysis with R . Un poco antiguo ya, no sé si Virgilio, Roger y Edzer están trabajando sobre nueva edición o no"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "",
    "text": "La gente de Yandex es gente lista y son los que están detrás de catboost. Ya el pasado mes de Abril conté como hacían la regresión cuantil y obtenían estimación de varios cuantiles a la vez aquí"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#codificación-de-las-categóricas.",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#codificación-de-las-categóricas.",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "Codificación de las categóricas.",
    "text": "Codificación de las categóricas.\nCatboost por defecto usa one-hot-encoding pero si por algo es conocido es por tener otro método de codificación, el cual viene descrito en la docu. Otro sitio dónde viene relativamente bien explicado es en este post\nVamos a ver el detalle, para cuándo hay variables categóricas y la variable a predecir es binaria.\nLa idea en la que se basan tiene que ver con los test de permutación. Lo que hacen son varias iteraciones desordenando los datos y en cada iteración\n\nDesordenan las filas del data frame de forma aleatoria de forma que se crea un nuevo orden\nLa codificación del nivel de la variable categórica se calcula para cada fila como: avg_target=countInClass+priortotalCount+1\\text{avg_target} = \\dfrac{countInClass + prior}{totalCount +1} pero usando sólo los datos previos a esa fila\n\nY luego para cada observación toman como codificación la media de las codificaciones obtenidas en las diferentes permutaciones.\nEn avg_target\\text{avg_target} de la fila i se tiene que\n\ncountInClass: Cuenta las veces que en todos los datos previos a la fila i, se tiene un target = 1 para cuando el nivel de la variable categórica es igual al de la fila i.\nprior: Constante que se define al principio del algoritmo. Puede ser la proporción de 1’s en los datos por ejemplo.\ntotalCount: El número de observaciones con el mismo nivel en la variable categórica que tiene la fila i, en los datos previos.\n\nEn el segundo post podemos ver la siguiente figura.\n\n\n\nFigura\n\n\nEn este caso, si queremos calcular el valor de avg_target\\text{avg_target} para la quinta observación es tan sencillo como\n\nprior : La fijamos a 0.05, por ejemplo\ncountInClass: En los datos previos sólo había un caso en que el target fuera 1 y la marca Toyota, por lo tanto 1.\ntotalCount: En los datos previos hay 2 observaciones con marca Toyota.\n\nAsí que avg_target=countInClass+priortotalCount+1=1+0.052+1=0.35\\text{avg_target} = \\dfrac{countInClass + prior}{totalCount +1} = \\dfrac{1+0.05}{2+1} = 0.35\nLos autores de catboost reconocen que de esta forma si sólo haces una permutación de los datos para los primeros valores no se tiene info suficiente para obtener una buena codificación, así que proponen hacer varias permutaciones y tomar como codificación la media de las codificaciones anteriores."
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/delete.html",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/delete.html",
    "title": "Math",
    "section": "",
    "text": "FMVGAPimplied=100*(Implied_Interest_RateFMV_Interest_Rate−1)\n\\mbox{FMV}_{GAP}^{implied}  = 100* (\\frac{\\mbox{Implied_Interest_Rate}}{\\mbox{FMV_Interest_Rate}} - 1)"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#pensamientos",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#pensamientos",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "Pensamientos",
    "text": "Pensamientos\nPero, pero…\n\n¿No os recuerda un poco a como se hace un aprendizaje bayesiano? . Es decir parto de una priori (puede que poco informativa) y conforme voy obteniendo datos voy actualizando la distribución de mi parámetro, y de esa forma puedo obtener la posterior predictive distribution, que es la que aplicaría por ejemplo a un dato no visto.\nDe hecho al hacer varias permutaciones ¿ no está convergiendo de alguna manera la solución de catboost hacia la aproximación bayesiana?\n¿No os parece un poco de sobreingeniería, para algo que quizá con una aproximación estilo compadre bayesiana se podría obtener algo muy similar y con menos esfuerzo?\n\n\nPruebecilla\nEl ejemplo que viene en la docu, dónde se ejemplifica con un pequeño dataset de 7 filas y muestran una de las permutaciones generadas.\n\n\n\npermutation1\n\n\nQue tras aplicar el algoritmo quedaría para esta permutación queda como \nReplicamos en código\n\n\nCode\nlibrary(tidyverse) # pa 4 tontás de hacer sample y de groups bys que hago luego\nlibrary(parallel) # para usar varios cores con mclapply\nlibrary(patchwork) # pa juntar ggplots\n\n\n\n\nCode\n\nmydf &lt;- tribble(\n    ~id,~f2, ~cat, ~label, \n    1,53,\"rock\",  0,\n    2,55,\"indie\", 0, \n    3,40,\"rock\",  1, \n    4,42,\"rock\",  1,\n    5,34,\"pop\",   1,\n    6,48,\"indie\", 1,\n    7,45, \"rock\",  0\n)\n\nmydf\n#&gt; # A tibble: 7 × 4\n#&gt;      id    f2 cat   label\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     1    53 rock      0\n#&gt; 2     2    55 indie     0\n#&gt; 3     3    40 rock      1\n#&gt; 4     4    42 rock      1\n#&gt; 5     5    34 pop       1\n#&gt; 6     6    48 indie     1\n#&gt; 7     7    45 rock      0\n\n\nFuncioncita para obtener la codificación a lo catboost\n\n\nCode\navg_target &lt;- function(prev_df, nivel, prior = 0.05){\n    countInClass &lt;- sum(prev_df[['label']][prev_df[['cat']]== nivel])\n    totalCount &lt;- sum(prev_df[['cat']]==nivel)\n    res &lt;-  (countInClass + prior) /(totalCount + 1)\n    return(res)\n}\n\n\nA la primer fila se le asigna siempre la prior\n\n\nCode\n# No estaba fino para ver como podría hacerlo sin iterar sobre todas las filas. \n\nfoo1 &lt;-  function(df, prior = 0.05) {\n    \n    df$cat_code[1] &lt;-  prior\n    \n    for (fila in 2:nrow(df)) {\n        prev_df &lt;- df[1:(fila - 1),]\n        df$cat_code[fila] &lt;-\n            avg_target(prev_df = prev_df, nivel = df$cat[fila], prior = prior)\n    }\n    return(df)\n}\n\n\n\n\nCode\nres1 &lt;- foo1(mydf, prior = 0.05)\nres1\n#&gt; # A tibble: 7 × 5\n#&gt;      id    f2 cat   label cat_code\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      0    0.05 \n#&gt; 2     2    55 indie     0    0.05 \n#&gt; 3     3    40 rock      1    0.025\n#&gt; 4     4    42 rock      1    0.35 \n#&gt; 5     5    34 pop       1    0.05 \n#&gt; 6     6    48 indie     1    0.025\n#&gt; 7     7    45 rock      0    0.512\n\n\nAhora lo repetimos varias veces. Dónde en cada iteración hacemos una permutación de las filas\n\n\nCode\nfoo2 &lt;-  function(df, prior = 0.05) {\n    require(tidyverse)\n    mynew_df &lt;-  df |&gt; slice_sample(prop = 1, replace = FALSE)\n    mynew_df$cat_code[1] &lt;-  prior\n    \n    for (fila in 2:nrow(mynew_df)) {\n        prev_df &lt;- mynew_df[1:(fila - 1),]\n        mynew_df$cat_code[fila] &lt;-\n            avg_target(prev_df = prev_df, nivel = mynew_df$cat[fila], prior = prior)\n    }\n    return(mynew_df)\n}\n\n\n\n\nCode\n\niteraciones &lt;-  1000\n\nres2 &lt;-  bind_rows(mclapply(1:iteraciones, FUN = function(x) foo2(mydf,prior = 0.05), mc.cores = 10))\n\ndim(res2)\n#&gt; [1] 7000    5\n\n(res2 &lt;- res2 |&gt;\n    group_by(id) |&gt;\n    mutate(cat_code = mean(cat_code)) |&gt;\n    distinct() |&gt; \n    arrange(id) )\n#&gt; # A tibble: 7 × 5\n#&gt; # Groups:   id [7]\n#&gt;      id    f2 cat   label cat_code\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      0   0.340 \n#&gt; 2     2    55 indie     0   0.284 \n#&gt; 3     3    40 rock      1   0.175 \n#&gt; 4     4    42 rock      1   0.194 \n#&gt; 5     5    34 pop       1   0.05  \n#&gt; 6     6    48 indie     1   0.0373\n#&gt; 7     7    45 rock      0   0.342\n\n\n\n\nAproximación compadre bayesiana\n¿Y si tomamos como priori una ℬ(2,2)\\mathcal{B}(2,2) y para cada categoría {rock, indie, pop} tomamos como su distribución a posteriori\nℬ(2+exitos en datos,2+fracasos en los datos)\\mathcal{B}(2 + \\text{exitos en datos},2 + \\text{fracasos en los datos}) y para obtener un valor de la codificación para cada observación simplemente extraemos un valor aleatorio de esa distribución a posteriori?\n\n\nCode\n\nres3 &lt;-  mydf |&gt; \n    group_by(cat) |&gt; \n    mutate(\n        n = n(), \n        exitos = sum(label)\n    )  |&gt; \n    ungroup() |&gt; \n    mutate(\n     cat_code = map2_dbl(exitos, n, function(exitos, n) rbeta(1, exitos + 2 , n - exitos + 2))\n    ) \n\nres3\n#&gt; # A tibble: 7 × 7\n#&gt;      id    f2 cat   label     n exitos cat_code\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      0     4      2    0.407\n#&gt; 2     2    55 indie     0     2      1    0.467\n#&gt; 3     3    40 rock      1     4      2    0.530\n#&gt; 4     4    42 rock      1     4      2    0.532\n#&gt; 5     5    34 pop       1     1      1    0.287\n#&gt; 6     6    48 indie     1     2      1    0.290\n#&gt; 7     7    45 rock      0     4      2    0.801\n\n\nPues tiene pinta de que esta aproximación podría ser tan válida como la que describen los de catboost y en principio es más sencilla.\n\n\nMás datos\nCreemos un dataset artificial partiendo de estos mismos datos.\n\n\nCode\nn= 200\nmydf_big= mydf[rep(seq_len(nrow(mydf)), n), ]\nmydf_big$id &lt;- 1:nrow(mydf_big)\n\nnrow(mydf_big)\n#&gt; [1] 1400\n\nhead(mydf_big, 10)\n#&gt; # A tibble: 10 × 4\n#&gt;       id    f2 cat   label\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1     1    53 rock      0\n#&gt;  2     2    55 indie     0\n#&gt;  3     3    40 rock      1\n#&gt;  4     4    42 rock      1\n#&gt;  5     5    34 pop       1\n#&gt;  6     6    48 indie     1\n#&gt;  7     7    45 rock      0\n#&gt;  8     8    53 rock      0\n#&gt;  9     9    55 indie     0\n#&gt; 10    10    40 rock      1\n\n\n\n\nCode\n## Cambiamos de forma aleatoria el valor de label para un % de las observaciones, para que no sea 200 veces exactamente el original\n\ntable(mydf_big$label)\n#&gt; \n#&gt;   0   1 \n#&gt; 600 800\n\nmydf_big$label &lt;- rbinom(n = nrow(mydf_big), size =1,  prob = ifelse(mydf_big$label==0, 0.3, 0.9))\ntable(mydf_big$label)\n#&gt; \n#&gt;   0   1 \n#&gt; 508 892\n\n# vemos que hemos cambiado algunos valores en label\nhead(mydf_big, 10)\n#&gt; # A tibble: 10 × 4\n#&gt;       id    f2 cat   label\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;\n#&gt;  1     1    53 rock      1\n#&gt;  2     2    55 indie     1\n#&gt;  3     3    40 rock      1\n#&gt;  4     4    42 rock      1\n#&gt;  5     5    34 pop       1\n#&gt;  6     6    48 indie     1\n#&gt;  7     7    45 rock      0\n#&gt;  8     8    53 rock      0\n#&gt;  9     9    55 indie     0\n#&gt; 10    10    40 rock      1\n\n\n\n\nComparamos\nPara elegir las priori poco informativa para ambos métodos vemos una muestra de los datos de tamaño 10\n\n\nCode\n(muestra &lt;- mydf_big |&gt; \n    slice_sample(n = 10) |&gt; \n    group_by(label) |&gt;\n    count())\n#&gt; # A tibble: 2 × 2\n#&gt; # Groups:   label [2]\n#&gt;   label     n\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     0     7\n#&gt; 2     1     3\n\n\n(prior_shape1 &lt;-  muestra$n[muestra$label==1])\n#&gt; [1] 3\n(prior_shape2 &lt;-  muestra$n[muestra$label==0])\n#&gt; [1] 7\n\n\n(prior_catboost &lt;- prior_shape1 /(prior_shape1 + prior_shape2))\n#&gt; [1] 0.3\n\n\n\nCatboost codificación\n\n\nCode\niteraciones = 50\ntictoc::tic(\"catbost_code\")\n\ncod_catboost &lt;- bind_rows(mclapply(1:iteraciones, FUN = function(x) foo2(mydf_big, prior = prior_catboost), mc.cores = 10))\n\n\ntictoc::toc(log=TRUE)\n#&gt; catbost_code: 3.293 sec elapsed\n\ndim(cod_catboost)\n#&gt; [1] 70000     5\n\n\n\n\nCode\ncod_catboost &lt;- cod_catboost |&gt;\n    group_by(id) |&gt;\n    mutate(cat_code = mean(cat_code)) |&gt;\n    distinct() |&gt; \n    arrange(id)\n\n\ndim(cod_catboost)\n#&gt; [1] 1400    5\n\nhead(cod_catboost)\n#&gt; # A tibble: 6 × 5\n#&gt; # Groups:   id [6]\n#&gt;      id    f2 cat   label cat_code\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      1    0.587\n#&gt; 2     2    55 indie     1    0.567\n#&gt; 3     3    40 rock      1    0.577\n#&gt; 4     4    42 rock      1    0.586\n#&gt; 5     5    34 pop       1    0.910\n#&gt; 6     6    48 indie     1    0.582\n\n\n\n\nEstilo compadre codificación\n\n\nCode\ntictoc::tic(\"estilo compadre\")\nestilo_compadre &lt;-  mydf_big |&gt; \n    group_by(cat) |&gt; \n    mutate(\n        n = n(), \n        exitos = sum(label)\n    )  |&gt; \n    ungroup() |&gt; \n    mutate(\n     cat_code = map2_dbl(exitos, n, function(exitos, n) rbeta(1, exitos + prior_shape1 , n - exitos + prior_shape2))\n    ) \n\ntictoc::toc(log = TRUE)\n#&gt; estilo compadre: 0.023 sec elapsed\n\nhead(estilo_compadre)\n#&gt; # A tibble: 6 × 7\n#&gt;      id    f2 cat   label     n exitos cat_code\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1     1    53 rock      1   800    474    0.604\n#&gt; 2     2    55 indie     1   400    233    0.539\n#&gt; 3     3    40 rock      1   800    474    0.572\n#&gt; 4     4    42 rock      1   800    474    0.563\n#&gt; 5     5    34 pop       1   200    185    0.854\n#&gt; 6     6    48 indie     1   400    233    0.558\n\n\n¿cómo de parecidas son las dos codificaciones?\nPor el momento parece que bastante\n\n\nCode\ncor(cod_catboost$cat_code, estilo_compadre$cat_code)\n#&gt; [1] 0.9819561\n\n\nParece que la codificación estilo compadre es un poco más dispersa, lo cual no tiene por qué ser necesariamente malo.\n\n\nCode\n\ncod_catboost |&gt; \n    group_by(cat) |&gt; \n    summarise(media = mean(cat_code), \n              low = quantile(cat_code, 0.05), \n              high = quantile(cat_code, 0.95), \n              sd_value = sd(cat_code))\n#&gt; # A tibble: 3 × 5\n#&gt;   cat   media   low  high sd_value\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 indie 0.577 0.567 0.587  0.00635\n#&gt; 2 pop   0.906 0.888 0.916  0.00871\n#&gt; 3 rock  0.585 0.575 0.592  0.00520\n\n\nestilo_compadre |&gt; \n    group_by(cat) |&gt; \n    summarise(media = mean(cat_code), \n              low = quantile(cat_code, 0.05), \n              high = quantile(cat_code, 0.95), \n              sd_value = sd(cat_code))\n#&gt; # A tibble: 3 × 5\n#&gt;   cat   media   low  high sd_value\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 indie 0.578 0.541 0.616   0.0240\n#&gt; 2 pop   0.896 0.857 0.933   0.0209\n#&gt; 3 rock  0.589 0.561 0.617   0.0173\n\n\n\n\nCode\np1 &lt;- cod_catboost |&gt; \n    ggplot(aes(x=cat_code, fill=cat))  +\n    geom_density(alpha = 0.4) +\n    labs(title = \"Gráfico densidad de las codificanes\", \n         subtitle = \"catboost\")\n\np2 &lt;- estilo_compadre |&gt; \n    ggplot(aes(x=cat_code, fill=cat))  +\n    geom_density(alpha = 0.4) +\n        labs(title = \"Gráfico densidad de las codificanes\", \n         subtitle = \"Estilo bayesian compadre\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nY si hacemos un modelito tonto usando estas codificaciones. Si, ya sé que son datos fakes y que no tiene mucho sentido y tal, y que lo suyo sería con unos datos reales (mandadme algunos !! )\n\n\nCode\n# set.seed(47)\n\nid_train &lt;-  sample(1:nrow(mydf_big), size = 700)\n\n\ntrain_predict_simple &lt;- function(df){\n    train &lt;-  df[id_train, ]\n    test &lt;- df[-id_train, ]\n    \n    fit_base &lt;- glm(label ~ f2+ cat, data = train, family = binomial)\n    fit &lt;-  glm(label ~  f2 +  cat_code, data = train, family = binomial)\n    \n    auc_base &lt;- pROC::auc(test$label, predict(fit_base, test, type = \"response\"))\n    auc &lt;- pROC::auc(test$label, predict(fit, test, type = \"response\"))\n    \n    return(list(auc_base = auc_base, auc = auc))\n    \n}\n\n\nmclapply(list(cod_catboost, estilo_compadre), train_predict_simple, mc.cores = 2)\n#&gt; [[1]]\n#&gt; [[1]]$auc_base\n#&gt; Area under the curve: 0.8231\n#&gt; \n#&gt; [[1]]$auc\n#&gt; Area under the curve: 0.7693\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; [[2]]$auc_base\n#&gt; Area under the curve: 0.8231\n#&gt; \n#&gt; [[2]]$auc\n#&gt; Area under the curve: 0.7658"
  },
  {
    "objectID": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#más-pensamientos",
    "href": "2023/06/09/categoricas_a_lo_catboost_pensamientos/index.html#más-pensamientos",
    "title": "Categóricas a lo catboost. Pensamientos",
    "section": "Más pensamientos",
    "text": "Más pensamientos\n\nEl ejemplo que he hecho no es del todo válido puesto que tanto para la codificación con catboost como la de estilo compadre han intervenido todos los datos.\nLa variable categórica que codifico sólo tiene 3 niveles, de hecho no haría falta hacer este tipo de codificación. Tengo pendiente probar con algo como códigos postales o similar.\nLa forma en que catboost hace esta codificación me parece que está en mitad entre la aproximación bayesiana y hacer un target_encoding al uso. De hecho si hay un nivel con muy pocos valores el valor de la codificación de catboost para ese nivel va a parecerse más a la prior que elijas que a la proporción de éxitos en esa categoría, lo cual es muy parecido a la estimación bayesiana compadre.\nSe podrían utilizar codificaciones basadas en modelos mixtos o algún tipo de combinación convexa entre la información particular que tiene una categoría y la general aportada por el conjunto de los datos."
  },
  {
    "objectID": "2019/07/02/el-randomforest-no-nos-deja-ver-el-árbol/index.html",
    "href": "2019/07/02/el-randomforest-no-nos-deja-ver-el-árbol/index.html",
    "title": "El randomforest no nos deja ver el árbol",
    "section": "",
    "text": "En primer lugar, el título de este post se lo debo a Mario Passani y no va sobre estadística.\nEn realidad va sobre que últimamente me da la sensación de que la profesión del científico de datos se está centrado más en las formas que en el fondo. Os cuento un par de ejemplos.\nHace un tiempo en una empresa en la que trabajé se estaban implementando temas de análisis exploratorios usando pyspark de forma que se generaban unos bonitos htmls con info sobre las variables de un sparkdataframe. Hasta aquí todo bien, el git del proyecto estaba muy bonito, había gente que programaba de forma más que aceptable, pero oye, de repente cuando me pongo a utilizar la librería y pido que me saque un boxplot resulta que me salta a la vista la ausencia de los puntitos típicos de los outliers (y sabía que había varios), total, que cuando me pongo a bichear el código resulta que como bigotes del boxplot me habían puesto el mínimo y el máximo. Hay que decir que el gráfico era muy bonito y todo eso, pero era inútil, no habían implementado un boxplot, se le parecía pero no era.\nOtro caso me lo contó un compañero de una telco. Resulta que un compañero suyo había implementado toda una etl que juntaba varias tablas para al final obtener datos sobre la fuga de clientes, mi compañero al ver el resultado le dijo que algo estaba mal porque salían unas tasas de “churn” bastante mayores que las de meses anteriores. El hacedor de la etl le dijo que se peinara y que eso estaba bien porque, ojo ¡¡había pasado los test unitarios!! y por lo que se ve acabó la cosa un pelín disputada.\nA lo que me refiero, noto que últimamente nos estamos centrado más en el cómo implementamos las cosas que en saber lo que estamos haciendo, y no quiero decir con esto que las formas no sean importantes, sino simplemente que tengamos claro qué narices estamos haciendo."
  },
  {
    "objectID": "2019/06/25/malditas-proporciones-pequeñas-ii/index.html",
    "href": "2019/06/25/malditas-proporciones-pequeñas-ii/index.html",
    "title": "Malditas proporciones pequeñas II",
    "section": "",
    "text": "¿Cuál sería el tamaño muestral mínimo para estimar un incremento del 15% en una proporción de digamos 0.004?\nEn realidad me gustaría hacer como cuenta mi amigo Carlos en este post, pero no puedo ir examinando unidades y actualizar los intervalos de credibilidad hasta que la anchura me convenza, porque ni siquiera conozco al tío de la furgoneta que ha de ir “examinando” cada unidad experimental, amén de que para conseguir 4 tiene que examinar cerca de 1000. Así que veamos como se ha hecho toda la vida.\nNos interesa es minimizar los errore tipo I y tipo II. Recordemos lo que eran.\n\nError tipo I : Error de falso positivo, decir que hay diferencias cuando en realidad no las hay (H0 es cierta pero digo que no)\nError tipo II: Error falso negativo, error que cometo al decir que no hay diferencias cuando en realidad si las hay. (H0 es falsa pero decimos que es verdadera) 8 \n\nSi utilizamos el paquete de R pwr podemos calcularlo para un error tipo I, (el alpha de siempre) de 0.05 y una potencia (1 - error tipo II) de 0.9\n\np1 <- 0.004\np2 <- p1 * 1.15\nlibrary(pwr)\n\npotencia_0.9 <- pwr.p.test(ES.h(p1=p1, p2= p2),\n                           sig.level= 0.05, power = 0.9) \nplot(potencia_0.9)\n\n\n\n\n\n\n\n\nY nos sale que el tamaño de muestra mínimo está en torno a 125 mil .\nHay otras librerías para calcular dicho tamaño muestral, por ejemplo SampleSizeProportions que según pone en la documentación lo hace teniendo en cuenta el intervalo de credibilidad deseado. Tengo que mirar mejor esta librería.\n\nlibrary(SampleSizeProportions)\n\nlen <-  (p1 * 1.15 - p1/1.15)\nc1 <- 40\nc2 <- 46\nd1 <- 9960\nd2 <- 9954\n\npropdiff.modwoc(len = len, c1 = c1, d1 = d1, c2 = c2, d2 = d2)\n#> [1] 120910 120910\n\nY sale unos 120 mil para cada grupo.\nO también podemos ver cosas como está que comentan los amigos de WinVector, aquí o aquí\n\nestimateT <- function(lowProb,difference,errorProb) {\n  -log(errorProb/2)*lowProb/(difference^2)\n}\n\n# detectar diferencias de 15%\nlowProb <- 0.004\nincremento <- 0.15\nestimateT(lowProb, lowProb*incremento,  0.0006 )\n#> [1] 90130.31\n\nY nos sale un tamaño aproximado de 90 mil. Sea como fuere el tener que determinar tamaños de muestra para poder medir variaciones de un 15% en proporciones pequeñas implica tener muestras de tamaño 100000, así que como dice el título, ¡malditas proporciones pequeñas!"
  },
  {
    "objectID": "2019/06/24/malditas-proporciones-pequeñas-i/index.html",
    "href": "2019/06/24/malditas-proporciones-pequeñas-i/index.html",
    "title": "Malditas proporciones pequeñas I",
    "section": "",
    "text": "Cuando uno está en esto de ganarse la vida mediante la ciencia de datos, se da cuenta de que la vida no es tan maravillosa como lo cuentan los libros de texto ni los cursos de los másters y ni siquiera los concursos de kaggle.\nRecientemente en un proyecto nos piden detectar un efecto de un incremento del 15% en una proporción entre dos grupos, digamos en forma canónica, grupo de control y tratamiento. Hasta aquí todo normal y uno podría hacer calcular intervalos de confianza (o de credibilidad si nos vamos al mundo bayesiano) de manera más o menos fácil. Veamos como sería utilizando simulación.\nSupongamos una p1 = 0.5 y una p2 = p1 * 1.16 ( un pelín superior al 15%) Supongamos también que nuestros grupos tienen tamaño n1 = n2 = 100000. Así que vamos a calcular mediante simulación la probabilidad que p2/p1 >= 1.15. ¿Fácil, verdad?\np1 y p2 los simulamos suponiendo una distribución beta. Supongamos que p2 es un 16% mayor que p1\n\nset.seed(-1) # prueba a poner una semilla negativa en python a ver que pasa\np1 <- 0.5\np2 <- p1 * 1.16\nn <- 1E5\nsim <- 1E6\n\nsim_beta1 <- rbeta(sim, p1 * n, n - p1 * n)\nsim_beta2 <- rbeta(sim, p2*n, n - p2 * n)\n\nmedian(sim_beta1)\n#> [1] 0.5000039\nmedian(sim_beta2)\n#> [1] 0.5800018\n\nSi dibujamos sus funciones de densidad con R base sin muchas zirigoncias.\n\nplot(density(sim_beta1), main = \"prop\", col = \"darkblue\", lty = 2, xlim = c(0.45, 0.6))\n\nlines(density(sim_beta2), col = \"darkred\")\n\n\n\n\n\n\n\n\nY claramente si que parece que están separadas. Podemos estimar la probabilidad de que p2 sea un 15% mayor como la proporción de veces que p2/p1 >= 1.15 en el millón de simulaciones\n\nmean(sim_beta2/sim_beta1 >= 1.15)\n#> [1] 0.98152\n\nY podríamos estar bastante seguros de que p2 es al menos un 15% mayor que p1.\nPero ¿qué pasa si las proporciones son pequeñas, digamos de un 4 por mil?\n\nset.seed(-1)\np1 <- 0.004\np2 <- p1 * 1.16\nn <- 1E5\nsim <- 1E6\n\nsim_beta1 <- rbeta(sim, p1 * n, n - p1 * n)\nsim_beta2 <- rbeta(sim, p2*n, n - p2 * n)\n\nmedian(sim_beta1)\n#> [1] 0.003997118\nmedian(sim_beta2)\n#> [1] 0.004636583\n\n\nplot(density(sim_beta1), main = \"prop\", col = \"darkblue\", lty=2, xlim = c(0.003, 0.006)  )\n\nlines(density(sim_beta2), col = \"darkred\")\n\n\n\n\n\n\n\n\n\nmean(sim_beta2/sim_beta1 >= 1.15)\n#> [1] 0.550476\n\nPues ya no podemos estar tan seguros de que el incremento haya sido de alrededor un 15%.\nEn próximas entradas veremos como calcular de forma clásica el tamaño muestral necesario para detectar ese efecto."
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html",
    "href": "2023/01/21/regresion-poisson-brms/index.html",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "",
    "text": "Me llamó la atención ayer el excelente post de Carlos sobre regresión de poisson casi trivival con numpyro y le dije que iba a ver si podía replicarlo usando brms u otra cosa que se comunique con stan. También estuve bicheando su código que tiene colgado aquí\nLo primero, es que se trata de un ejercicio relativamente común, que es la de estimar el punto en que una serie de datos cambia de tendencia. Existen cosas como prophet (que por debajo es Stan) o librerías como mcp cmp o changepoint en R específicas para este tipo de cosas. Pero a Carlos le gusta , con buen criterio, especificar directamente el modelo.\nHe de reconocer que me ha gustado la sintaxis de numpyro\nBueno vamos al lío."
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#datos-y-librerías",
    "href": "2023/01/21/regresion-poisson-brms/index.html#datos-y-librerías",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Datos y librerías",
    "text": "Datos y librerías\nVoy implementar los dos primeros modelos que hay en el notebook de Carlos usando la función ulam de la librería rethinking de Richard McElreath. Dicha librería es un interfaz, incompleto aunque didáctico, de hacer modelos bayesianos con Stan.\nEl último modelo, el que detecta el punto de cambio no he sido capaz de hacerlo con esta librería, pero se podría hacer usando stan directamente, como aquí.\nLos datos, aunque él ha puesto la variable del tiempo de 1 a 23, yo la voy a poner de 1999 a 2021, porque intuía a qué datos se refería y que los he buscado.\n\nlibrary(rethinking) \nlibrary(cmdstanr)\nlibrary(brms)\n\n# por si uso splines\nlibrary(mgcv)\n\n\n# uso cmdstan como backend para stan desde R en vez de rstan\nset_cmdstan_path(\"/home/jose/cmdstan\")\nset_ulam_cmdstan(TRUE)\n\n\n\nd  <- list(\n      y = c(54, 63, 50, 54, 71, 72, 57, 69, 71, \n    76, 57, 73, 62, 51, 54, 55, 60, 49, \n    50, 53, 56, 49, 48),\n\n    t = 1999:2021\n\n)"
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#modelo-lambda-constante",
    "href": "2023/01/21/regresion-poisson-brms/index.html#modelo-lambda-constante",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Modelo lambda constante",
    "text": "Modelo lambda constante\n\nm0 <- ulam(\n    alist(y ~ poisson(lambda),\n          lambda <- a    ,\n          a ~ normal(60, 5)\n          ) ,\n    data = d,\n    chains = 2,\n    cores = 2 ,\n    sample = TRUE,\n    iter = 3000\n    \n)\n#> Running MCMC with 2 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 1 finished in 0.0 seconds.\n#> Chain 2 finished in 0.0 seconds.\n#> \n#> Both chains finished successfully.\n#> Mean chain execution time: 0.0 seconds.\n#> Total execution time: 0.4 seconds.\n\n\nprecis(m0)\n#>            result\n#> mean    58.976852\n#> sd       1.542638\n#> 5.5%    56.450528\n#> 94.5%   61.456426\n#> n_eff 1004.327368\n#> Rhat     1.000203"
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#modelo-dónde-lambda-es-una-recta",
    "href": "2023/01/21/regresion-poisson-brms/index.html#modelo-dónde-lambda-es-una-recta",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Modelo dónde lambda es una recta",
    "text": "Modelo dónde lambda es una recta\n\nm1 <- ulam(\n    alist(\n        y ~ poisson(lambda),\n        lambda <- a + b * t   ,\n        a ~ normal(60, 5),\n        b ~ normal(0, 1)\n        \n    ) ,\n    data = d,\n    chains = 2,\n    cores = 2 ,\n    sample = TRUE,\n    iter = 3000\n    \n)\n#> Running MCMC with 2 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \n#> Chain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \n#> Chain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \n#> Chain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \n#> Chain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \n#> Chain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \n#> Chain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \n#> Chain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \n#> Chain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \n#> Chain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \n#> Chain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \n#> Chain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \n#> Chain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \n#> Chain 1 finished in 0.2 seconds.\n#> Chain 2 finished in 0.2 seconds.\n#> \n#> Both chains finished successfully.\n#> Mean chain execution time: 0.2 seconds.\n#> Total execution time: 0.3 seconds.\n\nprecis(m1)\n#>            mean          sd         5.5%        94.5%    n_eff    Rhat4\n#> a 60.2808693333 5.071538760 52.390828500 68.641990000 454.4379 1.005453\n#> b -0.0007081383 0.002636486 -0.005088067  0.003417291 452.2575 1.005254"
  },
  {
    "objectID": "2023/01/21/regresion-poisson-brms/index.html#modelo-para-detectar-el-punto-de-cambio.",
    "href": "2023/01/21/regresion-poisson-brms/index.html#modelo-para-detectar-el-punto-de-cambio.",
    "title": "Una regresión de poisson, plagiando a Carlos",
    "section": "Modelo para detectar el punto de cambio.",
    "text": "Modelo para detectar el punto de cambio.\nLa verdad que me habría gustado seguir usando ulam para el modelo pero he sido incapaz de replicar este cacho de código de numpyro\ndef model02(t, datos):\n\n  knot = numpyro.sample(\"knot\", dist.Normal(len(t)/2, len(t)/4))\n\n  a0 = numpyro.sample(\"a0\", dist.Normal(60, 5))\n  b0 = numpyro.sample(\"b0\", dist.Normal( 0, 1))\n\n  b1 = numpyro.sample(\"b1\", dist.Normal(0, 1))  \n\n  λ = a0 + t * b0 + jnp.where(t > knot, (t - knot) * b1, 0)\n\n  with numpyro.plate(\"data\", len(t)):\n    numpyro.sample(\"obs\", dist.Poisson(λ), obs=datos)\n\nEl problema reside en jnp.where(t > knot, (t - knot) * b1, 0) , para resolverlo habría que programar directamente en stan o que ulam pudiera usar la función step de Stan, que si x es menor que 0 vale 0 y 1 en otro caso. El código en ulam si funcionara sería así\n\nm2 <- ulam(\n        alist(\n            y ~ poisson(lambda),\n            lambda <- b0 +  b1 * t + b2 * (t - knot) * step(t - knot) ,\n            knot ~ normal(23/2, 23/4),\n            b0 ~ normal(60, 5),\n            b1 ~ normal(0, 1),\n            b2 ~ normal(0, 1),\n\n        ) ,\n        data=d, chains=2, cores=1 , sample=TRUE, \n        iter = 3000)\n\npero no funciona , así que vamos a ver como sería usando brms. brms tiene una sintaxis digamos que peculiar, y su objetivo es parecerse a la especificación que se hace en R de los modelos lineales con lm y la que se hace con lme4. Es decir sería algo similar a esto\n\n\n brm( y ~ x1 + x2 + (1 | var_efecto_aleatorio) , \n                 family = poisson(\"identity\"), \n                 data = df)\n\ndónde no se especifican los \\(\\beta_i\\) de forma explícita. Pero también tiene sintaxis para hacerlo explícito. Veamos.\n\n# Datos en dataframe\ndf  <- data.frame(y = d$y, t = d$t)\n\n\nbform <- bf(\n  y ~ b0 + b1 * t + \n  \n  # brms si acepta la función step de Stan \n  # cuando t-change >= 0  entonces step(t-change) = 1, es decir, cuanto t > change y 0 en otro caso     \n  b2 * (t-change) * step( t - change),\n  \n  # Hay que poner que estime el \"intercept\" de los parámetros y el nl = TRUE para que brms sepa que son parámetros\n  # y no variables en los datos \n  b0 ~ 1,\n  b1 ~ 1, \n  b2 ~ 1,\n  change ~ 1, \n  nl = TRUE\n)\n\nEspecificamos las priors. En brms se escriben las priors y se “concatenan” mediante +\n\nbprior <- prior(normal(60, 5), nlpar = \"b0\") +\n          prior(normal(0, 1), nlpar = \"b1\") +\n          prior(normal(0, 1), nlpar = \"b2\") +\n         # para el cambio ponemos como media la mitad del intervalo y unos 5 años de desviación típica\n          prior(normal( 2010, 23/4), nlpar = \"change\")\n\nY ya podemos escribir el modelo en brms que lo compilara a stan y hace el mcmc\n\nmbrm <-\n    brm(\n        bform,\n        family = poisson(\"identity\"), # pongo de link la identidad en vez del log por hacer como en el post original\n        prior = bprior,\n        data = df,\n        backend = \"cmdstanr\",\n        cores = 6,\n        chains = 6,\n        iter = 4000,\n        warmup = 500\n    )\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 5 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 6 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 6 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 6 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 5 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 2 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 2 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 2 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 5 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 5 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 5 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 5 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 1 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 5 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 1 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 6 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 5 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 5 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 5 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 5 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 6 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 3 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 5 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 5 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 6 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 5 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 finished in 2.5 seconds.\n#> Chain 5 finished in 2.3 seconds.\n#> Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 4 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 4 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 6 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 6 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 6 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 6 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 1 finished in 2.9 seconds.\n#> Chain 3 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 6 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 6 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 6 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 6 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 6 finished in 3.0 seconds.\n#> Chain 4 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 3 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 finished in 4.4 seconds.\n#> Chain 3 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 3 finished in 18.0 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 5.5 seconds.\n#> Total execution time: 18.1 seconds.\n\nY bueno, no ha tardado mucho, unos 3 segundos por cadena.\n\nsummary(mbrm)\n#>  Family: poisson \n#>   Links: mu = identity \n#> Formula: y ~ b0 + b1 * t + b2 * (t - change) * step(t - change) \n#>          b0 ~ 1\n#>          b1 ~ 1\n#>          b2 ~ 1\n#>          change ~ 1\n#>    Data: df (Number of observations: 23) \n#>   Draws: 6 chains, each with iter = 4000; warmup = 500; thin = 1;\n#>          total post-warmup draws = 21000\n#> \n#> Population-Level Effects: \n#>                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> b0_Intercept        50.33     22.21     1.59    69.57 1.40       13       10\n#> b1_Intercept         0.01      0.01    -0.00     0.03 1.37       13       23\n#> b2_Intercept        -1.07      0.48    -1.93    -0.32 1.00     8488     8579\n#> change_Intercept  2008.67      3.31  2002.91  2014.20 1.00     9493     7851\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPodemos pintar la curva media estimada y su intervalo de credibilidad\n\nplot(conditional_effects(mbrm),\n     points = TRUE)\n\n\n\n\n\n\n\n\nTambién la posterior predict function de los diferentes puntos. Evidentemente esto tiene más variabilidad que la posterior predict de la media condicionada que era el gráfico anterior.\n\nplot(conditional_effects(mbrm, method = \"posterior_predict\"),\n     points = TRUE)\n\n\n\n\n\n\n\n\nY básicamente, obtenermos los mismo resultados que Carlos con numpyro.\nPodemos obtener un histograma de la posterior del punto de cambio\n\npunto_cambio_posterior <- as_draws_df(mbrm, variable = \"b_change_Intercept\")\n\nggplot2::ggplot(punto_cambio_posterior, aes(x =b_change_Intercept ))   +\n    ggplot2::geom_histogram()\n\n\n\n\n\n\n\n\n\nposterior <- as_draws_df(mbrm)\n\nhead(posterior)\n#> # A draws_df: 6 iterations, 1 chains, and 6 variables\n#>   b_b0_Intercept b_b1_Intercept b_b2_Intercept b_change_Intercept lprior lp__\n#> 1             52        0.00607          -1.33               2010   -9.0  -87\n#> 2             69       -0.00512          -0.56               2006   -9.1  -89\n#> 3             61        0.00087          -0.40               2006   -7.3  -87\n#> 4             63       -0.00099          -0.48               2006   -7.5  -86\n#> 5             59        0.00275          -1.17               2006   -7.9  -85\n#> 6             58        0.00339          -1.18               2006   -8.1  -86\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\ndim(posterior)\n#> [1] 21000     9\n\nPintando las posterior predict directamente\n\n\n# Muy old R base, pero es que es la hora de comer y no tengo azúcar en el cerebro\n\nn_samples <- 10000\nidx_sample <-  sample(1:nrow(posterior), size = n_samples, replace = TRUE)\n\nposterior_df <-  as.data.frame(posterior)\n\n\nmake_curve_functions <-  function(fila) {\n    \n    b0     <-  posterior_df[fila, \"b_b0_Intercept\"]\n    b1     <-  posterior_df[fila, \"b_b1_Intercept\"]\n    b2     <-  posterior_df[fila, \"b_b2_Intercept\"]\n    change <-  posterior_df[fila, \"b_change_Intercept\"]\n    lambda <- b0 + b1 * t + ifelse(t > change, (t-change) * b2, 0) \n    return(lambda)\n    \n}\n\n\nt <-  df$t\n\nres <- sapply(idx_sample, make_curve_functions)\n\n\nplot(t, res[,1], pch = 19, col = scales::alpha(\"lightblue\", 0.7), ylim = c(40, 80), type = \"l\")\n\nfor (id in 2:n_samples) {\n    points(t, res[,id], pch = 19, col = scales::alpha(\"lightblue\", 0.7), type = \"l\")\n    \n}\n\npoints(t, df$y, pch = 19)\n\n\n\n\n\n\n\n\nUna cosa interesante de brms es que nos construye código en Stan que luego nosotros podemos modificar\n\nmake_stancode(bform, data = df,prior= bprior, family = poisson(\"identity\"))\n#> // generated with brms 2.19.0\n#> functions {\n#> }\n#> data {\n#>   int<lower=1> N;  // total number of observations\n#>   int Y[N];  // response variable\n#>   int<lower=1> K_b0;  // number of population-level effects\n#>   matrix[N, K_b0] X_b0;  // population-level design matrix\n#>   int<lower=1> K_b1;  // number of population-level effects\n#>   matrix[N, K_b1] X_b1;  // population-level design matrix\n#>   int<lower=1> K_b2;  // number of population-level effects\n#>   matrix[N, K_b2] X_b2;  // population-level design matrix\n#>   int<lower=1> K_change;  // number of population-level effects\n#>   matrix[N, K_change] X_change;  // population-level design matrix\n#>   // covariate vectors for non-linear functions\n#>   int C_1[N];\n#>   int prior_only;  // should the likelihood be ignored?\n#> }\n#> transformed data {\n#> }\n#> parameters {\n#>   vector[K_b0] b_b0;  // population-level effects\n#>   vector[K_b1] b_b1;  // population-level effects\n#>   vector[K_b2] b_b2;  // population-level effects\n#>   vector[K_change] b_change;  // population-level effects\n#> }\n#> transformed parameters {\n#>   real lprior = 0;  // prior contributions to the log posterior\n#>   lprior += normal_lpdf(b_b0 | 60, 5);\n#>   lprior += normal_lpdf(b_b1 | 0, 1);\n#>   lprior += normal_lpdf(b_b2 | 0, 1);\n#>   lprior += normal_lpdf(b_change | 2010, 23/4);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] nlp_b0 = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] nlp_b1 = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] nlp_b2 = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] nlp_change = rep_vector(0.0, N);\n#>     // initialize non-linear predictor term\n#>     vector[N] mu;\n#>     nlp_b0 += X_b0 * b_b0;\n#>     nlp_b1 += X_b1 * b_b1;\n#>     nlp_b2 += X_b2 * b_b2;\n#>     nlp_change += X_change * b_change;\n#>     for (n in 1:N) {\n#>       // compute non-linear predictor values\n#>       mu[n] = (nlp_b0[n] + nlp_b1[n] * C_1[n] + nlp_b2[n] * (C_1[n] - nlp_change[n]) * step(C_1[n] - nlp_change[n]));\n#>     }\n#>     target += poisson_lpmf(Y | mu);\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#> }\n#> generated quantities {\n#> }\n\n\nNotas\nUna forma fácil de ajustar estos datos es usando splines.\n\nm_spline  <- brm(\n                 y ~ s(t), \n                 family = poisson(\"identity\"), \n                 data = df, \n                 backend= \"cmdstanr\"\n)\n#> Running MCMC with 4 sequential chains...\n#> \n#> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 1 finished in 0.3 seconds.\n#> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 2 finished in 0.4 seconds.\n#> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 finished in 0.3 seconds.\n#> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \n#> Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \n#> Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \n#> Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \n#> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \n#> Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 4 finished in 0.3 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 0.3 seconds.\n#> Total execution time: 1.9 seconds.\n\nsummary(m_spline)\n#>  Family: poisson \n#>   Links: mu = identity \n#> Formula: y ~ s(t) \n#>    Data: df (Number of observations: 23) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Smooth Terms: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sds(st_1)    12.78      7.77     1.41    30.74 1.00     1376     1528\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    58.83      1.57    55.82    61.89 1.00     3590     2668\n#> st_1         -3.76     31.71   -56.38    67.67 1.00      906      572\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(conditional_smooths(m_spline, method = \"posterior_predict\") \n    ) \n\n\n\n\n\n\n\n\nY tendríamos la posterior del spline , pero la verdad, ahora no tengo ganas de buscar como interpretar ese spline para traducir a punto de cambio de tendencia.\nPor otro lado, leyendo por ahí he visto una implementación un pelín diferente\n\nbform_alt <- bf(\n    y ~ b0 +\n        # aqui es donde cambia un poco\n        b1 * (t - change) * step(change - t) +\n        # post cambio\n        b2 * (t - change) * step(t - change),\n    b0 + b1 + b2 + change ~ 1,\n    nl = TRUE\n)\n\nbprior <- prior(normal(60, 5), nlpar = \"b0\") +\n    prior(normal(0, 1), nlpar = \"b1\") +\n    prior(normal(0, 1), nlpar = \"b2\") +\n    prior(normal(2010, 23 / 4), nlpar = \"change\")\n\n\nmbrm_alt <-\n    brm(\n        bform_alt, family = poisson(\"identity\"),\n        prior = bprior,\n        data = df,\n        backend = \"cmdstanr\",\n        cores = 6,\n        chains = 6,\n        iter = 4000,\n        warmup = 500\n    )\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 5 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 6 Iteration:    1 / 4000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 5 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 5 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 6 Iteration:  100 / 4000 [  2%]  (Warmup) \n#> Chain 6 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 5 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 1 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 1 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 6 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 3 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 3 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 5 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 5 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 5 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 3 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#> Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 5 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 6 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 6 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 6 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 4 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 4 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 4 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 1 finished in 1.4 seconds.\n#> Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#> Chain 3 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 5 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 6 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  501 / 4000 [ 12%]  (Sampling) \n#> Chain 2 Iteration:  600 / 4000 [ 15%]  (Sampling) \n#> Chain 2 Iteration:  700 / 4000 [ 17%]  (Sampling) \n#> Chain 2 Iteration:  800 / 4000 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  900 / 4000 [ 22%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 4000 [ 25%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#> Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 5 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 6 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 6 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#> Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 5 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 5 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 5 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 5 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 6 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 6 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 3 finished in 1.7 seconds.\n#> Chain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#> Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#> Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 5 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 6 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 5 finished in 1.7 seconds.\n#> Chain 6 finished in 1.6 seconds.\n#> Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#> Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#> Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 4 finished in 1.8 seconds.\n#> Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#> Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#> Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#> Chain 2 finished in 2.0 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 1.7 seconds.\n#> Total execution time: 2.2 seconds.\n\nY sale prácticamente lo mismo\n\nsummary(mbrm_alt)\n#>  Family: poisson \n#>   Links: mu = identity \n#> Formula: y ~ b0 + b1 * (t - change) * step(change - t) + b2 * (t - change) * step(t - change) \n#>          b0 ~ 1\n#>          b1 ~ 1\n#>          b2 ~ 1\n#>          change ~ 1\n#>    Data: df (Number of observations: 23) \n#>   Draws: 6 chains, each with iter = 4000; warmup = 500; thin = 1;\n#>          total post-warmup draws = 21000\n#> \n#> Population-Level Effects: \n#>                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> b0_Intercept        63.38      2.92    57.00    68.66 1.00     7444     7067\n#> b1_Intercept         0.50      0.68    -0.71     1.92 1.00     6217     7902\n#> b2_Intercept        -1.05      0.42    -1.90    -0.27 1.00     8058     8853\n#> change_Intercept  2007.95      2.88  2002.84  2014.69 1.00     7429     6453\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nMe queda pendiente hacerlo con Turing.jl Saludos"
  },
  {
    "objectID": "2023/metanalisis.html",
    "href": "2023/metanalisis.html",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "",
    "text": "Ya en 2022 os mostraba uno de los ingredientes principales de la cocina electoral, al menos de la tradicional, no de la postmoderna Alaminos-Tezanos.\nHoy os quiero contar como haría yo la agregación de encuestas, cuando no se tienen los datos brutos. En primer lugar aviso de lo que sigue a continuación sólo lo he hecho por diversión y faltaría mucho más trabajo para considerarlo un intento serio.\nLa diversión viene por este tweet de Anabel Forte que puso como contestación a un hilo dónde Kiko Llaneras explicaba su modelo de predicción agregando encuestas y haciendo simulaciones. Aquí el hilo de kiko y en la imagen la respuesta de Ana.\n\n\n\nTweet de Ana\n\n\nTotal, que dado que conozco a Ana y a Virgilio y son bayesianos y yo sólo un aprendiz de la cosa, pues he intentado un metaanálisis bayesiano sencillo juntando varias encuestas."
  },
  {
    "objectID": "2023/metanalisis.html#introducción",
    "href": "2023/metanalisis.html#introducción",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "",
    "text": "Ya en 2022 os mostraba uno de los ingredientes principales de la cocina electoral, al menos de la tradicional, no de la postmoderna Alaminos-Tezanos.\nHoy os quiero contar como haría yo la agregación de encuestas, cuando no se tienen los datos brutos. En primer lugar aviso de lo que sigue a continuación sólo lo he hecho por diversión y faltaría mucho más trabajo para considerarlo un intento serio.\nLa diversión viene por este tweet de Anabel Forte que puso como contestación a un hilo dónde Kiko Llaneras explicaba su modelo de predicción agregando encuestas y haciendo simulaciones. Aquí el hilo de kiko y en la imagen la respuesta de Ana.\n\n\n\nTweet de Ana\n\n\nTotal, que dado que conozco a Ana y a Virgilio y son bayesianos y yo sólo un aprendiz de la cosa, pues he intentado un metaanálisis bayesiano sencillo juntando varias encuestas."
  },
  {
    "objectID": "2023/metanalisis.html#datos",
    "href": "2023/metanalisis.html#datos",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Datos",
    "text": "Datos\nLo primero era intentar encontrar datos de las encuestas que se han hecho, importante que tengan tanto la estimación como el tamaño muestral. Si, ya sé que cada empresa tiene su cocina y sus cosas, que unas son telefónicas, que otras son tracking o paneles y tal, pero ya he dicho que lo estoy haciendo por diversión..\nBueno, pues aquí he encontrado la info que buscaba. El tema es que la tabla está en una tabla de datawrapper enlace_table y no he sido capaz de escrapear de forma programática, que se le va a hacer, no vale uno pa to.\nComo eran muchas encuestas pues he ido seleccionando algunas del mes de julio y al final me he quedado con unas 23. Para cada encuesta he puesto su tamaño muestral, la diferencia entre la fecha de las elecciones y la fecha de la realización de la encuesta, variable time , también he convertido a votos la estimación que dan para pp, psoe, sumar, vox y resto, simplemente multiplicando la estimación que dan por su tamaño muestral.\nMejor vemos la tabla\n\nlibrary(tidyverse)\nlibrary(DT)\ndf &lt;-  read_csv(here::here(\"data/encuestas_agregadas.csv\")) |&gt; \n    select(empresa, time, partido, everything())\n\ndatatable(df)\n\n\n\n\n\n\nPintamos\n\n\ncolores &lt;-  c(\n    \"pp\" = \"#005999\",\n    \"psoe\" = \"#FF0126\", \n    \"sumar\" = \"#A00B85\", \n    \"vox\" = \"#51962A\", \n    \"resto\" = \"grey\"\n    )\n\ndf |&gt; \n    ggplot(aes(x = time, y = estim,color = partido )) +\n    geom_point() +\n    scale_color_manual(values = colores) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nLa selección de encuestas la he hecho sin mucho orden, son todas del mes de julio, algunas empresas repiten como sigma2 , gad3, simple_logica o sociométrica, otras veces he puesto como nombre el medio (okdiario o prisa).\nBueno, pues vamos a ver como hago el metaanálisis.\n\nPreparación datos\nVoy a poner los datos en un formato que me conviene más para lo que quiero hacer.\n\nn es tamaño de muestra\ntime : es días hasta elecciones, -7 quiere decir qeu la encuesta se publicó (o se hizo, no lo sé) 7 días antes del 23 de julio\nColumnas resultantes de multiplicar la estimación en la encuesta para cada partido por el tamaño muestral\n\nComo vemos voy a considerar 23 encuestas.\n\ndf_wider &lt;- df |&gt; \n    select(-estim) |&gt; \n    pivot_wider( id_cols = c(empresa, n, time),\n                 names_from = partido, \n                 values_from = votos) |&gt; \n    arrange(empresa)\n\nDT::datatable(df_wider)\n\n\n\n\n\n\nPues con esto ya puedo hacer mi intento de meta-análisis, que es probable que esté mal, que no soy un experto en estas cosas."
  },
  {
    "objectID": "2023/metanalisis.html#meta-análisis",
    "href": "2023/metanalisis.html#meta-análisis",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Meta-análisis",
    "text": "Meta-análisis\nPues lo voy a hacer de forma bayesiana. Los datos los tenemos a nivel de encuesta, por lo que puedo considerar que los votos estimados a cada partido en cada encuesta siguen una distribución multinomial , dónde n (tamaño muestral) es el número de intentos y tengo el vector de votos a cada partido que se obtendría. La suma de pp+psoe+sumar+vox+resto es igual a n para cada fila de los datos.\nTambién puedo considerar que las estimaciones de varias encuestas realizadas por la misma empresa no son independientes, no es descabellado ¿verdad?. Y también podría considerar que las estimaciones varían conforme se acerca la fecha de las elecciones y que esta variación podría ser diferente para cada empresa encuestadora. Pues con estos ingredientes ya puedo hacer el “meta-análisis”\nUtilizo la librería brms que me va a permitir hacerlo con una interfaz sencilla. Y en algún momento del futuro miraré como hacerlo con numpyro que me está picando con eso Carlos\n\n\nlibrary(cmdstanr)\nlibrary(brms)\nlibrary(tidybayes)\n\noptions(brms.backend=\"cmdstanr\")\n\nCreamos una columna que una las columnas de los votos a partidos para que sea nuestro vector de respuesta multinomial\n\n\ndf_wider$cell_counts &lt;- with(df_wider, cbind(pp, psoe,sumar, vox, resto))\n\nDT::datatable(head(df_wider))\n\n\n\n\n\n\nY pasamos a ajustar el modelo, dónde vamos a considerar como efecto aleatorio la empresa y como efecto fijo el tiempo, aunque diferente para cada empresa.\nEn la fórmula de brms añadimos informacióna la variable respuesta, en este caso añadimos la info del tamaño muestral. Mirando cosas sobre meta-análisis con brms se puede añadir cosas como desviación estándar de la estimación del efecto y cosas así.\n\n\nformula &lt;- brmsformula(\n    cell_counts | trials(n) ~  (time |empresa))\n\n# vemos las priors por defecto qeu h\n(priors &lt;- get_prior(formula, df_wider, family = multinomial()))\n#&gt;                 prior     class      coef   group resp    dpar nlpar lb ub\n#&gt;                lkj(1)       cor                                           \n#&gt;                lkj(1)       cor           empresa                         \n#&gt;                (flat) Intercept                                           \n#&gt;  student_t(3, 0, 2.5) Intercept                         mupsoe            \n#&gt;  student_t(3, 0, 2.5)        sd                         mupsoe        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa       mupsoe        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa       mupsoe        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa       mupsoe        0   \n#&gt;  student_t(3, 0, 2.5) Intercept                        muresto            \n#&gt;  student_t(3, 0, 2.5)        sd                        muresto        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa      muresto        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa      muresto        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa      muresto        0   \n#&gt;  student_t(3, 0, 2.5) Intercept                        musumar            \n#&gt;  student_t(3, 0, 2.5)        sd                        musumar        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa      musumar        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa      musumar        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa      musumar        0   \n#&gt;  student_t(3, 0, 2.5) Intercept                          muvox            \n#&gt;  student_t(3, 0, 2.5)        sd                          muvox        0   \n#&gt;  student_t(3, 0, 2.5)        sd           empresa        muvox        0   \n#&gt;  student_t(3, 0, 2.5)        sd Intercept empresa        muvox        0   \n#&gt;  student_t(3, 0, 2.5)        sd      time empresa        muvox        0   \n#&gt;        source\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;       default\n#&gt;       default\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n\nAjustamos el modelo\n\nmodel_multinomial &lt;-\n    brm(\n        formula,\n        df_wider,\n        multinomial(),\n        prior = priors,\n        iter = 4000,\n        warmup = 1000,\n        cores = 4,\n        chains = 4,\n        seed = 47,\n        backend = \"cmdstanr\",\n        control = list(adapt_delta = 0.95), \n        refresh = 0\n    )\n#&gt; Running MCMC with 4 parallel chains...\n#&gt; \n#&gt; Chain 1 finished in 8.4 seconds.\n#&gt; Chain 3 finished in 8.5 seconds.\n#&gt; Chain 4 finished in 8.6 seconds.\n#&gt; Chain 2 finished in 9.9 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 8.9 seconds.\n#&gt; Total execution time: 10.0 seconds.\n\nsummary(model_multinomial)\n#&gt;  Family: multinomial \n#&gt;   Links: mupsoe = logit; musumar = logit; muvox = logit; muresto = logit \n#&gt; Formula: cell_counts | trials(n) ~ (time | empresa) \n#&gt;    Data: df_wider (Number of observations: 23) \n#&gt;   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 12000\n#&gt; \n#&gt; Group-Level Effects: \n#&gt; ~empresa (Number of levels: 10) \n#&gt;                                     Estimate Est.Error l-95% CI u-95% CI Rhat\n#&gt; sd(mupsoe_Intercept)                    0.05      0.03     0.00     0.13 1.00\n#&gt; sd(mupsoe_time)                         0.00      0.00     0.00     0.01 1.00\n#&gt; sd(musumar_Intercept)                   0.14      0.06     0.05     0.28 1.00\n#&gt; sd(musumar_time)                        0.00      0.00     0.00     0.02 1.00\n#&gt; sd(muvox_Intercept)                     0.11      0.06     0.01     0.25 1.00\n#&gt; sd(muvox_time)                          0.01      0.01     0.00     0.02 1.00\n#&gt; sd(muresto_Intercept)                   0.05      0.04     0.00     0.16 1.00\n#&gt; sd(muresto_time)                        0.01      0.00     0.00     0.01 1.00\n#&gt; cor(mupsoe_Intercept,mupsoe_time)       0.06      0.58    -0.94     0.95 1.00\n#&gt; cor(musumar_Intercept,musumar_time)     0.28      0.56    -0.88     0.98 1.00\n#&gt; cor(muvox_Intercept,muvox_time)        -0.03      0.56    -0.95     0.93 1.00\n#&gt; cor(muresto_Intercept,muresto_time)     0.21      0.58    -0.91     0.98 1.00\n#&gt;                                     Bulk_ESS Tail_ESS\n#&gt; sd(mupsoe_Intercept)                    3451     3599\n#&gt; sd(mupsoe_time)                         2876     5041\n#&gt; sd(musumar_Intercept)                   3697     3162\n#&gt; sd(musumar_time)                        3236     4666\n#&gt; sd(muvox_Intercept)                     2951     2225\n#&gt; sd(muvox_time)                          1842     4433\n#&gt; sd(muresto_Intercept)                   5233     4971\n#&gt; sd(muresto_time)                        3078     3653\n#&gt; cor(mupsoe_Intercept,mupsoe_time)       5869     6716\n#&gt; cor(musumar_Intercept,musumar_time)     7015     7025\n#&gt; cor(muvox_Intercept,muvox_time)         6490     6740\n#&gt; cor(muresto_Intercept,muresto_time)     4284     6808\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; mupsoe_Intercept     -0.20      0.03    -0.25    -0.15 1.00     6469     6941\n#&gt; musumar_Intercept    -0.99      0.05    -1.09    -0.90 1.00     4232     5876\n#&gt; muvox_Intercept      -0.97      0.05    -1.06    -0.87 1.00     3835     6313\n#&gt; muresto_Intercept    -1.10      0.03    -1.15    -1.04 1.00     7182     7017\n#&gt; \n#&gt; Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).\n\nY ya tendríamos el modelo."
  },
  {
    "objectID": "2023/metanalisis.html#predicción-estimación",
    "href": "2023/metanalisis.html#predicción-estimación",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "¿Predicción / estimación?",
    "text": "¿Predicción / estimación?\nVuelvo a decir que esto es sólo por diversión, para hacer algo serio tendría que haber usado mayor número de encuestas y realizadas en diferentes momentos del tiempo, tener las estimaciones que daban en cada provincia y realizar la estimación de escaños. Todo eso y más ya lo hace, y muy bien, Kiko Llaneras para El País\n¿Cómo podríamos estimar lo que va a pasar el día de las elecciones con este modelo?\nPues podríamos considerar que las elecciones fueran una encuesta realizada por una empresa que no tengo en los datos (un nuevo nivel de la variable empresa) , en este caso el gobierno, y ponemos la variable time = 0\n\n# pongo n = 1 para que me devuelva las probabilidades , si ponenemos n = 15000000 nos devolvería una # estimación de cuántos votos van a cada partido\n\nnewdata &lt;- tibble(\n    empresa = \"votaciones_dia_23\", \n    time = 0,\n    n= 1)\n\nnewdata\n#&gt; # A tibble: 1 × 3\n#&gt;   empresa            time     n\n#&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 votaciones_dia_23     0     1\n\nAhora utilizando una función de la librería tidybayes tenemos una forma fácil de añadir las posterior predict\n\nestimaciones &lt;-  newdata %&gt;% \n    add_epred_draws(model_multinomial, allow_new_levels = TRUE) %&gt;% \n    mutate(partido = as_factor(.category)) |&gt; \n    select(-.category)\n\ndim(estimaciones )\n#&gt; [1] 60000    10\n\nDT::datatable(head(estimaciones, 100))\n\n\n\n\n\n\nY tenemos 12000 estimaciones de la posteriori para cada partido. Esto se podría decir que es equivalente a las 15000 simulaciones que hace Kiko con su modelo. Bueno, salvo que él en cada simulación calcula más cosas, como los escaños obtenidos etc..\nPodemos hacer un summary de las posteriores y ver intervalo de credibilidad al 80% por ejemplo\n\nestimaciones |&gt; \n    group_by(partido) |&gt; \n    summarise(\n        media = mean(.epred), \n        mediana= median(.epred), \n        low = quantile(.epred, 0.05), \n        high= quantile(.epred, 0.95)\n    )\n#&gt; # A tibble: 5 × 5\n#&gt;   partido media mediana   low  high\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 pp      0.344   0.344 0.325 0.364\n#&gt; 2 psoe    0.282   0.283 0.265 0.299\n#&gt; 3 sumar   0.127   0.127 0.110 0.148\n#&gt; 4 vox     0.131   0.131 0.115 0.148\n#&gt; 5 resto   0.115   0.115 0.105 0.125\n\nO pintar las distribuciones. .\n\n\nestimaciones %&gt;% \n    ggplot(aes(x=.epred, fill = partido)) +\n    geom_density(alpha = 0.5 ) +\n    scale_x_continuous(labels = scales::percent) +\n    scale_fill_manual(values = colores) +\n    labs(title=\"Agregando encuestas por diversión. Resultado\",\n        x = \"Porcentaje estimado\", \n        y = \"Density\")\n\n\n\n\n\n\n\n\n¿Qué más cosas podemos hacer? Ya que tengo las posterioris puedo usarlas y calcular las posterioris del bloque PP+ Vox o de cualquier otra cosa.\nSupongamos que hubiera 15 millones de votos válidos.\n\nvotantes &lt;- 15e6\nposterioris &lt;- estimaciones  |&gt; \n    ungroup() |&gt; \n    mutate(votos = votantes * .epred) |&gt; \n    select(partido, votos) |&gt; \n    pivot_wider(names_from = partido, values_from = votos) \n\n# tenemos columnas que son listas.  hay que hacer un unnest\nposterioris\n#&gt; # A tibble: 1 × 5\n#&gt;   pp             psoe           sumar          vox            resto         \n#&gt;   &lt;list&gt;         &lt;list&gt;         &lt;list&gt;         &lt;list&gt;         &lt;list&gt;        \n#&gt; 1 &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt; &lt;dbl [12,000]&gt;\n\n\nposterioris &lt;- posterioris  |&gt; \n    unnest(c(pp, psoe, sumar, vox, resto)) \n\nhead(posterioris)\n#&gt; # A tibble: 6 × 5\n#&gt;         pp     psoe    sumar      vox    resto\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 5133515. 4446758. 1812278. 1943958. 1663491.\n#&gt; 2 5009867. 4145069. 2138741. 2050803. 1655520.\n#&gt; 3 5497166. 4231178. 1557841. 1776698. 1937118.\n#&gt; 4 5372578. 4119759. 1768716. 1833665. 1905282.\n#&gt; 5 5211844. 4157799. 1955837. 1962067. 1712452.\n#&gt; 6 5310599. 4135619. 1959110. 1763744. 1830928.\n\nSumo votos de los bloques para cada una de las 12000 filas. Además. añado al bloque de la izquierda el 50% de los votos que están en resto.\n\nposterioris &lt;- posterioris |&gt; \n    mutate(\n        derecha = pp + vox, \n        izquierda = psoe + sumar + 0.5*resto) |&gt; \n    mutate(derecha_posterior = derecha / votantes, \n           izquierda_posterior = izquierda/votantes)\n\nposterioris |&gt; \n    head(20)\n#&gt; # A tibble: 20 × 9\n#&gt;          pp     psoe    sumar     vox  resto derecha izquierda derecha_posterior\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n#&gt;  1 5133515. 4446758. 1812278.  1.94e6 1.66e6  7.08e6  7090782.             0.472\n#&gt;  2 5009867. 4145069. 2138741.  2.05e6 1.66e6  7.06e6  7111570.             0.471\n#&gt;  3 5497166. 4231178. 1557841.  1.78e6 1.94e6  7.27e6  6757578.             0.485\n#&gt;  4 5372578. 4119759. 1768716.  1.83e6 1.91e6  7.21e6  6841116.             0.480\n#&gt;  5 5211844. 4157799. 1955837.  1.96e6 1.71e6  7.17e6  6969863.             0.478\n#&gt;  6 5310599. 4135619. 1959110.  1.76e6 1.83e6  7.07e6  7010193.             0.472\n#&gt;  7 5232029. 4022503. 2050199.  2.06e6 1.64e6  7.29e6  6891489.             0.486\n#&gt;  8 5497116. 4104486. 1877294.  1.78e6 1.74e6  7.28e6  6851583.             0.485\n#&gt;  9 5037253. 4327570. 2111705.  1.86e6 1.66e6  6.90e6  7268684.             0.460\n#&gt; 10 5127698. 4210917. 1935521.  1.91e6 1.82e6  7.04e6  7055529.             0.469\n#&gt; 11 5486238. 4199075. 1588582.  1.86e6 1.87e6  7.35e6  6720877.             0.490\n#&gt; 12 4936325. 4342781. 1959659.  2.08e6 1.68e6  7.02e6  7142121.             0.468\n#&gt; 13 4759372. 4465230. 2329286.  1.84e6 1.60e6  6.60e6  7596283.             0.440\n#&gt; 14 5113801. 4232229. 1859841.  2.12e6 1.67e6  7.24e6  6927872.             0.482\n#&gt; 15 5188795. 4362614. 1766206.  1.97e6 1.72e6  7.16e6  6986615.             0.477\n#&gt; 16 5172707. 4361351. 1689878.  1.85e6 1.93e6  7.02e6  7014039.             0.468\n#&gt; 17 5022602. 4422880. 1906715.  2.02e6 1.63e6  7.04e6  7143928.             0.469\n#&gt; 18 5136992. 4341198. 1916981.  1.87e6 1.74e6  7.00e6  7126623.             0.467\n#&gt; 19 5335322. 4263027. 1794167.  1.83e6 1.78e6  7.17e6  6945986.             0.478\n#&gt; 20 5092228. 4262031. 1813527.  2.14e6 1.69e6  7.23e6  6920418.             0.482\n#&gt; # ℹ 1 more variable: izquierda_posterior &lt;dbl&gt;\n\nAhora nos podemos hacer preguntas como , ¿en cuántas de estas posterioris gana el bloque de la derecha así construido al de la izquierda? o ¿en cuántas la diferencia que le saca el bloque de la derecha es mayor a un punto porcentual?\n\nposterioris |&gt; \n    mutate(gana_derecha = derecha_posterior&gt;izquierda_posterior, \n           gana_derecha_mas1_pct = (derecha_posterior - izquierda_posterior) &gt;= 0.01) |&gt; \n    summarise(\n        mean(gana_derecha),\n        mean(gana_derecha_mas1_pct)\n        )\n#&gt; # A tibble: 1 × 2\n#&gt;   `mean(gana_derecha)` `mean(gana_derecha_mas1_pct)`\n#&gt;                  &lt;dbl&gt;                         &lt;dbl&gt;\n#&gt; 1                0.670                         0.480"
  },
  {
    "objectID": "2023/metanalisis.html#coda",
    "href": "2023/metanalisis.html#coda",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Coda",
    "text": "Coda\nBueno, pues así es como he pasado el sábado. ¿Cosas que le faltaría a esto para ser algo serio?\n\nQue tuviera en cuenta más encuestas y analizara mejor qué tipo de encuestas son, sus cambios de estimación según el tiempo\nQue añadiera estimación de escaños, lo cual no es trivial.\nAñadir encuestas a nivel autónomico o datos de las municipales y poder hacer un modelo jerárquico en condiciones.\nHablando con Virgilio o Ana, queda claro que al ser encuestas a nivel nacional y tener un error muy grande a nivel provincial, es muy complicado hacer luego la predicción de escaños.\n\nY como decía al principio, seguramente esto del meta-análisis se pueda hacer de otra manera, mucho mejor, así que si alguien sabe, que lo ponga en los comentarios."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#intro",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#intro",
    "title": "Conformal prediction. Estilo compadre",
    "section": "",
    "text": "El jueves pasado asistí al más que recomendable meetup de PyData Madrid, que cuenta entre sus organizadores con el gran Juan Luis Cano Rodríguez, antiguo compañero mío de curro y tocayo de iniciales.\nEl caso es que en una de las charlas, Ignacio Peletier, mencionó de pasada lo del “Conformal Prediction”. Y siendo que Ignacio es un gran científico de datos, y que hacía unos meses que había tenido varias charlas con Carlos sobre el particular, pues he decidido ver un poco más en detalle de qué iba el asunto ."
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#intro",
    "href": "2023/02/19/Arrow_y_s3/index.html#intro",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#intro",
    "href": "2023/01/29/explicatividad_no_usual/index.html#intro",
    "title": "Explicatividad no usual",
    "section": "",
    "text": "Buscando en el portátil sobre otras cosas me he encontrado un pequeño ejercicio para implementar la idea que se comenta aquí\nLa idea es muy sencilla, tal y como comenta Carlos. Si tengo un modelo que sea tipo randomForest\n\nDe cada observación a predecir se anota en qué nodo terminal ha caído en cada árbol\nPara cada nodo terminal en cada árbol se recorre el camino hacia “arriba” para saber qué variables están implicadas en ese nodo\nSe cuenta para cada nodo terminal el número de veces que aparece una variable en cada árbol (o se da más importancia a las qeu estén más altos en el árbol)\nSe agrega de alguna manera para cada observación qué variables y cómo de importantes están en los caminos de los nodos terminales en los que han caído.\nEsa info sería la importancia de las variables a nivel individual\nSe podría clusterizar observaciones con similares variables importantes\n\nAntes de nada, sí, ya sé que existen cosas como los shap values y que a partir de ellos se puede hacer algo parecido. Pero no está de más de vez en cuando buscarse uno las habichueleas de forma más artesanal.."
  },
  {
    "objectID": "2023/metanalisis.html#el-código-que-genera-en-stan",
    "href": "2023/metanalisis.html#el-código-que-genera-en-stan",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "El código que genera en Stan",
    "text": "El código que genera en Stan\nCon la función stancode podemos ver las tripas de lo qeu se ha mandado a Stan\n\nstancode(model_multinomial)\n#&gt; // generated with brms 2.19.0\n#&gt; functions {\n#&gt;   /* compute correlated group-level effects\n#&gt;    * Args:\n#&gt;    *   z: matrix of unscaled group-level effects\n#&gt;    *   SD: vector of standard deviation parameters\n#&gt;    *   L: cholesky factor correlation matrix\n#&gt;    * Returns:\n#&gt;    *   matrix of scaled group-level effects\n#&gt;    */\n#&gt;   matrix scale_r_cor(matrix z, vector SD, matrix L) {\n#&gt;     // r is stored in another dimension order than z\n#&gt;     return transpose(diag_pre_multiply(SD, L) * z);\n#&gt;   }\n#&gt;   /* multinomial-logit log-PMF\n#&gt;    * Args:\n#&gt;    *   y: array of integer response values\n#&gt;    *   mu: vector of category logit probabilities\n#&gt;    * Returns:\n#&gt;    *   a scalar to be added to the log posterior\n#&gt;    */\n#&gt;   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#&gt;     return multinomial_lpmf(y | softmax(mu));\n#&gt;   }\n#&gt; }\n#&gt; data {\n#&gt;   int&lt;lower=1&gt; N; // total number of observations\n#&gt;   int&lt;lower=2&gt; ncat; // number of categories\n#&gt;   array[N, ncat] int Y; // response array\n#&gt;   array[N] int trials; // number of trials\n#&gt;   // data for group-level effects of ID 1\n#&gt;   int&lt;lower=1&gt; N_1; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_1; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_1; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_1_mupsoe_1;\n#&gt;   vector[N] Z_1_mupsoe_2;\n#&gt;   int&lt;lower=1&gt; NC_1; // number of group-level correlations\n#&gt;   // data for group-level effects of ID 2\n#&gt;   int&lt;lower=1&gt; N_2; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_2; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_2; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_2_musumar_1;\n#&gt;   vector[N] Z_2_musumar_2;\n#&gt;   int&lt;lower=1&gt; NC_2; // number of group-level correlations\n#&gt;   // data for group-level effects of ID 3\n#&gt;   int&lt;lower=1&gt; N_3; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_3; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_3; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_3_muvox_1;\n#&gt;   vector[N] Z_3_muvox_2;\n#&gt;   int&lt;lower=1&gt; NC_3; // number of group-level correlations\n#&gt;   // data for group-level effects of ID 4\n#&gt;   int&lt;lower=1&gt; N_4; // number of grouping levels\n#&gt;   int&lt;lower=1&gt; M_4; // number of coefficients per level\n#&gt;   array[N] int&lt;lower=1&gt; J_4; // grouping indicator per observation\n#&gt;   // group-level predictor values\n#&gt;   vector[N] Z_4_muresto_1;\n#&gt;   vector[N] Z_4_muresto_2;\n#&gt;   int&lt;lower=1&gt; NC_4; // number of group-level correlations\n#&gt;   int prior_only; // should the likelihood be ignored?\n#&gt; }\n#&gt; transformed data {\n#&gt;   \n#&gt; }\n#&gt; parameters {\n#&gt;   real Intercept_mupsoe; // temporary intercept for centered predictors\n#&gt;   real Intercept_musumar; // temporary intercept for centered predictors\n#&gt;   real Intercept_muvox; // temporary intercept for centered predictors\n#&gt;   real Intercept_muresto; // temporary intercept for centered predictors\n#&gt;   vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations\n#&gt;   matrix[M_1, N_1] z_1; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_1] L_1; // cholesky factor of correlation matrix\n#&gt;   vector&lt;lower=0&gt;[M_2] sd_2; // group-level standard deviations\n#&gt;   matrix[M_2, N_2] z_2; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_2] L_2; // cholesky factor of correlation matrix\n#&gt;   vector&lt;lower=0&gt;[M_3] sd_3; // group-level standard deviations\n#&gt;   matrix[M_3, N_3] z_3; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_3] L_3; // cholesky factor of correlation matrix\n#&gt;   vector&lt;lower=0&gt;[M_4] sd_4; // group-level standard deviations\n#&gt;   matrix[M_4, N_4] z_4; // standardized group-level effects\n#&gt;   cholesky_factor_corr[M_4] L_4; // cholesky factor of correlation matrix\n#&gt; }\n#&gt; transformed parameters {\n#&gt;   matrix[N_1, M_1] r_1; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_1] r_1_mupsoe_1;\n#&gt;   vector[N_1] r_1_mupsoe_2;\n#&gt;   matrix[N_2, M_2] r_2; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_2] r_2_musumar_1;\n#&gt;   vector[N_2] r_2_musumar_2;\n#&gt;   matrix[N_3, M_3] r_3; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_3] r_3_muvox_1;\n#&gt;   vector[N_3] r_3_muvox_2;\n#&gt;   matrix[N_4, M_4] r_4; // actual group-level effects\n#&gt;   // using vectors speeds up indexing in loops\n#&gt;   vector[N_4] r_4_muresto_1;\n#&gt;   vector[N_4] r_4_muresto_2;\n#&gt;   real lprior = 0; // prior contributions to the log posterior\n#&gt;   // compute actual group-level effects\n#&gt;   r_1 = scale_r_cor(z_1, sd_1, L_1);\n#&gt;   r_1_mupsoe_1 = r_1[ : , 1];\n#&gt;   r_1_mupsoe_2 = r_1[ : , 2];\n#&gt;   // compute actual group-level effects\n#&gt;   r_2 = scale_r_cor(z_2, sd_2, L_2);\n#&gt;   r_2_musumar_1 = r_2[ : , 1];\n#&gt;   r_2_musumar_2 = r_2[ : , 2];\n#&gt;   // compute actual group-level effects\n#&gt;   r_3 = scale_r_cor(z_3, sd_3, L_3);\n#&gt;   r_3_muvox_1 = r_3[ : , 1];\n#&gt;   r_3_muvox_2 = r_3[ : , 2];\n#&gt;   // compute actual group-level effects\n#&gt;   r_4 = scale_r_cor(z_4, sd_4, L_4);\n#&gt;   r_4_muresto_1 = r_4[ : , 1];\n#&gt;   r_4_muresto_2 = r_4[ : , 2];\n#&gt;   lprior += student_t_lpdf(Intercept_mupsoe | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(Intercept_musumar | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(Intercept_muvox | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(Intercept_muresto | 3, 0, 2.5);\n#&gt;   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n#&gt;   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n#&gt;   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_3 | 1);\n#&gt;   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#&gt;             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#&gt;   lprior += lkj_corr_cholesky_lpdf(L_4 | 1);\n#&gt; }\n#&gt; model {\n#&gt;   // likelihood including constants\n#&gt;   if (!prior_only) {\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] mupsoe = rep_vector(0.0, N);\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] musumar = rep_vector(0.0, N);\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] muvox = rep_vector(0.0, N);\n#&gt;     // initialize linear predictor term\n#&gt;     vector[N] muresto = rep_vector(0.0, N);\n#&gt;     // linear predictor matrix\n#&gt;     array[N] vector[ncat] mu;\n#&gt;     mupsoe += Intercept_mupsoe;\n#&gt;     musumar += Intercept_musumar;\n#&gt;     muvox += Intercept_muvox;\n#&gt;     muresto += Intercept_muresto;\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       mupsoe[n] += r_1_mupsoe_1[J_1[n]] * Z_1_mupsoe_1[n]\n#&gt;                    + r_1_mupsoe_2[J_1[n]] * Z_1_mupsoe_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       musumar[n] += r_2_musumar_1[J_2[n]] * Z_2_musumar_1[n]\n#&gt;                     + r_2_musumar_2[J_2[n]] * Z_2_musumar_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       muvox[n] += r_3_muvox_1[J_3[n]] * Z_3_muvox_1[n]\n#&gt;                   + r_3_muvox_2[J_3[n]] * Z_3_muvox_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       // add more terms to the linear predictor\n#&gt;       muresto[n] += r_4_muresto_1[J_4[n]] * Z_4_muresto_1[n]\n#&gt;                     + r_4_muresto_2[J_4[n]] * Z_4_muresto_2[n];\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       mu[n] = transpose([0, mupsoe[n], musumar[n], muvox[n], muresto[n]]);\n#&gt;     }\n#&gt;     for (n in 1 : N) {\n#&gt;       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#&gt;     }\n#&gt;   }\n#&gt;   // priors including constants\n#&gt;   target += lprior;\n#&gt;   target += std_normal_lpdf(to_vector(z_1));\n#&gt;   target += std_normal_lpdf(to_vector(z_2));\n#&gt;   target += std_normal_lpdf(to_vector(z_3));\n#&gt;   target += std_normal_lpdf(to_vector(z_4));\n#&gt; }\n#&gt; generated quantities {\n#&gt;   // actual population-level intercept\n#&gt;   real b_mupsoe_Intercept = Intercept_mupsoe;\n#&gt;   // actual population-level intercept\n#&gt;   real b_musumar_Intercept = Intercept_musumar;\n#&gt;   // actual population-level intercept\n#&gt;   real b_muvox_Intercept = Intercept_muvox;\n#&gt;   // actual population-level intercept\n#&gt;   real b_muresto_Intercept = Intercept_muresto;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_1] cor_1;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_2] cor_2;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_3] Cor_3 = multiply_lower_tri_self_transpose(L_3);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_3] cor_3;\n#&gt;   // compute group-level correlations\n#&gt;   corr_matrix[M_4] Cor_4 = multiply_lower_tri_self_transpose(L_4);\n#&gt;   vector&lt;lower=-1, upper=1&gt;[NC_4] cor_4;\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_1) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n#&gt;     }\n#&gt;   }\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_2) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n#&gt;     }\n#&gt;   }\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_3) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_3[choose(k - 1, 2) + j] = Cor_3[j, k];\n#&gt;     }\n#&gt;   }\n#&gt;   // extract upper diagonal of correlation matrix\n#&gt;   for (k in 1 : M_4) {\n#&gt;     for (j in 1 : (k - 1)) {\n#&gt;       cor_4[choose(k - 1, 2) + j] = Cor_4[j, k];\n#&gt;     }\n#&gt;   }\n#&gt; }"
  },
  {
    "objectID": "2023/metanalisis.html#actualización-2023-07-24",
    "href": "2023/metanalisis.html#actualización-2023-07-24",
    "title": "Meta-análisis. Agregando encuestas",
    "section": "Actualización [2023-07-24]",
    "text": "Actualización [2023-07-24]\nYa fueron las elecciones generales y escrutado al 100% se tiene que\n\n\n\n\n\n\n\nY claramente el promedio de encuestas infraestimó el voto al psoe. Para pp, sumar o o vox si que acertaron relativamente bien, aunque pp y vox se han quedado más cerda del extremo inferior del intervalo que del punto medio.\n\n¿Si hubiera considerado más encuestas habría cambiado algo?\n\ndf_update &lt;-  read_csv(here::here(\"data/encuestas_agregadas_39.csv\")) |&gt; \n    select(empresa, time, partido, everything())\n\ndatatable(df)\n\n\n\n\n\n\n\n\ndf_wider &lt;- df_update |&gt; \n    select(-estim) |&gt; \n    pivot_wider( id_cols = c(empresa, n, time),\n                 names_from = partido, \n                 values_from = votos) |&gt; \n    arrange(empresa)\n\ndf_wider$cell_counts &lt;- with(df_wider, cbind(resto,pp, psoe,sumar, vox))\n\nDT::datatable(df_wider)\n\n\n\n\n\n\n\n\n# cambio un poco la formla pq quiero efecto fijo del tiempo además del varying \n# slope\nformula &lt;- brmsformula(\n    cell_counts | trials(n) ~ time +  (time |empresa))\n\nmodel_multinomial_update &lt;- brm(formula, df_wider, multinomial(),\n                          iter = 4000, warmup = 1000, cores = 4, chains = 4,\n                          seed = 3,\n                          backend = \"cmdstanr\", \n                         control = list(adapt_delta = 0.95)\n)\n#&gt; Running MCMC with 4 parallel chains...\n#&gt; \n#&gt; Chain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \n#&gt; Chain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \n#&gt; Chain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \n#&gt; Chain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \n#&gt; Chain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \n#&gt; Chain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \n#&gt; Chain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \n#&gt; Chain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 3 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \n#&gt; Chain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 3 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \n#&gt; Chain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 1 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 2 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 3 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \n#&gt; Chain 1 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 2 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 3 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 1 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \n#&gt; Chain 4 Iteration: 1001 / 4000 [ 25%]  (Sampling) \n#&gt; Chain 2 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 3 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 1 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 4 Iteration: 1100 / 4000 [ 27%]  (Sampling) \n#&gt; Chain 2 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 3 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 1 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 4 Iteration: 1200 / 4000 [ 30%]  (Sampling) \n#&gt; Chain 2 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 3 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 1 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 4 Iteration: 1300 / 4000 [ 32%]  (Sampling) \n#&gt; Chain 2 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 3 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 1 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 4 Iteration: 1400 / 4000 [ 35%]  (Sampling) \n#&gt; Chain 2 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 3 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 1 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 4 Iteration: 1500 / 4000 [ 37%]  (Sampling) \n#&gt; Chain 2 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 3 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 1 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 4 Iteration: 1600 / 4000 [ 40%]  (Sampling) \n#&gt; Chain 2 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 1 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 4 Iteration: 1700 / 4000 [ 42%]  (Sampling) \n#&gt; Chain 2 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 4 Iteration: 1800 / 4000 [ 45%]  (Sampling) \n#&gt; Chain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 4 Iteration: 1900 / 4000 [ 47%]  (Sampling) \n#&gt; Chain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \n#&gt; Chain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \n#&gt; Chain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \n#&gt; Chain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \n#&gt; Chain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \n#&gt; Chain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \n#&gt; Chain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \n#&gt; Chain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \n#&gt; Chain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \n#&gt; Chain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \n#&gt; Chain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \n#&gt; Chain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \n#&gt; Chain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \n#&gt; Chain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \n#&gt; Chain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \n#&gt; Chain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \n#&gt; Chain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \n#&gt; Chain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 3 finished in 24.8 seconds.\n#&gt; Chain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \n#&gt; Chain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 1 finished in 25.3 seconds.\n#&gt; Chain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \n#&gt; Chain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 2 finished in 25.8 seconds.\n#&gt; Chain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \n#&gt; Chain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \n#&gt; Chain 4 finished in 26.5 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 25.6 seconds.\n#&gt; Total execution time: 26.5 seconds.\n\n\nestimaciones_update &lt;-  newdata %&gt;% \n    add_epred_draws(model_multinomial_update, allow_new_levels = TRUE) %&gt;% \n    mutate(partido = as_factor(.category)) |&gt; \n    select(-.category)\n\nPues parece que no cambia mucho. Si acaso añade más incertidumbre y ahora el resultado del psoe está dentro del intervalo de credibilidad del 90%\n\n\nestimaciones_update |&gt; \n    group_by(partido) |&gt; \n    summarise(\n        media = mean(.epred), \n        mediana= median(.epred), \n        low = quantile(.epred, 0.05), \n        high= quantile(.epred, 0.95)\n    )\n#&gt; # A tibble: 5 × 5\n#&gt;   partido media mediana    low  high\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 resto   0.115   0.115 0.108  0.126\n#&gt; 2 pp      0.342   0.342 0.309  0.375\n#&gt; 3 psoe    0.292   0.290 0.274  0.322\n#&gt; 4 sumar   0.125   0.124 0.113  0.139\n#&gt; 5 vox     0.125   0.128 0.0792 0.150\n\n\n\nestimaciones_update %&gt;% \n    ggplot(aes(x=.epred, fill = partido)) +\n    geom_density(alpha = 0.5 ) +\n    scale_x_continuous(labels = scales::percent) +\n    scale_fill_manual(values = colores) +\n    labs(title=\"Agregando encuestas por diversión. Resultado\",\n         subtitle = \"Actualización con más encuestas\",\n        x = \"Porcentaje estimado\", \n        y = \"Density\")\n\n\n\n\n\n\n\n\nCosas interesantes que puedo sacar del modelo. Por ejemplo los efectos aleatorios asociados a las diferentes empresas encuestadoras. Por ejemplo con mupsoe_Intercept se ve por ejemplo que la de prisa(40db) ,simple_logica (para el diario.es) sigma2 o el cis fueron las más favorables hacia el psoe.\nEsto parece sugerir que los resultados de las diferentes encuestas parecen estar afectados por la mano que les da de comer.\n\nranef(model_multinomial_update)\n#&gt; $empresa\n#&gt; , , mupp_Intercept\n#&gt; \n#&gt;                  Estimate  Est.Error        Q2.5      Q97.5\n#&gt; celeste_tel    0.04983451 0.05847759 -0.05785386 0.17452635\n#&gt; cis           -0.11269392 0.07921043 -0.28268055 0.01887259\n#&gt; electo_mania   0.01266378 0.05495308 -0.08769483 0.12890022\n#&gt; gad3           0.12708807 0.04538762  0.04470506 0.22400860\n#&gt; gesop         -0.09336239 0.06633783 -0.23445718 0.02701568\n#&gt; invymark      -0.08845348 0.08144761 -0.26558825 0.06216075\n#&gt; nc_report      0.06977150 0.05506073 -0.03028796 0.18586357\n#&gt; ok_diario      0.04076435 0.05285644 -0.05784015 0.15009835\n#&gt; prisa         -0.02726022 0.04755849 -0.11914163 0.07052230\n#&gt; sigma2         0.03550888 0.04398473 -0.04942665 0.12580797\n#&gt; simple_logica -0.02710374 0.04962525 -0.12671515 0.06897185\n#&gt; socio_metrica -0.02427102 0.05004341 -0.12195298 0.07523895\n#&gt; target_point   0.03521051 0.09644025 -0.16213082 0.23375662\n#&gt; \n#&gt; , , mupp_time\n#&gt; \n#&gt;                    Estimate    Est.Error         Q2.5        Q97.5\n#&gt; celeste_tel    5.601833e-04 0.0010752752 -0.001462165 0.0028845335\n#&gt; cis           -1.858769e-03 0.0014818803 -0.005107945 0.0003605464\n#&gt; electo_mania   8.326875e-04 0.0013083572 -0.001403193 0.0038301443\n#&gt; gad3           6.870950e-04 0.0018649834 -0.003052465 0.0044324950\n#&gt; gesop         -6.383980e-04 0.0018839697 -0.004635597 0.0029928842\n#&gt; invymark       3.140981e-05 0.0019661982 -0.004053128 0.0040827335\n#&gt; nc_report      6.845445e-04 0.0014938625 -0.002058980 0.0040297510\n#&gt; ok_diario      5.167656e-04 0.0012166924 -0.001778642 0.0031886107\n#&gt; prisa          6.750862e-04 0.0009813955 -0.001163746 0.0027679542\n#&gt; sigma2        -3.516238e-04 0.0012475810 -0.003085294 0.0019836857\n#&gt; simple_logica -1.126181e-03 0.0010989386 -0.003559223 0.0006837115\n#&gt; socio_metrica  3.710038e-04 0.0012584726 -0.002067272 0.0031104447\n#&gt; target_point  -3.996651e-04 0.0017999138 -0.004385952 0.0030570600\n#&gt; \n#&gt; , , mupsoe_Intercept\n#&gt; \n#&gt;                   Estimate  Est.Error        Q2.5      Q97.5\n#&gt; celeste_tel   -0.002276998 0.02808656 -0.06507048 0.05559588\n#&gt; cis            0.019702197 0.04093116 -0.05067533 0.11701995\n#&gt; electo_mania   0.011364728 0.02912626 -0.03663608 0.08641640\n#&gt; gad3           0.015223792 0.02628927 -0.02780848 0.07836367\n#&gt; gesop         -0.016066261 0.03623679 -0.10444262 0.04282177\n#&gt; invymark      -0.028486511 0.04541277 -0.14301735 0.04049496\n#&gt; nc_report     -0.007220812 0.02850193 -0.07316730 0.04673424\n#&gt; ok_diario     -0.030797624 0.03886857 -0.12636292 0.02279682\n#&gt; prisa          0.009762308 0.02631351 -0.03663878 0.07402995\n#&gt; sigma2         0.011121029 0.02579794 -0.03593884 0.07250188\n#&gt; simple_logica  0.016102746 0.03142286 -0.03983507 0.08999656\n#&gt; socio_metrica -0.008354206 0.02858118 -0.07234223 0.04903350\n#&gt; target_point   0.009988969 0.03623881 -0.05608511 0.09806407\n#&gt; \n#&gt; , , mupsoe_time\n#&gt; \n#&gt;                    Estimate   Est.Error          Q2.5         Q97.5\n#&gt; celeste_tel    6.351476e-05 0.001199362 -2.456705e-03  2.396963e-03\n#&gt; cis           -3.270677e-03 0.001262601 -5.934026e-03 -9.475839e-04\n#&gt; electo_mania   1.584091e-04 0.001340476 -2.472783e-03  2.911473e-03\n#&gt; gad3          -1.278420e-03 0.001959843 -5.409934e-03  2.486353e-03\n#&gt; gesop          1.001596e-03 0.002732244 -4.253542e-03  6.961284e-03\n#&gt; invymark       3.399821e-03 0.001909472  1.128583e-05  7.474699e-03\n#&gt; nc_report      9.755046e-04 0.001750050 -2.300850e-03  4.728513e-03\n#&gt; ok_diario      2.827258e-03 0.001539038 -1.292829e-05  5.969975e-03\n#&gt; prisa         -4.973800e-04 0.001138912 -2.882389e-03  1.753342e-03\n#&gt; sigma2        -1.277384e-03 0.001435137 -4.328619e-03  1.434950e-03\n#&gt; simple_logica -2.356142e-03 0.001293664 -5.124468e-03 -7.625871e-05\n#&gt; socio_metrica  1.594284e-03 0.001489799 -1.132845e-03  4.691291e-03\n#&gt; target_point  -1.411879e-03 0.001590356 -4.812724e-03  1.507244e-03\n#&gt; \n#&gt; , , musumar_Intercept\n#&gt; \n#&gt;                    Estimate  Est.Error         Q2.5      Q97.5\n#&gt; celeste_tel   -0.0358445986 0.04957444 -0.150229100 0.04743706\n#&gt; cis            0.0132971726 0.04651727 -0.069758368 0.12317857\n#&gt; electo_mania   0.0070341761 0.04248783 -0.083039127 0.09161290\n#&gt; gad3          -0.0353641132 0.03666900 -0.115859275 0.02688924\n#&gt; gesop          0.0373218097 0.05536161 -0.054462743 0.16499042\n#&gt; invymark      -0.0089751397 0.04854726 -0.111963050 0.08780055\n#&gt; nc_report     -0.0406315429 0.04748837 -0.150190250 0.03561324\n#&gt; ok_diario     -0.0102878996 0.04171049 -0.098574120 0.07398172\n#&gt; prisa          0.0225994343 0.04031133 -0.054578180 0.10777645\n#&gt; sigma2        -0.0082246939 0.03496083 -0.080907860 0.06160265\n#&gt; simple_logica  0.0751187593 0.05402766 -0.009799251 0.18905942\n#&gt; socio_metrica -0.0002657433 0.03985462 -0.084348557 0.08278825\n#&gt; target_point  -0.0165754581 0.05666641 -0.142857825 0.09367493\n#&gt; \n#&gt; , , musumar_time\n#&gt; \n#&gt;                    Estimate    Est.Error         Q2.5        Q97.5\n#&gt; celeste_tel    3.103610e-04 0.0008602449 -0.001373319 0.0022302135\n#&gt; cis            3.347250e-04 0.0008624901 -0.001128795 0.0024411762\n#&gt; electo_mania  -6.618639e-04 0.0011008107 -0.003507239 0.0008992042\n#&gt; gad3           2.429828e-05 0.0012769107 -0.002911007 0.0026252740\n#&gt; gesop         -8.011793e-05 0.0013149711 -0.002928823 0.0027984967\n#&gt; invymark       1.338833e-04 0.0011282724 -0.002083171 0.0027425217\n#&gt; nc_report      2.097078e-04 0.0011791976 -0.002183350 0.0028841215\n#&gt; ok_diario      3.148319e-04 0.0009717670 -0.001482126 0.0026558482\n#&gt; prisa         -6.035426e-04 0.0008646081 -0.002661508 0.0007422272\n#&gt; sigma2         4.400805e-04 0.0010397252 -0.001275172 0.0030627337\n#&gt; simple_logica -7.506860e-04 0.0010609271 -0.003165005 0.0009917626\n#&gt; socio_metrica  1.864726e-05 0.0009601066 -0.002061034 0.0020858427\n#&gt; target_point   3.113929e-04 0.0011178886 -0.001753471 0.0030024427\n#&gt; \n#&gt; , , muvox_Intercept\n#&gt; \n#&gt;                   Estimate  Est.Error        Q2.5      Q97.5\n#&gt; celeste_tel   -0.027563257 0.10163227 -0.22536030  0.1727010\n#&gt; cis           -0.565764453 0.13587027 -0.84565335 -0.3140112\n#&gt; electo_mania   0.066364066 0.09618556 -0.12254760  0.2570871\n#&gt; gad3          -0.008670455 0.07859782 -0.15700788  0.1495601\n#&gt; gesop          0.147990408 0.10824879 -0.05956420  0.3635356\n#&gt; invymark       0.033535523 0.14445059 -0.24711252  0.3263141\n#&gt; nc_report     -0.070586246 0.09473895 -0.25996155  0.1156743\n#&gt; ok_diario      0.054799569 0.09046204 -0.12134520  0.2336971\n#&gt; prisa          0.119210914 0.08427223 -0.04170232  0.2898681\n#&gt; sigma2         0.054515552 0.08308845 -0.10485597  0.2270518\n#&gt; simple_logica  0.138785392 0.08591571 -0.02525560  0.3128597\n#&gt; socio_metrica  0.078037685 0.08942748 -0.09257302  0.2565346\n#&gt; target_point  -0.011986902 0.20139152 -0.41809745  0.3930733\n#&gt; \n#&gt; , , muvox_time\n#&gt; \n#&gt;                    Estimate   Est.Error         Q2.5        Q97.5\n#&gt; celeste_tel    7.203336e-04 0.001784125 -0.002744172  0.004351591\n#&gt; cis           -7.170722e-03 0.002312615 -0.011891795 -0.002824443\n#&gt; electo_mania  -1.162150e-03 0.002241739 -0.005934080  0.002965156\n#&gt; gad3           4.264346e-04 0.002470284 -0.004084816  0.005839640\n#&gt; gesop          1.762831e-03 0.002799074 -0.003799805  0.007700488\n#&gt; invymark       3.921615e-04 0.003261884 -0.006150279  0.007273604\n#&gt; nc_report     -1.506608e-03 0.002381401 -0.006585859  0.002923375\n#&gt; ok_diario      8.105272e-05 0.001922182 -0.003800808  0.003791748\n#&gt; prisa          2.484377e-03 0.001586320 -0.000466300  0.005714418\n#&gt; sigma2         2.661615e-03 0.002236228 -0.001143362  0.007573518\n#&gt; simple_logica  1.070490e-03 0.001608866 -0.002146405  0.004289484\n#&gt; socio_metrica  2.899506e-04 0.002028250 -0.003889796  0.004245027\n#&gt; target_point   7.433332e-06 0.003551183 -0.007202358  0.007231719"
  }
]
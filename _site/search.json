[
  {
    "objectID": "2022.html",
    "href": "2022.html",
    "title": "2022",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 4, 2022\n\n\nConsejos para dejar spss\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html",
    "href": "2022/06/21/io-parte-1/index.html",
    "title": "IO Parte 1",
    "section": "",
    "text": "Allá por el año 1997 más o menos andaba yo estudiando Investigación Operativa en la Universidad de Granada. Recuerdo aprender el archiconocido algoritmo del simplex y algo también sobre programación entera (dónde el dominio de las variables está en \\(\\mathcal{Z}\\) ). No se me daba muy bien al principio, pero si recuerdo que luego me acabó gustando y el día que encuentre mis apuntes os pondré una demostración que desarrollé para un teorema que tenía algo que ver con la relación entre espacio primal y el dual.\nBueno, dejando de lado las batallitas de final del siglo pasado y debido a que estuve hace poco en la SEIO 2022 en Granada y coincidí con grandes profesionales de este tema, como por ejemplo el gran Alberto Torrejón Valenzuela, estoy convencido de que la investigación operativa es una de las grandes áreas que aún queda por explotar en las empresas.\nSe lleva haciendo Investigación Operativa desde hace tiempo, véase esto. Por otro lado creo que va a ser el próximo hype por las señales que estoy viendo, la primera de ellas es el cambio de nombre de la materia, ahora estoy empezando escuchar Analítica prescriptiva en vez de Investigación Operativa. Y como es norma en este mundillo, el cambio de nombre precede a la ¿burbuja?.\nPues vamos al grano, en esto de la investigación operativa se ha desarrollado mucho software para resolver este tipo de problemas, a partir de ahora los llamaremos solvers. Dentro de estos podemos destacar solvers comerciales como CPlex y Gurobi, pero también hay software libre como GLPK dentro del proyecto GNU, o sin olvidarnos de la fundación COIN-OR dónde se han desarrollado muchos solvers de manera opensource.\nDentro de nuestros 3 lenguajes favoritos (R, Julia y Python) hay librerías que permiten hacer de API para los diferentes solvers, tanto los de software libre como los comerciales. Paso a enumerar 3 proyectos, uno para cada lenguaje y pongo enlace.\nEn realidad, en principio uno podría decir que se trata sólo de sintaxis y que da igual cual uses pues al final todos utilizan los mismos solvers en el backend . No obstante, hay problemas en los que la misma construcción del mismo para pasárselo al solver puede tardar bastante y, según esto, JuMP parece estar a la altura con respecto a los softwares comerciales como GAMS en la generación del modelo y envío al solver.\nVeamos ahora un ejemplillo tonto de programación lineal y cual es la sintaxis (al fin y al cabo es sólo eso) en cada uno de los lenguajes.\nTenemos el siguiente problema de programación lineal\nY el objetivo es encontrar los valores de x e y que cumpliendo las restricciones minimicen la función 7x + 8y. Para los 3 lenguajes vamos a usar GLPK como solver"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#r",
    "href": "2022/06/21/io-parte-1/index.html#r",
    "title": "IO Parte 1",
    "section": "R",
    "text": "R\nlibrary(ROI)\n## ROI: R Optimization Infrastructure\n## Registered solver plugins: nlminb, alabama, glpk.\n## Default solver: auto.\nlibrary(ROI.plugin.glpk)\nHay que definir el objetivo, las restricciones y los límites de la variable. Para las restricciones en R hay que poner los coeficientes en forma de matriz , de ahí el uso de rbind\nobjetivo          = L_objective(c(7, 8), names = c(\"x\", \"y\"))\nrestricciones     = L_constraint(L = rbind(c(3, 4), c(2, 1)),\n                                   dir = c(\"==\", \">=\"),\n                                   rhs = c(9, 3))\nlimites_variables = V_bound(\n  li = 1:2,\n  ui = 1:2,\n  lb = c(-100, -Inf),\n  ub = c(Inf, 100)\n)\nAhora usamos la función OP que es el constructor del problema, por defecto considera que se trata de un problema de maximización OP(objective, constraints, types, bounds, maximum = FALSE)\nlp  <- OP(\n  objective = objetivo,\n  constraints = restricciones,\n  bounds = limites_variables\n)\nROI_applicable_solvers(lp)\n## [1] \"alabama\" \"glpk\"\nResolvemos con glpk\n(sol <- ROI_solve(lp, solver = \"glpk\"))\n## Optimal solution found.\n## The objective value is: 1.860000e+01\nY vemos cuáles son los valores de x e y que minimizan la función objetivo\nsolution(sol)\n##   x   y \n## 0.6 1.8"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#julia",
    "href": "2022/06/21/io-parte-1/index.html#julia",
    "title": "IO Parte 1",
    "section": "Julia",
    "text": "Julia\nEn Julia tenemos este librito online que va contando estupendamente como usar JuMP y las diferentes formas de utilizarlo.\n\n\nusing JuMP, GLPK\n\nm = Model(GLPK.Optimizer)\n## A JuMP Model\n## Feasibility problem with:\n## Variables: 0\n## Model mode: AUTOMATIC\n## CachingOptimizer state: EMPTY_OPTIMIZER\n## Solver name: GLPK\n\n# Variables y límites\n@variable(m, -100 <= x)\n## x\n@variable(m, y <= 100)\n## y\n\n# Objtivo\n@objective(m, Min, 7x + 8y)\n## 7 x + 8 y\n\n# restricciones\n@constraint(m, constraint1, 3x +  4y == 9)\n## constraint1 : 3 x + 4 y = 9.0\n@constraint(m, constraint2,  2x + 1y  >= 3)\n## constraint2 : 2 x + y ≥ 3.0\n\n# Solving the optimization problem\nJuMP.optimize!(m)\n\n# Printing the optimal solutions obtained\nprintln(\"Optimal Solutions:\")\n## Optimal Solutions:\nprintln(\"x = \", JuMP.value(x))\n## x = 0.5999999999999943\nprintln(\"y = \", JuMP.value(y))\n## y = 1.8000000000000114\nLa verdad que me ha gustado la sintaxis de JuMP , es casi un calco de como lo escribirías a mano. Como curiosidad fijaros en que se puede poner 3x +  4y == 9 , y no hace falta poner 3 * x  + 4 * y\nOtra curiosidad es que desde Julia puedes obtener el modelo en latex\nlatex_formulation(m)\n## $$ \\begin{aligned}\n## \\min\\quad & 7 x + 8 y\\\\\n## \\text{Subject to} \\quad & 3 x + 4 y = 9.0\\\\\n##  & 2 x + y \\geq 3.0\\\\\n##  & x \\geq -100.0\\\\\n##  & y \\leq 100.0\\\\\n## \\end{aligned} $$"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#python",
    "href": "2022/06/21/io-parte-1/index.html#python",
    "title": "IO Parte 1",
    "section": "Python",
    "text": "Python\nEn python usaremos pyomo, el cual también tiene una sintaxis clara.\nfrom pyomo.environ import *\n\nmodel = ConcreteModel()\nmodel.x = Var(domain=NonNegativeReals)\nmodel.y = Var(domain=NonNegativeReals)\n\nmodel.objetivo = Objective(expr = 7*model.x + 8*model.y, sense=minimize)\n\nmodel.constraint1 = Constraint(expr = 3* model.x +  4*model.y == 9)\nmodel.constraint2 = Constraint(expr = 2*model.x + 1*model.y  >= 3)\nresults = SolverFactory('glpk').solve(model)\nif results.solver.status == 'ok':\n    model.pprint()\n## 2 Var Declarations\n##     x : Size=1, Index=None\n##         Key  : Lower : Value : Upper : Fixed : Stale : Domain\n##         None :     0 :   0.6 :  None : False : False : NonNegativeReals\n##     y : Size=1, Index=None\n##         Key  : Lower : Value : Upper : Fixed : Stale : Domain\n##         None :     0 :   1.8 :  None : False : False : NonNegativeReals\n## \n## 1 Objective Declarations\n##     objetivo : Size=1, Index=None, Active=True\n##         Key  : Active : Sense    : Expression\n##         None :   True : minimize : 7*x + 8*y\n## \n## 2 Constraint Declarations\n##     constraint1 : Size=1, Index=None, Active=True\n##         Key  : Lower : Body      : Upper : Active\n##         None :   9.0 : 3*x + 4*y :   9.0 :   True\n##     constraint2 : Size=1, Index=None, Active=True\n##         Key  : Lower : Body    : Upper : Active\n##         None :   3.0 : 2*x + y :  +Inf :   True\n## \n## 5 Declarations: x y objetivo constraint1 constraint2\nprint('Objetivo = ', model.objetivo())\n## Objetivo =  18.6\nprint('\\nDecision Variables')\n## \n## Decision Variables\nprint('x = ', model.x())\n## x =  0.6\nprint('y = ', model.y())\n## y =  1.8"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#miscelánea",
    "href": "2022/06/21/io-parte-1/index.html#miscelánea",
    "title": "IO Parte 1",
    "section": "Miscelánea",
    "text": "Miscelánea\nLa investigación operativa es un campo muy amplio, desde problemas de asignación de turnos, optimizar contenedores en grandes buques que pasan por varios puertos, optimización de gasto en medios para maximizar ventas (Marketing Mix Modelling), problemas de localización (dónde poner una tienda o gasolinera nueva dada una demanda existente y competencia), problemas de grafos, etc.\nHay muchos investigadores que se están dedicando a resolver este tipo de cosas, y dónde no se trata tanto de usar tal o cual solver, sino de formular bien el problema, ya que diferentes formulaciones del mismo problema igualmente válidas dan lugar a tiempos de cómputo muy diferentes. No es de extrañar que se tarden muchas horas en encontrar soluciones a problemas determinados.\nPor otro lado, no puedo dejar de señalar el problema de los incentivos perversos. Me explico, a los investigadores que están en estos temas se les valora por paper publicado en revista de alto impacto, por lo que una vez le han publicado un artículo pasan al siguiente, olvidando la necesaria transferencia entre universidad y empresa. No es culpa suya, el sistema funciona así. Así que animo a todos aquellos que se dedican en serio a estos temas a meter la patita en el mundo de la empresa. Hay mucho trabajo que hacer y dinero que ganar."
  },
  {
    "objectID": "2022/02/06/pluralista/index.html",
    "href": "2022/02/06/pluralista/index.html",
    "title": "Pluralista",
    "section": "",
    "text": "Ando viendo los vídeos de Richard McElreath , Statistical Rethinking 2022 y ciertamente me están gustando mucho. En la segunda edición de su libro hace hincapié en temas de inferencia causal. Cuenta bastante bien todo el tema de los “confounders”, “forks”, “colliders” y demás. Además lo hace simulando datos, por lo que entiende todo de forma muy sencilla. Un par de conceptos que me han llamado la atención son por ejemplo cuando dice que condicionar por una variable no significa lo mismo en un modelo de regresión al uso que en uno bayesiano, en el segundo caso significa incluir esa variable en la distribución conjunta. Esto permite por ejemplo que bajo el marco de un modelo bayesiano se pueda condicionar incluso por un “collider” cosa que los entendidos de la inferencia causal prohíben expresamente pues eso abre un camino no causal en el DAG definido.\nSegún la RAE , pluralismo significa\ny en los videos se toma dicha postura, por ejemplo, se especifica el modelo teórico utilizando los diagramas causales y el Back door criterio para ver sobre qué variables hay que condicionar o no , para ver el efecto total de X sobre Y o para estimar el efecto directo.\nHay un ejemplo muy bueno en este post de Richard.\nNota: Este post es simplemente para entender un poco el post de Richard, el mérito es totalmente de él.\nBásicamente es una situación dónde se quiere estimar el efecto que tiene sobre el número de hijos que tiene una mujer, el número de hijos que tuvo su madre. En el diagrama causal también se indica la influencia que tiene el orden de nacimiento de de la madre y de la hija.\nDiagrama causal:\nSi queremos estimar el efecto global o el directo de M sobre D, habría que condicionar por U (siguiendo el backdoor criterio), y al ser no observable, no se puede estimar.\n¿Cómo podemos “estimar” el efecto de M sobre D dado que no podemos condicionar (en el sentido clásico) sobre U? .\nRichard propone lo que el llama “full luxury bayesian” que consiste en estimar a la vez todo el DAG y luego generar simulaciones usando la distribución conjunta obtenida para medir el efecto de la “intervención” y poder obtener el efecto causal.\nNótese que cuando en el DAG las relaciones se pueden expresar como modelos lineales, se puede estimar todo el DAG usando técnicas como los modelos de ecuaciones estructurales o el path analysis."
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#simulación",
    "href": "2022/02/06/pluralista/index.html#simulación",
    "title": "Pluralista",
    "section": "Simulación",
    "text": "Simulación\nSimulamos unos datos de forma qué vamos a conocer la “verdad” de la relaciones entre variables, que para eso simulamos.\n\n\nset.seed(1908)\nN <- 1000 # número de pares, 1000 madres y 1000 hijas\n\n\nU <- rnorm(N,0,1) # Simulamos el confounder\n\n# orden de nacimiento y \nB1 <- rbinom(N,size=1,prob=0.5)  # 50% de madres nacieeron en primer lugar\nM <- rnorm( N , 2 * B1 + U )\n\nB2 <- rbinom(N,size=1,prob=0.5) # 50% son las primogénitas\nD <- rnorm( N , 2  *B2 + U + 0 * M )\n\nEn esta simulación se ha forzado que el efecto del número de hijos de la madre sobre el núemro de hijos de la hija sea nulo. Por tanto sabemos que el efecto de M sobre D es 0..\nSi hacemos un modelo sin condicionar, tenemos sesgo\n\nlm(D ~ M)\n#> \n#> Call:\n#> lm(formula = D ~ M)\n#> \n#> Coefficients:\n#> (Intercept)            M  \n#>      0.7108       0.2930\n\nCondicionando por B1 también, de hecho tenemos la situación de amplificación del sesgo\n\nlm(D ~ M + B1)\n#> \n#> Call:\n#> lm(formula = D ~ M + B1)\n#> \n#> Coefficients:\n#> (Intercept)            M           B1  \n#>      1.0356       0.4606      -1.0441\n\n\nlm(D ~ M + B1 +B2 )\n#> \n#> Call:\n#> lm(formula = D ~ M + B1 + B2)\n#> \n#> Coefficients:\n#> (Intercept)            M           B1           B2  \n#>    -0.01621      0.46913     -0.91307      2.01487\n\n\nlm(D ~ M + B2)\n#> \n#> Call:\n#> lm(formula = D ~ M + B2)\n#> \n#> Coefficients:\n#> (Intercept)            M           B2  \n#>     -0.3204       0.3231       2.0550\n\nEn esta situación, no podemos estimar el efecto de M sobre D utilizando un solo modelo.\nUna forma de estimar el efecto de M sobre D es tirar de path analysis, que en este caso se puede al ser las relaciones lineales.\nSea:\n\nb: Efecto de B1 sobre M\nm: Efecto de M sobre D\n\nSe tiene que\n\\[Cov(B1, D ) = b\\cdot m \\cdot Var(B1)\\] Y como\n\\[b = \\dfrac{Cov(B1,M)}{Var(B1)} \\]\nPodemos estimar \\(m\\) como\n\\[m = \\dfrac{Cov(B1,D)}{b \\cdot Var(B1)} = \\dfrac{Cov(B1,D)}{Cov(B1,M)} \\] Y\n\n(m_hat = cov(B1,D) / cov(B1,M))\n#> [1] -0.0563039\n\ny esta estimación está menos sesgada, antes era del orden de 0.1 o 0.2 y ahora la estimación es del orden 0.01. Pero con esta estimación no tenemos información de su distribución sino sólo de esta estimación puntual. Y si las relaciones no fueran lineales no podría usarse, en cambio la siguiente aproximación si funciona"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#full-luxury-bayes",
    "href": "2022/02/06/pluralista/index.html#full-luxury-bayes",
    "title": "Pluralista",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nUtilizamos la librería de Richard rethinking y también cmdstanr para expresar el modelo causal completo y ajustarlo con Stan.\nAhora estimamos el DAG completo, aquí es dónde es diferente de la aproximación causal de Pearl, de esta forma podemos “condicionar” incluso por los colliders, porque condicionar en este marco significa meter esa información dentro de la distribución conjunta.\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\n# No metemos U al ser no observable\ndat <- list(\n  N = N,\n  M = M,\n  D = D,\n  B1 = B1,\n  B2 = B2\n)\nset.seed(1908)\n\nflbi <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 + k*U[i],\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M + k*U[i],\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n    # unmeasured confound\n    vector[N]:U ~ normal(0,1),\n    # priors\n    c(a1,a2,b,m) ~ normal( 0 , 0.5 ),\n    c(k,sigma,tau) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n#> Running MCMC with 4 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 4 finished in 17.1 seconds.\n#> Chain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 3 finished in 17.5 seconds.\n#> Chain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 1 finished in 17.6 seconds.\n#> Chain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 2 finished in 17.9 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 17.6 seconds.\n#> Total execution time: 18.0 seconds.\n\n\npost <- extract.samples(flbi)\nprecis(flbi)\n#>             mean         sd        5.5%      94.5%      n_eff     Rhat4\n#> m     0.01029393 0.04047150 -0.05430275 0.07409054   890.4789 1.0018319\n#> b     1.98864256 0.05811342  1.89487725 2.08168275  3042.1496 1.0007825\n#> a2    0.02836364 0.07310509 -0.08729441 0.14659932  1335.7564 1.0027553\n#> a1    0.06834714 0.05394291 -0.01785119 0.15621615  3747.6036 1.0007151\n#> tau   0.98041340 0.03617180  0.92352906 1.03914055  2745.6394 1.0007016\n#> sigma 1.07286195 0.05249312  0.98902178 1.15557275   909.1510 1.0015771\n#> k     0.98545837 0.05638324  0.89516212 1.07461055   828.2759 1.0013545\n#> p     0.48012384 0.01111044  0.46233213 0.49808700 16117.4910 0.9996995\n\nVemos que no aparece la estimación de U, pero en la posterior se ha estimado un valor de U para cada uno de las observaciones. 1000 observaciones y\n\n\ndim(post$U)\n#> [1] 8000 1000\npost$U[1:4, 1:5]\n#>          [,1]       [,2]       [,3]      [,4]        [,5]\n#> [1,] 0.382053 -0.4087260 -1.5231200  0.486262  0.00381229\n#> [2,] 0.733459  0.2971870  0.0441252 -1.424160 -0.98469300\n#> [3,] 0.254011 -0.0201819 -0.7499000 -1.444340 -0.51772400\n#> [4,] 0.468004  0.1923180 -1.8694400 -1.314570 -0.43902700"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#efecto-de-m-sobre-d.",
    "href": "2022/02/06/pluralista/index.html#efecto-de-m-sobre-d.",
    "title": "Pluralista",
    "section": "Efecto de M sobre D.",
    "text": "Efecto de M sobre D.\nEste era el efecto que queríamos obtener y el cuál no podíamos estimar al no poder condicionar sobre U. Aquí es tan sencillo como ver su distribución a posteriori.\n\n\nquantile(post$m)\n#>          0%         25%         50%         75%        100% \n#> -0.14804300 -0.01706778  0.01080760  0.03813360  0.15027000\n\nggplot() +\n  geom_density(aes(post$m)) + \n  labs(title = \"Efecto directo de M sobre D\", \n       x = \"m\")\n\n\n\n\n\n\n\n\n\nEfecto de B1 sobre D\nComo ya sabíamos, al haber simulado los datos de forma que las relaciones entre las variables sean lineales, el efecto de B1 sobre D no es más que el efecto de B1 sobre M multiplicado por el efecto de M sobre D.\nUtilizando la distribución a posteriori.\n\n# Efecto de B1 sobre D \nquantile( with(post,b*m) )\n#>          0%         25%         50%         75%        100% \n#> -0.27728158 -0.03379382  0.02148739  0.07658919  0.30614657\n\nggplot() +\n  geom_density(aes(post$b * post$m))+\n  labs(title = \"Efecto de B1 sobre D\", \n       x = \"b1 x m\")\n\n\n\n\n\n\n\n\n\n\nEfecto de B1 sobre D, simulando\nTal y como dice en su curso, el efecto causal puede ser visto como hacer una intervención supuesto cierto el modelo causal.\nSimplemente utilizamos las posterioris obtenidas y vamos simulando , en primer lugar B1 = 0 y simulamos qué M se obtendría, y lo hacemos también para B1 = 1 y restamos para obtener el efecto causal, que coindice con b * m\n\n# \n\n# B1 = 0\n# B1 -> M\nM_B1_0 <- with( post , a1 + b*0 + k*0 )\n# M -> D\nD_B1_0 <- with( post , a2 + b*0 + m*M_B1_0 + k*0 )\n\n# now same but with B1 = 1\nM_B1_1 <- with( post , a1 + b*1 + k*0 )\nD_B1_1 <- with( post , a2 + b*0 + m*M_B1_1 + k*0 )\n\n# difference to get causal effect\nd_D_B1 <- D_B1_1 - D_B1_0\nquantile(d_D_B1)\n#>          0%         25%         50%         75%        100% \n#> -0.27728158 -0.03379382  0.02148739  0.07658919  0.30614657\n\nPues como dice el título , ser pluralista no está tan mal, puedes usar el DAG y el backdoor criterio para entender qué variables ha de tener en cuenta para estimar tu efecto causal, y a partir de ahí podrías usar el “full luxury bayesian” en situaciones más complicadas."
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html",
    "title": "Mediator. Full luxury bayes",
    "section": "",
    "text": "Continuando con la serie sobre cosas de inferencia causal y full luxury bayes, antes de que empiece mi amigo Carlos Gil, y dónde seguramente se aprenderá más.\nEste ejemplo viene motivado precisamente por una charla que tuve el otro día con él.\nSea el siguiente diagrama causal\nSe tiene que z es un mediador entre x e y, y la teoría nos dice que si quiero obtener el efecto directo de x sobre y he de condicionar por z , y efectivamente, así nos lo dice el backdoor criterio. Mientras que si quiero el efecto total de x sobre y no he de condicionar por z."
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#datos-simulados",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#datos-simulados",
    "title": "Mediator. Full luxury bayes",
    "section": "Datos simulados",
    "text": "Datos simulados\n\n\nset.seed(155)\nN <- 1000 \n\nx <- rnorm(N, 2, 1) \nz <- rnorm(N, 4+ 4*x , 2 ) # a z le ponemos más variabilidad, pero daría igual\ny <- rnorm(N, 2+ 3*x + z, 1)\n\nEfecto total de x sobre y \nTal y como hemos simulado los datos, se esperaría que el efecto total de x sobre y fuera\n\\[ \\dfrac{cov(x,y)} {var(x)} \\approx 7 \\]\nY qué el efecto directo fuera de 3\nEfectivamente\nEfecto total\n\n# efecto total\nlm(y~x)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>       5.881        7.112\n\n\n# efecto directo\nlm(y~x+z)\n#> \n#> Call:\n#> lm(formula = y ~ x + z)\n#> \n#> Coefficients:\n#> (Intercept)            x            z  \n#>      2.0318       3.0339       0.9945"
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#full-luxury-bayes",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#full-luxury-bayes",
    "title": "Mediator. Full luxury bayes",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nHagamos lo mismo pero estimando el dag completo\n\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\ndat <- list(\n  N = N,\n  x = x,\n  y = y,\n  z = z\n)\nset.seed(1908)\n\nflbi <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a0,\n    z ~ normal( mu , sigma ),\n    \n    mu <- a1 + bx * x,\n   \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx2 * x + bz * z,\n\n    # priors\n    \n    c(a0,a1,a2,bx,bx2, bz) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n#> Running MCMC with 10 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 finished in 10.6 seconds.\n#> Chain 3 finished in 10.6 seconds.\n#> Chain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 8 finished in 11.0 seconds.\n#> Chain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 6 finished in 11.5 seconds.\n#> Chain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 5 finished in 12.4 seconds.\n#> Chain 9 finished in 12.4 seconds.\n#> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 finished in 12.9 seconds.\n#> Chain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 7 finished in 13.3 seconds.\n#> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 finished in 13.6 seconds.\n#> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 4 finished in 13.9 seconds.\n#> \n#> All 10 chains finished successfully.\n#> Mean chain execution time: 12.2 seconds.\n#> Total execution time: 14.0 seconds.\n\nY recuperamos los coeficientes y varianzas\n\n\nprecis(flbi)\n#>            mean         sd      5.5%    94.5%     n_eff     Rhat4\n#> bz    1.0131931 0.01557219 0.9883466 1.038241  8226.042 1.0005919\n#> bx2   2.9610334 0.07098661 2.8447278 3.073501  9640.122 1.0004040\n#> bx    4.1535508 0.06180592 4.0552767 4.252771  8753.661 1.0001456\n#> a2    1.9440850 0.09315785 1.7952495 2.093027 11266.267 1.0001373\n#> a1    3.7089914 0.13578637 3.4916007 3.926010  8709.448 1.0002893\n#> a0    1.9857103 0.03108199 1.9357773 2.035712 16252.310 0.9997523\n#> k     0.9719412 0.02166791 0.9378896 1.007410 15276.158 0.9999036\n#> tau   0.9859183 0.02216068 0.9512944 1.021932 15735.140 1.0001540\n#> sigma 1.9636543 0.04382554 1.8950489 2.034130 15855.098 1.0000898\n\n\n# extraemos 10000 muestras de la posteriori \npost <- extract.samples(flbi, n = 10000) \n\nY el efecto directo de x sobre y sería\n\nquantile(post$bx2, probs = c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 2.823659 2.962330 3.099855\n\nEn este ejemplo sencillo, podríamos estimar el efecto causal de x sobre y simplemente sumando las posterioris\n\nquantile(post$bx + post$bx2, c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 6.928046 7.114180 7.299252\n\nTambién podríamos obtener el efecto causal total de x sobre y simulando una intervención. En este caso, al ser la variable continua, lo que queremos obtener como de diferente es y cuando \\(X = x_i\\) versus cuando \\(X = x_i+1\\).\nSiempre podríamos ajustar otro modelo bayesiano dónde no tuviéramos en cuenta a z y obtendríamos la estimación de ese efecto total de x sobre y, pero siguiendo a Rubin y Gelman, la idea es incluir en nuestro modelo toda la información disponible. Y tal y como dice Richard McElreath , Statistical Rethinking 2022, el efecto causal se puede estimar simulando la intervención.\nAsí que el objetivo es dado nuestro modelo que incluye la variable z, simular la intervención cuando \\(X = x_i\\) y cuando \\(X = x_i+1\\) y una estimación del efecto directo es la resta de ambas distribuciones a posteriori de y.\nCreamos función para calcular el efecto de la intervención y_do_x\n\nget_total_effect <- function(x_value = 0, incremento = 0.5) {\n  n <- length(post$bx)\n  with(post, {\n    # simulate z, y  for x= x_value\n    z_x0 <- rnorm(n, a1 + bx * x_value  , sigma)\n    y_x0 <- rnorm(n, a2  + bz * z_x0 + bx * x_value , tau)\n    \n    # simulate z,y for x= x_value +1 \n    z_x1 <- rnorm(n, a1 + bx * (x_value + incremento)  , sigma)\n    y_x1 <- rnorm(n, a2  + bz * z_x1 + bx2 * (x_value + incremento) , tau)\n    \n    \n    # compute contrast\n    y_do_x <- (y_x1 - y_x0) / incremento\n    return(y_do_x)\n  })\n  \n}\n\nDado un valor de x, podemos calcular el efecto total\n\ny_do_x_0_2 <- get_total_effect(x_value = 0.2) \n\nquantile(y_do_x_0_2)\n#>         0%        25%        50%        75%       100% \n#> -15.324628   2.395551   6.702379  10.987520  28.909002\n\nPodríamos hacer lo mismo para varios valores de x\n\nx_seq <- seq(-0.5, 0.5, length = 1000)\ny_do_x <-\n  mclapply(x_seq,  function(lambda)\n    get_total_effect(x_value = lambda))\n\nY para cada uno de estos 1000 valores tendría 10000 valores de su efecto total de x sobre y.\n\nlength(y_do_x[[500]])\n#> [1] 10000\n\nhead(y_do_x[[500]])\n#> [1]  3.055107  4.988595  5.030397 12.469616 14.944735 22.881773\n\nCalculamos los intervalos de credibilidad del efecto total de x sobre y para cada valor de x.\n\n# lo hacemos simplemente por cuantiles, aunque podríamos calcular el hdi también, \n\nintervalos_credibilidad <-  mclapply( y_do_x, function(x) quantile(x, probs = c(0.025, 0.5, 0.975)))\n\n# Media e intervalor de credibilidad para el valor de x_seq en la posición 500 \nmean(y_do_x[[500]])\n#> [1] 7.278184\nintervalos_credibilidad[[500]]\n#>      2.5%       50%     97.5% \n#> -4.867513  7.337732 19.761850\n\nintervalo de predicción clásico con el lm\nHabría que calcular la predicción de y para un valor de x y para el de x + 1, podemos calcular los intervalos de predicción clásicos parea cada valor de x, pero no para la diferencia ( al menos con la función predict)\n\n\nmt_lm <- lm(y~x)\npredict(mt_lm, newdata = list(x= x_seq[[500]]), interval  = \"prediction\")\n#>        fit      lwr     upr\n#> 1 5.877439 1.578777 10.1761\npredict(mt_lm, newdata = list(x= x_seq[[500]] +1), interval  = \"prediction\")\n#>        fit      lwr     upr\n#> 1 12.98993 8.698051 17.2818\n\nPero como sería obtener el intervalo de credibilidad para la media de los efectos totales?\nCalculando para cada valor de x la media de la posteriori del efecto global y juntando todas las medias.\n\nslopes_mean <- lapply(y_do_x, mean)\n\nquantile(unlist(slopes_mean), c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 6.023455 7.180465 8.326128\n\nQue tiene mucha menos variabilidad que el efecto global en un valor concreto de x, o si juntamos todas las estimaciones\n\nquantile(unlist(y_do_x),  c(0.025, 0.5, 0.975))\n#>      2.5%       50%     97.5% \n#> -5.219611  7.168985 19.562729\n\nEvidentemente, podríamos simplemente no haber tenido en cuenta la variable z en nuestro modelo bayesiano.\n\nflbi_2 <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a1,\n    \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx * x ,\n    \n    # priors\n    \n    c(a1,a2,bx) ~ normal( 0 , 0.5 ),\n    c(tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n#> Running MCMC with 10 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 5 finished in 1.1 seconds.\n#> Chain 6 finished in 1.1 seconds.\n#> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 1 finished in 1.4 seconds.\n#> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 finished in 1.4 seconds.\n#> Chain 4 finished in 1.5 seconds.\n#> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 finished in 1.6 seconds.\n#> Chain 8 finished in 1.5 seconds.\n#> Chain 9 finished in 1.5 seconds.\n#> Chain 10 finished in 1.4 seconds.\n#> Chain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 finished in 1.6 seconds.\n#> \n#> All 10 chains finished successfully.\n#> Mean chain execution time: 1.4 seconds.\n#> Total execution time: 1.8 seconds.\n\nY obtenemos directamente el efecto total de x sobre y.\n\nprecis(flbi_2)\n#>          mean         sd      5.5%    94.5%    n_eff     Rhat4\n#> bx  7.1948083 0.06787458 7.0863389 7.302933 10446.14 1.0001211\n#> a2  5.6085615 0.14948341 5.3685467 5.848723 10598.69 1.0001680\n#> a1  1.9860307 0.03058033 1.9368689 2.034631 14116.13 1.0004122\n#> k   0.9721054 0.02159773 0.9382407 1.006761 15044.78 0.9995902\n#> tau 2.1898100 0.04953736 2.1120600 2.269831 14995.33 0.9999084\n\n\npost2 <- extract.samples(flbi_2,  n = 10000)\n\nquantile(post2$bx, probs = c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 7.061389 7.194525 7.327361"
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#pensamientos-finales",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#pensamientos-finales",
    "title": "Mediator. Full luxury bayes",
    "section": "Pensamientos finales",
    "text": "Pensamientos finales\n\nParece que no es tan mala idea incluir en tu modelo bayesiano toda la información disponible.\nSer pluralista es una buena idea, usando el backdoor criterio dado que nuestro dag sea correcto, nos puede llevar a un modelo más simple y fácil de estimar.\nComo dije en el último post, estimar todo el dag de forma conjunta puede ser útil en varias situaciones.\nYa en 2009 se hablaba de esto aquí"
  },
  {
    "objectID": "2022/02/09/collider-bias/index.html",
    "href": "2022/02/09/collider-bias/index.html",
    "title": "Collider Bias?",
    "section": "",
    "text": "Continuando con temas del post anterior. Dice Pearl, con buen criterio, que si condicionas por un collider abres ese camino causal y creas una relación espuria entre las dos variables “Tratamiento” y “Respuesta” y por lo tanto si condicionas por el collider, aparece un sesgo.\nHablando estilo compadre. Si Tratamiento -> Collider y Respuesta -> Collider, si condiciono en el Collider, es decir, calculo la relación entre Tratamiento y Respuesta para cada valor de C, se introduce un sesgo.\nSi \\[C = 2\\cdot Tratamiento + 3 \\cdot respuesta\\]\nSi sé que C = 3, y que Tratamiento = 4 , ya hay relación entre Tratamiento y respuesta aunque sean independientes, porque ambos son causa de C.\nSimulemos, que es una buena forma de ver qué pasa si condiciono por el collider, siendo el tratamiento y la respuesta independientes.\nSi no ajusto por el collider, obtengo que no hay efecto del tratamiento , correcto\nCondicionando, aparece el sesgo\nRetomando el ejemplo del último post, pero en vez de tener una variable de confusión no observable, tenemos un collider.\nUsando la función adjustmentSets de dagitty nos dice sobre qué variables hay que condicionar si quiero el efecto causal total y directo de M sobre D, siguiendo las reglas de Pearl, ver por ejemplo (J. Pearl (2009), Causality: Models, Reasoning and Inference. Cambridge University Press.)\nSimulo unos datos dónde fuerzo a que no haya efecto causal de M a D.\nEn grafo sería\nY vemos lo de antes,\nVemos si hay efecto causal de M sobre D (uso modelos lineales por simplicidad).\nEl modelo correcto sería sin condicionar por el collider. Y bien, hace lo que debe, no hay efecto de M sobre D, tal y como sabemos que pasa en la realidad\nCondicionando ahora por el collider, tenemos sesgo.\nQueda como curiosidad que si condicionas por B1 en vez de por el collider también hay sesgo, pero si condicionas solo por B2, no hay."
  },
  {
    "objectID": "2022/02/09/collider-bias/index.html#full-luxury-bayes",
    "href": "2022/02/09/collider-bias/index.html#full-luxury-bayes",
    "title": "Collider Bias?",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nNo todos los DAg’s son tan sencillos como el que he puesto, hay veces en los que una misma variable puede ser a la vez un collider y una variable de confusión, porque puede haber varios path causales y tenga diferente rol. En esos casos, condicionar por el collider te abre un path, y si no condicionas te abre otro. Ante esas situaciones, y suponiendo que el dag es correcto, no se podría estimar el efecto causal.\nSin embargo, condicionar en la red bayesiana no significa lo mismo que condicionar en un sólo modelo, sino que significa que introduzco la información que me proporciona el collider en la distribución conjunta y que me obtenga la posteriori.\nAl estimar el DAG completo, usando Stan por ejemplo, se estima tanto el modelo para M, como para D de forma conjunta.\n\nModelo bayesiano sin condicionar por el collider\nFormulamos el modelo usando la librería rethinking y lo ajustamos usando la función ulam que por debajo llama a Stan\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\n\ndat <- list(\n  N = N,\n  M = M,\n  D = D,\n  B1 = B1,\n  B2 = B2, \n  C = C\n)\n\nset.seed(155)\n\nflbi <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 ,\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M ,\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n\n    # priors\n    c(a1,a2,b,m) ~ normal( 0 , 0.5 ),\n    c(k,sigma,tau) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n\nWarning in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 6, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 7, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 39, column 4 to column 29)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 39, column 4 to column 29)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 35, column 4 to column 27)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 finished in 2.2 seconds.\nChain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 finished in 2.3 seconds.\nChain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 finished in 2.4 seconds.\nChain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 finished in 2.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.4 seconds.\nTotal execution time: 2.6 seconds.\n\n\nVemos los parámetros estimados y sus intervalos de credibilidad y extraemos la posteriori\n\nprecis(flbi)\n\n              mean         sd         5.5%      94.5%    n_eff     Rhat4\nm     -0.008692293 0.02258036 -0.044472871 0.02734239 6673.459 1.0005257\nb      1.961391786 0.04356684  1.891207250 2.03241110 5380.781 0.9998034\na2     0.059745778 0.04370034 -0.009909845 0.12931849 5500.657 1.0000543\na1     0.026656585 0.03702030 -0.032118024 0.08601562 6410.710 0.9997482\ntau    0.984889552 0.02213892  0.949803835 1.02046055 7752.122 1.0000469\nsigma  0.967785086 0.02155725  0.933665955 1.00275110 8907.866 1.0001535\nk      0.998568923 1.01036660  0.056543283 2.88207040 9005.060 1.0000657\np      0.482860133 0.01114257  0.465039000 0.50049459 8082.865 1.0000378\n\npost <- extract.samples(flbi)\n\nPintamos la distribución a posteriori del efecto y cómo ya sabíamos, al no condicionar por el collider, se estima sin sesgo que no hay efecto causal de M a D.\n\nplot(bayestestR::hdi(post$m, ci = c( 0.80, 0.95))) +\n  labs(title = \"Estimación sin collider\")\n\n\n\n\n\n\n\n\n\n\nModelo condicionando por el collider\nYa sabemos que no es necesario de hecho condicionar por el collider, más aún, que hacerlo induce un sesgo en la estimación del efecto, ¿pero qué pasa si estimamos el dag al completo?\n\nset.seed(155)\n\nflbi_collider <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 ,\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M ,\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n    \n    # Collider\n    \n     C ~ normal( cmu , csigma ),\n     cmu <- a3 + cm * M  + cd * D,\n\n    # priors\n    c(a1,a2,a3,b,m, cm, cd) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, csigma) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n\nWarning in '/tmp/Rtmpsdysi4/model-8eef4e4ce260.stan', line 3, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/tmp/Rtmpsdysi4/model-8eef4e4ce260.stan', line 4, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 finished in 4.4 seconds.\nChain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 1 finished in 4.5 seconds.\nChain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 finished in 4.5 seconds.\nChain 3 finished in 4.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.5 seconds.\nTotal execution time: 4.6 seconds.\n\n\nViendo la distribución posterior de los parámetros resulta que hemos podido estimar el verdadero efecto causal de M sobre D (que sabemos que es 0), incluso aunque hayamos “condicionado” por el collider.\n\nprecis(flbi_collider)\n\n               mean         sd        5.5%      94.5%     n_eff     Rhat4\ncd      3.968243739 0.04611450  3.89560945 4.04327110  8248.105 1.0004150\ncm      2.934145831 0.04687289  2.85989505 3.00919165  8890.321 1.0001715\nm      -0.009309562 0.02249104 -0.04553421 0.02674973  8503.878 1.0000069\nb       1.962677924 0.04345328  1.89429945 2.03282660  7107.343 0.9999206\na3      0.160077627 0.09126337  0.01252729 0.30475624  7756.140 1.0006626\na2      0.060140941 0.04359355 -0.01003761 0.13036986  7499.209 0.9996584\na1      0.025057624 0.03711509 -0.03519740 0.08448685  7396.713 1.0005212\ncsigma  2.064163930 0.04708394  1.99060945 2.14121220 10185.107 1.0002583\ntau     0.984744502 0.02189981  0.95050602 1.01991110  8571.627 0.9998603\nsigma   0.968314337 0.02180198  0.93405345 1.00380110  9457.782 1.0001085\np       0.482931190 0.01117531  0.46538884 0.50083727 10342.731 0.9998451\n\npost_with_collider <- extract.samples(flbi_collider)\n\n\nquantile(post_with_collider$m)\n\n          0%          25%          50%          75%         100% \n-0.095140500 -0.024429525 -0.009353655  0.005972277  0.072001900 \n\nplot(bayestestR::hdi(post_with_collider$m, ci = c( 0.80, 0.95))) +\n  labs(title = \"Estimación con collider\")\n\n\n\n\n\n\n\n\nAsí, que siendo “pluralista”, estimar el dag completo nos puede ayudar en situaciones dónde el backdoor criterio nos diga que no se puede estimar el efecto causal."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html",
    "href": "2022/01/01/cocinando/index.html",
    "title": "Cocinando",
    "section": "",
    "text": "Lo primero, feliz año a todos (no me da la gana de poner todas y todes), y espero que este año sea mejor que el pasado.\nHoy voy a hablar un poco de la “cocina” electoral en los barómetros de opinión, pero de forma muy simplificada.\nUna de las primeras cosas que se hacía era comparar el recuerdo de voto declarado en la encuesta con el resultado real de las elecciones a las que hacía referencia.\nCuándo no coinciden una de las cosas que se hacían era imputar el recuerdo de voto para aquellos que no contestaron a la pregunta. Esto se hacía utilizando variables de la encuesta, típicamente variables de autoposición idelógica y similares.\nUna vez imputado el recuerdo de voto se comparaba de nuevo con el resultado real de las elecciones y si variaba se recurría a la ponderación por el recuerdo de voto real. Esto es, se estimaban unos pesos de forma que la distribución del recuerdo de voto en la encuesta fuera lo más similar posible a los resultados reales.\nEsta “reponderación” corre el riesgo de descalibrar la encuesta en otras variables, tales como sexo y edad, por poner un ejemplo. La solución podría ser postestratificar la muestra, pero para eso deberíamos saber los valores poblaciones en cada combinación de sexo, edad (posiblemente agrupada) y recuerdo de voto. Es decir, tener la distribución conjunta, lo cual implica tener por ejemplo todas las combinaciones de edad y partido al que votó en la muestra y también saber la distribución poblacional (en las elecciones consideradas). Evidentemente no siempre es posible tener tanta información, por lo que se opta por al menos ajustar las distribuciones marginales.\nPara obtener esos pasos se utiliza un procedimiento iterativo llamado raking\nPara ver como se haría esa parte de la “cocina” (lo de imputar los nulos en recuerdo de voto usando un modelo no lo voy a hacer), utilizando la librería survey de Thomas Lumley."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#fuentes-de-datos.",
    "href": "2022/01/01/cocinando/index.html#fuentes-de-datos.",
    "title": "Cocinando",
    "section": "Fuentes de datos.",
    "text": "Fuentes de datos.\n\nBarómetro del CIS, noviembre de 2021.\nResultados electorales oficiales utilizando la fantástica librería infoelectoral\nPadrón oficial de habitantes a 1 de Enero de 2020."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#obtención-datos",
    "href": "2022/01/01/cocinando/index.html#obtención-datos",
    "title": "Cocinando",
    "section": "Obtención datos",
    "text": "Obtención datos\n\nEncuesta CIS.\n\nlibrary(tidyverse)\nlibrary(infoelectoral)\nlibrary(magrittr) # \nlibrary(patchwork) #\n \ndf <- haven::read_sav(\"/home/jose/Rstudio_projects/raking_ejemplo/MD3340/3340.sav\")\ndf %<>%  ## change in place\n  rename_all(tolower)\n\n\n# Convierto a factor algunas variables para que pillen el label que \n# viene del fichero de spss. \n\ndf <- df %>% \n  mutate(across(.cols = c(ccaa, sexo, recuerdo, recuvotogr, \n                          intenciongalter, intenciong, intenciongr, \n                          intenciongalterr), .fns = as_factor))\n\n# categorizmaos la edad\ndf <- df %>%\n  mutate(\n    gedad =\n      case_when(\n        edad >= 100 ~ \"100 años y más\",\n        edad >= 18 & edad <= 24 ~ \"18-24 años\",\n        edad >= 25 & edad <= 29 ~ \"25-29 años\",\n        edad >= 30 & edad <= 34 ~ \"30-34 años\",\n        edad >= 35 & edad <= 39 ~ \"35-39 años\",\n        edad >= 40 & edad <= 44 ~ \"40-44 años\",\n        edad >= 45 & edad <= 49 ~ \"45-49 años\",\n        edad >= 50 & edad <= 54 ~ \"50-54 años\",\n        edad >= 55 & edad <= 59 ~ \"55-59 años\",\n        edad >= 60 & edad <= 64 ~ \"60-64 años\",\n        edad >= 65 & edad <= 69 ~ \"65-69 años\",\n        edad >= 70 & edad <= 74 ~ \"70-74 años\",\n        edad >= 75 & edad <= 79 ~ \"75-79 años\",\n        edad >= 80 & edad <= 84 ~ \"80-84 años\",\n        edad >= 85 & edad <= 89 ~ \"85-89 años\",\n        edad >= 90 & edad <= 94 ~ \"90-94 años\",\n        edad >= 95 & edad <= 99 ~ \"95-99 años\",\n       \n      )\n  )\n\n\ndf$gedad <- as.factor(df$gedad)\nhead(df)\n#> # A tibble: 6 × 254\n#>   estudio     registro  cues tipo_tel  ccaa     prov    mun      tamuni  entrev \n#>   <dbl+lbl>      <dbl> <dbl> <dbl+lbl> <fct>    <dbl+l> <dbl+lb> <dbl+l> <dbl+l>\n#> 1 3340 [3340]     3982     1 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 1 [Men… 0 [Ano…\n#> 2 3340 [3340]     8979     2 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 3 [10.… 0 [Ano…\n#> 3 3340 [3340]    11104     3 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 3 [10.… 0 [Ano…\n#> 4 3340 [3340]     4205     4 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 3 [10.… 0 [Ano…\n#> 5 3340 [3340]     1191     5 1 [Fijo]  Andaluc… 4 [Alm… 13 [Alm… 5 [100… 0 [Ano…\n#> 6 3340 [3340]     1203     6 2 [Móvil] Andaluc… 4 [Alm… 13 [Alm… 5 [100… 0 [Ano…\n#> # … with 245 more variables: capital <dbl+lbl>, sexo <fct>, edad <dbl+lbl>,\n#> #   p0 <dbl+lbl>, p1 <dbl+lbl>, p2 <dbl+lbl>, p3 <dbl+lbl>, p3_1_1 <dbl+lbl>,\n#> #   p3_1_2 <dbl+lbl>, p3_1_3 <dbl+lbl>, p3_1_4 <dbl+lbl>, p3_1_5 <dbl+lbl>,\n#> #   p3_1_6 <dbl+lbl>, p4 <dbl+lbl>, sersanientre <dbl+lbl>, p4anno <dbl+lbl>,\n#> #   p4mes <dbl+lbl>, p4a0_1_20 <dbl+lbl>, p4a0_2_21 <dbl+lbl>,\n#> #   servientre_1 <dbl+lbl>, servientre_2 <dbl+lbl>, servientre_3 <dbl+lbl>,\n#> #   servientre_4 <dbl+lbl>, servientre_5 <dbl+lbl>, servientre_7 <dbl+lbl>, …\n\nPodemos ver cuántos encuestados hay por cada sexo,edad, y recuerdo de voto, contando los datos brutos o los datos utilizando la ponderación que ha calculado el CIS.\n\ndf %>% \n  group_by(sexo, gedad, recuerdo) %>% \n  summarise(total = n(),\n            peso_tot = sum(peso))\n#> # A tibble: 509 × 5\n#> # Groups:   sexo, gedad [31]\n#>    sexo   gedad      recuerdo       total peso_tot\n#>    <fct>  <fct>      <fct>          <int>    <dbl>\n#>  1 Hombre 18-24 años PP                 7    6.82 \n#>  2 Hombre 18-24 años PSOE              17   18.1  \n#>  3 Hombre 18-24 años C's                2    2.11 \n#>  4 Hombre 18-24 años En Comú Podem      4    4.10 \n#>  5 Hombre 18-24 años Més Compromís      1    1.03 \n#>  6 Hombre 18-24 años EAJ-PNV            2    2.06 \n#>  7 Hombre 18-24 años Na+                1    0.563\n#>  8 Hombre 18-24 años VOX                4    4.13 \n#>  9 Hombre 18-24 años Unidas Podemos    13   13.7  \n#> 10 Hombre 18-24 años BNG                2    2.05 \n#> # … with 499 more rows\n\nPara normalizar las siglas usamos otra tabla de lookup\n\nsiglas_cis <-  read_csv(\"/home/jose/Rstudio_projects/raking_ejemplo/siglas_cis.csv\")\n\nsiglas_cis\n#> # A tibble: 28 × 2\n#>    recuerdo       key       \n#>    <chr>          <chr>     \n#>  1 No recuerda    abstencion\n#>  2 No votó        abstencion\n#>  3 C's            CIUDADANOS\n#>  4 PSOE           PSOE      \n#>  5 PP             PP        \n#>  6 N.C.           abstencion\n#>  7 VOX            VOX       \n#>  8 En blanco      abstencion\n#>  9 Unidas Podemos PODEMOS-IU\n#> 10 PACMA          PACMA     \n#> # … with 18 more rows\n\n\n\n# le pegamos las siglas normalizadas\ndf <- df %>% \n  left_join(siglas_cis) \n\n\n\n# Vemos los totales por recuerdo de voto , usando la ponderacińo del cis\ndf %>% \n  group_by(key) %>% \n  summarise(frq = sum(peso))\n#> # A tibble: 16 × 2\n#>    key           frq\n#>    <chr>       <dbl>\n#>  1 abstencion 1064. \n#>  2 BILDU        21.5\n#>  3 BNG          23.6\n#>  4 CIUDADANOS  239. \n#>  5 COMPROMIS    22.1\n#>  6 CUP          17.4\n#>  7 ERC          81.9\n#>  8 JXCAT        40.0\n#>  9 MAS PAIS     28.7\n#> 10 OTROS        62.4\n#> 11 PACMA        26.4\n#> 12 PNV          40.5\n#> 13 PODEMOS-IU  426. \n#> 14 PP          525. \n#> 15 PSOE        917. \n#> 16 VOX         244.\n\n# Vemos total de datos en la encuesta y comprobamos que la suma de las \n# ponderaciones coincide con el total de encuestados. \n\ndf %>% \n  summarise(n(), \n            sum(peso))\n#> # A tibble: 1 × 2\n#>   `n()` `sum(peso)`\n#>   <int>       <dbl>\n#> 1  3779       3779.\n\n# Convertimos a factor la variable con el recuerdo de voto normalizado\ndf$key <- as.factor(df$key)\n\nAhora lo que nos hace falta es saber los totales de recuerdo de voto, sexo y edad que debería haber tenido la encuesta.\n\n\nResultados electorales\n\n\n\ncongress_2019 <- municipios(tipo_eleccion = \"congreso\", anno = 2019, mes = \"11\")\n\n(votos_summary <-  congress_2019 %>%\n  group_by(codigo_ccaa, codigo_provincia,\n           codigo_municipio, municipio) %>%\n  summarise(\n    abstencion = first(censo_ine) -\n      first(votos_blancos) -\n      first(votos_nulos) -\n      first(votos_candidaturas),\n    censo_ine = first(censo_ine), \n    votos_blancos = first(votos_blancos),\n    votos_nulos = first(votos_nulos),\n    votos_candidaturas = first(votos_candidaturas)  ) %>%\n    ungroup() %>% \n  summarise(\n    abstencion = sum(abstencion, na.rm = TRUE) +\n      sum(votos_blancos, na.rm = TRUE) + \n      sum(votos_nulos, na.rm = TRUE),\n    censo_ine = sum(censo_ine, na.rm = TRUE), \n    votos_blancos = sum(votos_blancos, na.rm = TRUE),\n    votos_nulos = sum(votos_nulos, na.rm = TRUE),\n    votos_candidaturas = sum(votos_candidaturas, na.rm = TRUE)\n  ))\n#> # A tibble: 1 × 5\n#>   abstencion censo_ine votos_blancos votos_nulos votos_candidaturas\n#>        <dbl>     <dbl>         <dbl>       <dbl>              <dbl>\n#> 1   10973466  34870481        216249      248543           23897015\n\n# ponemos abstencion como partido\nabstencion <-  votos_summary %>%\n  select( abstencion) %>%\n  mutate(siglas = \"abstencion\", \n         denominacion = \"abstencion\", \n         codigo_partido = \"abstencion\") %>%  \n  rename(votos = abstencion)\n\n\n\n\nvotos_partidos <-  congress_2019 %>% \n  group_by(codigo_partido, siglas, denominacion) %>% \n  summarise(votos = sum(votos))\n\n\nvotos_final <- votos_partidos %>% \n  bind_rows(abstencion) %>% \n  bind_cols(votos_summary %>% \n               select( censo_ine)) %>% # debe ser pob > = 18 \n  ungroup() %>%\n    mutate(prop_voto = votos/censo_ine) %>% \n  arrange(-prop_voto) \n  \n\nDT::datatable(votos_final)\n\n\n\n\n\n\nComo las siglas de los partidos en la info oficial y las que vienen en la encuesta no están normalizadas, me construí una tabla de “lookup” para eso.\n\nsiglas_infoelectoral <- read_csv(\"/home/jose/Rstudio_projects/raking_ejemplo/siglas_infoelectoral.csv\")\n\nDT::datatable(siglas_infoelectoral)\n\n\n\n\n\n\n\n(votos_final_summary <- votos_final %>% \n  left_join(siglas_infoelectoral) %>% \n  group_by(key) %>% \n  summarise(prop_voto = sum(prop_voto, na.rm=TRUE)))\n#> # A tibble: 16 × 2\n#>    key        prop_voto\n#>    <chr>          <dbl>\n#>  1 abstencion   0.315  \n#>  2 BILDU        0.00793\n#>  3 BNG          0.00343\n#>  4 CIUDADANOS   0.0470 \n#>  5 COMPROMIS    0.00502\n#>  6 CUP          0.00702\n#>  7 ERC          0.0249 \n#>  8 JXCAT        0.0151 \n#>  9 MAS PAIS     0.0115 \n#> 10 OTROS        0.0309 \n#> 11 PACMA        0.00649\n#> 12 PNV          0.0108 \n#> 13 PODEMOS-IU   0.0732 \n#> 14 PP           0.144  \n#> 15 PSOE         0.194  \n#> 16 VOX          0.104\n\n\nvotos_final_summary$key <- as.factor(votos_final_summary$key)\n\n(pop_revoto <-  votos_final_summary %>% \n  mutate(Freq = prop_voto * sum(df$peso)) %>% \n  select(key, Freq ) )\n#> # A tibble: 16 × 2\n#>    key          Freq\n#>    <fct>       <dbl>\n#>  1 abstencion 1189. \n#>  2 BILDU        30.0\n#>  3 BNG          13.0\n#>  4 CIUDADANOS  177. \n#>  5 COMPROMIS    19.0\n#>  6 CUP          26.5\n#>  7 ERC          94.3\n#>  8 JXCAT        57.1\n#>  9 MAS PAIS     43.5\n#> 10 OTROS       117. \n#> 11 PACMA        24.5\n#> 12 PNV          40.9\n#> 13 PODEMOS-IU  276. \n#> 14 PP          544. \n#> 15 PSOE        732. \n#> 16 VOX         395.\n\nY vemos que no coincide mucho con la que hay en la encuesta\n\ndf %>% \n  group_by(key) %>% \n  summarise(Freq = sum(peso))\n#> # A tibble: 16 × 2\n#>    key          Freq\n#>    <fct>       <dbl>\n#>  1 abstencion 1064. \n#>  2 BILDU        21.5\n#>  3 BNG          23.6\n#>  4 CIUDADANOS  239. \n#>  5 COMPROMIS    22.1\n#>  6 CUP          17.4\n#>  7 ERC          81.9\n#>  8 JXCAT        40.0\n#>  9 MAS PAIS     28.7\n#> 10 OTROS        62.4\n#> 11 PACMA        26.4\n#> 12 PNV          40.5\n#> 13 PODEMOS-IU  426. \n#> 14 PP          525. \n#> 15 PSOE        917. \n#> 16 VOX         244.\n\nEn la encuesta hay más personas que recuerdan haber votado al psoe que las que debería haber, así como también hay menos que recuerdan haber votado a Mas País o a Vox. Había una hipótesis por ahi que decía que el votante de derechas está infrarrepresentado en las encuestas.\n\n\nPadrón\nEsto es solo un csv con la info de población por sexo y edad exacta (quité los menores de 18 años)\nLeemos el csv, que lamentablemente viene en encoding de windows y con separador decimal y tal.\n\npadron <- read_delim(\n  \"/home/jose/Rstudio_projects/raking_ejemplo/pad_2021.csv\",\n  delim = \";\",\n  escape_double = FALSE,\n  locale = locale(\n    date_names = \"es\",\n    decimal_mark = \",\",\n    grouping_mark = \".\",\n    encoding = \"WINDOWS-1252\"\n  ),\n  trim_ws = TRUE\n)\n\nCategorizamos la edad y vemos cuales habrían sido las frecuencias de edad en la encuestas si fueran representativas de la estructura de población del padrón de 2020\n\npop_edad <- padron %>% \n  mutate(\n    gedad =\n      case_when(\n        edad >= 100 ~ \"100 años y más\",\n        edad >= 18 & edad <= 24 ~ \"18-24 años\",\n        edad >= 25 & edad <= 29 ~ \"25-29 años\",\n        edad >= 30 & edad <= 34 ~ \"30-34 años\",\n        edad >= 35 & edad <= 39 ~ \"35-39 años\",\n        edad >= 40 & edad <= 44 ~ \"40-44 años\",\n        edad >= 45 & edad <= 49 ~ \"45-49 años\",\n        edad >= 50 & edad <= 54 ~ \"50-54 años\",\n        edad >= 55 & edad <= 59 ~ \"55-59 años\",\n        edad >= 60 & edad <= 64 ~ \"60-64 años\",\n        edad >= 65 & edad <= 69 ~ \"65-69 años\",\n        edad >= 70 & edad <= 74 ~ \"70-74 años\",\n        edad >= 75 & edad <= 79 ~ \"75-79 años\",\n        edad >= 80 & edad <= 84 ~ \"80-84 años\",\n        edad >= 85 & edad <= 89 ~ \"85-89 años\",\n        edad >= 90 & edad <= 94 ~ \"90-94 años\",\n        edad >= 95 & edad <= 99 ~ \"95-99 años\",\n        \n      )\n  ) %>% \n  group_by(gedad) %>% \n  summarise(pob = sum(total)) %>% \n  ungroup() %>%\n  mutate(pct = pob/sum(pob)) %>% \n  mutate(Freq = pct* sum(df$peso)) %>% \n  select(gedad, Freq) %>% \n  filter(gedad!=\"100 años y más\")\n\npop_sexo <-   padron %>% \n  mutate(sexo = ifelse(sexo == \"Hombres\", \"Hombre\", \"Mujer\")) %>% \n  group_by(sexo) %>% \n  summarise(pob = sum(total)) %>% \n  ungroup() %>%\n  mutate(pct = pob/sum(pob)) %>% \n  mutate(Freq = pct* sum(df$peso)) %>% \n  select(sexo, Freq)\n\npop_sexo$sexo <- as.factor(pop_sexo$sexo)\n\npop_edad$gedad <- as.factor(pop_edad$gedad)\n\n\npop_edad\n#> # A tibble: 16 × 2\n#>    gedad       Freq\n#>    <fct>      <dbl>\n#>  1 18-24 años 352. \n#>  2 25-29 años 269. \n#>  3 30-34 años 213. \n#>  4 35-39 años 313. \n#>  5 40-44 años 377. \n#>  6 45-49 años 381. \n#>  7 50-54 años 356. \n#>  8 55-59 años 362. \n#>  9 60-64 años 286. \n#> 10 65-69 años 240. \n#> 11 70-74 años 191. \n#> 12 75-79 años 187. \n#> 13 80-84 años 123. \n#> 14 85-89 años  77.9\n#> 15 90-94 años  41.5\n#> 16 95-99 años  11.1\n\nY veamos si se parece a lo que hay en la encuesta\n\ndf %>% \n  group_by(gedad) %>% \n  summarise(Freq = sum(peso))\n#> # A tibble: 16 × 2\n#>    gedad        Freq\n#>    <fct>       <dbl>\n#>  1 18-24 años 235.  \n#>  2 25-29 años 214.  \n#>  3 30-34 años 214.  \n#>  4 35-39 años 268.  \n#>  5 40-44 años 402.  \n#>  6 45-49 años 382.  \n#>  7 50-54 años 392.  \n#>  8 55-59 años 336.  \n#>  9 60-64 años 334.  \n#> 10 65-69 años 375.  \n#> 11 70-74 años 282.  \n#> 12 75-79 años 177.  \n#> 13 80-84 años 114.  \n#> 14 85-89 años  40.0 \n#> 15 90-94 años  12.8 \n#> 16 95-99 años   1.05\n\nPues en la encuesta ha caído más gente joven de la que debería, cosas que pasan."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#raking",
    "href": "2022/01/01/cocinando/index.html#raking",
    "title": "Cocinando",
    "section": "Raking",
    "text": "Raking\nPues ya tenemos todo para hacer el ejercicio simple de raking.\nImportamos la librería survery, comprobamos que los niveles de las variables que vamos a considerar en el raking son los mismos en los datos de las encuestas y en los dataframes auxiliares\n\n\n# Comprobamos niveles\nall.equal(levels(pop_revoto$key) , levels(df$key))\n#> [1] TRUE\nall.equal(levels(pop_edad$gedad) , levels(df$gedad))\n#> [1] TRUE\nall.equal(levels(pop_sexo$sexo),  levels(df$sexo))\n#> [1] TRUE\n\nConstruimos un diseño muestral inicial utilizando los pesos que facilita el CIS.\n\nlibrary(survey)\n\ndisenno <- svydesign(id=~1, weight=~peso,data=df)\n\nVemos los totales por recuerdo de voto por ejemplo, con la estimación de su error estándar\n\nsvytotal(~key, disenno)\n#>                  total      SE\n#> keyabstencion 1064.301 28.1710\n#> keyBILDU        21.475  4.4926\n#> keyBNG          23.630  4.9128\n#> keyCIUDADANOS  238.593 15.1738\n#> keyCOMPROMIS    22.099  4.8096\n#> keyCUP          17.429  4.2181\n#> keyERC          81.939  9.0654\n#> keyJXCAT        39.984  6.3702\n#> keyMAS PAIS     28.734  5.4132\n#> keyOTROS        62.353  7.5990\n#> keyPACMA        26.359  5.2625\n#> keyPNV          40.515  6.4151\n#> keyPODEMOS-IU  425.598 19.7388\n#> keyPP          525.044 21.6608\n#> keyPSOE        917.318 26.7850\n#> keyVOX         244.059 15.4552\n\nPara hacer el raking utilizamos la funcion rake que toma argumentos el diseño muestral original, una lista con el nombre de las variables (en formula) en la encuesta , y una lista con los dataframes auxiliares cada uno con dos columnas, la variable que se corresponde con la de la encuesta y una columna numérica con el valor de cuántos individuos habría de haber en la muestra para que la distribución fuera igual a la de la población. Otros parámetros, serían el número de iteraciones máximas y el criterio de parada (epsilon) del procedimiento iterativo.\n\n\nponderacion_1 <- \n  rake (\n  design             = disenno,\n  sample.margins     = list(~gedad, ~key, ~sexo), \n  population.margins = list(pop_edad, pop_revoto, pop_sexo)\n  ) \n\nY ahora podemos comprobar qué tal lo ha hecho\nEdad\n\npop_edad # dist poblacional\n#> # A tibble: 16 × 2\n#>    gedad       Freq\n#>    <fct>      <dbl>\n#>  1 18-24 años 352. \n#>  2 25-29 años 269. \n#>  3 30-34 años 213. \n#>  4 35-39 años 313. \n#>  5 40-44 años 377. \n#>  6 45-49 años 381. \n#>  7 50-54 años 356. \n#>  8 55-59 años 362. \n#>  9 60-64 años 286. \n#> 10 65-69 años 240. \n#> 11 70-74 años 191. \n#> 12 75-79 años 187. \n#> 13 80-84 años 123. \n#> 14 85-89 años  77.9\n#> 15 90-94 años  41.5\n#> 16 95-99 años  11.1\n\n\nsvytotal(~gedad, disenno) # usando ponderaciones cis\n#>                    total      SE\n#> gedad18-24 años 234.8523 15.0942\n#> gedad25-29 años 214.4397 14.4131\n#> gedad30-34 años 214.3337 14.3916\n#> gedad35-39 años 267.7623 16.0524\n#> gedad40-44 años 401.8541 19.2163\n#> gedad45-49 años 382.1934 18.8006\n#> gedad50-54 años 392.2205 19.0815\n#> gedad55-59 años 335.9379 17.7668\n#> gedad60-64 años 334.4781 17.7006\n#> gedad65-69 años 374.9334 18.6334\n#> gedad70-74 años 281.8288 16.3995\n#> gedad75-79 años 176.6998 13.0458\n#> gedad80-84 años 114.0455 10.6935\n#> gedad85-89 años  40.0390  6.3841\n#> gedad90-94 años  12.7567  3.6124\n#> gedad95-99 años   1.0536  1.0536\n\n\nsvytotal(~gedad, ponderacion_1)\n#>                   total     SE\n#> gedad18-24 años 351.958 0.0022\n#> gedad25-29 años 269.150 0.0022\n#> gedad30-34 años 213.023 0.0019\n#> gedad35-39 años 312.727 0.0023\n#> gedad40-44 años 377.061 0.0024\n#> gedad45-49 años 380.727 0.0024\n#> gedad50-54 años 355.685 0.0022\n#> gedad55-59 años 361.652 0.0023\n#> gedad60-64 años 286.424 0.0018\n#> gedad65-69 años 239.837 0.0016\n#> gedad70-74 años 190.638 0.0013\n#> gedad75-79 años 186.745 0.0016\n#> gedad80-84 años 123.233 0.0013\n#> gedad85-89 años  77.958 0.0010\n#> gedad90-94 años  41.526 0.0011\n#> gedad95-99 años  11.083 0.0000\n\nsexo\n\npop_sexo\n#> # A tibble: 2 × 2\n#>   sexo    Freq\n#>   <fct>  <dbl>\n#> 1 Hombre 1889.\n#> 2 Mujer  1891.\n\nEn la encuesta original hay sobrepresentacion de mujeres\n\nsvytotal(~sexo, disenno)\n#>             total     SE\n#> sexoHombre 1820.6 31.548\n#> sexoMujer  1958.8 31.519\n\n\nsvytotal(~sexo, ponderacion_1)\n#>             total SE\n#> sexoHombre 1888.6  0\n#> sexoMujer  1890.8  0\n\nRecuerdo de voto\n\npop_revoto\n#> # A tibble: 16 × 2\n#>    key          Freq\n#>    <fct>       <dbl>\n#>  1 abstencion 1189. \n#>  2 BILDU        30.0\n#>  3 BNG          13.0\n#>  4 CIUDADANOS  177. \n#>  5 COMPROMIS    19.0\n#>  6 CUP          26.5\n#>  7 ERC          94.3\n#>  8 JXCAT        57.1\n#>  9 MAS PAIS     43.5\n#> 10 OTROS       117. \n#> 11 PACMA        24.5\n#> 12 PNV          40.9\n#> 13 PODEMOS-IU  276. \n#> 14 PP          544. \n#> 15 PSOE        732. \n#> 16 VOX         395.\n\n\nsvytotal(~key, disenno)\n#>                  total      SE\n#> keyabstencion 1064.301 28.1710\n#> keyBILDU        21.475  4.4926\n#> keyBNG          23.630  4.9128\n#> keyCIUDADANOS  238.593 15.1738\n#> keyCOMPROMIS    22.099  4.8096\n#> keyCUP          17.429  4.2181\n#> keyERC          81.939  9.0654\n#> keyJXCAT        39.984  6.3702\n#> keyMAS PAIS     28.734  5.4132\n#> keyOTROS        62.353  7.5990\n#> keyPACMA        26.359  5.2625\n#> keyPNV          40.515  6.4151\n#> keyPODEMOS-IU  425.598 19.7388\n#> keyPP          525.044 21.6608\n#> keyPSOE        917.318 26.7850\n#> keyVOX         244.059 15.4552\n\n\nsvytotal(~key, ponderacion_1)\n#>                  total    SE\n#> keyabstencion 1189.358 5e-04\n#> keyBILDU        29.972 1e-04\n#> keyBNG          12.962 0e+00\n#> keyCIUDADANOS  177.463 2e-04\n#> keyCOMPROMIS    18.969 1e-04\n#> keyCUP          26.547 1e-04\n#> keyERC          94.271 2e-04\n#> keyJXCAT        57.097 1e-04\n#> keyMAS PAIS     43.529 1e-04\n#> keyOTROS       116.681 2e-04\n#> keyPACMA        24.515 1e-04\n#> keyPNV          40.915 1e-04\n#> keyPODEMOS-IU  276.473 2e-04\n#> keyPP          544.268 3e-04\n#> keyPSOE        731.849 4e-04\n#> keyVOX         394.559 4e-04\n\nPues podríamos dar por buena la calibración alcanzada\nLos pesos los podemos extraer usando la función weights\n\nweights(ponderacion_1)[1:10]\n#>         1         2         3         4         5         6         7         8 \n#> 1.8324806 1.1606015 0.5372897 1.4168519 0.6115565 0.9731935 0.8024724 0.7280362 \n#>         9        10 \n#> 1.4168519 1.2670284"
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#estimación-simple-de-la-intención-de-voto.",
    "href": "2022/01/01/cocinando/index.html#estimación-simple-de-la-intención-de-voto.",
    "title": "Cocinando",
    "section": "Estimación simple de la intención de voto.",
    "text": "Estimación simple de la intención de voto.\nPara realizar una “buena” estimación de voto tendria que haber hecho algo más aparte del “raking”, tal vez un modelo para tener voto probable de los indecisos etcétera.\nNo obstante vamos a ver qué estimación saldría simplemente utilizando los pesos originales y los pesos calibrados.\n\n(estim_cis <- svytotal(~intenciongr, disenno))\n#>                                            total      SE\n#> intenciongrPP                           531.4821 21.8379\n#> intenciongrPSOE                         745.3097 24.8698\n#> intenciongrCiudadanos                   111.9348 10.5990\n#> intenciongrMés Compromís                 23.1522  4.9224\n#> intenciongrERC                           61.6780  7.9014\n#> intenciongrJxCat                         31.7818  5.6855\n#> intenciongrEAJ-PNV                       39.4836  6.3334\n#> intenciongrEH Bildu                      13.0444  3.5539\n#> intenciongrCCa-NC                        10.4548  3.4812\n#> intenciongrNA+                            9.4790  2.4073\n#> intenciongrPACMA                         29.3263  5.4769\n#> intenciongrVOX                          320.9019 17.4761\n#> intenciongrCUP                           13.3279  3.6906\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               402.1001 19.2592\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           18.4930  4.3490\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            3.1434  1.6474\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      60.9837  7.8825\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  2.1852  1.5450\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.9441  6.7160\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  74.6794  8.5264\n#> intenciongrEn blanco                    172.4132 13.0502\n#> intenciongrNo votaría                   352.4158 18.0842\n#> intenciongrNo sabe todavía              548.6834 21.9338\n#> intenciongrN.C.                         158.0310 12.4293\n\n\n(estim_calibrada <- svytotal(~intenciongr, ponderacion_1))\n#>                                            total      SE\n#> intenciongrPP                           566.2767 19.2580\n#> intenciongrPSOE                         635.4325 18.5286\n#> intenciongrCiudadanos                    96.7651  8.9299\n#> intenciongrMés Compromís                 21.7971  3.8757\n#> intenciongrERC                           69.4503  6.1644\n#> intenciongrJxCat                         43.1817  5.2533\n#> intenciongrEAJ-PNV                       37.1996  4.1088\n#> intenciongrEH Bildu                      17.3237  4.0213\n#> intenciongrCCa-NC                        14.3188  4.9252\n#> intenciongrNA+                           16.8526  4.4475\n#> intenciongrPACMA                         31.5831  5.6846\n#> intenciongrVOX                          434.0434 17.2445\n#> intenciongrCUP                           18.8067  4.1303\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               302.0278 12.7507\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           11.5789  2.4683\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            4.1735  2.2022\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      69.1656  7.8136\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  3.6379  2.6304\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.0260  6.7859\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  84.9910  9.7962\n#> intenciongrEn blanco                    186.4136 14.3569\n#> intenciongrNo votaría                   376.8699 19.3696\n#> intenciongrNo sabe todavía              524.4669 21.7009\n#> intenciongrN.C.                         169.0461 13.5892\n\nVamos a pintarlas. El objeto devuelto por svytotal no es muy manejable, pero podemos utilizar lo que devuelve el print.\n\n\n\n\nestim_simple1 <- print(svytotal(~intenciongr, disenno))\n#>                                            total      SE\n#> intenciongrPP                           531.4821 21.8379\n#> intenciongrPSOE                         745.3097 24.8698\n#> intenciongrCiudadanos                   111.9348 10.5990\n#> intenciongrMés Compromís                 23.1522  4.9224\n#> intenciongrERC                           61.6780  7.9014\n#> intenciongrJxCat                         31.7818  5.6855\n#> intenciongrEAJ-PNV                       39.4836  6.3334\n#> intenciongrEH Bildu                      13.0444  3.5539\n#> intenciongrCCa-NC                        10.4548  3.4812\n#> intenciongrNA+                            9.4790  2.4073\n#> intenciongrPACMA                         29.3263  5.4769\n#> intenciongrVOX                          320.9019 17.4761\n#> intenciongrCUP                           13.3279  3.6906\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               402.1001 19.2592\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           18.4930  4.3490\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            3.1434  1.6474\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      60.9837  7.8825\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  2.1852  1.5450\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.9441  6.7160\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  74.6794  8.5264\n#> intenciongrEn blanco                    172.4132 13.0502\n#> intenciongrNo votaría                   352.4158 18.0842\n#> intenciongrNo sabe todavía              548.6834 21.9338\n#> intenciongrN.C.                         158.0310 12.4293\nestim_simple2 <- print(svytotal(~intenciongr, ponderacion_1))\n#>                                            total      SE\n#> intenciongrPP                           566.2767 19.2580\n#> intenciongrPSOE                         635.4325 18.5286\n#> intenciongrCiudadanos                    96.7651  8.9299\n#> intenciongrMés Compromís                 21.7971  3.8757\n#> intenciongrERC                           69.4503  6.1644\n#> intenciongrJxCat                         43.1817  5.2533\n#> intenciongrEAJ-PNV                       37.1996  4.1088\n#> intenciongrEH Bildu                      17.3237  4.0213\n#> intenciongrCCa-NC                        14.3188  4.9252\n#> intenciongrNA+                           16.8526  4.4475\n#> intenciongrPACMA                         31.5831  5.6846\n#> intenciongrVOX                          434.0434 17.2445\n#> intenciongrCUP                           18.8067  4.1303\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               302.0278 12.7507\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           11.5789  2.4683\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            4.1735  2.2022\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      69.1656  7.8136\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  3.6379  2.6304\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.0260  6.7859\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  84.9910  9.7962\n#> intenciongrEn blanco                    186.4136 14.3569\n#> intenciongrNo votaría                   376.8699 19.3696\n#> intenciongrNo sabe todavía              524.4669 21.7009\n#> intenciongrN.C.                         169.0461 13.5892\n\ncis_estim <- estim_simple1 %>% \n  as.data.frame() %>%  # as.data.frame para no perder los nombre de filas\n  rownames_to_column(var = \"partido\") %>% \n  mutate(partido       = str_sub(partido, 12, -1),\n         tot_low       = total - 1.96 * SE , # intervalos simples\n         tot_high      = total + 1.96 * SE, \n         pct_voto      = total    / 3779.429, \n         pct_voto_low  = tot_low  / 3779.429, \n         pct_voto_high = tot_high / 3779.429\n           ) \n\n\ncañi_estim <- estim_simple2 %>% \n  as.data.frame() %>%  # as.data.frame para no perder los nombre de filas\n  rownames_to_column(var = \"partido\") %>% \n  mutate(partido       = str_sub(partido, 12, -1),\n         tot_low       = total - 1.96 * SE , \n         tot_high      = total + 1.96 * SE, \n         pct_voto      = total    / 3779.429, \n         pct_voto_low  = tot_low  / 3779.429, \n         pct_voto_high = tot_high / 3779.429\n  ) \n\n\np_cis <- cis_estim %>% \n  top_n(22, pct_voto) %>% \n  ggplot(aes(y = reorder(partido, pct_voto ), x = pct_voto))  +\n  geom_point(color = \"darkred\", size = rel(3)) +\n  geom_errorbarh(aes(xmin = pct_voto_low, xmax = pct_voto_high)) +\n  scale_x_continuous(labels = scales::percent, \n                     limits = c(0, 0.22)) +\n  labs(title = \"Estimación intención voto (CIS)\", \n       subtitle = \"Usando ponderación cis\",\n       x = \"Proporción voto\",\n       y = \"Partido\")\n      \np_cañi <- cañi_estim %>% \n  top_n(22, pct_voto) %>% \n  ggplot(aes(y = reorder(partido, pct_voto ), x = pct_voto))  +\n  geom_point(color = \"darkblue\", size = rel(3)) +\n  geom_errorbarh(aes(xmin = pct_voto_low, xmax = pct_voto_high)) +\n  scale_x_continuous(labels = scales::percent, \n                     limits = c(0, 0.22)) +\n  labs(title = \"Estimación intención voto (con raking) \", \n       subtitle = \"Ajustando ponderación por edad,\\nsexo y recuerdo voto\",\n       x = \"Proporción voto\",\n       y = \"Partido\")\n\np_cis + p_cañi"
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#nota.",
    "href": "2022/01/01/cocinando/index.html#nota.",
    "title": "Cocinando",
    "section": "Nota.",
    "text": "Nota.\nEn vez de raking es usual utilizar modelos como MRP (multilevel regression and estratification), pero este último tiene el incoveniente (aunque muchas otras ventajas) de que necesita saber la distribución conjunta de las variables por las que se postestratifica. Aquí os dejo un artículo interesante"
  },
  {
    "objectID": "2022/01/08/cachitos-2021/index.html",
    "href": "2022/01/08/cachitos-2021/index.html",
    "title": "Cachitos 2021",
    "section": "",
    "text": "Retomando la entrada de cachitos de la nochevieja de 2020\nActualizo el script para bajar el video de la nochevieja de este año, extraer los fotogramas y tener los subtítulos.\nEste año parece (o yo no me he enterado) que ha habido menos polémica. Pero como siempre, nos hemos reído bastante.\nEjemplo:\n\n\n\n\n\nY el texto extraído con tesseract\nEl cámara se arrima, pero sin tocar.... NW,\nSN\n4 como el PSOE con la monarquía. | aaa\nAquí os dejo el script para bajar el vídeo y extraer los subtítulos.\n#!/bin/bash\n\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n\nif [ \"$ANNO\" == \"2021\" ] ;\nthen\n    wget https://lote5-vod-hls-geoblockurl.akamaized.net/resources/TE_GLUCA/mp4/4/0/1641020001504.mp4 \n    mv 1641020001504.mp4 $video_file\nfi\n \nif [ \"$ANNO\" == \"2020\" ] ;\nthen\n    wget http://mediavod-lvlt.rtve.es/resources/TE_GLUCA/mp4/2/4/1609487028742.mp4\n    mv 1609487028742.mp4 $video_file\nfi\n\nif [ \"$ANNO\" == \"2019\" ] ;\nthen\n    wget https://rtvehlsvod2020a-fsly.vod-rtve.cross-media.es/resources/TE_GLUCA/mp4/0/9/1577860099590.mp4\n    mv 1577860099590.mp4 $video_file\nfi\n\n# Pasar a jpg uno de cada 220 fotogramas\n\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 6 mogrify -resize 642x480 {}\n\n# Seleccionar cacho dond estan subtitulos\nfind . -name '*.jpg' |  parallel -j 6 convert {} -crop 460x50+90+295 +repage -compress none -depth 8 {}.subtitulo.tif\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 6 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 6 tesseract -l spa {} {}\n\n# mover a directorio texto\nmkdir -p $root_directory/$ANNO$suffix_txt_dir\n\nmv *.txt $root_directory/$ANNO$suffix_txt_dir\n\ncd $root_directory"
  },
  {
    "objectID": "2022/01/16/cachitos-tercera-parte/index.html",
    "href": "2022/01/16/cachitos-tercera-parte/index.html",
    "title": "Cachitos. Tercera parte",
    "section": "",
    "text": "Cómo aún ando medio “covitoso”, reciclo el código y comentarios de la entrada de 2021 y con solo cambiar la ruta del fichero de subtítulos ya nos vale todo el código.\nEl csv con el texto de los subtítulos para 2021 lo tenéis en este enlace.\nVamos al lío\n\n\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno <- \"2021\"\n\nLeemos el csv. Uso DT y así podéis ver todos los datos o buscar cosas, por ejemplo Ayuso o pandemia , monarquía o podemos\n\nsubtitulos_proces <-  read_csv(str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\nsubtitulos_proces %>% \n  select(texto, n_fichero, n_caracteres) %>% \n  DT::datatable()\n\n\n\n\n\n\nOye, pues sólo con esto ya nos valdría ¿no?\nQuitamos stopwords\n\nto_remove <- c(tm::stopwords(\"es\"),\n               \"110\", \"4\",\"1\",\"2\",\"7\",\"10\",\"0\",\"ñ\",\"of\",\n               \"5\",\"á\",\"i\",\"the\",\"3\", \"n\", \"p\",\n               \"ee\",\"uu\",\"mm\",\"ema\", \"zz\",\n               \"wr\",\"wop\",\"wy\",\"x\",\"xi\",\"xl\",\"xt\",\n               \"xte\",\"yí\", \"your\", \"si\")\n\nhead(to_remove, 40)\n#>  [1] \"de\"      \"la\"      \"que\"     \"el\"      \"en\"      \"y\"       \"a\"      \n#>  [8] \"los\"     \"del\"     \"se\"      \"las\"     \"por\"     \"un\"      \"para\"   \n#> [15] \"con\"     \"no\"      \"una\"     \"su\"      \"al\"      \"lo\"      \"como\"   \n#> [22] \"más\"     \"pero\"    \"sus\"     \"le\"      \"ya\"      \"o\"       \"este\"   \n#> [29] \"sí\"      \"porque\"  \"esta\"    \"entre\"   \"cuando\"  \"muy\"     \"sin\"    \n#> [36] \"sobre\"   \"también\" \"me\"      \"hasta\"   \"hay\"\n\nPero en nuestros datos, las palabras no están separadas, tendríamos que separarlas y luego quitar las que no queremos. Para eso voy a utilizar la librería tidytext\n\nlibrary(tidytext)\n\n# Con unnest token pasamos a un dataframe qeu tiene tantas filas como palabras\n\nprint(str_glue(\"Filas datos originales: {tally(subtitulos_proces)}\"))\n#> Filas datos originales: 687\n\nsubtitulos_proces_one_word <- subtitulos_proces %>% \n    unnest_tokens(input = texto,\n                  output = word) %>% \n    filter(! word %in% to_remove) %>% # quito palabras de la lista \n    filter(nchar(word)>1) # Nos quedamos con palabras que tengan más de un cáracter\n\n\nprint(str_glue(\"Filas datos tokenizado: {tally(subtitulos_proces_one_word)}\"))\n#> Filas datos tokenizado: 4735\n\nsubtitulos_proces_one_word %>% \n  select(name,n_fichero,word, n_caracteres)\n#> # A tibble: 4,735 × 4\n#>     name n_fichero                      word          n_caracteres\n#>    <dbl> <chr>                          <chr>                <dbl>\n#>  1    14 00000014.jpg.subtitulo.tif.txt servicio               118\n#>  2    14 00000014.jpg.subtitulo.tif.txt meteorológico          118\n#>  3    14 00000014.jpg.subtitulo.tif.txt cachitos               118\n#>  4    14 00000014.jpg.subtitulo.tif.txt informa                118\n#>  5    14 00000014.jpg.subtitulo.tif.txt prevén                 118\n#>  6    14 00000014.jpg.subtitulo.tif.txt vientos                118\n#>  7    14 00000014.jpg.subtitulo.tif.txt fiesta                 118\n#>  8    14 00000014.jpg.subtitulo.tif.txt fuertes                118\n#>  9    14 00000014.jpg.subtitulo.tif.txt próximas               118\n#> 10    14 00000014.jpg.subtitulo.tif.txt tres                   118\n#> # … with 4,725 more rows\n\nUna cosa simple que podemos hacer es contar palabras, y vemos que lo que más se repite es canción, obvio\n\npalabras_ordenadas <- subtitulos_proces_one_word %>% \n    group_by(word) %>% \n    summarise(veces = n()) %>% \n    arrange(desc(veces))\n\npalabras_ordenadas %>% \n    slice(1:20) %>% \n    ggplot(aes(x = reorder(word, veces), y = veces)) +\n    geom_col(show.legend = FALSE) +\n    ylab(\"veces\") +\n    xlab(\"\") +\n    coord_flip() +\n    theme_bw()\n\n\n\n\n\n\n\n\nO pintarlas en plan nube de palabras.\n\nlibrary(wordcloud)\npal <- brewer.pal(8,\"Dark2\")\nsubtitulos_proces_one_word %>% \n    group_by(word) %>% \n    count() %>% \n    with(wordcloud(word, n, random.order = FALSE, max.words = 80, colors=pal))    \n\n\n\n\n\n\n\n\nPues una vez que tenemos las palabras de cada subtítulo separadas podemos buscar palabras polémicas, aunque antes al usar la librería DT ya podíamos buscar, veamos como sería con el código.\nCreamos lista de palabras a buscar.\n\npalabras_1 <- c(\"monarca\",\"pp\",\"vox\",\"rey\",\"coron\",\"zarzuela\",\n                \"prisión\", \"democracia\", \"abascal\",\"casado\",\n                \"ultra\",\"ciudada\", \"oposición\",\"derech\",\n                \"podem\",\"sanchez\",\"iglesias\",\"errejon\",\"izquier\",\n                \"gobierno\",\"illa\",\"redondo\",\"ivan\",\"celaa\",\n                \"guardia\",\"príncipe\",\"principe\",\"ayuso\",\n                \"tezanos\",\"cis\",\"republic\", \"simon\", \"pandem\",\"lazo\",\"arrim\",\n                \"toled\",\"alber\",\"fach\", \"zarzu\", \"democr\",\"vicepre\", \"minist\",\n                \"irene\",\"montero\",\"almeida\", \"monarq\")\n\nConstruimos una regex para que encuentre las palabras que empiecen así.\n\n(exp_regx <- paste0(\"^\",paste(palabras_1, collapse = \"|^\")))\n#> [1] \"^monarca|^pp|^vox|^rey|^coron|^zarzuela|^prisión|^democracia|^abascal|^casado|^ultra|^ciudada|^oposición|^derech|^podem|^sanchez|^iglesias|^errejon|^izquier|^gobierno|^illa|^redondo|^ivan|^celaa|^guardia|^príncipe|^principe|^ayuso|^tezanos|^cis|^republic|^simon|^pandem|^lazo|^arrim|^toled|^alber|^fach|^zarzu|^democr|^vicepre|^minist|^irene|^montero|^almeida|^monarq\"\n\nY nos creamos una variable que valga TRUE cuando suceda esto\n\n\nsubtitulos_proces_one_word <- subtitulos_proces_one_word %>% \n    mutate(polemica= str_detect(word, exp_regx))\n\nsubtitulos_proces_one_word %>% \n  filter(polemica) %>% \n  select(name, word, n_fichero) \n#> # A tibble: 32 × 3\n#>     name word      n_fichero                     \n#>    <dbl> <chr>     <chr>                         \n#>  1   139 reyes     00000139.jpg.subtitulo.tif.txt\n#>  2   169 arrima    00000169.jpg.subtitulo.tif.txt\n#>  3   169 monarquía 00000169.jpg.subtitulo.tif.txt\n#>  4   330 pandemia  00000330.jpg.subtitulo.tif.txt\n#>  5   397 ministro  00000397.jpg.subtitulo.tif.txt\n#>  6   398 pandemia  00000398.jpg.subtitulo.tif.txt\n#>  7   404 guardia   00000404.jpg.subtitulo.tif.txt\n#>  8   581 iglesias  00000581.jpg.subtitulo.tif.txt\n#>  9   621 podemos   00000621.jpg.subtitulo.tif.txt\n#> 10   641 illa      00000641.jpg.subtitulo.tif.txt\n#> # … with 22 more rows\n\nPodríamos ver el texto de los subtítulos, para eso, nos quedamos con un identificador, como el nombre del fichero txt, que nos servirá luego para leer la imagen.\nPues en realidad tenemos sólo 27 subtítulos polémicos de los de alrededor de 680 que hay\n\nsubtitulos_polemicos <- subtitulos_proces_one_word %>% \n    filter(polemica) %>% \n    pull(n_fichero) %>% \n    unique()\nsubtitulos_polemicos\n#>  [1] \"00000139.jpg.subtitulo.tif.txt\" \"00000169.jpg.subtitulo.tif.txt\"\n#>  [3] \"00000330.jpg.subtitulo.tif.txt\" \"00000397.jpg.subtitulo.tif.txt\"\n#>  [5] \"00000398.jpg.subtitulo.tif.txt\" \"00000404.jpg.subtitulo.tif.txt\"\n#>  [7] \"00000581.jpg.subtitulo.tif.txt\" \"00000621.jpg.subtitulo.tif.txt\"\n#>  [9] \"00000641.jpg.subtitulo.tif.txt\" \"00000665.jpg.subtitulo.tif.txt\"\n#> [11] \"00000671.jpg.subtitulo.tif.txt\" \"00000680.jpg.subtitulo.tif.txt\"\n#> [13] \"00000763.jpg.subtitulo.tif.txt\" \"00000828.jpg.subtitulo.tif.txt\"\n#> [15] \"00000853.jpg.subtitulo.tif.txt\" \"00000865.jpg.subtitulo.tif.txt\"\n#> [17] \"00000866.jpg.subtitulo.tif.txt\" \"00000955.jpg.subtitulo.tif.txt\"\n#> [19] \"00000980.jpg.subtitulo.tif.txt\" \"00000981.jpg.subtitulo.tif.txt\"\n#> [21] \"00001135.jpg.subtitulo.tif.txt\" \"00001169.jpg.subtitulo.tif.txt\"\n#> [23] \"00001176.jpg.subtitulo.tif.txt\" \"00001183.jpg.subtitulo.tif.txt\"\n#> [25] \"00001189.jpg.subtitulo.tif.txt\" \"00001228.jpg.subtitulo.tif.txt\"\n#> [27] \"00001233.jpg.subtitulo.tif.txt\" \"00001254.jpg.subtitulo.tif.txt\"\n#> [29] \"00001262.jpg.subtitulo.tif.txt\"\n\n\n(texto_polemicos <- subtitulos_proces %>% \n    filter(n_fichero %in% subtitulos_polemicos) %>% \n    arrange(n_fichero) %>% \n    pull(texto))\n#>  [1] \"ella se conformaba con poquita cosa no como tú que sigues poniendo el scalextric en la carta a los reyes\"             \n#>  [2] \"el cámara se arrima pero sin tocar nw sn 4 como el psoe con la monarquía aaa\"                                         \n#>  [3] \"el mítico rockero henry stephen autor de esta canción también murió en 2021 a causa de la pandemia\"                   \n#>  [4] \"con el fary perdimos un buen taxista y un mejor ministro de economía\"                                                 \n#>  [5] \"ha hecho falta una crisis financiera de 10 años y una pandemia para que alguien hiciera caso a el fary\"               \n#>  [6] \"si en una verbena gallega la tortilla está muy hecha o no suena ana kiro viene la guardia civil y la cierra\"          \n#>  [7] \"4 la italiana le supo sacar más partido al bamboleo y que julio iglesias o los gypsy kings\"                           \n#>  [8] \"anita triunfó con un tema que al principio no le molaba nada como sánchez con podemos\"                                \n#>  [9] \"illa rusa tt y los coches del pasado\"                                                                                 \n#> [10] \"con tanto escenario para ellos solos parece la sede de ciudadanos\"                                                    \n#> [11] \"himno de los nuevos románticos al que también pertenecieron ultravox duran duran o hace poco pablo alborán y lópez\"   \n#> [12] \"marian gold era una mezcla entre isa serra e ivonne reyes\"                                                            \n#> [13] \"a una canción de pimpinela b sánchez y casado en una sesión de control\"                                               \n#> [14] \"se ganó la corona del rey de la bachata como líder del grupo aventura\"                                                \n#> [15] \"olvidaste la tabla periódica y a reyes godos pero esto sigue en algún lugar de tu cerebro junto al fijo de tus padres\"\n#> [16] \"sherpa a la izquierda solo de la imagen sigue siendo igual de barón pero bastante menos rojo\"                         \n#> [17] \"como buen sherpa es capaz de cargar con el pack completo plandemia inmigrantes invasores gobierno comunista\"          \n#> [18] \"cuentan que iván redondo susurraba este estribillo al oído de pedro sánchez un par de veces al día\"                   \n#> [19] \"o iglesias an e y canto a galicia dn d\"                                                                               \n#> [20] \"lo único virgen alrededor de julio iglesias eran 4 las islas en las que invertía su patrimonio\"                       \n#> [21] \"a estas alturas de la noche es cuando te arrimas y sentencias ni héroes ni mecano el último\"                          \n#> [22] \"e podría haber sido la música de la campaña electoral de ayuso 4 ye la campañn\"                                       \n#> [23] \"creéis que rodrigo de lorenzo sabe que no lleva maraca en la mano izquierda\"                                          \n#> [24] \"aparte de ser el mejor letrista en español el intelectual de la salsa y ministro de turismo hizo\"                     \n#> [25] \"este clásico tiene más versiones que ministros este gobierno\"                                                         \n#> [26] \"22 en realidad esas coletas siempre fueron polémicas me lo milagroso es llegar a vicepresidente con una\"              \n#> [27] \"azúcar no bueno mejor asi alberto garzón\"                                                                             \n#> [28] \"concretamente falín a vuestra izquierda era el padre de falete\"                                                       \n#> [29] \"amanece se recorta en el horizonte el perfil de santiago abascal p\"\n\nPodemos ver las imágenes\n\n(polemica_fotogramas <- unique(substr(subtitulos_polemicos, 1,12)))\n#>  [1] \"00000139.jpg\" \"00000169.jpg\" \"00000330.jpg\" \"00000397.jpg\" \"00000398.jpg\"\n#>  [6] \"00000404.jpg\" \"00000581.jpg\" \"00000621.jpg\" \"00000641.jpg\" \"00000665.jpg\"\n#> [11] \"00000671.jpg\" \"00000680.jpg\" \"00000763.jpg\" \"00000828.jpg\" \"00000853.jpg\"\n#> [16] \"00000865.jpg\" \"00000866.jpg\" \"00000955.jpg\" \"00000980.jpg\" \"00000981.jpg\"\n#> [21] \"00001135.jpg\" \"00001169.jpg\" \"00001176.jpg\" \"00001183.jpg\" \"00001189.jpg\"\n#> [26] \"00001228.jpg\" \"00001233.jpg\" \"00001254.jpg\" \"00001262.jpg\"\n\npolemica_fotogramas_full <- paste0(str_glue(\"{root_directory}video/{anno}_jpg/\"), polemica_fotogramas)\n\nsubtitulos_polemicos_full <- paste0(polemica_fotogramas_full,\".subtitulo.tif\")\n\nY ahora utilizando la librería magick en R y un poco de programación funcional (un simple map), tenemos la imagen leída\n\nlibrary(magick)\n\nfotogramas_polemicos_img <- map(polemica_fotogramas_full, image_read)\nsubtitulos_polemicos_img <- map(subtitulos_polemicos_full, image_read)\n\n\nsubtitulos_polemicos_img[[18]]\n\n\n\n\n\n\n\n\n\nfotogramas_polemicos_img[[18]]\n\n\n\n\n\n\n\n\nPodemos ponerlos todos juntos.\n\nlista_fotogram_polemicos <- map(fotogramas_polemicos_img, grid::rasterGrob)\ngridExtra::grid.arrange(grobs=lista_fotogram_polemicos)"
  },
  {
    "objectID": "2022/01/10/cachitos-segunda-parte/index.html",
    "href": "2022/01/10/cachitos-segunda-parte/index.html",
    "title": "Cachitos. Segunda parte",
    "section": "",
    "text": "Nada, esto es sólo para leernos con R los subtítulos del post anterior.\n\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno <- \"2021\"\n\n# Construims un data frame con los nombrs de los ficheros \n\nnombre_ficheros <- list.files(path = str_glue(\"{root_directory}{anno}_txt/\")) %>% \n    enframe() %>% \n    rename(n_fichero = value)\n\nnombre_ficheros\n#> # A tibble: 1,384 × 2\n#>     name n_fichero                     \n#>    <int> <chr>                         \n#>  1     1 00000001.jpg.subtitulo.tif.txt\n#>  2     2 00000002.jpg.subtitulo.tif.txt\n#>  3     3 00000003.jpg.subtitulo.tif.txt\n#>  4     4 00000004.jpg.subtitulo.tif.txt\n#>  5     5 00000005.jpg.subtitulo.tif.txt\n#>  6     6 00000006.jpg.subtitulo.tif.txt\n#>  7     7 00000007.jpg.subtitulo.tif.txt\n#>  8     8 00000008.jpg.subtitulo.tif.txt\n#>  9     9 00000009.jpg.subtitulo.tif.txt\n#> 10    10 00000010.jpg.subtitulo.tif.txt\n#> # … with 1,374 more rows\n\nAhora los podemos leer en orden\n\nsubtitulos <-  list.files(path = str_glue(\"{root_directory}{anno}_txt/\"), \n                        pattern = \"*.txt\", full.names = TRUE) %>% \n    map(~read_file(.)) %>% \n    enframe() %>%  \n  # hacemos el join con el dataframe anterior para tener el nombre del fichero original\n    left_join(nombre_ficheros)\n\nglimpse(subtitulos)\n#> Rows: 1,384\n#> Columns: 3\n#> $ name      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n#> $ value     <list> \"\\f\", \"\\f\", \"FUN MÚSICA Y CINTAS DE VÍDEO\\n\\f\", \" \\n\\f\", \"\\…\n#> $ n_fichero <chr> \"00000001.jpg.subtitulo.tif.txt\", \"00000002.jpg.subtitulo.ti…\nsubtitulos\n#> # A tibble: 1,384 × 3\n#>     name value     n_fichero                     \n#>    <int> <list>    <chr>                         \n#>  1     1 <chr [1]> 00000001.jpg.subtitulo.tif.txt\n#>  2     2 <chr [1]> 00000002.jpg.subtitulo.tif.txt\n#>  3     3 <chr [1]> 00000003.jpg.subtitulo.tif.txt\n#>  4     4 <chr [1]> 00000004.jpg.subtitulo.tif.txt\n#>  5     5 <chr [1]> 00000005.jpg.subtitulo.tif.txt\n#>  6     6 <chr [1]> 00000006.jpg.subtitulo.tif.txt\n#>  7     7 <chr [1]> 00000007.jpg.subtitulo.tif.txt\n#>  8     8 <chr [1]> 00000008.jpg.subtitulo.tif.txt\n#>  9     9 <chr [1]> 00000009.jpg.subtitulo.tif.txt\n#> 10    10 <chr [1]> 00000010.jpg.subtitulo.tif.txt\n#> # … with 1,374 more rows\n\nen n_fichero tenemos el nombre y en value el texto\n\n\nsubtitulos %>% \n  pull(value) %>%\n  ## usamos `[[` que es el operador para acceder a la lista el que normalemente se usa [[nombre_elemento]]\n  `[[`(16)\n#> [1] \"Ella resume a la perfección la filosofía de Cachitos:\\nmontar “La fiesta” “Buscando en el baúl de los recuerdos”.\\n\\n \\n\\f\"\n\n# equivalentemente\n\n# subtitulos %>% \n#     pull(value) %>% \n#     pluck(16)\n\nComo sabemos que hay muchos ficheros sin texto podemos contar letras.\n\nsubtitulos <- subtitulos %>% \n    mutate(n_caracteres = nchar(value)) \n\nsubtitulos %>% \n    group_by(n_caracteres) %>% \n    count()\n#> # A tibble: 128 × 2\n#> # Groups:   n_caracteres [128]\n#>    n_caracteres     n\n#>           <int> <int>\n#>  1            1   428\n#>  2            3    97\n#>  3            4    19\n#>  4            5    13\n#>  5            6    15\n#>  6            7     8\n#>  7            8     6\n#>  8            9     3\n#>  9           10     3\n#> 10           11     2\n#> # … with 118 more rows\n\nsubtitulos %>% \n    group_by(n_caracteres) %>% \n    count() %>% \n  ggplot(aes(x = n_caracteres, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\nY vemos que hay muchos subtitulos con pocos caracteres. Si vemos por ejemplo los que tienen menos de 12 caracteres\n\nsubtitulos %>% \n    filter(n_caracteres <12) %>% \n    pull(value) %>% \n    head(10)\n#> [[1]]\n#> [1] \"\\f\"\n#> \n#> [[2]]\n#> [1] \"\\f\"\n#> \n#> [[3]]\n#> [1] \" \\n\\f\"\n#> \n#> [[4]]\n#> [1] \"\\f\"\n#> \n#> [[5]]\n#> [1] \" \\n\\f\"\n#> \n#> [[6]]\n#> [1] \" \\n\\f\"\n#> \n#> [[7]]\n#> [1] \"\\f\"\n#> \n#> [[8]]\n#> [1] \"\\f\"\n#> \n#> [[9]]\n#> [1] \"\\f\"\n#> \n#> [[10]]\n#> [1] \"\\f\"\n\nQue se corresponden con haber pillado parte no del subtítulo sino del nombre de la actuación\n\nsubtitulos %>% \n    filter(n_caracteres ==15)\n#> # A tibble: 2 × 4\n#>    name value     n_fichero                      n_caracteres\n#>   <int> <list>    <chr>                                 <int>\n#> 1   571 <chr [1]> 00000571.jpg.subtitulo.tif.txt           15\n#> 2  1361 <chr [1]> 00001361.jpg.subtitulo.tif.txt           15\n\nUsando la librería magick en R que permite usar imagemagick en R, ver post de Raúl Vaquerizo y su homenaje a Sean Connery, podemos ver el fotgrama correspondiente\n\nlibrary(magick)\n(directorio_imagenes <- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#> /media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/video/2021_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00000018.jpg\"))\n\n\n\n\n\n\n\n\nTambién podemos ver hasta cuando pasa eso, por ejemplo si vemos subtítulos con 18 caracteres\n\nsubtitulos %>% \n    filter(n_caracteres ==18) %>% \n    pull(value)\n#> [[1]]\n#> [1] \" \\n\\nJ0 EN EL AMOR\\n\\f\"\n#> \n#> [[2]]\n#> [1] \"¿EDITH BROOKS\\nch\\n\\f\"\n#> \n#> [[3]]\n#> [1] \" \\n\\nmme Tha Power\\n\\f\"\n#> \n#> [[4]]\n#> [1] \"  \\n\\n\\\"RONTERA\\n\\n  \\n\\f\"\n\n\nsubtitulos <- subtitulos %>% \n    filter(n_caracteres > 17) \n\nglimpse(subtitulos)\n#> Rows: 778\n#> Columns: 4\n#> $ name         <int> 3, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 27, 28, 29, 30…\n#> $ value        <list> \"FUN MÚSICA Y CINTAS DE VÍDEO\\n\\f\", \"El servicio meteoro…\n#> $ n_fichero    <chr> \"00000003.jpg.subtitulo.tif.txt\", \"00000014.jpg.subtitulo…\n#> $ n_caracteres <int> 30, 118, 82, 117, 117, 25, 100, 97, 86, 88, 84, 43, 52, 8…\n\nCon el fin de detectar cuáles están duplicados y aprovechando que están en orden de aparición, podemos hacer utilizar distancias de texto para calcular la distancia de cada subtítulo con el anterior, y si la distancia es pequeña es que es el mismo rótulo.\nPrimero hacemos una mini-limpieza.\n\nstring_mini_clean <-  function(string){\n    string <- gsub(\"?\\n|\\n\", \" \", string)\n    string <- gsub(\"\\r|?\\f|=\", \" \", string)\n    string <- gsub('“|”|—|>',\" \", string)\n    \n    string <- gsub(\"[[:punct:][:blank:]]+\", \" \", string)\n    string <- tolower(string)\n    string <- gsub(\"  \", \" \", string)\n    \n    return(string)\n}\n\n# Haciendo uso de programacion funciona con purrr es muy fácil pasar esta función a cada elemento. y decirle que # el reultado es string con map_chr\n\nsubtitulos_proces <- subtitulos %>% \n    mutate(texto = map_chr(value, string_mini_clean)) %>% \n    select(-value)\n\nsubtitulos_proces %>% \n  select(texto)\n#> # A tibble: 778 × 1\n#>    texto                                                                        \n#>    <chr>                                                                        \n#>  1 \"fun música y cintas de vídeo \"                                              \n#>  2 \"el servicio meteorológico de cachitos informa se prevén vientos de fiesta m…\n#>  3 \"no es para menos llevamos dos años conformándonos solo con aires de siesta \"\n#>  4 \"ella resume a la perfección la filosofía de cachitos montar la fiesta busca…\n#>  5 \" ella resume a la perfección la filosofía de cachitos montar la fiesta busc…\n#>  6 \" oncé a2y in love \"                                                         \n#>  7 \"esperamos que tengáis una tele bien grande no sabemos si cabrá tanto flow e…\n#>  8 \"liberté egalité fraternité vacunaté y beyoncé la lola flores negra ejercien…\n#>  9 \"mirad su pelo ya os dijimos que el aire de fiesta iba a soplar fuerte esta …\n#> 10 \"mirad su pelo ya os dijimos que el aire de fiesta iba a soplar fuerte esta …\n#> # … with 768 more rows\n\nY ya vemos a simple vista que hay algun duplicado. Calculemos ahora la distancia de strings, utilizando la función stringdist de la librería del mismo nombre.\n\n\nsubtitulos_proces %>% \n    mutate(texto_anterior = lag(texto)) %>% \n    # calculamos distancias con método lcs (que no me he leído que hace exactamente)\n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\")) %>% \n  # veamos algunos elementos\n    filter(distancia < 19) %>% \n    arrange(desc(distancia) ) %>% \n    select(texto, texto_anterior, distancia) \n#> # A tibble: 89 × 3\n#>    texto                                                         texto…¹ dista…²\n#>    <chr>                                                         <chr>     <dbl>\n#>  1 \" la rosalía emérita \"                                        \" alía…      18\n#>  2 \"chango llegó a españa como aspirante a estrella del rock y … \"chang…      15\n#>  3 \"leonard cohen y el pitufo gruñón en el cuerpo de un italian… \"leona…      13\n#>  4 \" el stress del año 2000 nos llegó con casi 20 os de retraso… \" el s…       7\n#>  5 \"aquí ya llevaba cuatro años de carrera luis miguel tiene má… \"aquí …       7\n#>  6 \"imborrable siempre la sonrisa de jerry aunque un poco incóm… \"la im…       6\n#>  7 \"7 literalmente significa puedes tocarme la campanita funcio… \"liter…       6\n#>  8 \" las palabras no vienen fácilmente paradójicamente no fue e… \" las …       5\n#>  9 \"en españa el g arm nació ya vintage porque el británico nos… \"en es…       5\n#> 10 \"nosotros también lo sentimos si alguien se ofende y en nues… \" 4 no…       5\n#> # … with 79 more rows, and abbreviated variable names ¹​texto_anterior,\n#> #   ²​distancia\n\nY parece que funciona. Así que decido quitar las filas dónde la distancia sea menos que 19 y así eliminar muchos de los duplicados.\n\nsubtitulos_proces <- subtitulos_proces %>% \n    mutate(texto_anterior = lag(texto)) %>% \n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\")) %>% \n    filter(distancia > 19) %>% \n    select(-texto_anterior)\n\nsubtitulos_proces %>% \n  head()\n#> # A tibble: 6 × 5\n#>    name n_fichero                      n_caracteres texto                dista…¹\n#>   <int> <chr>                                 <int> <chr>                  <dbl>\n#> 1    14 00000014.jpg.subtitulo.tif.txt          118 \"el servicio meteor…     106\n#> 2    15 00000015.jpg.subtitulo.tif.txt           82 \"no es para menos l…     110\n#> 3    16 00000016.jpg.subtitulo.tif.txt          117 \"ella resume a la p…     102\n#> 4    18 00000018.jpg.subtitulo.tif.txt           25 \" oncé a2y in love \"     100\n#> 5    19 00000019.jpg.subtitulo.tif.txt          100 \"esperamos que teng…      92\n#> 6    20 00000020.jpg.subtitulo.tif.txt           97 \"liberté egalité fr…     105\n#> # … with abbreviated variable name ¹​distancia\n\n\nwrite_csv(subtitulos_proces,\n          file = str_glue(\"{root_directory}{anno}_txt_unido.csv\"))"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "",
    "text": "Voy a poner un ejemplo de como utilizar solvers para investigación operativa dentro de R.\nTenemos la siguiente información: * Listado de códigos postales de España con la longitud y latitud del centroide del polígono. * Listado de códigos postales de la ubicación de las sedes de una empresa. * En la empresa hay jefes e indios, no es necesario que haya un jefe por sede.\nSe quiere, para cada provincia de España\n\nAsignar cada código postal de esa provincia a un empleado de la empres (jefe o indio).\nUn mismo código postal no puede estar asignado a más de un empleado.\nEn la medida de lo posible asignar a los empleados los códigos postales más cercanos al lugar de su sede.\nA igualdad de distancia entre un código postal y una sede, se debería asignar ese código postal a un indio.\nNingún indio debe tener asignados menos códigos postales que ningún jefe.\nLos jefes como máximo han de tener 7 códigos postales asignados.\nLos indios como mínimo han de tener 3 códigos postales asignados.\nNo puede haber ningún empleado que esté “desasignado”.\n\nDados estos requisitos debería plantear como es la definición del problema, pero no tengo ganas de ponerme a escribir fórmulas en latex, así que en vez de eso voy a utilizar unos datos simulados y directamente al código.."
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#carga-de-datos-y-crear-datos-ficticios.",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#carga-de-datos-y-crear-datos-ficticios.",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "Carga de datos y crear datos ficticios.",
    "text": "Carga de datos y crear datos ficticios.\n\nCarga códigos postales\nCasualmente, tengo por mi pc un shapefile algo antiguo (de cuando está capa estaba en cartociudad) con la capa de códigos postales de España, la cual si se quiere actualizada vale un dinerillo. correos, 6000 Euros la versión sin actualizaciones.. Bueno, si hacienda y correos somos todos me gustaría al menos poder utilizar esto actualizado sin que me cueste 6k.\nVamos a cargar la capa, obtener los centroides, pasar la geometría a longitud y latitud\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(sf)\n## Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\ncod_postales_raw <- readRDS(here::here(\"data/cp_boundaries.rds\")) %>%\n  select(-cp_num, -cp_2_num)\n\nhead(cod_postales_raw)\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -1536953 ymin: 3373964 xmax: -41802.13 ymax: 5247186\n## Projected CRS: WGS 84 / Pseudo-Mercator\n##      cp cp_2   area_m2                       geometry\n## 1 35560   35 187875455 MULTIPOLYGON (((-1518970 33...\n## 2 27330   27   6659413 MULTIPOLYGON (((-821864.3 5...\n## 3 46680   46  69190773 MULTIPOLYGON (((-51610.46 4...\n## 4 49706   49  90229134 MULTIPOLYGON (((-641488.4 5...\n## 5 21120   21  20068648 MULTIPOLYGON (((-776955.2 4...\n## 6 16623   16 132859998 MULTIPOLYGON (((-256256.7 4...\nPintamos algunos códigos\nplot(st_geometry(cod_postales_raw[1:2000, ]))\n\nPara obtener los centroides, usamos la función st_centroid y pasamos la capa de polígonos a una de puntos\ncod_postales_raw <- st_centroid(cod_postales_raw)\n## Warning in st_centroid.sf(cod_postales_raw): st_centroid assumes attributes are\n## constant over geometries of x\nhead(cod_postales_raw)\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -1525406 ymin: 3382025 xmax: -47782.92 ymax: 5245455\n## Projected CRS: WGS 84 / Pseudo-Mercator\n##      cp cp_2   area_m2                  geometry\n## 1 35560   35 187875455  POINT (-1525406 3382025)\n## 2 27330   27   6659413 POINT (-823274.9 5245455)\n## 3 46680   46  69190773 POINT (-47782.92 4752325)\n## 4 49706   49  90229134 POINT (-637415.5 5057096)\n## 5 21120   21  20068648 POINT (-778872.1 4479315)\n## 6 16623   16 132859998 POINT (-262034.3 4818194)\nplot(st_geometry(cod_postales_raw[1:2000, ]), cex = 0.2)\n\nAhora extraemos de la geometría la longitud y latitud. Para eso hay que transformar la geometría.\ncod_postales_raw <- cod_postales_raw %>%\n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\ncod_postales <- cod_postales_raw %>%\n  mutate(\n    centroide_longitud = unlist(map(geometry, 1)),\n    centroide_latitud = unlist(map(geometry, 2))\n  ) %>%\n  st_drop_geometry() %>% # quitamos la geometría y nos quedamos solo con la longitud y latitud\n  rename(\n    cod_postal = cp,\n    cod_prov = cp_2\n  ) %>%\n  filter(!is.na(centroide_longitud)) # tenía un polígono con NAS\n\nhead(cod_postales)\n##   cod_postal cod_prov   area_m2 centroide_longitud centroide_latitud\n## 1      35560       35 187875455        -13.7029565          29.05011\n## 2      27330       27   6659413         -7.3956047          42.56144\n## 3      46680       46  69190773         -0.4292412          39.21368\n## 4      49706       49  90229134         -5.7260007          41.30272\n## 5      21120       21  20068648         -6.9967272          37.28791\n## 6      16623       16 132859998         -2.3538946          39.67063\nPor otro lado me interesa añadir el literal de provincia, tengo una tabla extraída del INE con la correspondencia entre cod_prov y el literal\nprovincia <- read_csv(here::here(\"data/codprov.csv\"))\n## Rows: 52 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (2): CODIGO, LITERAL\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(provincia)\n## # A tibble: 6 × 2\n##   CODIGO LITERAL         \n##   <chr>  <chr>           \n## 1 02     Albacete        \n## 2 03     Alicante/Alacant\n## 3 04     Almería         \n## 4 01     Araba/Álava     \n## 5 33     Asturias        \n## 6 05     Ávila\nNormalizo a mayúsculas y sin tildes y se lo pego a los códigos postales\nprovincia <- provincia %>%\n  mutate(provincia = toupper(stringi::stri_trans_general(LITERAL, \"Latin-ASCII\")))\n\ncod_postales <- cod_postales %>%\n  left_join(provincia %>%\n    select(\n      CODIGO,\n      provincia\n    ),\n  by = c(\"cod_prov\" = \"CODIGO\")\n  )\n\ndim(cod_postales)\n## [1] 10808     6\nhead(cod_postales %>%\n  select(provincia, cod_prov, everything()))\n##           provincia cod_prov cod_postal   area_m2 centroide_longitud\n## 1       PALMAS, LAS       35      35560 187875455        -13.7029565\n## 2              LUGO       27      27330   6659413         -7.3956047\n## 3 VALENCIA/VALENCIA       46      46680  69190773         -0.4292412\n## 4            ZAMORA       49      49706  90229134         -5.7260007\n## 5            HUELVA       21      21120  20068648         -6.9967272\n## 6            CUENCA       16      16623 132859998         -2.3538946\n##   centroide_latitud\n## 1          29.05011\n## 2          42.56144\n## 3          39.21368\n## 4          41.30272\n## 5          37.28791\n## 6          39.67063\n\n\nDatos ficticios de las sedes de las empresas\nLo que voy a hacer es seleccionar aleatoriamente un número de códigos postales en cada provincia, que serán las sedes de la empresa. En cada provincia pongo al menos a un empleado de tipo = “jefe”. Luego, reparto de forma aleatoria entre los códigos postales que han sido elegidos como sedes otros 120 jefes y 480 indios.\nset.seed(155)\n\n## En cada provincia nos quedamos con  un 6% de códigos postales\n\nsedes_alea <- cod_postales %>%\n  group_by(provincia) %>%\n  slice_sample(prop = 0.06)\n\n\n\n# en cada provincia al menos un jefe y resto de empleados de forma aleatoria, en las diferentes sedes elegidas\n\npersonal <- bind_rows(\n  sedes_alea %>%\n    select(provincia, cod_postal) %>%\n    group_by(provincia) %>%\n    slice_sample(n = 1) %>%\n    ungroup() %>%\n    select(cod_postal) %>%\n    mutate(tipo = \"jefe\"),\n  tibble(\n    tipo = c(rep(\"jefe\", 120), rep(\"indio\", 360)),\n    cod_postal = sample(sedes_alea$cod_postal, size = 480, replace = TRUE)\n  )\n)\nCreamos data set sedes\nsedes <- personal %>%\n  left_join(sedes_alea)\n## Joining, by = \"cod_postal\"\ndim(sedes)\n## [1] 530   7\nsample_n(sedes, 7)\n## # A tibble: 7 × 7\n##   cod_postal tipo  cod_prov    area_m2 centroide_longitud centroide_la…¹ provi…²\n##   <fct>      <chr> <chr>         <dbl>              <dbl>          <dbl> <chr>  \n## 1 37660      indio 37        36448279.              -5.99           40.5 SALAMA…\n## 2 41770      jefe  41       183345907.              -5.55           37.0 SEVILLA\n## 3 08011      jefe  08          968836.               2.16           41.4 BARCEL…\n## 4 34479      indio 34        49891663.              -4.42           42.4 PALENC…\n## 5 34859      indio 34       118812672.              -4.59           42.8 PALENC…\n## 6 09348      indio 09       249695400.              -3.61           42.0 BURGOS \n## 7 29750      indio 29        14389642.              -4.04           36.8 MALAGA \n## # … with abbreviated variable names ¹​centroide_latitud, ²​provincia"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-en-granada",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-en-granada",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "IO al servicio del mal en GRANADA",
    "text": "IO al servicio del mal en GRANADA\nComo ejemplo, vamos a ver como sería para Granada\ncod_postales_granada <- cod_postales %>%\n  filter(provincia == \"GRANADA\") %>%\n  mutate(id = row_number())\n\nsedes_granada <- sedes %>%\n  filter(provincia == \"GRANADA\") %>%\n  arrange(desc(tipo)) %>%\n  mutate(id_sede = row_number())\n\nsedes_granada\n## # A tibble: 11 × 8\n##    cod_postal tipo  cod_prov    area_m2 centroide_long…¹ centr…² provi…³ id_sede\n##    <fct>      <chr> <chr>         <dbl>            <dbl>   <dbl> <chr>     <int>\n##  1 18328      jefe  18        58574459.            -3.87    37.2 GRANADA       1\n##  2 18006      jefe  18         3245912.            -3.61    37.2 GRANADA       2\n##  3 18516      jefe  18       146541813.            -3.24    37.2 GRANADA       3\n##  4 18516      jefe  18       146541813.            -3.24    37.2 GRANADA       4\n##  5 18197      indio 18        10003524.            -3.61    37.2 GRANADA       5\n##  6 18516      indio 18       146541813.            -3.24    37.2 GRANADA       6\n##  7 18414      indio 18        40411565.            -3.34    36.9 GRANADA       7\n##  8 18197      indio 18        10003524.            -3.61    37.2 GRANADA       8\n##  9 18369      indio 18        17670871.            -4.01    37.2 GRANADA       9\n## 10 18611      indio 18        33542783.            -3.60    36.8 GRANADA      10\n## 11 18514      indio 18       110524485.            -3.08    37.2 GRANADA      11\n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nEs importante haber ordenado por tipo , porque vamos a utilizar el mismo índice j para empleados jefe y empleados indios.\nAhora definimos: * m como el número de empleados en las sedes de Granada * n como el número de códigos postales a asignar en Granada * n_sedes como el número de sedes * njefes como el número de jefes * n_indios como el número de indios\nm <- nrow(sedes_granada)\nn <- nrow(cod_postales_granada)\nn_sedes <- length(unique(sedes_granada$cod_postal))\n\nnjefes <- sedes_granada %>%\n  filter(tipo == \"jefe\") %>%\n  count() %>%\n  pull(n)\n\nn_indios <- m - njefes\nNecesitamos definir una función de distancia entre los códigos postales a asignar y las sedes. Para eso usamos la distancia Haversine que está implementada en la librería geosphere. Y aquí ya introducimos uno de los requerimientos. Básicamente aumentamos la distancia un 10% si el empleado es un jefe, de forma que sea peor asignarle ese código postal al jefe en términos de minimizar el total de distancias.\ntransportcost_granada <- function(i, j) {\n  cliente <- cod_postales_granada[i, ]\n  comercial <- sedes_granada[j, ]\n  distancia <-\n    geosphere::distHaversine(\n      c(cliente$centroide_longitud, cliente$centroide_latitud),\n      c(comercial$centroide_longitud, comercial$centroide_latitud)\n    )\n\n  if (comercial[, \"tipo\"] == \"jefe\") {\n    distancia <- distancia * 1.1\n  }\n\n  return(distancia / 1000) # devolvemos la disancia en km\n}\n\n# distancia entre sede 1 y empleado 3\ntransportcost_granada(1, 3)\n## [1] 51.54738\nPintamos los códigos postales y las sedes. Los granadinos reconoceremos la forma de la provincia.\np <-\n  ggplot(\n    cod_postales_granada,\n    aes(centroide_longitud, centroide_latitud)\n  ) +\n  geom_point(size = rel(2), shape = 4) +\n  geom_point(\n    data = sedes_granada,\n    size = rel(3),\n    color = \"darkorange\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank()\n  )\np + ggtitle(\"Sin asignar\")"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#optimización",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#optimización",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "Optimización",
    "text": "Optimización\nPara optimizar el problema vamos a usar la librería ompr que permite plantear el problema de optimización lineal entera de forma sencilla, y se conecta a la librería ROI que es la que al final llama al solver. Como solver vamos a utilizar glpk que es software libre y lo suficientemente bueno para este ejemplo.\nlibrary(ompr)\nlibrary(ompr.roi)\nlibrary(ROI.plugin.glpk)\nlibrary(patchwork) # pa unir los ggplots resultantes\nDefinimos el modelo\nmip_model_granada <- MIPModel() %>%\n  # variable indicadora que indica si una tienda i se asigna a comercial j\n  add_variable(x[i, j], i = 1:n, j = 1:m, type = \"binary\") %>%\n  # Minimizar el objetivo de distancia\n  set_objective(sum_over(transportcost_granada(i, j) * x[i, j], i = 1:n, j = 1:m), \"min\") %>%\n  # cada tienda (código postal) solo debe ir a un comerciial. el comercial puede atender varios\n  add_constraint(sum_over(x[i, j], j = 1:m) == 1, i = 1:n) %>%\n  # todo el mundo tiene que atender al minimo a una tienda\n  add_constraint(sum_over(x[i, j], i = 1:n) >= 1, j = 1:m) %>%\n  #   Los jefes curran menos, como máximo 7 tiendas\n  add_constraint(sum_over(x[i, j], i = 1:n) <= 7, j = 1:njefes) %>%\n  #\n  # # Los indios al menos atienden a 3 tiendas\n  add_constraint(sum_over(x[i, j], i = 1:n) >= 3, j = (njefes + 1):m) %>%\n  # para no sobrecargar mucho a los indios, les pongo un máximo que sea 1.5 veces el núemro de tiendas entre total currantes (jefes + indios)\n  add_constraint(sum_over(x[i, j], i = 1:n) <= round(1.5 * n / m), j = (njefes + 1):m) %>%\n  add_constraint(sum_over(x[i, j], i = 1:n) >= sum_over(x[i, k], i = 1:n), j = (njefes + 1):m, k = 1:njefes)\nAlgunas aclaraciones de la sintaxis anterior.\n\nNuestra variable auxilizar es \\(X_{i,j}\\) dónde la i son los códigos postales y la j cada empleado.\nSe trata de minimizar la suma total de distancias cuando se asigna un código postal a un empleado, para todos los códigos postales y todos los empleados.\nLa restricción add_constraint(sum_over(x[i, j], j = 1:m) == 1  , i = 1:n) si nos fijamos en el sum_over significa sumar en j (empleados) para cada código postal (i) y que esa suma valga 1. Es decir, para cada código postal (i) sólo se permite que sea asignado a un empleado\nadd_constraint(sum_over(x[i, j], i = 1:n) >= 1  , j = 1:m) Que para cada empleado (j) la suma de todos los códigos postales que se le asignen sea mayor o igual que 1. Vamos que no se quede ninguno ocioso.\nadd_constraint(sum_over( x[i,j], i = 1:n)  <= 7, j = 1:njefes) por eso ordeanmos por tipo para que el índice 1:njefes corresponda a los empleados jefes, esta restricción asegura que a un jefe no se le asignen más de 7 códigos postales.\nadd_constraint(sum_over( x[i,j], i = 1:n)  >=  3 , j = (njefes +1):m) Mínimo 3 códigos postales para los indios.\nadd_constraint(sum_over( x[i,j], i = 1:n)  <=  round(1.5 * n/m) , j = (njefes +1):m) Esta restricción intenta equilibrar el número de asignaciones para los indios, de forma que como mucho a un empleado tenga 1.5 veces la media de códigos postales por empleado.\nadd_constraint(sum_over( x[i,j], i = 1:n)  >=  sum_over( x[i,k], i = 1:n) , j = (njefes +1):m, k = 1:njefes) En esta restricción es dónde aseguramos que ningún empleado tenga menos asignaciones que ningún jefe, por eso se ha usado el índice k.\n\nPues el problema tiene 2200 variables (todas binarias) y 257 restricciones.\nmip_model_granada\n## Mixed integer linear optimization problem\n## Variables:\n##   Continuous: 0 \n##   Integer: 0 \n##   Binary: 2200 \n## Model sense: minimize \n## Constraints: 257\nResolvemos con glpk\nresult_granada <- solve_model(mip_model_granada, with_ROI(solver = \"glpk\", verbose = TRUE))\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 257 rows, 2200 columns, 19200 non-zeros\n##       0: obj =   0.000000000e+00 inf =   2.320e+02 (218)\n##     397: obj =   9.427540716e+03 inf =   5.627e-13 (0) 1\n## *   870: obj =   3.723682515e+03 inf =   0.000e+00 (0) 2\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 257 rows, 2200 columns, 19200 non-zeros\n## 2200 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +   870: mip =     not found yet >=              -inf        (1; 0)\n## +   870: >>>>>   3.723682515e+03 >=   3.723682515e+03   0.0% (1; 0)\n## +   870: mip =   3.723682515e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\nresult_granada\n## Status: success\n## Objective value: 3723.683\nY ahora procedemos a ver las asignaciones. Para eso utilizamos la función get_solution que nos va a devolver la solución obtenida para nuestra variable \\(X_{i,j}\\)\nmatching <- result_granada %>%\n  get_solution(x[i, j]) %>%\n  select(i, j, value) %>%\n  filter(value > 0) # nons quedamos con las asignaciones\n\nmatching\n##       i  j value\n## 1    16  1     1\n## 2    27  1     1\n## 3    55  1     1\n## 4    68  1     1\n## 5   119  1     1\n## 6   157  1     1\n## 7   173  1     1\n## 8    13  2     1\n## 9    37  2     1\n## 10   96  2     1\n## 11  113  2     1\n## 12  161  2     1\n## 13  169  2     1\n## 14  178  2     1\n## 15  141  3     1\n## 16   34  4     1\n## 17    1  5     1\n## 18    8  5     1\n## 19   23  5     1\n## 20   30  5     1\n## 21   32  5     1\n## 22   71  5     1\n## 23   98  5     1\n## 24  102  5     1\n## 25  108  5     1\n## 26  112  5     1\n## 27  117  5     1\n## 28  120  5     1\n## 29  122  5     1\n## 30  124  5     1\n## 31  130  5     1\n## 32  132  5     1\n## 33  134  5     1\n## 34  137  5     1\n## 35  138  5     1\n## 36  140  5     1\n## 37  149  5     1\n## 38  170  5     1\n## 39  182  5     1\n## 40  191  5     1\n## 41  192  5     1\n## 42  194  5     1\n## 43  198  5     1\n## 44   10  6     1\n## 45   15  6     1\n## 46   20  6     1\n## 47   65  6     1\n## 48   69  6     1\n## 49   82  6     1\n## 50   83  6     1\n## 51   86  6     1\n## 52   87  6     1\n## 53   92  6     1\n## 54   93  6     1\n## 55  116  6     1\n## 56  128  6     1\n## 57  133  6     1\n## 58  135  6     1\n## 59  144  6     1\n## 60  151  6     1\n## 61  153  6     1\n## 62  163  6     1\n## 63  168  6     1\n## 64  174  6     1\n## 65  177  6     1\n## 66  190  6     1\n## 67  199  6     1\n## 68    2  7     1\n## 69    5  7     1\n## 70    6  7     1\n## 71    7  7     1\n## 72   11  7     1\n## 73   12  7     1\n## 74   17  7     1\n## 75   24  7     1\n## 76   26  7     1\n## 77   28  7     1\n## 78   31  7     1\n## 79   44  7     1\n## 80   48  7     1\n## 81   53  7     1\n## 82   56  7     1\n## 83   72  7     1\n## 84   77  7     1\n## 85   91  7     1\n## 86  104  7     1\n## 87  105  7     1\n## 88  131  7     1\n## 89  147  7     1\n## 90  156  7     1\n## 91  166  7     1\n## 92  171  7     1\n## 93  187  7     1\n## 94  193  7     1\n## 95   14  8     1\n## 96   39  8     1\n## 97   40  8     1\n## 98   47  8     1\n## 99   54  8     1\n## 100  59  8     1\n## 101  60  8     1\n## 102  62  8     1\n## 103  70  8     1\n## 104  73  8     1\n## 105  75  8     1\n## 106  78  8     1\n## 107  79  8     1\n## 108  84  8     1\n## 109  85  8     1\n## 110  90  8     1\n## 111  97  8     1\n## 112  99  8     1\n## 113 101  8     1\n## 114 109  8     1\n## 115 110  8     1\n## 116 118  8     1\n## 117 126  8     1\n## 118 167  8     1\n## 119 185  8     1\n## 120 189  8     1\n## 121 195  8     1\n## 122   9  9     1\n## 123  25  9     1\n## 124  29  9     1\n## 125  33  9     1\n## 126  35  9     1\n## 127  46  9     1\n## 128  50  9     1\n## 129  51  9     1\n## 130  57  9     1\n## 131  63  9     1\n## 132  67  9     1\n## 133  74  9     1\n## 134  80  9     1\n## 135  88  9     1\n## 136 103  9     1\n## 137 107  9     1\n## 138 111  9     1\n## 139 114  9     1\n## 140 115  9     1\n## 141 125  9     1\n## 142 136  9     1\n## 143 162  9     1\n## 144 172  9     1\n## 145 175  9     1\n## 146 179  9     1\n## 147 180  9     1\n## 148 196  9     1\n## 149   3 10     1\n## 150   4 10     1\n## 151  22 10     1\n## 152  36 10     1\n## 153  38 10     1\n## 154  45 10     1\n## 155  49 10     1\n## 156  61 10     1\n## 157  64 10     1\n## 158  76 10     1\n## 159  89 10     1\n## 160 106 10     1\n## 161 127 10     1\n## 162 129 10     1\n## 163 139 10     1\n## 164 143 10     1\n## 165 148 10     1\n## 166 152 10     1\n## 167 154 10     1\n## 168 155 10     1\n## 169 159 10     1\n## 170 176 10     1\n## 171 181 10     1\n## 172 183 10     1\n## 173 186 10     1\n## 174  18 11     1\n## 175  19 11     1\n## 176  21 11     1\n## 177  41 11     1\n## 178  42 11     1\n## 179  43 11     1\n## 180  52 11     1\n## 181  58 11     1\n## 182  66 11     1\n## 183  81 11     1\n## 184  94 11     1\n## 185  95 11     1\n## 186 100 11     1\n## 187 121 11     1\n## 188 123 11     1\n## 189 142 11     1\n## 190 145 11     1\n## 191 146 11     1\n## 192 150 11     1\n## 193 158 11     1\n## 194 160 11     1\n## 195 164 11     1\n## 196 165 11     1\n## 197 184 11     1\n## 198 188 11     1\n## 199 197 11     1\n## 200 200 11     1\nAhora vemos cuántas asignaciones tiene cada empleado y pintamos los resultados\nasignaciones <- matching %>%\n  group_by(j) %>%\n  summarise(asignaciones = sum(value)) %>%\n  arrange(desc(asignaciones)) %>%\n  left_join(sedes_granada, by = c(\"j\" = \"id_sede\"))\n\nasignaciones\n## # A tibble: 11 × 9\n##        j asignaciones cod_postal tipo  cod_prov  area_m2 centr…¹ centr…² provi…³\n##    <int>        <dbl> <fct>      <chr> <chr>       <dbl>   <dbl>   <dbl> <chr>  \n##  1     5           27 18197      indio 18         1.00e7   -3.61    37.2 GRANADA\n##  2     7           27 18414      indio 18         4.04e7   -3.34    36.9 GRANADA\n##  3     8           27 18197      indio 18         1.00e7   -3.61    37.2 GRANADA\n##  4     9           27 18369      indio 18         1.77e7   -4.01    37.2 GRANADA\n##  5    11           27 18514      indio 18         1.11e8   -3.08    37.2 GRANADA\n##  6    10           25 18611      indio 18         3.35e7   -3.60    36.8 GRANADA\n##  7     6           24 18516      indio 18         1.47e8   -3.24    37.2 GRANADA\n##  8     1            7 18328      jefe  18         5.86e7   -3.87    37.2 GRANADA\n##  9     2            7 18006      jefe  18         3.25e6   -3.61    37.2 GRANADA\n## 10     3            1 18516      jefe  18         1.47e8   -3.24    37.2 GRANADA\n## 11     4            1 18516      jefe  18         1.47e8   -3.24    37.2 GRANADA\n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nplot_assignment <- matching %>%\n  inner_join(cod_postales_granada, by = c(\"i\" = \"id\")) %>%\n  inner_join(sedes_granada, by = c(\"j\" = \"id_sede\"), suffix = c(\"_clientes\", \"_comerciales\"))\n\n\n\n\n\np_jefes <- p +\n  geom_segment(\n    data = plot_assignment %>%\n      filter(tipo == \"jefe\"),\n    aes(\n      x = centroide_longitud_comerciales,\n      y = centroide_latitud_comerciales,\n      xend = centroide_longitud_clientes,\n      yend = centroide_latitud_clientes\n    )\n  ) +\n  ggtitle(paste0(\"Asignaciones para los jefes\"))\n\n\np_indios <- p +\n  geom_segment(\n    data = plot_assignment %>%\n      filter(tipo == \"indio\"),\n    aes(\n      x = centroide_longitud_comerciales,\n      y = centroide_latitud_comerciales,\n      xend = centroide_longitud_clientes,\n      yend = centroide_latitud_clientes\n    )\n  ) +\n  ggtitle(paste0(\"Asignaciones para los indios\"))\n\n\np_or <- p +\n  labs(\n    title = \"sin asignar\",\n    subtitle = \"Granada\"\n  )\np_final <- p_or / p_jefes / p_indios\n\np_final"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-eligiendo-provincia",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-eligiendo-provincia",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "IO al servicio del mal eligiendo provincia",
    "text": "IO al servicio del mal eligiendo provincia\nCreo función (francamente mejorable y modularizable) para poder elegir provincia o provincias\nget_asignaciones_x_provincia <- function(cod_postales = cod_postales, sedes = sedes,\n                                         provincia_sel = \"MADRID\", plot = TRUE, ...) {\n  cod_postales_filt <- cod_postales %>%\n    filter(provincia %in% provincia_sel) %>%\n    mutate(id = row_number())\n\n  sedes_filt <- sedes %>%\n    filter(provincia %in% provincia_sel) %>%\n    arrange(desc(tipo)) %>%\n    mutate(id_sede = row_number())\n\n  m <- nrow(sedes_filt)\n  n <- nrow(cod_postales_filt)\n  n_sedes <- length(unique(sedes_filt$cod_postal))\n\n  njefes <- sedes_filt %>%\n    filter(tipo == \"jefe\") %>%\n    count() %>%\n    pull(n)\n\n  n_indios <- m - njefes\n\n  transportcost <- function(i, j) {\n    cliente <- cod_postales_filt[i, ]\n    comercial <- sedes_filt[j, ]\n    distancia <- geosphere::distHaversine(\n      c(cliente$centroide_longitud, cliente$centroide_latitud),\n      c(comercial$centroide_longitud, comercial$centroide_latitud)\n    )\n\n    if (comercial[, \"tipo\"] == \"jefe\") distancia <- distancia * 1.1\n\n    return(distancia / 1000)\n  }\n\n\n  p <- ggplot(cod_postales_filt, aes(centroide_longitud, centroide_latitud)) +\n    geom_point(size = rel(2), shape = 4) +\n    geom_point(data = sedes_filt, size = rel(3), color = \"darkorange\") +\n    # scale_x_continuous(limits = c(0, grid_size+1)) +\n    # scale_y_continuous(limits = c(0, grid_size+1)) +\n    theme(\n      axis.title = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text = element_blank(), panel.grid = element_blank()\n    )\n\n  mip_model <- MIPModel() %>%\n    # variable indicadora que indica si una tienda i se asigna a comercial j\n    add_variable(x[i, j], i = 1:n, j = 1:m, type = \"binary\") %>%\n    # Minimizar el objetivo de distancia\n    set_objective(sum_over(transportcost(i, j) * x[i, j], i = 1:n, j = 1:m), \"min\") %>%\n    # cada tienda (código postal) solo debe ir a un comerciial. el comercial puede atender varios\n    add_constraint(sum_over(x[i, j], j = 1:m) == 1, i = 1:n) %>%\n    # todo el mundo tiene que atender al minimo a una tienda\n    add_constraint(sum_over(x[i, j], i = 1:n) >= 1, j = 1:m) %>%\n    # %>%\n\n    #   Los jefes curran menos, como máximo 7 tiendas\n    add_constraint(sum_over(x[i, j], i = 1:n) <= 7, j = 1:njefes) %>%\n    #\n    # # Los indios al menos atienden a 3 tiendas\n    add_constraint(sum_over(x[i, j], i = 1:n) >= 3, j = (njefes + 1):m) %>%\n    # para no sobrecargar mucho a los indios, les pongo un máximo que sea 1.5 veces el núemro de tiendas entre total currantes (jefes + indios)\n    add_constraint(sum_over(x[i, j], i = 1:n) <= round(1.5 * n / m), j = (njefes + 1):m) %>%\n    add_constraint(sum_over(x[i, j], i = 1:n) >= sum_over(x[i, k], i = 1:n), j = (njefes + 1):m, k = 1:njefes)\n\n\n  result2 <- solve_model(mip_model, with_ROI(solver = \"glpk\", verbose = TRUE))\n\n\n  matching <- result2 %>%\n    get_solution(x[i, j]) %>%\n    select(i, j, value) %>%\n    filter(value > 0)\n\n\n\n  asignaciones <- matching %>%\n    group_by(j) %>%\n    summarise(asignaciones = sum(value)) %>%\n    arrange(desc(asignaciones)) %>%\n    left_join(sedes_filt, by = c(\"j\" = \"id_sede\"))\n\n\n  plot_assignment <- matching %>%\n    inner_join(cod_postales_filt, by = c(\"i\" = \"id\")) %>%\n    inner_join(sedes_filt, by = c(\"j\" = \"id_sede\"), suffix = c(\"_clientes\", \"_comerciales\"))\n\n\n\n  p_jefes <- p +\n    geom_segment(\n      data = plot_assignment %>%\n        filter(tipo == \"jefe\"),\n      aes(\n        x = centroide_longitud_comerciales,\n        y = centroide_latitud_comerciales,\n        xend = centroide_longitud_clientes,\n        yend = centroide_latitud_clientes\n      )\n    ) +\n    ggtitle(paste0(\"Asignaciones para los jefes\"))\n\n\n  p_indios <- p +\n    geom_segment(\n      data = plot_assignment %>%\n        filter(tipo == \"indio\"),\n      aes(\n        x = centroide_longitud_comerciales,\n        y = centroide_latitud_comerciales,\n        xend = centroide_longitud_clientes,\n        yend = centroide_latitud_clientes\n      )\n    ) +\n    ggtitle(paste0(\"Asignaciones para los indios\"))\n\n  subtitulo <- reduce(provincia_sel, function(x, y) paste(x, y, sep = \"-\"))\n  p_or <- p +\n    labs(\n      title = \"sin asignar\",\n      subtitle = subtitulo\n    )\n  p_final <- p_or / p_jefes / p_indios\n\n  if (plot) print(p_final)\n\n  return(list(\n    comerciales = sedes_filt,\n    cod_postales = cod_postales_filt,\n    matching = matching, tot_asignaciones = asignaciones, plot_final = p_final\n  ))\n}\nY veamos algunos ejemplos.\n\nMADRID\nmadrid <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = \"MADRID\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 385 rows, 4425 columns, 45725 non-zeros\n##       0: obj =   0.000000000e+00 inf =   3.400e+02 (320)\n##     498: obj =   1.415569938e+04 inf =   5.690e-14 (0) 1\n## Perturbing LP to avoid stalling [939]...\n## Removing LP perturbation [1341]...\n## *  1341: obj =   5.881701905e+03 inf =   0.000e+00 (0) 4\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 385 rows, 4425 columns, 45725 non-zeros\n## 4425 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1341: mip =     not found yet >=              -inf        (1; 0)\n## +  1341: >>>>>   5.881701905e+03 >=   5.881701905e+03   0.0% (1; 0)\n## +  1341: mip =   5.881701905e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\nPodemos ver cuántos códigos postales le han tocado a cada empleado.\nSe ve que se cumplen las restricciones. Seguramente para ser más equitativo habría que tocar algo a mano, para que a los empleados indios de la misma sede se repartan mejor los códigos postales. pero como primera aproximación no está mal\nmadrid$tot_asignaciones %>% \n  arrange(cod_postal)\n## # A tibble: 15 × 9\n##        j asignaciones cod_postal tipo  cod_prov  area_m2 centr…¹ centr…² provi…³\n##    <int>        <dbl> <fct>      <chr> <chr>       <dbl>   <dbl>   <dbl> <chr>  \n##  1     2            7 28011      jefe  28         3.03e7   -3.75    40.4 MADRID \n##  2     1            7 28015      jefe  28         2.59e6   -3.71    40.4 MADRID \n##  3     5            7 28015      jefe  28         2.59e6   -3.71    40.4 MADRID \n##  4    12           30 28035      indio 28         2.20e7   -3.74    40.5 MADRID \n##  5    14           30 28213      indio 28         8.52e7   -4.19    40.4 MADRID \n##  6     9           30 28521      indio 28         3.53e7   -3.50    40.3 MADRID \n##  7     7           30 28668      indio 28         3.65e6   -3.84    40.4 MADRID \n##  8    13           30 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n##  9     6           25 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 10     8           17 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 11    11            8 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 12     4            7 28817      jefe  28         6.04e7   -3.26    40.5 MADRID \n## 13    15           30 28901      indio 28         1.62e6   -3.73    40.3 MADRID \n## 14    10           30 28931      indio 28         8.78e5   -3.86    40.3 MADRID \n## 15     3            7 28931      jefe  28         8.78e5   -3.86    40.3 MADRID \n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nPodemos ver el detalle, por ejemplo qué códigos postales le toca al empleado j=4\nmadrid_asignaciones <-  madrid$tot_asignaciones  %>% \n  left_join(madrid$matching, by = \"j\") %>% \n  left_join(madrid$cod_postales, by = c(\"i\" = \"id\"), suffix = c(\"\",\"_tienda\")) \n\nmadrid_asignaciones %>% \n  filter(j==4) %>% \n  select(tipo ,j, i, cod_postal, cod_postal_tienda)\n## # A tibble: 7 × 5\n##   tipo      j     i cod_postal cod_postal_tienda\n##   <chr> <int> <int> <fct>      <fct>            \n## 1 jefe      4    61 28817      28810            \n## 2 jefe      4    71 28817      28812            \n## 3 jefe      4    89 28817      28818            \n## 4 jefe      4   121 28817      28515            \n## 5 jefe      4   155 28817      28804            \n## 6 jefe      4   172 28817      28817            \n## 7 jefe      4   219 28817      28811\n\n\nBarcelona\nbarcelona <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel =\"BARCELONA\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 471 rows, 5715 columns, 59055 non-zeros\n##       0: obj =   0.000000000e+00 inf =   4.260e+02 (406)\n##     600: obj =   1.389502410e+04 inf =   9.258e-13 (0) 1\n## Perturbing LP to avoid stalling [1077]...\n## Removing LP perturbation [1716]...\n## *  1716: obj =   7.841913058e+03 inf =   0.000e+00 (0) 5\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 471 rows, 5715 columns, 59055 non-zeros\n## 5715 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1716: mip =     not found yet >=              -inf        (1; 0)\n## +  1716: >>>>>   7.841913058e+03 >=   7.841913058e+03   0.0% (1; 0)\n## +  1716: mip =   7.841913058e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\n\n\nSevilla\nsevilla <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = \"SEVILLA\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 182 rows, 1064 columns, 7448 non-zeros\n##       0: obj =   0.000000000e+00 inf =   1.710e+02 (163)\n##     243: obj =   8.653234667e+03 inf =   5.145e-13 (0)\n## *   572: obj =   3.623165871e+03 inf =   0.000e+00 (0) 1\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 182 rows, 1064 columns, 7448 non-zeros\n## 1064 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +   572: mip =     not found yet >=              -inf        (1; 0)\n## +   572: >>>>>   3.623165871e+03 >=   3.623165871e+03   0.0% (1; 0)\n## +   572: mip =   3.623165871e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\n\n\nGranada y Málaga juntas\ngranada_malaga <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = c(\"GRANADA\",\"MALAGA\"))\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 488 rows, 7160 columns, 80550 non-zeros\n##       0: obj =   0.000000000e+00 inf =   4.230e+02 (393)\n##     515: obj =   2.754380624e+04 inf =   4.807e-13 (0) 1\n## Perturbing LP to avoid stalling [1388]...\n## Removing LP perturbation [1688]...\n## *  1688: obj =   7.728634950e+03 inf =   0.000e+00 (0) 5\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 488 rows, 7160 columns, 80550 non-zeros\n## 7160 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1688: mip =     not found yet >=              -inf        (1; 0)\n## +  1688: >>>>>   7.728634950e+03 >=   7.728634950e+03   0.0% (1; 0)\n## +  1688: mip =   7.728634950e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\nY hasta aquí ha llegado el uso de la IO para el mal. Feliz verano !!"
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "",
    "text": "Tengo una relación extraña con Julia, por un lado me gusta bastante y por otro me parece que aún le falta algo para que lo adopte de forma más seria. Quizá tenga que ver con mi forma de aprender (que seguro que no es óptima), en vez de irme a los tutoriales típicos, me voy directamente a ver cómo se hace algo que me interesa. En este caso hacer modelos bayesianos con Julia usando Turing.\nTuring es una librería escrita en Julia para programación probabilística, podría considerarse como un competidor de Stan, aunque todavía es una librería joven. Turing añade sólo una pequeña capa de programación probabilística, y promete cosas como modelos de redes neuronales dónde los pesos sigan una distribución probabilística\nNo me voy a meter en esos lares, yo soy más prosaico y por el momento sólo quiero ejemplificar con Turing el modelo que cuento en pluralista.\nRecordemos que habías simulado unos datos tal que así.\nEn la simulación se ha forzado que el efecto del número de hijos de la madre (M) sobre el número de hijos de la hija (D) sea cero.\nEl DAG era algo así. En este dag para estimar el efecto de M sobre D, hace falta condicionar por U, pero al ser una variable de confusión no observada, no habría forma de estimarlo de la forma tradicional (a lo Pearl). La solución es estimar el DAG completo."
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#ajuste-en-turing",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#ajuste-en-turing",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Ajuste en Turing",
    "text": "Ajuste en Turing\nRecordemos que nuestra U es una variable que no tenemos, se podría asimilar a una variable con todos sus valores perdidos y cada uno de esos valores perdidos es un parámetro a estimar.\nLibrerías : Aparte de Turing, hace falta ReverseDiff (diferenciación automática) y alguna más.\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nLeo los datos simulados que había guardado en un csv previamente\n\npl = DataFrame(CSV.File(\"data/pluralista.csv\"))\ndescribe(pl)\njulia> describe(pl)\n4×7 DataFrame\n Row │ variable  mean     min       median    max      nmissing  eltype   \n     │ Symbol    Float64  Real      Float64   Real     Int64     DataType \n─────┼────────────────────────────────────────────────────────────────────\n   1 │ D         1.00621  -3.55365  0.986136  6.03293         0  Float64\n   2 │ M         1.00836  -3.91626  0.90395   6.69591         0  Float64\n   3 │ B1        0.473     0        0.0       1               0  Int64\n   4 │ B2        0.487     0        0.0       1               0  Int64\nNos construimos el modelo con Turing.\nAlgunas cosas a comentar.\n\nEl uso de filldist para crear el vector de U y que cada valor siga una Normal(0,1).\n.+ para sumar un escalar como a1 con un vector. El uso del “.operacion” es habitual en julia para hacer broadcast.\nMvNormal al final. Esto lo he leído por ahí para que haga mejor el sampleo.\nAl igual que en Stan se tiene que escribir en cierto orden (y si no no funciona bien) porque Turing no es declarativo.\n\n@model function pluralista(D, M, B1, B2)\n\n    N = Int(length(D))\n\n    # Variable no observada\n    U ~ filldist(Normal(0, 1), N)\n\n\n    # Prior coeficientes\n    a1 ~ Normal(0, 0.5)\n    a2 ~ Normal(0, 0.5)\n    m  ~ Normal(0, 0.5)\n    b  ~ Normal(0, 0.5)\n    p  ~ Beta(2,2)\n    \n    \n    k ~  Exponential(1)\n    σ₁ ~ Exponential(1)\n    σ₂ ~ Exponential(1)\n    \n    B1 ~ Bernoulli(p)\n    B2 ~ Bernoulli(p)\n    \n    #  transformed parameters\n    mu1 = a1 .+ b * B1 + k * U\n    mu2 = a2 .+ b * B2 + m * M + k * U\n    \n    # likelihood\n\n\n    M ~ MvNormal(mu1, σ₁ * I) \n    D ~ MvNormal(mu2, σ₂ * I)\n\nend\nComparando con el código del mismo modelo en Stan (al final del post) se observa que la sintaxis es parecida."
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#muestreo-de-la-posterior-en-turing",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#muestreo-de-la-posterior-en-turing",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Muestreo de la posterior en Turing",
    "text": "Muestreo de la posterior en Turing\nHay que usar reversediff porque si no no acaba nunca.\nRandom.seed!(155)\n\n\nTuring.setadbackend(:reversediff)\nTuring.setrdcache(true)\n\nflbi = sample(\n    pluralista(pl.D, pl.M, pl.B1, pl.B2), \n    NUTS(1000, 0.65),\n    MCMCThreads(),\n    2_000, 4)\njulia> flbi = sample(\n           pluralista(pl.D, pl.M, pl.B1, pl.B2), \n           NUTS(1000, 0.65),\n           MCMCThreads(),\n           2_000, 4)\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.025\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n\nChains MCMC chain (2000×1020×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 136.29 seconds\nCompute duration  = 510.14 seconds\nY ha tardado unos 2 minutos por cadena. Ciertamente no está mal, pero no se acerca a la velocidad de Stan, que lo hace en unos 18 segundos.\nY podemos extraer un resumen de los parámetros que nos interesan con\njulia> summarize(flbi[[:a1, :a2, :b, :m, :σ₁, :σ₂]])\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n          a1    0.0682    0.0538     0.0006    0.0009   3268.9064    1.0007        6.4079\n          a2    0.0326    0.0759     0.0008    0.0024   1015.7923    1.0059        1.9912\n           m    0.0063    0.0430     0.0005    0.0018    554.1348    1.0096        1.0862\n           b    1.9865    0.0593     0.0007    0.0012   2403.5462    1.0008        4.7116\n          σ₁    1.1427    0.1205     0.0013    0.0049    535.2307    1.0086        1.0492\n          σ₂    0.9621    0.0719     0.0008    0.0016   2496.8176    1.0009        4.8944\n          \nY efectivamente, lo ha hecho bien y ha recuperado los verdaderos valores de los parámetros y estimado que el efecto de M sobre D es 0.\nmyplot = plot(flbi[[:a1, :a2, :b, :m, :σ₁, :σ₂]])\n\nsavefig(myplot,\"plurarlista_turing.png\")\n\n\n\nimagen"
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#reflexiones.",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#reflexiones.",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Reflexiones.",
    "text": "Reflexiones.\n\nMe ha parecido fácil escribir un modelo bayesiano como este en Turing\nNo he conseguido ver como hacer que me funcione un predict sobre nuevos datos que tengan B1 y B2, pero no M y D. Cuestión de empezar más poco a poco con los tutoriales que hay por ahí.\nPor el momento parece que Stan sigue siendo el estado del arte en estas cosas, aunque lo de integrar Turing con Flux por ejemplo, promete.\n\nMismo modelo en Stan.\n\ndata{\n    int N;\n    vector[N] D;\n    vector[N] M;\n    int B1[N];\n    int B2[N];\n}\n\n\nparameters{\n    vector[N] U;\n    real m;\n    real b;\n    real a2;\n    real a1;\n    real<lower=0> tau;\n    real<lower=0> sigma;\n    real<lower=0> k;\n    real<lower=0,upper=1> p;\n}\n\ntransformed parameters {\n  vector[N] nu;\n  vector[N] mu;\n\n\n  for ( i in 1:N ) {\n        nu[i] = a2 + b * B2[i] + m * M[i] + k * U[i];\n    }\n    \n  for ( i in 1:N ) {\n        mu[i] = a1 + b * B1[i] + k * U[i];\n    }\n\n\n}\n\nmodel{\n    \n    U ~ normal( 0 , 1 );\n    \n    a1 ~ normal( 0 , 0.5 );\n    a2 ~ normal( 0 , 0.5 );\n    m  ~ normal( 0 , 0.5 );\n    b  ~ normal( 0 , 0.5 );\n    p  ~ beta( 2 , 2 );\n    \n    k ~ exponential( 1 );\n    sigma ~ exponential( 1 );\n    tau   ~ exponential( 1 );\n    B2    ~ bernoulli( p );\n    B1    ~ bernoulli( p );\n\n    D ~ normal( nu , tau );\n    M ~ normal( mu , sigma );\n}\n\n// genero point_loglikelihood, util para evaluar modelo con psis loo\ngenerated quantities {\n vector[N] log_lik_D;\n vector[N] log_lik_M;\n\n  for (i in 1:N)\n    log_lik_D[i] = normal_lpdf(D[i] | nu[i], tau);\n\n  for (i in 1:N)\n    log_lik_M[i] = normal_lpdf(M[i] | mu[i], sigma);\n\n\n  }"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html",
    "href": "2022/09/18/veloooosidad/index.html",
    "title": "Veeelooosidad",
    "section": "",
    "text": "No, este post no va sobre la canción de Medina Azahara sino de comparar un par de librerías para lectura y procesamiento de datos. A saber, polars escrita en Rust y con api en python versus vroom en combinación con librerías como data.table o collapse en R. Estas últimas usan por debajo C++, así que tanto por el lado de python como por el de R el principal mérito se debe a usar Rust y C++."
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#datos-hardware-y-entornos",
    "href": "2022/09/18/veloooosidad/index.html#datos-hardware-y-entornos",
    "title": "Veeelooosidad",
    "section": "Datos, hardware y entornos",
    "text": "Datos, hardware y entornos\nPara hacer la comparación vamos a usar un dataset de 100 millones de filas y 9 columnas, el mismo que se usa en h2o.ai db-benchmark.\nLo voy a probar en mi pc, que es un slimbook de justo antes de la pandemia, con 1gb de ssd, 32Gb de RAM y procesador Intel i7-9750H (12) @ 4.500GHz con 6 núcleos (12 hilos) y corriendo Linux Mint 20.\n\nR\nPara R voy a chequear vroom y data.table para leer los datos y data.table, tidytable y collapse para el procesamiento\nR: Uso R version 4.2.1 (2022-06-23) – “Funny-Looking Kid” vroom: 1.5.7 data.table: 1.14.2 tidytable: 0.8.1.9 collapse: 1.8.8\n\n\nPython\nUso un entorno de conda con python 3.6.12 polars: ‘0.12.5’"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#scripts",
    "href": "2022/09/18/veloooosidad/index.html#scripts",
    "title": "Veeelooosidad",
    "section": "Scripts",
    "text": "Scripts\n\nR\nEn R voy a usar microbenchmark para realizar varias ejecuciones\nFichero: tests.R\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vroom)\nlibrary(tidytable)\n\nlibrary(microbenchmark)\n\n# Check lectu\n\nsetDTthreads(0L)\n\nlectura <- microbenchmark(\n    vroom  = vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE), \n    data.table = data.table::fread(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\"),\n    times = 3L\n)\n\nprint(lectura)\n\n# group by sum\n\nx <- vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE)\n\n# x= sample_frac(x, size = 0.1)\nx_dt <- qDT(x)\n\ngroup_by_performance <- microbenchmark(\n    data.table = x_dt[, lapply(.SD, mean, na.rm = TRUE), keyby = id1, .SDcols = 7:9],\n    # dplyr      = x %>%\n    #     group_by(id1, id2) %>%\n    #     summarise(v1 = sum(v1, na.rm = TRUE)) %>% \n    #     ungroup(),\n    tidytable = x_dt %>%\n        summarize.(v1 = sum(v1),\n                   v2 = sum(v2),\n                   v3 = sum(v3),\n                   .by = c(id1, id2)),\n    # base_R = tapply(x$v1, list(x$id1, x$id2), sum, na.rm = TRUE),\n\n    collapse= x_dt %>%\n        fgroup_by(id1, id2) %>%\n        fsummarise(v1 = fsum(v1),\n                   v2 = fsum(v2),\n                   v3 = fsum(v3)),\n\n    collapse_pure = {\n        g <- GRP(x, ~ id1 +id2)\n        fsum(x$v1, g)\n        fsum(x$v2, g)\n    },\n    times = 5L\n)\n\nprint(group_by_performance)\n\n\nPython\nFichero: tests.py\nimport polars as pl\nimport time\n\nstart = time.time()\ndf = pl.read_csv(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\")\nend = time.time()\n\nprint(end -start)\n\nstart = time.time()\n\n(\n    df\n.lazy()\n    .groupby(['id1','id2'])\n    .agg(\n        [\n            pl.col(\"v1\").sum().alias('v1_sum'),\n            pl.col(\"v2\").sum().alias('v2_sum'),\n            pl.col(\"v3\").sum().alias('v3_sum')\n        ]\n    )\n.collect()\n)\nend = time.time()\nprint(end - start)"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#resultados",
    "href": "2022/09/18/veloooosidad/index.html#resultados",
    "title": "Veeelooosidad",
    "section": "Resultados",
    "text": "Resultados\nPara comparar, ejecuto los scripts desde consola y teniendo cerrado navegadores, ides y demás.\n\nR\nRscript tests.R\nLectura en R\nUnit: seconds\n       expr       min        lq      mean    median        uq       max neval\n      vroom  7.783958  7.953598  8.185716  8.123239  8.386596  8.649953     3\n data.table 41.914928 42.809751 45.213309 43.704575 46.862499 50.020424     3\n\nGroup by y sum en R.\nUnit: seconds\n          expr      min       lq     mean   median       uq       max neval cld\n    data.table 1.469617 1.476545 1.550360 1.486647 1.633409  1.685581     5   a\n     tidytable 1.182273 1.189111 1.291734 1.279313 1.314287  1.493686     5   a\n      collapse 1.799175 1.813744 6.255215 1.891603 2.076616 23.694936     5   a\n collapse_pure 1.553002 1.555598 1.570758 1.566454 1.571605  1.607132     5   a\n\nPor lo que más o menos, usar vroom para leer y tidytable, data.table o collapse para hacer el cálculo sale por unos 10 segundos o un poco menos.\n\n\nPython\npython tests.py \n7.755492448806763\n1.8228027820587158\n\nY vemos que con polars tenemos más o menos los mismos tiempos."
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#conclusiones",
    "href": "2022/09/18/veloooosidad/index.html#conclusiones",
    "title": "Veeelooosidad",
    "section": "Conclusiones",
    "text": "Conclusiones\nTanto en R como en Python tenemos librerías muy rápidas que , si tenemos suficiente RAM podemos trabajar con conjunto de datos bastante tochos y hacer cosas en tiempos más que razonables.\nPolars es una librería muy nueva y muy bien hecha, ojalá hagan api para R. No obstante, data.table lleva tiempo en R y su desempeño es consistente en múltiples situaciones. Mi consejo es echarle un ojo al fastverse."
  },
  {
    "objectID": "2022/04/10/transparente/2022-04-10-transparente.html",
    "href": "2022/04/10/transparente/2022-04-10-transparente.html",
    "title": "Transparente",
    "section": "",
    "text": "El otro día le decía a mis compañeros que hay cosas que no entiendo de la jerga del mundillo en el que nos movemos, (para echar unas risas ver el video de Pantomima Full) .\nYa lo de “tengo una call”, o lo de “estamos alineados” me toca bastante los … pero bueno. Pero hay varias expresiones que me repatean profundamente, y voy a enumerarlas por orden decreciente de odio.\n\n“Reglas de negocio”. Esta frase te la sueltan cuándo no quieren decirte que lo que se hace son 3 “wheres” que alguien decidió en su día y que ni se evaluó su eficacia entonces, ni ahora. Es como un mantra bajo el cual cabe todo, cuándo no te quieren decir alguna cosa absurda que se hace, se dice “son reglas de negocio”, y ya es como que no puedes preguntar de qué se trata, es eso y fin.\n“Hacer foco”. ¿Cómo que hacer foco? ¿quíén ha empezado a maltratar el idioma de tal manera? Uno se enfoca, se pone el foco, etc, pero no se hace foco. Con lo bonito que sería decir “énfasis” o “hincapié”. Supongo que esto lo dijo alguien con poder en el pasado, y ya nadie se atrevió a corregir, y no sólo eso, sino que se adoptó y ahora es ley.\n“Transparente”. Igual que con reglas de negocio, alguien dice, “para vosotros esto será transparente” y te quedas igual, no sabes si significa que te va a afectar lo que sea que se haga, si no te afecta, si te afecta pero poco, o vete tu a saber el qué. Con lo que fácil que sería un “sujeto , verbo, predicado”.\n\nEn fin, buena semana santa, no hagan muchos “quick win” , beban algo “transparente” y si no saben como explicar algún desastre digan que “son reglas de negocio” y que “hay que hacer foco” en “el roadmap” para que estemos todos “alineados” y llegar a los “Okr’s” del próximo “Q”.\nAdvertencia: Huyan de todo áquel que use estas expresiones más de 2 veces cada media hora. Su productividad aumentará"
  },
  {
    "objectID": "2022/05/29/no-mentir-s/index.html",
    "href": "2022/05/29/no-mentir-s/index.html",
    "title": "No mentirás",
    "section": "",
    "text": "Hay veces que uno se deja llevar por la emoción cuando hace algo y a veces se exagera un poco con lo que hace tu criatura.\nTal es el caso de la librería Nanyml, la cual tiene buena pinta pero exagera en al menos dos partes. La primera y más evidente es cuándo dice que puede estimar el desempeño futuro de un modelo sin comparar con lo que realmente pase, así promete el Estimating Performance without Targets\nOs juro que me he leído la documentación varias veces e incluso he visto el código y en ningún lado he visto que haga eso que promete.\nEn realidad lo que hace no es más que basarse en dos asunciones que, si se leen en primer lugar, hace que la afirmación presuntuosa de estimar el desempeño de un modelo sin ver el target se caiga por su propio peso. A saber, las dos asunciones son.\n\nEl modelo retorna probabilidades bien calibradas siempre.\nLa relación de \\(P[y | X]\\) no cambia .\n\nEstas dos asunciones por si solas lo que nos dicen es que vas a medir el desempeño de un modelo (sin ver el verdadero valor del target) asumiendo de partida que el modelo es tan bueno como lo era cuando lo entrenaste.\nLa segunda parte es en lo que denomina CBPE algorithm que si se lee con atención no es otra cosa que simplemente utilizar el modelo para obtener predicciones sobre un nuevo conjunto de datos.\nAsí, para calcular el AUC estimado, lo que hace es asumir que el modelo es bueno, y obtener las diferentes matrices de confusión que se derivan de escoger los posibles puntos de corte y, aquí viene el tema, considerar que el valor predicho por el modelo, es el verdadero valor.\nCon estas asunciones , cualquier cambio en la métrica del AUC se debería sólo y exclusivamente a cambios en la estructura de la población y no a que el modelo haya dejado de ser bueno (lo cual es imposible puesto que es una de las asunciones)..\nEjemplo. Si tenemos 3 grupos distintos dónde tenemos un evento binario. Supongamos que el primero de ellos viene de una población con proporción igual a 0.25, el segundo grupo viene de una población con proporción de 0.8 y el tercero de una población con proporción de 0.032. Si tomamos 1000, 300 y 600 observaciones de cada población respectivamente podemos simular tener un score que cumpla la condición de estar bien calibrado\n\nps1 <- rbeta(1000, 1, 3)\nps2 <- rbeta(300, 4, 1)\nps3 <- rbeta(600, 2, 60)\n\nps <- c(ps1, ps2, ps3)\n\nmean(ps1) ;  mean(ps2); mean(ps3)\n#> [1] 0.2516992\n#> [1] 0.8020855\n#> [1] 0.03233513\n\nLa distribución de los “scores” sería\n\n\n\n\n\n\n\n\n\nPues el CBPE no sería otra cosa que calcular el auc del modelo ¡¡asumiendo que las probabilidades estimadas son correctas!! . Es como intentar demostrar algo teniendo como asunción que es cierto. Pero vayamos al cálculo.\nSiguiendo lo descrito por la documentación y comprobando con el código de la librería se tendría que\n\n\ntpr_fpr <- function(threshold, ps) {\n  yj <- ifelse(ps >= threshold, 1, 0) \n  p_false = abs(yj - ps)\n  p_true = 1- p_false\n  n <- length(yj)\n  tp <- sum(p_true[yj == 1])\n  fp <- sum(p_false[yj==1])\n  tn <- sum(p_true[yj==0] )\n  fn <- sum(p_false[yj==0] )\n  tpr <- tp / (tp + fn)\n  fpr <- fp /(fp + tn)\n  return(data.frame(tpr = tpr, fpr = fpr))\n}\n\n\npscortes = sort(unique(ps), decreasing = TRUE)\n\ndfs <-  lapply(pscortes, function(x) tpr_fpr(x, ps))\n\nvalores <- do.call(rbind, dfs)\n\n\nplot(valores$fpr, valores$tpr, type = \"l\")\n\n\n\n\n\n\n\n\nsimple_auc <- function(TPR, FPR){\n  # inputs already sorted, best scores first \n  dFPR <- c(diff(FPR), 0)\n  dTPR <- c(diff(TPR), 0)\n  sum(TPR * dFPR) + sum(dTPR * dFPR)/2\n}\n\nwith(valores, simple_auc(tpr, fpr))\n#> [1] 0.8914538\n\nCómo se ve, para calcular el auc sólo se tiene en cuenta las probabilidades estimadas, por lo que pierde todo el sentido para obtener un desempeño de cómo de bien lo hace el modelo.\nDe hecho, si hubiera simulado para cada observación una bernoulli tomando como probabilidad de éxito el score tendría lo siguiente, y tomo esa simulación como el valor real , obtengo el mismo auc que con CBPE.\n\nlabels <- rbinom(length(ps), 1, ps)\n(res <- pROC::auc(labels, ps))\n#> Area under the curve: 0.8962\n\nEs decir, en la misma definición de lo que es una matriz de confusión y las métricas asociadas va implícita la idea de comparar la realidad con la estimación, si sustituyes la realidad por la estimación , entonces pierde el sentido.\nPero veamos para qué si puede servir esta cosa. Pues nos puede servir para detectar cambios de distribuciones conjuntas entre dos conjuntos de datos. Me explico, supongamos que quiero predecir sobre un conjunto de datos que en vez de tener 1000 observaciones de la primera población hay 200, y que de la segunda hay 100 y 10000 de la tercera. Pues en este caso, el cambio en el auc se debe solo a eso, al cambio de la estructura de la población global.\n\n\nps1_new <- rbeta(200, 1, 3)\nps2_new <- rbeta(100, 4, 1)\nps3_new <- rbeta(10000, 2, 60)\n\nps_new <- c(ps1_new, ps2_new, ps3_new)\n\n\nlabels <- rbinom(length(ps_new), 1, ps_new)\n(res <- pROC::auc(labels, ps_new))\n#> Area under the curve: 0.7616\n\nLa bajada del “auc estimado” solo se debe a cambios en la estructura de la nueva población que tiene muchas más observaciones de la población 3.\nPor lo tanto, lo que nannyml hace y no está mal, ojo, es simplemente ver cuál serían métricas agregadas (como el auc) cuando cambia la estructura pero no la probabilidad condicionada de y con respecto a las variables independientes.\nLo que no me parece bien es poner en la documentación que calcula el desempeño de un modelo sin ver el target, puesto que confunde y ya ha dado lugar a algún post en “towards data science” (gente, formaros primero con libros antes de leer post de estos sitios) con más humo que madera.\nY como se suele decir “No mentirás”."
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "",
    "text": "En Palabras para Julia parte 3 hablaba de modelos bayesianos con Turing.jl, y me quedé con una espinita clavada, que era la de poder predecir de forma relativamente fácil con Turing, o incluso guardar de alguna forma la “posterior samples” y poder usar mi modelo en otra sesión de Julia.\nEmpiezo una serie de entradas cuyo objetivo es ver si puedo llegar a la lógica para poner “en producción” un modelo bayesiando con Turing, pero llegando incluso a crear un binario en linux que me permita predecir con un modelo y desplegarlo incluso en entornos dónde no está instalado Julia. La verdad, que no sé si lo conseguiré, pero al menos aprendo algo por el camino.\nSi, ya sé que existen los dockers y todo eso, pero no está de más saber que existen alternativas que quizá sean mejores. Ya en el pasado he tratado temas de cómo productivizar modelos de h2o sobre spark aquí o con Julia aquí. El objetivo final será llegar a tener un binario en linux que tome como argumento la ruta dónde se haya guardado las posterior samples de un modelo bayesiano y la ruta con especificación de dicho modelo en texto (para que Turing sepa como usar esas posterior samples) y que nos genere la posterior predictive para nuevos datos.\nAsí que vamos al lío. Empezamos por ver como entrenamos un modelo bayesiano con Turing y como se puede guardar y utilizar posteriormente."
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html#entrenamiento-con-julia",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html#entrenamiento-con-julia",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "Entrenamiento con Julia",
    "text": "Entrenamiento con Julia\nVamos a hacer un ejemplo sencillo, entrenando una regresión lineal múltiple de forma bayesiana. El dataset forma parte del material del libro Introduction to Statistical Learning. Advertising\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\n\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\nmm = DataFrame(CSV.File(\"data/Advertising.csv\"))\ndescribe(mm)\n\n\n200×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │       1    230.1     37.8       69.2     22.1\n   2 │       2     44.5     39.3       45.1     10.4\n   3 │       3     17.2     45.9       69.3      9.3\n  ⋮  │    ⋮        ⋮        ⋮         ⋮         ⋮\n 198 │     198    177.0      9.3        6.4     12.8\n 199 │     199    283.6     42.0       66.2     25.5\n 200 │     200    232.1      8.6        8.7     13.4\n                                     194 rows omitted\n\njulia> describe(mm)\n5×7 DataFrame\n Row │ variable   mean      min   median   max    nmissing  eltype   \n     │ Symbol     Float64   Real  Float64  Real   Int64     DataType \n─────┼───────────────────────────────────────────────────────────────\n   1 │ Column1    100.5      1     100.5   200           0  Int64\n   2 │ TV         147.043    0.7   149.75  296.4         0  Float64\n   3 │ radio       23.264    0.0    22.9    49.6         0  Float64\n   4 │ newspaper   30.554    0.3    25.75  114.0         0  Float64\n   5 │ sales       14.0225   1.6    12.9    27.0         0  Float64\nEspecificamos el modelo, y aquí tengo que comentar un par de cosas. Una que julia gracias a que implementa eficazmente el Multiple dispatch, podemos tener una misma función que devuelva cosas diferentes dependiendo de que le pasemos, así una función puede tener diferentes métodos. El otro aspecto es el uso del condition en Turing (alias |) se puede especificar el modelo sin pasar como argumento la variable dependiente y usarla solo para obtener la posterior, lo cual nos va a permitir hacer algo como predict( modelo(Xs), cadena_mcmc), y no tener que pasar la y como un valor perdido.\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n    \n    radio::AbstractVector{<:Real},\n    newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\nAhora tenemos el mismo modelo, que me a servir tanto para pasarle como argumentos escalares como vectores, nótese que la función Normal tomo como argumento la desviación típica, mientrar que MvNormal toma una matriz de varianzas/covarianzas. Se aconseja el uso de MvNormal en Turing pues mejora el tiempo de cálculo de la posteriori.\nObtenemos la posteriori de los parámetros, pasándole como datos el dataset de Advertising. Es importante que la columna de la variable dependiente se pase como NamedTuple, esto se puede hacer en julia usando (; vector_y) .\n\n# utilizamos 4 cadenas con n_samples = 2000  para cada una\n\n# usamos | para pasarle los datos de Y que no habiamos pasado en la especificacion del modelo\n\nchain = sample(mm_model_sin_sales(mm.TV, mm.radio, mm.newspaper) | (; mm.sales),\n    NUTS(0.65),MCMCThreads(),\n    2_000, 4)\n    \nY en unos 18 segundos tenemos nuestra MCMC Chain.\nChains MCMC chain (2000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 18.06 seconds\nCompute duration  = 71.54 seconds\nparameters        = a, tv_coef, radio_coef, newspaper_coef, σ₁\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n      parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n          Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n               a    2.0952    0.2712     0.0030    0.0038   5123.1176    0.9999       71.6139\n         tv_coef    0.0481    0.0013     0.0000    0.0000   7529.0954    0.9998      105.2461\n      radio_coef    0.1983    0.0087     0.0001    0.0001   5230.9995    1.0000       73.1220\n  newspaper_coef    0.0040    0.0059     0.0001    0.0001   6203.9490    1.0002       86.7224\n              σ₁    1.7205    0.0874     0.0010    0.0011   5441.9631    1.0000       76.0709\n\nQuantiles\n      parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n          Symbol   Float64   Float64   Float64   Float64   Float64 \n\n               a    1.5635    1.9104    2.0982    2.2788    2.6182\n         tv_coef    0.0455    0.0472    0.0481    0.0489    0.0508\n      radio_coef    0.1814    0.1923    0.1983    0.2042    0.2155\n  newspaper_coef   -0.0077    0.0001    0.0040    0.0078    0.0157\n              σ₁    1.5585    1.6607    1.7169    1.7781    1.8997\nVale, estupendo,en chain tenemos las 8000 samples para cada uno de los 5 parámetros , y también las de temas del ajuste interno por HMC, de ahí lo de (2000×17×4 Array{Float64, 3}).\nPero ¿cómo podemos predecir para nuevos datos?\nPues podemos pasarle simplemente 3 escalares correspondientes a las variables TV, radio y newspaper.\nEs necesario pasarle a la función predict la llamada al modelo con los nuevos datos mm_model_sin_sales(tv_valor, radio_valor,newspaper_valor) y las posterioris (la cadena MCMC) de los parámetros.\n\njulia> predict(mm_model_sin_sales(2, 5, 7), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales    3.2203    1.7435     0.0195    0.0176   8053.9924    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales   -0.1863    2.0441    3.2553    4.4030    6.6547\nTambién podemos pasarle más valores\n\njulia> mm_last = last(mm, 3)\n3×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │     198    177.0      9.3        6.4     12.8\n   2 │     199    283.6     42.0       66.2     25.5\n   3 │     200    232.1      8.6        8.7     13.4\n\n\njulia> predicciones = predict(mm_model_sin_sales(mm_last.TV, mm_last.radio, mm_last.newspaper), chain)\nChains MCMC chain (2000×3×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales[1], sales[2], sales[3]\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n    sales[1]   12.5192    1.7427     0.0195    0.0170   8270.6268    1.0000\n    sales[2]   24.3266    1.7560     0.0196    0.0222   7720.4172    1.0001\n    sales[3]   14.9901    1.7327     0.0194    0.0188   8039.4940    0.9999\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n    sales[1]    9.0888   11.3344   12.5241   13.6990   15.9571\n    sales[2]   20.8369   23.1519   24.3414   25.4967   27.7429\n    sales[3]   11.6549   13.8304   14.9617   16.1471   18.3733\nPodría quedarme con las predicciones para sales[1] y calcular el intervalo de credibilidad el 80%\njulia> quantile(reshape(Array(predicciones[\"sales[1]\"]), 8000), [0.1, 0.5, 0.9])\n3-element Vector{Float64}:\n 10.28185973755853\n 12.524091380928425\n 14.74877121738519"
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html#guardar-cadena-y-predecir",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html#guardar-cadena-y-predecir",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "Guardar cadena y predecir",
    "text": "Guardar cadena y predecir\nAhora viene la parte que nos interesa a los que nos dedicamos a esto y queremos usar un modelo entrenado hace 6 meses sobre datos de hoy. Guardar lo que hicimos y predecir sin necesidad de reentrenar.\nGuardamos la posteriori\n\n\nwrite( \"cadena.jls\", chain)\nY ahora, cerramos julia y abrimos de nuevo.\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\n# posteriori guardada\nchain = read(\"cadena.jls\", Chains)\n\n# Especificación del modelo (esto puede ir en otro fichero .jl)\n\n# Si tengo en un fichero jl el código de @model, lo puedo incluir ahí. \n\n\n# ruta = \"especificacion_modelo.jl\"\n# include(ruta)\n\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n     radio::AbstractVector{<:Real},\n      newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\n\n\n\nY aqui viene la parte importante. En la que utilizamos el modelo guardado, que no es más que las posterioris de los parámetros que hemos salvado en disco previamente.\n\n## predecimos la misma observación , fila 198 del dataset\n\npredict(mm_model_sin_sales(177, 9.3, 6.4 ), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales   12.4723    1.7285     0.0193    0.0186   8326.7650    0.9998\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales    9.0844   11.3106   12.4727   13.6334   15.7902\n       \nY voilá. Sabiendo que se puede guardar la posteriori y usarla luego , veo bastante factible poder llegar al objetivo de crear un “motor de predicción” de modelos bayesianos con Turing, que sea un ejecutable y que tome como argumentos la posteriori guardada de un modelo ajustado y en texto (con extensión jl ) y escriba el resultado en disco. Y lo dicho, que pueda desplegar este ejecutable en cualquier sistema linux, sin tener que instalar docker ni nada, solo hacer un unzip"
  },
  {
    "objectID": "2022/10/26/sigo-trasteando-con-julia/index.html",
    "href": "2022/10/26/sigo-trasteando-con-julia/index.html",
    "title": "Sigo trasteando con julia",
    "section": "",
    "text": "Siguiendo con lo que contaba aquí me he construido un binario para predecir usando un modelo de xgboost con Julia. La ventaja es que tengo un tar.gz que puedo descomprimir en cualquier linux (por ejemplo un entorno de producción sin acceso a internet y que no tenga ni vaya a tener julia instalado, ni docker ni nada de nada), descomprimir y poder hacer un miapp_para_predecir mi_modelo_entrenado.jls csv_to_predict.csv resultado.csv y que funcione y vaya como un tiro.\nPongo aquí los ficheros relevantes.\nPor ejemplo mi fichero para entrenar un modelo y salvarlo .\nFichero train_ boston.jl\n# Training model julia\nusing  CSV,CategoricalArrays, DataFrames, MLJ, MLJXGBoostInterface\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\ndf1[:, :target] .= ifelse.(df1[!, :medv_20].== \"NG20\", 1, 0)\nconst target = CategoricalArray(df1[:, :target])\n\nconst X = df1[:, Not([:medv_20, :target])]\n\nTree = @load XGBoostClassifier pkg=XGBoost\ntree_model = Tree(objective=\"binary:logistic\", max_depth = 6, num_round = 800)\nmach = machine(tree_model, X, target)\n\nThreads.nthreads()\nevaluate(tree_model, X, target, resampling=CV(shuffle=true),measure=log_loss, verbosity=0)\nevaluate(tree_model, X, target,\n                resampling=CV(shuffle=true), measure=bac, operation=predict_mode, verbosity=0)\n\n\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\nyhat = predict(mach, X[test,:])\n\nevaluate(tree_model, X[test,:], target[test], measure=auc, operation=predict_mode, verbosity=0)\n\nniveles = levels.(yhat)[1]\nniveles[1]\n\nlog_loss(yhat, target[test]) |> mean\n\nres = pdf(yhat, niveles)\nres_df = DataFrame(res,:auto)\n\nMLJ.save(\"models/boston_xg.jls\", mach)\nY luego los ficheros que uso para construirme la app binaria .. Recordemos del post que mencionaba que lo que necesito es el código del programa principal (el main) y un fichero de precompilación que sirve para que al crear la app se compilen las funciones que voy a usar.\nfichero precomp.jl,\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"models/mimodelo_xg_binario.jls\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, [\"target_0\", \"target_1\"])\n\nCSV.write(\"data/predicciones.csv\", res_df)\nfichero xgboost_predict_binomial.jl , aquí es dónde está el main\nmodule xgboost_predict_binomial\n\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, ntasks= Sys.CPU_THREADS)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, [\"target_0\", \"target_1\"])          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\ny si todo está correcto y siguiendo las instrucciones del post anterior, se compilaría haciendo por ejemplo esto\nusing PackageCompiler\ncreate_app(\"../xgboost_predict_binomial\", \"../xg_binomial_inference\",\n precompile_execution_file=\"../xgboost_predict_binomial/src/precomp_file.jl\", force=true, filter_stdlibs = true, cpu_target = \"x86_64\")\nY esto me crea una estructura de directorios dónde está mi app y todo lo necesario para ejecutar julia en cualqueir linux.\n\n╰─ $ ▶ tree -L 2 xg_binomial_inference\nxg_binomial_inference\n├── bin\n│   ├── julia\n│   └── xgboost_predict_binomial\n├── lib\n│   ├── julia\n│   ├── libjulia.so -> libjulia.so.1.8\n│   ├── libjulia.so.1 -> libjulia.so.1.8\n│   └── libjulia.so.1.8\n└── share\n    └── julia\ny poner por ejemplo en el .bashrc el siguiente alias.\nalias motor_xgboost=/home/jose/Julia_projects/xgboost_model/xg_binomial_inference/bin/xgboost_predict_binomial\ny ya está listo.\nAhora tengo un dataset a predecir de 5 millones de filas\n\n╰─ $ ▶ wc -l data/test.csv \n5060001 data/test.csv\n\n head -n4 data/test.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\ny bueno, tardo unos 11 segundos en obtener las predicciones y escribir el resultado\n╰─ $ ▶ time motor_xgboost models/boston_xg.jls data/test.csv pred.csv\n\nreal    0m11,091s\nuser    0m53,293s\nsys 0m2,321s\n\ny comprobamos que lo ha hecho bien\n\n╰─ $ ▶ wc -l  pred.csv \n5060001 pred.csv\n\n\n╰─ $ ▶ head -n 5 pred.csv \ntarget_0,target_1\n0.9999237,7.63197e-5\n0.99120975,0.008790266\n0.99989164,0.00010834133\n0.99970543,0.00029458306\nY nada, pues esto puede servir para subir modelos a producción en entornos poco amigables (sin python3, sin R, sin julia, sin spark, sin docker, sin internet). Es un poco old style que me diría mi arquenazi favorito Rubén, pero\nOs dejo el tar.gz para que probéis, también os dejo el Project.tomly el Manifest.toml y el fichero con el que he entrenado los datos. para que uséis el mismo entorno de julia que he usado yo.\nenlace_drive"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html",
    "title": "Api y docker con R. parte 1",
    "section": "",
    "text": "Todo el mundo anda haciendo apis para poner modelos en producción, y oye, está bien. Si además lo complementas con dockerizarlo para tener un entorno controlado y que te valga para ponerlo en cualquier sitio dónde esté docker instalado pues mejor.\nAquí voy a contar un ejemplo de como se puede hacer con R usando plumber y docker, en siguentes post contaré como hacerlo con vetiver que es una librería que está para R y Python que tiene algún extra, como versionado de modelos y demás.\nLo primero de todo es trabajar en un proyecto nuevo y usar renv. renv es para gestionar entornos de R, ojo que también funciona bien si tienes que mezclar R y python. Tiene cosas interesantes como descubrir las librerías que usas en tu proyecto y aún mejor, si estas librerías ya las tienes instaladas pues te crea enlaces simbólicos a dónde están y te permite ahorrar un montón de espacio, que al menos yo, no he conseguido ver cómo hacer eso con conda."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#objetivo",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#objetivo",
    "title": "Api y docker con R. parte 1",
    "section": "Objetivo",
    "text": "Objetivo\nMi objetivo es ver cómo pondría un modelo bayesiano ajustado con brms para que me devuelva predicciones puntuales y las posterioris en un entorno de producción."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#entrenando-modelo",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#entrenando-modelo",
    "title": "Api y docker con R. parte 1",
    "section": "Entrenando modelo",
    "text": "Entrenando modelo\nPara eso voy a usar datos de un antiguo post.\nUna vez que estemos en ese nuevo proyecto, ajustamos y guardamos un modelo .\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n## ✔ tibble  3.1.8      ✔ dplyr   1.0.10\n## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n## ✔ readr   2.1.3      ✔ forcats 0.5.2 \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(brms)\n## Loading required package: Rcpp\n## Loading 'brms' package (version 2.18.0). Useful instructions\n## can be found by typing help('brms'). A more detailed introduction\n## to the package is available through vignette('brms_overview').\n## \n## Attaching package: 'brms'\n## \n## The following object is masked from 'package:stats':\n## \n##     ar\nlibrary(cmdstanr)\n## Warning: package 'cmdstanr' was built under R version 4.3.0\n## This is cmdstanr version 0.5.2\n## - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n## - Use set_cmdstan_path() to set the path to CmdStan\n## - Use install_cmdstan() to install CmdStan\n## Using all cores. 12 in my machine, y que haga las cadenas en paralelo\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n## CmdStan path set to: /home/jose/cmdstan\ntrain <- read_csv(here::here(\"data/train_local.csv\"))\n## Rows: 662 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): segmento, tipo, edad_cat\n## dbl (2): valor_cliente, n\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# guiña a librería antigua\ncar::some(train)\n## # A tibble: 10 × 5\n##    segmento tipo  valor_cliente edad_cat     n\n##    <chr>    <chr>         <dbl> <chr>    <dbl>\n##  1 Rec      SM                2 21- 40       4\n##  2 Best     SM                1 41-50      475\n##  3 Best     C                 4 >60       2807\n##  4 No_way   C                 1 41-50      356\n##  5 No_way   B                 5 40-60      221\n##  6 Rec      SF                2 >60        152\n##  7 Rec      B                 4 40-60      194\n##  8 Best     C                 5 41-50     4934\n##  9 No_way   B                 3 41-50     1064\n## 10 No_way   SF                8 41-50       29\nAjustamos un modelo bayesiano con efectos aleatorios y usando la columna n como pesos de las filas. (leer el post dónde usé estos datos para saber más)\ntrain <- train %>% \n    mutate(target1 = as_factor(ifelse(segmento == \"Best\", \"Best\", \"Other\")))\n\n\nformula <- brmsformula(\n    target1| resp_weights(n)  ~ (1 | edad_cat) + (1 | valor_cliente) + (1 | tipo)\n    )\n\nmod <- brm(\n    formula,\n     family = \"bernoulli\", data = train, \n    iter = 4000, warmup = 1000, cores = 4, chains = 4,\n    seed = 10,\n    backend = \"cmdstanr\", \n     refresh = 0) # refresh 0 qu eno quiero que se me llene el post de los output de las cadenas mcm\n\nsaveRDS(mod, here::here(\"brms_model.rds\"))"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#comprobamos-que-nuestro-modelo-funciona",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#comprobamos-que-nuestro-modelo-funciona",
    "title": "Api y docker con R. parte 1",
    "section": "Comprobamos que nuestro modelo funciona",
    "text": "Comprobamos que nuestro modelo funciona\nlibrary(tidybayes)\n## \n## Attaching package: 'tidybayes'\n## The following objects are masked from 'package:brms':\n## \n##     dstudent_t, pstudent_t, qstudent_t, rstudent_t\nmod_reload <- readRDS(here::here(\"brms_model.rds\"))\n \n# \n\ntest <-  read_csv(here::here(\"data/test_local.csv\"))\n## Rows: 656 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): segmento, tipo, edad_cat\n## dbl (2): valor_cliente, n\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# estimacion puntual\npredict(mod_reload, head(test))\n##        Estimate Est.Error Q2.5 Q97.5\n## [1,] 0.23216667 0.4222324    0     1\n## [2,] 0.13233333 0.3388669    0     1\n## [3,] 0.16075000 0.3673155    0     1\n## [4,] 0.13825000 0.3451766    0     1\n## [5,] 0.12716667 0.3331735    0     1\n## [6,] 0.07333333 0.2606937    0     1\n# full posterior\n# para 6 filas guarda los valores obtenidos en las 3000 iteraciones de cada cadena\n# 3000 * 4 * 6 = 72000 valores \n\nposterior_pred <- add_epred_draws(head(test), mod_reload) \n\nhead(posterior_pred )\n## # A tibble: 6 × 10\n## # Groups:   segmento, tipo, valor_cliente, edad_cat, n, .row [1]\n##   segmento tipo  valor_cliente edad_cat     n  .row .chain .itera…¹ .draw .epred\n##   <chr>    <chr>         <dbl> <chr>    <dbl> <int>  <int>    <int> <int>  <dbl>\n## 1 Rec      C                 0 21- 40     132     1     NA       NA     1  0.230\n## 2 Rec      C                 0 21- 40     132     1     NA       NA     2  0.234\n## 3 Rec      C                 0 21- 40     132     1     NA       NA     3  0.233\n## 4 Rec      C                 0 21- 40     132     1     NA       NA     4  0.230\n## 5 Rec      C                 0 21- 40     132     1     NA       NA     5  0.232\n## 6 Rec      C                 0 21- 40     132     1     NA       NA     6  0.226\n## # … with abbreviated variable name ¹​.iteration\ndim(posterior_pred)\n## [1] 72000    10\nPara la primer fila podemos tener la distribución a posteriori\nposterior_pred %>% \n  filter(.row == 1) %>% \n  ggplot(aes(x=.epred)) +\n  geom_density() \n\nPues listo, ya tenemos el modelo entrenado y guardado, ahora sólo queda escribir el código para la api y el Dockerfile.."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creando-el-plumber.r",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creando-el-plumber.r",
    "title": "Api y docker con R. parte 1",
    "section": "Creando el plumber.R",
    "text": "Creando el plumber.R\nUna cosa importante, si hemos usado renv es escribir el fichero con las dependencias que usamos. Eso se hace con renv::snapshot() y se crea un fichero dónde están descritas las dependencias versionadas de nuestro proyecto.\nPero quizá para el docker no necesitemos todas, en este caso, partiendo del fichero anterior nos creamos otro con sólo las dependencias necesarias. Yo lo he llamado vetiver_renv.lock porque empecé trasteando con vetiver y soy demasiado vago como para cambiar ahora el nombre. El contenido del fichero es\nvetiver_renv.lock\n{\n  \"R\": {\n    \"Version\": \"4.2.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"binarios\",\n        \"URL\": \"https://packagemanager.rstudio.com/all/latest\"\n      },\n      {\n        \"Name\": \"ropenspain\",\n        \"URL\": \"https://ropenspain.r-universe.dev\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"plumber\": {\n      \"Package\": \"plumber\",\n      \"Version\": \"1.2.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"8b65a7a00ef8edc5ddc6fabf0aff1194\",\n      \"Requirements\": [\n        \"R6\",\n        \"crayon\",\n        \"ellipsis\",\n        \"httpuv\",\n        \"jsonlite\",\n        \"lifecycle\",\n        \"magrittr\",\n        \"mime\",\n        \"promises\",\n        \"rlang\",\n        \"sodium\",\n        \"stringi\",\n        \"swagger\",\n        \"webutils\"\n      ]\n    },\n    \"brms\": {\n      \"Package\": \"brms\",\n      \"Version\": \"2.18.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"afcb0d871e1759b68b29eb6affd37a10\",\n      \"Requirements\": [\n        \"Matrix\",\n        \"Rcpp\",\n        \"abind\",\n        \"backports\",\n        \"bayesplot\",\n        \"bridgesampling\",\n        \"coda\",\n        \"future\",\n        \"ggplot2\",\n        \"glue\",\n        \"loo\",\n        \"matrixStats\",\n        \"mgcv\",\n        \"nleqslv\",\n        \"nlme\",\n        \"posterior\",\n        \"rstan\",\n        \"rstantools\",\n        \"shinystan\"\n      ]\n    },\n    \"tidybayes\": {\n      \"Package\": \"tidybayes\",\n      \"Version\": \"3.0.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"d501501261b724f35ec9f2b80f4421b5\",\n      \"Requirements\": [\n        \"arrayhelpers\",\n        \"coda\",\n        \"dplyr\",\n        \"ggdist\",\n        \"ggplot2\",\n        \"magrittr\",\n        \"posterior\",\n        \"rlang\",\n        \"tibble\",\n        \"tidyr\",\n        \"tidyselect\",\n        \"vctrs\",\n        \"withr\"\n      ]\n    }\n  }\n}\n\nCómo veis también he añadido la librería tidybayes, porque me va a resultar útil para sacar la posteriori de las predicciones de los nuevos datos.\nCreamos el fichero plumber.R que no es más que decir cómo se va a predecir y crear un par de endpoints que permiten tanto obtener estimaciones puntuales como la full posterior. Con la librería plumber se hace fácil sin más que usar decoradores.\nFichero plumber.R\nlibrary(brms)\nlibrary(plumber)\nlibrary(tidybayes)\n\nbrms_model <- readRDS(\"brms_model.rds\")\n\n\n#* @apiTitle brms predict Api\n#* @apiDescription Endpoints for working with brms model\n\n## ---- filter-logger\n#* Log some information about the incoming request\n#* @filter logger\nfunction(req){\n    cat(as.character(Sys.time()), \"-\",\n        req$REQUEST_METHOD, req$PATH_INFO, \"-\",\n        req$HTTP_USER_AGENT, \"@\", req$REMOTE_ADDR, \"\\n\")\n    forward()\n}\n\n## ---- post-data\n#* Submit data and get a prediction in return\n#* @post /predict\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    predict(brms_model, data) |>\n        as.data.frame()\n}\n\n\n#* @post /full_posterior\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    add_epred_draws(data, brms_model) \n        \n}\nNo tiene mucho misterio, los endpoint se crean usando\n#* @post  /nombre_endpoing\ny creando una función que va a tomar los datos que le pasemos en formato json a la api, los pasa a data.frame y usa el modelo previamente cargado para obtener las estimaciones puntuales en un caso y la full posterior (con add_epred_draws) en el otro."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creamos-el-docker",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creamos-el-docker",
    "title": "Api y docker con R. parte 1",
    "section": "Creamos el docker",
    "text": "Creamos el docker\nIba a contar lo que es docker, pero mejor lo miráis en su web. Sólo quedarnos con la idea que es como tener una máquina virtual que puedo usar en otro sitio, pero es mucho más ligera y puede usar cosas del sistema anfitrión e interactuar con él.\nPara crear nuestra imagen docker tenemos que crear un fichero que se llame Dockerfile dónde vamos a ir diciéndole como cree nuestra máquina virtual.\nEs importante que estén los ficheros anteriores, el modelo salvado , el plumber.R y el fichero .lock en las rutas correctas dónde los busca el Dockerfile, en mi caso, lo he puesto todo en el mismo sitio.\nContendido del Dockerfile\n# Docker file para modelo brms\n\nFROM rocker/r-ver:4.2.1\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  default-jdk \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  make \\\n  zlib1g-dev \\\n  libxml2-dev \\\n  libglpk-dev \\\n  && apt-get clean\n\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\n\n## Copio el modelo y el fichero de la api\nCOPY brms_model.rds /opt/ml/brms_model.rds\nCOPY plumber.R /opt/ml/plumber.R\n\nEXPOSE 8081\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8081)\"]\n\n\nImportante que el puerto que se exponga con EXPOSE sea el mismo que usa el plumber, en este caso el 8081.\nAhora para construir la imagen docker y ejecutarla\ndocker build -t mi_modelo_brms .\n\nY despues de un rato podemos ejecutarlo mapeando el puerto\nnohup docker container run --rm -p 8081:8081 mi_modelo_brms &"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#funciona",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#funciona",
    "title": "Api y docker con R. parte 1",
    "section": "¿Funciona?",
    "text": "¿Funciona?\nPodemos usar curl, python, php o cualquier otra cosa para mandar peticiones a la api y que nos devuelva predicciones, con R sería algo así.\ntest %>% \n    head(2) \n## # A tibble: 2 × 5\n##   segmento tipo  valor_cliente edad_cat     n\n##   <chr>    <chr>         <dbl> <chr>    <dbl>\n## 1 Rec      C                 0 21- 40     132\n## 2 Best     B                 0 41-50       19\nbase_url <- \"http://0.0.0.0:8081\"\n\napi_res <- httr::POST(url = paste0(base_url, \"/predict\"),\n                      body = head(test),\n                      encode = \"json\")\npredicted_values <- httr::content(api_res, as = \"text\", encoding = \"UTF-8\")\n\njsonlite::fromJSON(predicted_values)\n##   Estimate Est.Error Q2.5 Q97.5\n## 1   0.2283    0.4198    0     1\n## 2   0.1356    0.3424    0     1\n## 3   0.1604    0.3670    0     1\n## 4   0.1320    0.3385    0     1\n## 5   0.1215    0.3267    0     1\n## 6   0.0737    0.2612    0     1\napi_res2 <- httr::POST(url = paste0(base_url, \"/full_posterior\"),\n                      body = head(test,1),\n                      encode = \"json\")\nposterior_values <- httr::content(api_res2, as = \"text\", encoding = \"UTF-8\")\n\n\njsonlite::fromJSON(posterior_values)  %>% \n  head(100)\n##     segmento tipo valor_cliente edad_cat   n .row .draw .epred\n## 1        Rec    C             0   21- 40 132    1     1 0.2297\n## 2        Rec    C             0   21- 40 132    1     2 0.2341\n## 3        Rec    C             0   21- 40 132    1     3 0.2330\n## 4        Rec    C             0   21- 40 132    1     4 0.2296\n## 5        Rec    C             0   21- 40 132    1     5 0.2321\n## 6        Rec    C             0   21- 40 132    1     6 0.2256\n## 7        Rec    C             0   21- 40 132    1     7 0.2211\n## 8        Rec    C             0   21- 40 132    1     8 0.2215\n## 9        Rec    C             0   21- 40 132    1     9 0.2259\n## 10       Rec    C             0   21- 40 132    1    10 0.2245\n## 11       Rec    C             0   21- 40 132    1    11 0.2330\n## 12       Rec    C             0   21- 40 132    1    12 0.2263\n## 13       Rec    C             0   21- 40 132    1    13 0.2262\n## 14       Rec    C             0   21- 40 132    1    14 0.2426\n## 15       Rec    C             0   21- 40 132    1    15 0.2307\n## 16       Rec    C             0   21- 40 132    1    16 0.2348\n## 17       Rec    C             0   21- 40 132    1    17 0.2293\n## 18       Rec    C             0   21- 40 132    1    18 0.2281\n## 19       Rec    C             0   21- 40 132    1    19 0.2304\n## 20       Rec    C             0   21- 40 132    1    20 0.2277\n## 21       Rec    C             0   21- 40 132    1    21 0.2283\n## 22       Rec    C             0   21- 40 132    1    22 0.2355\n## 23       Rec    C             0   21- 40 132    1    23 0.2297\n## 24       Rec    C             0   21- 40 132    1    24 0.2257\n## 25       Rec    C             0   21- 40 132    1    25 0.2191\n## 26       Rec    C             0   21- 40 132    1    26 0.2275\n## 27       Rec    C             0   21- 40 132    1    27 0.2328\n## 28       Rec    C             0   21- 40 132    1    28 0.2312\n## 29       Rec    C             0   21- 40 132    1    29 0.2190\n## 30       Rec    C             0   21- 40 132    1    30 0.2370\n## 31       Rec    C             0   21- 40 132    1    31 0.2303\n## 32       Rec    C             0   21- 40 132    1    32 0.2252\n## 33       Rec    C             0   21- 40 132    1    33 0.2190\n## 34       Rec    C             0   21- 40 132    1    34 0.2269\n## 35       Rec    C             0   21- 40 132    1    35 0.2311\n## 36       Rec    C             0   21- 40 132    1    36 0.2309\n## 37       Rec    C             0   21- 40 132    1    37 0.2313\n## 38       Rec    C             0   21- 40 132    1    38 0.2361\n## 39       Rec    C             0   21- 40 132    1    39 0.2335\n## 40       Rec    C             0   21- 40 132    1    40 0.2414\n## 41       Rec    C             0   21- 40 132    1    41 0.2333\n## 42       Rec    C             0   21- 40 132    1    42 0.2283\n## 43       Rec    C             0   21- 40 132    1    43 0.2354\n## 44       Rec    C             0   21- 40 132    1    44 0.2314\n## 45       Rec    C             0   21- 40 132    1    45 0.2357\n## 46       Rec    C             0   21- 40 132    1    46 0.2240\n## 47       Rec    C             0   21- 40 132    1    47 0.2241\n## 48       Rec    C             0   21- 40 132    1    48 0.2355\n## 49       Rec    C             0   21- 40 132    1    49 0.2260\n## 50       Rec    C             0   21- 40 132    1    50 0.2268\n## 51       Rec    C             0   21- 40 132    1    51 0.2278\n## 52       Rec    C             0   21- 40 132    1    52 0.2213\n## 53       Rec    C             0   21- 40 132    1    53 0.2246\n## 54       Rec    C             0   21- 40 132    1    54 0.2316\n## 55       Rec    C             0   21- 40 132    1    55 0.2313\n## 56       Rec    C             0   21- 40 132    1    56 0.2209\n## 57       Rec    C             0   21- 40 132    1    57 0.2269\n## 58       Rec    C             0   21- 40 132    1    58 0.2323\n## 59       Rec    C             0   21- 40 132    1    59 0.2280\n## 60       Rec    C             0   21- 40 132    1    60 0.2357\n## 61       Rec    C             0   21- 40 132    1    61 0.2275\n## 62       Rec    C             0   21- 40 132    1    62 0.2387\n## 63       Rec    C             0   21- 40 132    1    63 0.2387\n## 64       Rec    C             0   21- 40 132    1    64 0.2231\n## 65       Rec    C             0   21- 40 132    1    65 0.2370\n## 66       Rec    C             0   21- 40 132    1    66 0.2313\n## 67       Rec    C             0   21- 40 132    1    67 0.2243\n## 68       Rec    C             0   21- 40 132    1    68 0.2335\n## 69       Rec    C             0   21- 40 132    1    69 0.2275\n## 70       Rec    C             0   21- 40 132    1    70 0.2340\n## 71       Rec    C             0   21- 40 132    1    71 0.2250\n## 72       Rec    C             0   21- 40 132    1    72 0.2373\n## 73       Rec    C             0   21- 40 132    1    73 0.2259\n## 74       Rec    C             0   21- 40 132    1    74 0.2405\n## 75       Rec    C             0   21- 40 132    1    75 0.2227\n## 76       Rec    C             0   21- 40 132    1    76 0.2210\n## 77       Rec    C             0   21- 40 132    1    77 0.2337\n## 78       Rec    C             0   21- 40 132    1    78 0.2306\n## 79       Rec    C             0   21- 40 132    1    79 0.2242\n## 80       Rec    C             0   21- 40 132    1    80 0.2235\n## 81       Rec    C             0   21- 40 132    1    81 0.2247\n## 82       Rec    C             0   21- 40 132    1    82 0.2188\n## 83       Rec    C             0   21- 40 132    1    83 0.2129\n## 84       Rec    C             0   21- 40 132    1    84 0.2415\n## 85       Rec    C             0   21- 40 132    1    85 0.2293\n## 86       Rec    C             0   21- 40 132    1    86 0.2312\n## 87       Rec    C             0   21- 40 132    1    87 0.2189\n## 88       Rec    C             0   21- 40 132    1    88 0.2236\n## 89       Rec    C             0   21- 40 132    1    89 0.2262\n## 90       Rec    C             0   21- 40 132    1    90 0.2317\n## 91       Rec    C             0   21- 40 132    1    91 0.2316\n## 92       Rec    C             0   21- 40 132    1    92 0.2288\n## 93       Rec    C             0   21- 40 132    1    93 0.2299\n## 94       Rec    C             0   21- 40 132    1    94 0.2288\n## 95       Rec    C             0   21- 40 132    1    95 0.2311\n## 96       Rec    C             0   21- 40 132    1    96 0.2264\n## 97       Rec    C             0   21- 40 132    1    97 0.2269\n## 98       Rec    C             0   21- 40 132    1    98 0.2287\n## 99       Rec    C             0   21- 40 132    1    99 0.2283\n## 100      Rec    C             0   21- 40 132    1   100 0.2191\nSeguramente usar una api para obtener la posteriori que tiene tantos valores para cada dato no sea lo más eficiente, porque lo devuelve en formato json y luego hay que convertirlo a data.frame, pero funciona."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#salvar-docker-en-un-tar.gz",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#salvar-docker-en-un-tar.gz",
    "title": "Api y docker con R. parte 1",
    "section": "Salvar docker en un tar.gz",
    "text": "Salvar docker en un tar.gz\nSi no tenemos un sitio estilo docker hub dónde registrar nuestros docker o por cualquier otra causa, podemos usar docker save para generar un fichero comprimido y docker load para importarlo.\nSería algo así como\n\ndocker save mi_modelo_brms | gzip > mi_modelo_brms_docker.tar.gz\nCopiar ese tar.gz a dónde toque\ndocker load < mi_modelo_brms_docker.tar.gz"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#adelanto-con-vetiver",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#adelanto-con-vetiver",
    "title": "Api y docker con R. parte 1",
    "section": "Adelanto con vetiver",
    "text": "Adelanto con vetiver\nCon la librería vetiver se simplifica todo este proceso, puesto que crea por ti el plumber.R y el dockerfile y tiene movidas para guardar la monitorización del modelo y demás. Está tanto para R como para python. En R soporta los modelos que estén en tidymodels y en python soporta scikit-learn, statmodels, xgboost y creo que también pytorch"
  },
  {
    "objectID": "2022/10/30/api-y-docker-con-r-parte-2.html",
    "href": "2022/10/30/api-y-docker-con-r-parte-2.html",
    "title": "Api y docker con R. parte 2",
    "section": "",
    "text": "En la entrada de api y docker con R parte I veíamos que es muy fácil construir una api y dockerizarla para tener un modelo bayesiano en producción. Pero hay un pequeño incoveniente, el docker que hemos creado se base en rocker/verse que se basan en ubuntu. Y ubuntu ocupa mucho. Pero gracias a gente como Gabor Csardi (autor entre otras librerías de igraph), tenemos r-hub/minimal, que permiten tener una imagen de docker con R basadas en alpine, de hecho una imagen de docker con R y dplyr son unos 50 mb.\nLo primero de todo es ver cuánto ocupa el docker creado en el primer post.\nPues son unos cuántos gigas, mayoritariamente al estar basado en ubuntu y al que los docker de rocker/verse instalan todo el software de R recomendado, los ficheros de ayuda, las capacidades gráficas, etc..\nPero con r-hub/minimal podemos dejar bastante limpio el tema. Leyendo el Readme del repo vemos que han configurado una utilidad a la que llaman installr que permite instalar librerías del sistema o de R, instalando los compiladores de C, fortran etc que haga falta y eliminarlos una vez están compiladas la librerías.\nSin más, cambiamos el Dockerfile del otro día por este otro .\nY haciendo docker build -t mi_modelo_brms_rminimal . pasado un rato puesto que ha de compilar las librerías tenemos nuestra api dockerizada con la misma funcionalidad que el otro día.\nY con un tamaño mucho más contenido\nque se va a unos 655 mb, de los cuales unos 300 MB se deben a stan y rstan. Pero vamos, no está mal, pasar de 3.4 Gb a 665MB."
  },
  {
    "objectID": "2022/10/30/api-y-docker-con-r-parte-2.html#actualización-usando-renv",
    "href": "2022/10/30/api-y-docker-con-r-parte-2.html#actualización-usando-renv",
    "title": "Api y docker con R. parte 2",
    "section": "Actualización, usando renv",
    "text": "Actualización, usando renv\nPor temas de buenas prácticas es recomendable usar renv para crear el archivo renv.lock dónde se guarda qué versión de las librerías estamos usando, y además porque usa por defecto un repo con las librerías compiladas.\nLo primero que hago es crearme un nuevo proyecto dónde pongo el modelo entrenado que queremos usar brms_model.rds que entrené en el primer post y el fichero plumber.R y ningún fichero más.\nFichero plumber.R\n\n\n#\n# This is a Plumber API. In RStudio 1.2 or newer you can run the API by\n# clicking the 'Run API' button above.\n#\n# In RStudio 1.1 or older, see the Plumber documentation for details\n# on running the API.\n#\n# Find out more about building APIs with Plumber here:\n#\n#    https://www.rplumber.io/\n#\n# save as bos_rf_score.R\n\nlibrary(brms)\nlibrary(plumber)\nlibrary(tidybayes)\n\nbrms_model <- readRDS(\"brms_model.rds\")\n\n\n#* @apiTitle brms predict Api\n#* @apiDescription Endpoints for working with brms model\n## ---- filter-logger\n#* Log some information about the incoming request\n#* @filter logger\nfunction(req){\n    cat(as.character(Sys.time()), \"-\",\n        req$REQUEST_METHOD, req$PATH_INFO, \"-\",\n        req$HTTP_USER_AGENT, \"@\", req$REMOTE_ADDR, \"\\n\")\n    forward()\n}\n\n## ---- post-data\n#* Submit data and get a prediction in return\n#* @post /predict\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    predict(brms_model, data) |>\n        as.data.frame()\n}\n\n\n#* @post /full_posterior\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    add_epred_draws(data, brms_model) \n    \n}\n\nA continuación activo renv en el proyecto\n\n renv::activate()\n* Project '~/Rstudio_projects/r-api-minimal' loaded. [renv 0.16.0]\n\nUna vez que está activado y el fichero plumber.R está creado en el directorio uso hydrate para que encuentre qué librerías se usan en el proyecto\n\n\n> renv::hydrate()\n* Discovering package dependencies ... Done!\n* Copying packages into the cache ... Done!\n\ny ya podemos crear el fichero renv::snapshot(), donde pone todas las librerías que se van a instalar y si vienen de CRAN , de GitHub o de RSPM(rstudio package manager)\n\nrenv::snapshot()\nThe following package(s) will be updated in the lockfile:\n\n# CRAN ===============================\n- Matrix           [* -> 1.5-1]\n- R6               [* -> 2.5.1]\n- RColorBrewer     [* -> 1.1-3]\n- Rcpp             [* -> 1.0.9]\n- base64enc        [* -> 0.1-3]\n- bslib            [* -> 0.4.0]\n- cachem           [* -> 1.0.6]\n- codetools        [* -> 0.2-18]\n- colorspace       [* -> 2.0-3]\n- ellipsis         [* -> 0.3.2]\n- fansi            [* -> 1.0.3]\n- farver           [* -> 2.1.1]\n- fastmap          [* -> 1.1.0]\n- generics         [* -> 0.1.3]\n- ggplot2          [* -> 3.3.6]\n- htmltools        [* -> 0.5.3]\n- jquerylib        [* -> 0.1.4]\n- labeling         [* -> 0.4.2]\n- lattice          [* -> 0.20-45]\n- lifecycle        [* -> 1.0.3]\n- magrittr         [* -> 2.0.3]\n- memoise          [* -> 2.0.1]\n- mgcv             [* -> 1.8-40]\n- mime             [* -> 0.12]\n- munsell          [* -> 0.5.0]\n- pkgconfig        [* -> 2.0.3]\n- prettyunits      [* -> 1.1.1]\n- processx         [* -> 3.7.0]\n- ps               [* -> 1.7.1]\n- rappdirs         [* -> 0.3.3]\n- rprojroot        [* -> 2.0.3]\n- sass             [* -> 0.4.2]\n- stringi          [* -> 1.7.8]\n- tibble           [* -> 3.1.8]\n- utf8             [* -> 1.2.2]\n- withr            [* -> 2.5.0]\n\n# GitHub =============================\n- glue             [* -> jimhester/fstrings@HEAD]\n\n# RSPM ===============================\n- BH               [* -> 1.78.0-0]\n- Brobdingnag      [* -> 1.2-9]\n- DT               [* -> 0.26]\n- HDInterval       [* -> 0.2.2]\n- MASS             [* -> 7.3-58.1]\n- RcppEigen        [* -> 0.3.3.9.2]\n- RcppParallel     [* -> 5.1.5]\n- StanHeaders      [* -> 2.21.0-7]\n- abind            [* -> 1.4-5]\n- arrayhelpers     [* -> 1.1-0]\n- backports        [* -> 1.4.1]\n- bayesplot        [* -> 1.9.0]\n- bridgesampling   [* -> 1.1-2]\n- brms             [* -> 2.18.0]\n- callr            [* -> 3.7.2]\n- checkmate        [* -> 2.1.0]\n- cli              [* -> 3.4.1]\n- coda             [* -> 0.19-4]\n- colourpicker     [* -> 1.1.1]\n- commonmark       [* -> 1.8.1]\n- cpp11            [* -> 0.4.3]\n- crayon           [* -> 1.5.2]\n- crosstalk        [* -> 1.2.0]\n- curl             [* -> 4.3.3]\n- desc             [* -> 1.4.2]\n- digest           [* -> 0.6.30]\n- distributional   [* -> 0.3.1]\n- dplyr            [* -> 1.0.10]\n- dygraphs         [* -> 1.1.1.6]\n- fontawesome      [* -> 0.3.0]\n- fs               [* -> 1.5.2]\n- future           [* -> 1.28.0]\n- ggdist           [* -> 3.2.0]\n- ggridges         [* -> 0.5.4]\n- globals          [* -> 0.16.1]\n- gridExtra        [* -> 2.3]\n- gtable           [* -> 0.3.1]\n- gtools           [* -> 3.9.3]\n- htmlwidgets      [* -> 1.5.4]\n- httpuv           [* -> 1.6.6]\n- igraph           [* -> 1.3.5]\n- inline           [* -> 0.3.19]\n- isoband          [* -> 0.2.6]\n- jsonlite         [* -> 1.8.2]\n- later            [* -> 1.3.0]\n- lazyeval         [* -> 0.2.2]\n- listenv          [* -> 0.8.0]\n- loo              [* -> 2.5.1]\n- markdown         [* -> 1.2]\n- matrixStats      [* -> 0.62.0]\n- miniUI           [* -> 0.1.1.1]\n- mvtnorm          [* -> 1.1-3]\n- nleqslv          [* -> 3.3.3]\n- nlme             [* -> 3.1-160]\n- numDeriv         [* -> 2016.8-1.1]\n- parallelly       [* -> 1.32.1]\n- pillar           [* -> 1.8.1]\n- pkgbuild         [* -> 1.3.1]\n- plumber          [* -> 1.2.1]\n- plyr             [* -> 1.8.7]\n- posterior        [* -> 1.3.1]\n- promises         [* -> 1.2.0.1]\n- purrr            [* -> 0.3.5]\n- renv             [* -> 0.16.0]\n- reshape2         [* -> 1.4.4]\n- rlang            [* -> 1.0.6]\n- rstan            [* -> 2.21.7]\n- rstantools       [* -> 2.2.0]\n- scales           [* -> 1.2.1]\n- shiny            [* -> 1.7.2]\n- shinyjs          [* -> 2.1.0]\n- shinystan        [* -> 2.6.0]\n- shinythemes      [* -> 1.2.0]\n- sodium           [* -> 1.2.1]\n- sourcetools      [* -> 0.1.7]\n- stringr          [* -> 1.4.1]\n- svUnit           [* -> 1.0.6]\n- swagger          [* -> 3.33.1]\n- tensorA          [* -> 0.36.2]\n- threejs          [* -> 0.3.3]\n- tidybayes        [* -> 3.0.2]\n- tidyr            [* -> 1.2.1]\n- tidyselect       [* -> 1.2.0]\n- vctrs            [* -> 0.4.2]\n- viridisLite      [* -> 0.4.1]\n- webutils         [* -> 1.1]\n- xfun             [* -> 0.34]\n- xtable           [* -> 1.8-4]\n- xts              [* -> 0.12.2]\n- yaml             [* -> 2.3.6]\n- zoo              [* -> 1.8-11]\n\nThe version of R recorded in the lockfile will be updated:\n- R                [*] -> [4.2.1]\n\nDo you want to proceed? [y/N]: y\n* Lockfile written to '~/Rstudio_projects/r-api-minimal/renv.lock'.\n\nY ya sólo queda crear el Dockerfile usando como base r-hub/minimal\nDockerfile\n# Docker file para modelo brms\n\nFROM rhub/r-minimal:4.2.1\n\n# copio fichero de las librerías\nCOPY renv.lock renv.lock\n\n# uso -c para que se queden instaladas los compiladores de c y fortran\n\nRUN installr -c -a \"curl-dev linux-headers gfortran libcurl libxml2 libsodium-dev libsodium automake autoconf\"\n\n#instalo renv\nRUN installr -c renv\n\n# uso renv para instlar la versión de las librerías que hay en renv.lock\nRUN Rscript -e \"renv::restore()\"\n\n## Copio el modelo y el fichero de la api\nCOPY brms_model.rds /opt/ml/brms_model.rds\nCOPY plumber.R /opt/ml/plumber.R\n\n# exponemos el puerto\nEXPOSE 8081\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8081)\"]\ny como antes construimos el docker image\ndocker build -t mi_modelo_brms_rminimal_renv .\nEl docker usando renv es sustancialmente más pesado, ocupa 1.29 Gb\nSeguramente se puede optimizar más si no usara brms, puesto que importa shinystan, bayesplot y otras librerías que no son estrictamente necesarias para nuestro propósito. Habrá que esperar a que Virgilio haga la función predict de INLA para darle una vuelta a esto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Me llamo José Luis Cañadas Reche y llevo unos cuantos años dedicado a esto del análisis de datos con éxito desigual."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Aquí estoy de nuevo",
    "section": "",
    "text": "Estoy cambiando el blog de blogdown a quarto Welcome!"
  },
  {
    "objectID": "posts/leaflet_ejemplo/index.html",
    "href": "posts/leaflet_ejemplo/index.html",
    "title": "Leaflet example",
    "section": "",
    "text": "library(leaflet)\n\nm <- leaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=174.768, lat=-36.852, popup=\"The birthplace of R\")\nm  # Print the map"
  },
  {
    "objectID": "2021/12/12/Modelos-mixtos-spark/index.html",
    "href": "2021/12/12/Modelos-mixtos-spark/index.html",
    "title": "Modelos mixtos en spark. Intento 1",
    "section": "",
    "text": "A los que nos dedicamos a esto siempre echamos de menos un lme4 en python o en Spark. En Julia afortunadamente tenemos MixedModels.jl.\nTotal que buscando alguna posible solución para poder usar esto en spark me encuentro con dos posibles soluciones.\n\nphoton-ml\nMomentMixedModels\n\nAmbos repos llevan un tiempo sin actualizarse así que no sé yo.\nphoton-ml es de linkedin y tiene buena pinta, al menos el tutorial, que tienes que bajarte un docker y tal, funciona. Aunque la sintaxis es rara. Aún tengo que probarlo más y probar a crear el jar del proyecto ya que no está en maven central y tal (y no me funcionó)\n\nEjemplo de sintaxis de photon-ml\n\n// Define another feature shard for our random effect coordinate, and create a new mapping\n// with both our 'global' and 'perUser' shards.\nval perUserFeatureShardId = \"perUser\"\nval perUserFeatureShard = Set(\"genreFeatures\", \"movieLatentFactorFeatures\")\nval mixedFeatureShardBags = Map(\n    globalFeatureShardId -> globalFeatureShard,\n    perUserFeatureShardId -> perUserFeatureShard)\n\n// Since we have a new shard, re-read the training and validation data into a new DataFrame\n// (and a new index map for the new feature shard).\nval (mixedInputData, mixedFeatureShardInputIndexMaps) = dataReader.readMerged(\n    Seq(\"/data/movielens/trainData.avro\"),\n    mixedFeatureShardBags,\n    numPartitions)\nval mixedValidateData = dataReader.readMerged(\n    Seq(\"/data/movielens/validateData.avro\"),\n    mixedFeatureShardInputIndexMaps,\n    mixedFeatureShardBags,\n    numPartitions)\nDonde mixedInputData es un dataframe de spark con esta pinta.\nmixedInputData.show()\n\n+----+--------+------+-------+------+------+--------------------+--------------------+\n| uid|response|userId|movieId|weight|offset|              global|             perUser|\n+----+--------+------+-------+------+------+--------------------+--------------------+\n|null|     4.0|     1|   1215|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|     1|   1350|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     1|   2193|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|     1|   3476|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|     1|   4993|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     3|   1544|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|    440|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|    914|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|   1894|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|   2112|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|   3524|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|   3911|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    11|    256|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    11|   1200|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.5|    11|  48394|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     1.0|    11|  56003|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     0.5|    11|  64508|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    14|    471|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|    14|   2018|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|    14|   6936|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n\nDonde las columna global y perUser son iguales, pero una se usa para estimar la parte de los efectos fijos y la otra para los aleatorios.\nY luego sigue con\n// A 'RandomEffectDataConfiguration' requires an identifier field to use for grouping data from the\n// same entity, in addition to the fields that a 'FixedEffectDataConfiguration' requires. It also has\n// some additional optional parameters not covered in this tutorial.\nval perUserRandomEffectId = \"userId\"\nval perUserDataConfig = RandomEffectDataConfiguration(\n    perUserRandomEffectId,\n    perUserFeatureShardId,\n    numPartitions,\n    projectorType = IndexMapProjection)\n\n// A 'RandomEffectOptimizationConfiguration' is defined much like a\n// 'FixedEffectOptimizationConfiguration'. The options below are varied from those above primarily\n// for variety and demonstration.\nval perUserOptimizerConfig = OptimizerConfig(\n    optimizerType = TRON,\n    tolerance = 1e-3,\n    maximumIterations = 4)\nval perUserRegularizationContext = L2RegularizationContext\nval perUserRegularizationWeight = 1\nval perUserOptimizationConfig = RandomEffectOptimizationConfiguration(\n    perUserOptimizerConfig,\n    perUserRegularizationContext,\n    perUserRegularizationWeight)\n\n// Assign a coordinate ID to the random effect configurations we defined above. This time, we have\n// multiple coordinates and need to determine the update sequence. In general, it's recommended to\n// order coordinates from least to most granular, i.e. those that correlate most with the response to\n// those that correlate least.\nval perUserCoordinateId = \"perUser\"\nval mixedCoordinateDataConfigs = Map(\n    globalCoordinateId -> globalDataConfig,\n    perUserCoordinateId -> perUserDataConfig)\nval mixedCoordinateOptConfigs = Map(\n    globalCoordinateId -> globalOptimizationConfig,\n    perUserCoordinateId -> perUserOptimizationConfig)\nval mixedUpdateSequence = Seq(globalCoordinateId, perUserCoordinateId)\n\n// Reset our estimator. The training task hasn't changed, but the data configurations and update\n// sequence have. Furthermore, since there are now multiple coordinates, we should try multiple\n// passes of coordinate descent.\nestimator.setCoordinateDataConfigurations(mixedCoordinateDataConfigs)\nestimator.setCoordinateUpdateSequence(mixedUpdateSequence)\nestimator.setCoordinateDescentIterations(2)\n\n// Train a new model.\nval (mixedModel, _, mixedModelConfig) = estimator.fit(\n    mixedInputData,\n    Some(mixedValidateData),\n    Seq(mixedCoordinateOptConfigs)).head\n\n// Save the trained model.\nModelProcessingUtils.saveGameModelToHDFS(\n    sc,\n    new Path(\"output/mixed\"),\n    mixedModel,\n    trainingTask,\n    mixedModelConfig,\n    None,\n    mixedFeatureShardInputIndexMaps)\n    \nY guarda los coeficientes en avro\n\"avro cat -n 1 ./output/mixed/random-effect/perUser/coefficients/part-00000.avro\" #| \"jq .\" !\n{\n  \"variances\": null,\n  \"means\": [\n    {\n      \"term\": \"Drama\",\n      \"name\": \"Genre\",\n      \"value\": -0.35129547272878503\n    },\n    {\n      \"term\": \"Musical\",\n      \"name\": \"Genre\",\n      \"value\": -0.2967514108349342\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"7\",\n      \"value\": -0.13789947075029355\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"14\",\n      \"value\": -0.13577029316450503\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"8\",\n      \"value\": -0.12850130065314527\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"26\",\n      \"value\": -0.11646520581859549\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"15\",\n      \"value\": -0.09620039918539182\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"6\",\n      \"value\": 0.08934738779979344\n    },\n    {\n      \"term\": \"Comedy\",\n      \"name\": \"Genre\",\n      \"value\": 0.08833383209245319\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"2\",\n      \"value\": -0.08756438537931642\n    },\n\nmore coefficients\n    \"modelClass\": \"com.linkedin.photon.ml.supervised.regression.LinearRegressionModel\",\n  \"lossFunction\": \"\",\n  \"modelId\": \"7\"\n}\nLo dicho, no tiene mala pinta y ajusta rápido, me falta probar a crear el jar del proyecto\nPor otro lado MomentMixedModels también parecía prometedora pero al intentar crear el jar con sbt (tampoco está en maven central) peta con (*:update) sbt.ResolveException: unresolved dependency: com.stitchfix.algorithms.spark#sfs3_2.11;0.7.0-spark2.2.0: not found y viendo el build.sbt hace referencia a http://artifactory.vertigo.stitchfix.com/artifactory/releases que parece que ya no existe, así que mi gozo en un pozo. La sintaxis parecía sencilla.\nval linearModelFitter = {\n    new MixedEffectsRegression()\n      .setResponseCol(\"Reaction\")\n      .setFixedEffectCols(Seq(\"Days\"))\n      .setRandomEffectCols(Seq(\"Days\"))\n      .setFamilyParam(\"gaussian\")\n      .setGroupCol(\"Subject\")\n  }\n\n  val linearModel = linearModelFitter.fit(sleepstudyData)\n  println(linearModel.β)\nPues nada, a ver si algún ingenazi con alma de analista se digna a hacer una implementación de lme4 en Spark , porque, reconozcámoslo Spark-ml es una ñapa. Lo único que medio funciona bien es usar los algoritmos de h2o sobre spark con sparkling-water y me falta probar un poco más su implementación de modelos jerárquicos\nHasta otra."
  },
  {
    "objectID": "2021/12/01/lectura-para-el-finde/index.html",
    "href": "2021/12/01/lectura-para-el-finde/index.html",
    "title": "Lecturas para el finde",
    "section": "",
    "text": "El Vol 100 del Journal Of Statistical Software promete, y mucho. Artículo del gran Virgilio y muchos más sobre software para estadística bayesiana. Virgilio, sólo falta que le eches un vistazo a las cositas que hay en Julia. Pues nada, ya tengo entretenimiento. Aquí os dejo el enlace"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "",
    "text": "¿Qué os parecería tener un modelo guardado y un binario en linux que tomando como parámetros el modelo y el dataset a predecir guardara las predicciones en un csv?\nY todo eso que funcione en cualquier Linux, de forma que puedas copiar esa aplicación de un Ubuntu a un EC2 con amazon linux (un centos) y que funcione igual sin tener que tener Julia instalado en el EC2.\nY no estoy hablando de tener un docker o tener un entorno de conda dónde lo despliegas y tu script dónde se predice necesita ser interpretado, sino de una aplicación compilada dónde tiene el runtime de Julia y todo lo necesario para correr. De hecho la estructura de esa aplicación sería algo así.\n $ ▶ tree -L 1 ../bin_blog\n../bin_blog\n├── artifacts\n├── bin\n└── lib\nBueno, pues vamos a ver cómo se consigue eso utilizando el paquete PackageCompiler En primer lugar algunas consideraciones sobre latencia y precompilados que podéis ver en el video de Kristoffer Carlson.\n\nJulia tiene los paquetes precompilados, por lo que cuando arrancas el pc y haces using paquete tarda un rato.\nUna vez compilado, la primera vez que lo invocas tarda un tiempo.\nAunque ya hayas hecho using paquete la primera vez que usas una función también tiene que compilarla y tarda otro rato.\n\npor ejemplo, vemos que la primera vez que hago using Plots tarda unos 3 segundos, o que la primera vez que uso la función plot también tarda, pero la siguiente vez es muy rápido.\njulia> @time using Plots\n  3.267061 seconds (7.93 M allocations: 551.989 MiB, 3.89% gc time, 0.17% compilation time)\n\njulia> @time using Plots\n  0.666359 seconds (665.66 k allocations: 37.314 MiB, 6.76% gc time, 99.99% compilation time)\n\n\njulia> @time p = plot(rand(2,2))\n  2.436100 seconds (3.11 M allocations: 186.676 MiB, 7.61% gc time, 57.23% compilation time)\n[ Info: Precompiling GR_jll [d2c73de3-f751-5644-a686-071e5b155ba9]\n\njulia> @time p = plot(rand(2,2))\n  0.000883 seconds (3.93 k allocations: 228.672 KiB)\nPero, ¿no era una de las características de Julia su velocidad, cómo podemos apañar esto?. Pues hay varias formas.\n\nCrear un archivo startup.jl en \\.julia\\config\\ dónde escribamos lo que queremos que se cargue al iniciar julia.\nCrear una sysimage de julia con PackageCompiler de forma que podamos hacer algo como julia --sysimage mi_sysimage.so y se inicie Julia con los paquetes que queramos ya compilados y cargados.\n\nEl paquete PackageCompiler permite también crear una aplicación de forma que crea una sysimage de julia junto con un script con una función main que es la que se ejecuta al llamar al binario que crea. La ventaja de esta aproximación es que podemos crear un binario que funcione en cualquier linux aunque no tengamos Julia instalado."
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#objetivo",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#objetivo",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Objetivo",
    "text": "Objetivo\nEl objetivo es construir un “Motor de Modelos” (recuerdos a mi amigo Roberto Sancho) que funciones en cualquier linux, y que dado un modelo previamente entrenado y la ruta de un fichero con los datos, haga la predicción del modelo y escriba un csv con el resultado.\nAl final se podría usar de la siguiente forma\nmotor_modelos modelo_entrenado.jlso datos_to_predict.csv resultado.csv\nEn las pruebas que he hecho, para predecir un fichero de 5 millones de filas y escribir el csv con el resultado de la predicción de un modelo randomForest ha tardado unos 15 segundos en todo el proceso."
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#creando-el-entorno-necesario",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#creando-el-entorno-necesario",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Creando el entorno necesario",
    "text": "Creando el entorno necesario\nLo primero que tenemos que hacer es seguir el proceso como para crear un paquete en julia. Iniciamos julia en el REPL y entrando en el modo paquete con ] utilizamos generate\n(@v1.6) pkg> generate decision_tree_app\n  Generating  project decision_tree_app:\n    decision_tree_app/Project.toml\n    decision_tree_app/src/decision_tree_app.jl\nEsto crea el directorio decision_tree_app así como un Project.toml dónde se va a ir guardando la referencia y las versiones de las librerías que usemos, y también crea el fichero src/decision_tree_app.jl con la estructura mínima.\n╭─ jose @ jose-PROX15 ~\n│\n╰─ $ ▶ cat decision_tree_app/src/decision_tree_app.jl \nmodule decision_tree_app\n\ngreet() = print(\"Hello World!\")\n\nend # module\nPues sobre esta base es la que vamos a trabajar. Ahora tenemos que activar el entorno con\n(@v1.6) pkg> activate .\n  Activating environment at `~/decision_tree_app/Project.toml`\n \nDe esta forma cada vez que añadamos un paquete con add nombrepquete se queda guardado la referencia en el el Project.toml y se creará un Manifest.toml, estos dos archivos son los que nos servirán para reproducir el mismo entorno en otro sitio, equivalente a un requirements en python."
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#crear-la-aplicación",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#crear-la-aplicación",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Crear la aplicación",
    "text": "Crear la aplicación\nEn el paquete PackageCompiler existe la función create_app que tomando como argumentos, el directorio de la aplicación, directorio dónde compilar y uno o varios ficheros de ejemplo del flujo que se va a realizar, creará la apliación compilada.\n\nFichero de precompilación\nEs importante tener un fichero de precompilación que sea ejemplo simple de lo que tiene que hacer la aplicación. A saber, leer un modelo, leer unos datos, predecir y escribir el resultado.\nPara eso, entrenamos un modelo sobre iris, y guardamos el modelo\nFichero train.jl\n\n# Training model julia\nusing CSV,CategoricalArrays, DataFrames, MLJ\n\n\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\n\nconst target =  CategoricalArray(df1[:, :Species])\nconst X = df1[:, Not(:Species)]\n\n\nTree = @load RandomForestClassifier pkg=DecisionTree\ntree = Tree(n_trees = 20)\ntree.max_depth = 3\n\nmach = machine(tree, X, target)\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\n\nMLJ.save(\"mimodelo.jlso\", mach, compression=:gzip)\n\nY una vez que tenemos el modelo guardado creamos el fichero de precompilación que pasaremos como argumento a create_app\nFichero precomp_file.jl en directorio src\nEn este fichero al llamar a las funciones CSV.read, predict, o CSV.write se consigue que al crear la aplicación compilada esas funciones se compilen y la latencia sea mínima.\nusing MLJ, CSV, DataFrames, MLJDecisionTreeInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"/home/jose/Julia_projects/decision_tree_app/data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"/home/jose/Julia_projects/decision_tree_app/mimodelo.jlso\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, niveles)\n\nCSV.write(\"/home/jose/Julia_projects/decision_tree_app/data/predicciones.csv\", res_df)\nCuando compilamos una aplicación con PackageCompiler lo que se ejecuta es la función julia_main que se encuentre en el módulo que creamos con el mismo nombre que el nombre de la aplicación.\nFichero decision_tree_app.jl en src\nmodule decision_tree_app\n\nusing MLJ, CSV, DataFrames, MLJDecisionTreeInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, tasks=10)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, niveles)          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\nAsí nos queda la estructura\n tree -L 1 src/\nsrc/\n├── decision_tree_app.jl\n├── precomp_file.jl\n└── train.jl\nAhora ya podemos compilar la aplicación\n\nusing PackageCompiler\n\ncreate_app(\"../decision_tree_app\", \"../bin_blog\", precompile_execution_file=\"../decision_tree_app/src/precomp_file.jl\", force=true, filter_stdlibs = true)\n\nY después de unos 30 minutos ya tenemos en el directorio bin_blog todo lo necesario, el runtime de julia embebido, las librerías compiladas , etcétera, de forma que copiando esa estructura en otro ordenador (con linux) ya funcionaría nuestra app sin tener Julia instalado.\n\ntree -L 1 bin_blog/\nbin_blog/\n├── artifacts\n├── bin\n└── lib\nPor ejemplo en lib tenemos\ntotal 272\ndrwxrwxr-x 3 jose jose   4096 ago 14 10:37 ./\ndrwxrwxr-x 5 jose jose   4096 ago 14 10:37 ../\ndrwxrwxr-x 2 jose jose   4096 ago 14 10:37 julia/\nlrwxrwxrwx 1 jose jose     15 ago 14 10:37 libjulia.so -> libjulia.so.1.6*\nlrwxrwxrwx 1 jose jose     15 ago 14 10:37 libjulia.so.1 -> libjulia.so.1.6*\n-rwxr-xr-x 1 jose jose 266232 ago 14 10:37 libjulia.so.1.6*\ny en lib/julia tiene este aspecto\ndrwxrwxr-x 2 jose jose     4096 ago 14 10:37 ./\ndrwxrwxr-x 3 jose jose     4096 ago 14 10:37 ../\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libamd.so -> libamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libamd.so.2 -> libamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    39059 ago 14 10:37 libamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libatomic.so.1 -> libatomic.so.1.2.0*\n-rwxr-xr-x 1 jose jose   147600 ago 14 10:37 libatomic.so.1.2.0*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libbtf.so -> libbtf.so.1.2.6*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libbtf.so.1 -> libbtf.so.1.2.6*\n-rwxr-xr-x 1 jose jose    13108 ago 14 10:37 libbtf.so.1.2.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcamd.so -> libcamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcamd.so.2 -> libcamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    43470 ago 14 10:37 libcamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    28704 ago 14 10:37 libccalltest.so*\n-rwxr-xr-x 1 jose jose    39128 ago 14 10:37 libccalltest.so.debug*\nlrwxrwxrwx 1 jose jose       19 ago 14 10:37 libccolamd.so -> libccolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       19 ago 14 10:37 libccolamd.so.2 -> libccolamd.so.2.9.6*\n-rwxr-xr-x 1 jose jose    47652 ago 14 10:37 libccolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libcholmod.so -> libcholmod.so.3.0.13*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libcholmod.so.3 -> libcholmod.so.3.0.13*\n-rwxr-xr-x 1 jose jose  1005880 ago 14 10:37 libcholmod.so.3.0.13*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libcolamd.so -> libcolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libcolamd.so.2 -> libcolamd.so.2.9.6*\n-rwxr-xr-x 1 jose jose    31250 ago 14 10:37 libcolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcurl.so -> libcurl.so.4.7.0*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcurl.so.4 -> libcurl.so.4.7.0*\n-rwxr-xr-x 1 jose jose   654080 ago 14 10:37 libcurl.so.4.7.0*\n-rwxr-xr-x 1 jose jose    22120 ago 14 10:37 libdSFMT.so*\n-rwxr-xr-x 1 jose jose   758680 ago 14 10:37 libgcc_s.so.1*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libgfortran.so.4 -> libgfortran.so.4.0.0*\n\ny en bin\ntotal 245180\ndrwxrwxr-x 2 jose jose      4096 ago 14 10:50 ./\ndrwxrwxr-x 5 jose jose      4096 ago 14 10:37 ../\n-rwxrwxr-x 1 jose jose     17928 ago 14 10:50 decision_tree_app*\n-rwxrwxr-x 1 jose jose 251030272 ago 14 10:50 decision_tree_app.so*"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#utilizando-la-aplicación",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#utilizando-la-aplicación",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Utilizando la aplicación",
    "text": "Utilizando la aplicación\nLo primero que comprobamos es si la aplicación funciona con el modelo que tenemos sobre iris.\nIntentamos predecir un fichero tal que así\nhead test_to_predict.csv \nSepal.Length,Sepal.Width,Petal.Length,Petal.Width\n5.1,3.5,1.4,0.2\n4.9,3,1.4,0.2\n4.7,3.2,1.3,0.2\n4.6,3.1,1.5,0.2\n5,3.6,1.4,0.2\n5.4,3.9,1.7,0.4\n4.6,3.4,1.4,0.3\n5,3.4,1.5,0.2\n4.4,2.9,1.4,0.2\nAhora ejecutamos la app pasándole el modelo guardado en train.jl y el csv\n╰─ $ ▶ time ../bin_blog/bin/decision_tree_app mimodelo.jlso test_to_predict.csv predicho.csv\n\nreal    0m2,046s\nuser    0m2,344s\nsys 0m0,581s\n╰─ $ ▶ head -20 predicho.csv \nsetosa,versicolor,virginica\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n0.9,0.1,0.0\n0.95,0.05,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n0.95,0.05,0.0"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#uso-como-motor-para-predecir.",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#uso-como-motor-para-predecir.",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Uso como motor para predecir.",
    "text": "Uso como motor para predecir.\nUna vez tenemos la app queremos utilizarla con otros modelos y otros datos sin necesidad de tener que compilar de nuevo.\nLo primero es usar las mismas versiones de las librerías que hemos usado en la app. Para eso copiamos los archivos Project.toml y Manifest.toml en otro directorio y activamos el entorno con activate . e instalamos los paquetes con instanstiate\n\n─ $ ▶ cd entorno_modelos/\n╭─ jose @ jose-PROX15 ~/Julia_projects/entorno_modelos\n│\n╰─ $ ▶ julia \n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.6.2 (2021-07-14)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\n(@v1.6) pkg> activate .\n  Activating environment at `~/Julia_projects/entorno_modelos/Project.toml`\n\n(decision_tree_app) pkg> instantiate\n\nY ya podemos entrenar un nuevo modelo. En este caso voy a entrenar un modelo sobre los datos del precio de las viviendas en Boston, pero dónde he creado variables categórica que diferencie entre si el precio es mayor que 20 o menor, variable medv_20 con niveles (G20, NG20)\n─ $ ▶ head data/boston.csv \n\"crim\",\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\",\"ptratio\",\"lstat\",\"medv_20\"\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98,\"G20\"\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14,\"G20\"\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03,\"G20\"\n0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,2.94,\"G20\"\n0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,5.33,\"G20\"\n0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,5.21,\"G20\"\n0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,12.43,\"G20\"\n0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,19.15,\"G20\"\n0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,29.93,\"NG20\"\nY nuestro fichero de entrenamiento es el siguiente. Dejo solo lo de entrenar y guardar el modelo, otro día vemos la validación cruzada etc..\n# Training model julia\nusing CSV,CategoricalArrays, DataFrames, MLJ\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\nconst target =  CategoricalArray(df1[:, :medv_20])\nconst X = df1[:, Not(:medv_20)]\n\nTree = @load RandomForestClassifier pkg=DecisionTree\ntree = Tree(n_trees = 20)\ntree.max_depth = 5\n\n\nmach = machine(tree, X, target)\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\n\nMLJ.save(\"boston_rf.jlso\", mach, compression=:gzip)\nY ya podemos usar nuestra aplicación compilada para predecir con el modelo que acabamos de entrenar. Para simular más un proceso real, usamos el conjunto de datos de boston pero con 5 millones de filas.\nFichero sin la variable a predecir\n╰─ $ ▶ head -10  boston_to_predict.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\n0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,2.94\n0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,5.33\n0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,5.21\n0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,12.43\n0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,19.15\n0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,29.93\n\n\n╰─ $ ▶ wc -l  boston_to_predict.csv \n5060001 boston_to_predict.csv\nY ahora utilizamos nuestreo “Motor de Modelos” . Y en mi pc, tarda en predecir y escribir el resultado en torno a los 15 segundos.\n╰─ $ ▶ time ../bin_blog/bin/decision_tree_app boston_rf.jlso boston_to_predict.csv res.csv\n\nreal    0m14,080s\nuser    0m28,949s\nsys 0m3,442s\n╰─ $ ▶ wc -l res.csv \n5060001 res.csv\n╭─ jose @ jose-PROX15 ~/Julia_projects/entorno_modelos\n│\n╰─ $ ▶ head -10 res.csv \nG20,NG20\n0.95,0.05\n0.95,0.05\n1.0,0.0\n1.0,0.0\n1.0,0.0\n1.0,0.0\n0.85,0.15\n0.4,0.6\n0.25,0.75"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#conclusión",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#conclusión",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Conclusión",
    "text": "Conclusión\nEsto es sólo un ejemplo de como crear una aplicación compilada con Julia, en este caso para temas de “machín lenin”, pero podría ser para otra cosa.\nComo ventaja, que podemos crear un tar.gz con toda la estructura creada en directorio bin_blog y ponerlo en otro linux y qué funcione sin necesidad de que sea la misma distribución de linux ni de que esté Julia instalado. Esto podría ser útil en entornos productivos en los que haya restricciones a la hora de instalar software.\nAún tengo que explorar como leer y escribir ficheros de s3 con AWSS3.jl y más cosas relacionadas.\nHasta otra."
  },
  {
    "objectID": "2021/08/07/palabras-para-julia-parte-1-n/index.html",
    "href": "2021/08/07/palabras-para-julia-parte-1-n/index.html",
    "title": "Palabras para Julia ( Parte 1/n)",
    "section": "",
    "text": "A pesar del título, no voy a hablar sobre la excelente canción de los Suaves, sino del lenguaje de programación Julia. Ya en otra entrada del blog de hace un par de años comparé glmer con INLA y la librería MixedModels. Por aquel entonces la versión de Julia era la 1.0.3, ya va por la 1.6.2. Debido a reciente entrada de Carlos dónde apostaba por Julia para el larguísimo plazo, he decidido echarle un vistazo un poco más en profundidad.\nLo cierto es que me está gustando bastante el lenguaje y voy a escribir un par de entradas dónde contar alguna cosilla. Ya Carlos mencionaba que Julia corre sobre LLVM, pero también cabe mencionar que Julia tiene características más que interesantes, como multiple dispatch o tipos abstractos que permiten al desarrollador escribir código sin preocuparse demasiado por el tipado y que sea el compilador el que cree los métodos específicos. Si, has oído bien, Julia compila las funciones, por lo que tiene la doble ventaja de ser un lenguaje rápido a la vez que sencillo, bueno, su lema dice “Tan fácil como Python, tan rápido como C”.\nEn esta primera entrada voy a poner un ejemplo sencillo de cómo sería hacer un modelo de “Machín Lenin” utilizando la librería MLJ, y en el post siguiente os contaré como tener un binario para predecir usando ese modelo de forma que funcione en cualquier Linux sin importar si está basado en Debian, Centos o lo que sea, y sin necesidad de tener instalado Julia, ni docker, ni nada."
  },
  {
    "objectID": "2021/08/07/palabras-para-julia-parte-1-n/index.html#modelo-con-mlj",
    "href": "2021/08/07/palabras-para-julia-parte-1-n/index.html#modelo-con-mlj",
    "title": "Palabras para Julia ( Parte 1/n)",
    "section": "Modelo con MLJ",
    "text": "Modelo con MLJ\nMLJ es una librería que pretende servir de interfaz común a otras muchas librerías. Veamos un ejemplo de como ajustar un RandomForest implementado en la librería DecisionTree.\nLo primero, para instalar paquetes podéis mirar esto, básicamente haces\nusing Pkg\nPkg.import(\"nombre_paquete\")\nO en el REPL de Julia entras en el modo Package pulsando ] y pones add nombre_paquete . Esto bajará la librería correspondiente precompilado y la añade a ~/.julia/packages/\nVamos al ejemplo. Aunque voy a usar chunks de julia (gracias a la librería JuliaCall) en el rmarkdown dónde escribo los posts, en realidad como editor par Julia me gusta VSCode.\nLos datos de ejemplo son de la librería bootde R . puedes ver la ayuda haciendo en R\n\n\nMostrar / ocultar código\nlibrary(boot)\nhelp(channing)\n\n\nChanning House Data\nDescription\nThe channing data frame has 462 rows and 5 columns.\n\nChanning House is a retirement centre in Palo Alto, California. These data were collected between the opening of the house in 1964 until July 1, 1975. In that time 97 men and 365 women passed through the centre. For each of these, their age on entry and also on leaving or death was recorded. A large number of the observations were censored mainly due to the resident being alive on July 1, 1975 when the data was collected. Over the time of the study 130 women and 46 men died at Channing House. Differences between the survival of the sexes, taking age into account, was one of the primary concerns of this study.\n\nUsage\nchanning\nFormat\nThis data frame contains the following columns:\n\nsex\nA factor for the sex of each resident (\"Male\" or \"Female\").\n\nentry\nThe residents age (in months) on entry to the centre\n\nexit\nThe age (in months) of the resident on death, leaving the centre or July 1, 1975 whichever event occurred first.\n\ntime\nThe length of time (in months) that the resident spent at Channing House. (time=exit-entry)\n\ncens\nThe indicator of right censoring. 1 indicates that the resident died at Channing House, 0 indicates that they left the house prior to July 1, 1975 or that they were still alive and living in the centre at that date.\n\nEn Julia podemos instalar la librería RDatasets y usar esos datos\n\n\nMostrar / ocultar código\nusing RDatasets, MLJ\nchanning = dataset(\"boot\", \"channing\")\n#> 462×5 DataFrame\n#>  Row │ Sex     Entry  Exit   Time   Cens\n#>      │ Cat…    Int32  Int32  Int32  Int32\n#> ─────┼────────────────────────────────────\n#>    1 │ Male      782    909    127      1\n#>    2 │ Male     1020   1128    108      1\n#>    3 │ Male      856    969    113      1\n#>    4 │ Male      915    957     42      1\n#>    5 │ Male      863    983    120      1\n#>    6 │ Male      906   1012    106      1\n#>    7 │ Male      955   1055    100      1\n#>    8 │ Male      943   1025     82      1\n#>   ⋮  │   ⋮       ⋮      ⋮      ⋮      ⋮\n#>  456 │ Female    986   1030     44      1\n#>  457 │ Female   1039   1132     93      1\n#>  458 │ Female    968    990     22      1\n#>  459 │ Female    955    990     35      1\n#>  460 │ Female    837    911     74      1\n#>  461 │ Female    861    915     54      1\n#>  462 │ Female    967    983     16      1\n#>                           447 rows omitted\n\n\nMLJ necesita que las columnas tenga los tipos correctos en scitypes. Podemos verlos con\n\n\nMostrar / ocultar código\nschema(channing)\n#> ┌─────────┬─────────────────────────────────┬───────────────┐\n#> │ _.names │ _.types                         │ _.scitypes    │\n#> ├─────────┼─────────────────────────────────┼───────────────┤\n#> │ Sex     │ CategoricalValue{String, UInt8} │ Multiclass{2} │\n#> │ Entry   │ Int32                           │ Count         │\n#> │ Exit    │ Int32                           │ Count         │\n#> │ Time    │ Int32                           │ Count         │\n#> │ Cens    │ Int32                           │ Count         │\n#> └─────────┴─────────────────────────────────┴───────────────┘\n#> _.nrows = 462\n\n\nQueremos modelar la variable exit. MLJ quiere la y por un lado y las X’s por otro, para eso vamos a usar la función unpack que además de permitir eso permite cambiar el tipo de las variables, y convertir la variable Cens a categórica por ejemplo\n\n\nMostrar / ocultar código\ny, X =  unpack(channing,\n                      ==(:Exit),            # con el doble igual seleccionamos la y\n                      !=(:Time);            # Quitamos variable Time\n                      :Exit=>Continuous,    # Convertimos al tipo correcto en scitypes\n                      :Entry=>Continuous,\n                      :Cens=>Multiclass)\n#> ([909.0, 1128.0, 969.0, 957.0, 983.0, 1012.0, 1055.0, 1025.0, 1043.0, 945.0  …  905.0, 1040.0, 926.0, 1030.0, 1132.0, 990.0, 990.0, 911.0, 915.0, 983.0], 462×3 DataFrame\n#>  Row │ Sex     Entry    Cens\n#>      │ Cat…    Float64  Cat…\n#> ─────┼───────────────────────\n#>    1 │ Male      782.0  1\n#>    2 │ Male     1020.0  1\n#>    3 │ Male      856.0  1\n#>    4 │ Male      915.0  1\n#>    5 │ Male      863.0  1\n#>    6 │ Male      906.0  1\n#>    7 │ Male      955.0  1\n#>    8 │ Male      943.0  1\n#>   ⋮  │   ⋮        ⋮      ⋮\n#>  456 │ Female    986.0  1\n#>  457 │ Female   1039.0  1\n#>  458 │ Female    968.0  1\n#>  459 │ Female    955.0  1\n#>  460 │ Female    837.0  1\n#>  461 │ Female    861.0  1\n#>  462 │ Female    967.0  1\n#>              447 rows omitted)\n\n\nAhora ya podemos ver como se hace el modelo.\n\n\nMostrar / ocultar código\nTree = @load RandomForestRegressor pkg=DecisionTree\n#> import MLJDecisionTreeInterface ✔\n#> MLJDecisionTreeInterface.RandomForestRegressor\ntree = Tree(n_trees = 20) # tambien se puede instanciar sin paraámetros \n#> RandomForestRegressor(\n#>     max_depth = -1,\n#>     min_samples_leaf = 1,\n#>     min_samples_split = 2,\n#>     min_purity_increase = 0.0,\n#>     n_subfeatures = -1,\n#>     n_trees = 20,\n#>     sampling_fraction = 0.7,\n#>     pdf_smoothing = 0.0,\n#>     rng = Random._GLOBAL_RNG())\u001b[34m @366\u001b[39m\n# y usar tree.n_trees = 20\n\n\nComo tenemos la variable Censque es categórica necesitamos codificarla, aquí entra como hacer un pipeline en MLJ, que es una de las cosas más potentes que tiene junto con los ComposingModels models que permite mezclar varios modelos.\n\n\nMostrar / ocultar código\n# Definimos un ContinuosEncoder, ver la ayuda con ?ContinousEncoder en el repl de julia\n\n  \n\n  # Unsupervised model for arranging all features (columns) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n  # \n  #   •  If ftr is already Continuous retain it.\n  # \n  #   •  If ftr is Multiclass, one-hot encode it.\n  # \n  #   •  If ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case\n  #      one-hot encode it.\n  # \n  #   •  If ftr is Count, replace it with coerce(ftr, Continuous).\n  # \n  #   •  If ftr is of some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n  # \n  # If drop_last=true is specified, then one-hot encoding always drops the last class indicator colum\n\nhot = ContinuousEncoder(one_hot_ordered_factors=true, drop_last=true)\n#> ContinuousEncoder(\n#>     drop_last = true,\n#>     one_hot_ordered_factors = true)\u001b[34m @117\u001b[39m\n\n\nUtilizamos la macro @pipeline para encadenar el onehot y el modelo\n\n\nMostrar / ocultar código\npipe = @pipeline hot tree\n#> Pipeline259(\n#>     continuous_encoder = ContinuousEncoder(\n#>             drop_last = true,\n#>             one_hot_ordered_factors = true),\n#>     random_forest_regressor = RandomForestRegressor(\n#>             max_depth = -1,\n#>             min_samples_leaf = 1,\n#>             min_samples_split = 2,\n#>             min_purity_increase = 0.0,\n#>             n_subfeatures = -1,\n#>             n_trees = 20,\n#>             sampling_fraction = 0.7,\n#>             pdf_smoothing = 0.0,\n#>             rng = Random._GLOBAL_RNG()))\u001b[34m @447\u001b[39m\n\n\nY ya podemos ajustar el modelo por ejemplo utilizando evaluate y validación cruzada\n\n\nMostrar / ocultar código\nevaluate(pipe, X, y, resampling=CV(nfolds=5), measure = [rmse, mae])\n#> PerformanceEvaluation object with these fields:\n#>   measure, measurement, operation, per_fold,\n#>   per_observation, fitted_params_per_fold,\n#>   report_per_fold, train_test_pairs\n#> extract:\n#> ┌─────────────────────────────────────────┬─────────────┬───────────┬───────────\n#> │ measure                                 │ measurement │ operation │ per_fold ⋯\n#> ├─────────────────────────────────────────┼─────────────┼───────────┼───────────\n#> │ RootMeanSquaredError()\\e[34m @216\\e[39m │ 54.6        │ predict   │ [47.2, 5 ⋯\n#> │ MeanAbsoluteError()\\e[34m @160\\e[39m    │ 45.9        │ predict   │ [40.1, 5 ⋯\n#> └─────────────────────────────────────────┴─────────────┴───────────┴───────────\n#>                                                                 1 column omitted\n\n\nHay más medidas que se pueden listar con measures()\nTambién podemos partir en train, test y similar\n\n\nMostrar / ocultar código\ntrain, test = partition(eachindex(y), 0.7, shuffle=true, rng=155);\n\n\nAhora instanciamos el modelo con machine especificando la X, y la y\n\n\nMostrar / ocultar código\nmodelo = machine(pipe, X,y)\n#> \u001b[34mMachine{Pipeline259,…} @838\u001b[39m trained 0 times; caches data\n#>   args: \n#>     1:   \u001b[34mSource @987\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`\n#>     2:   \u001b[34mSource @871\u001b[39m ⏎ `AbstractVector{Continuous}`\n\n\nY podemos usar fit! para ajustar “in place” (en julia si la función acaba en ! es una función que actua modificando el objeto que se le pasa) sin tener que crear otra variable\n\n\nMostrar / ocultar código\n\nfit!(modelo, rows = train)\n#> \u001b[34mMachine{Pipeline259,…} @838\u001b[39m trained 1 time; caches data\n#>   args: \n#>     1:   \u001b[34mSource @987\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`\n#>     2:   \u001b[34mSource @871\u001b[39m ⏎ `AbstractVector{Continuous}`\n\n\nY ya podríamos predecir sobre test, dónde se le aplicaría el onehot encoder que hemos definido en el pipeline\n\n\nMostrar / ocultar código\n# En julia podemos usar sintaxis latex por ejemplo \\beta\\hat  y tabulador despues de beta y hat en vscode \n# escribe β̂ (uso juliaMono https://juliamono.netlify.app/) como tipo de letra \nŷ = predict(modelo,X[test, :])\n#> 139-element Vector{Float64}:\n#>  1060.8833333333334\n#>  1090.125\n#>  1024.5666666666666\n#>  1039.45\n#>   931.9666666666668\n#>   975.8\n#>  1114.1\n#>   966.925\n#>   955.65\n#>   883.3666666666668\n#>     ⋮\n#>   881.6266666666667\n#>   854.2\n#>  1025.0\n#>   919.6333333333332\n#>   932.5\n#>   935.5\n#>  1061.0166666666667\n#>   929.9\n#>   958.9\n\n\nEn este caso hemos hecho un modelo para predecir una variable continua, cuando sea categórica existen funciones que devuelven la clase predicha o la probabilidad de cada clase.\nTambién podemos ver como varía el error según el número de árboles\n\n\nMostrar / ocultar código\nmodelo\n#> \u001b[34mMachine{Pipeline259,…} @838\u001b[39m trained 1 time; caches data\n#>   args: \n#>     1:   \u001b[34mSource @987\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`\n#>     2:   \u001b[34mSource @871\u001b[39m ⏎ `AbstractVector{Continuous}`\n\n\n\n\nMostrar / ocultar código\nr_tree = range(pipe, :(random_forest_regressor.n_trees), lower=2, upper=20)\n#> typename(MLJBase.NumericRange)(Int64, :(random_forest_regressor.n_trees), ... )\n\n\n\n\nMostrar / ocultar código\n\ncurve = MLJ.learning_curve(modelo;\n                           range=r_tree,\n                           resampling=CV(nfolds=5),\n                           measure=rmse)\n#> (parameter_name = \"random_forest_regressor.n_trees\",\n#>  parameter_scale = :linear,\n#>  parameter_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n#>  measurements = [57.62183128809188, 56.51529950303111, 55.56904469933379, 54.8531983408167, 54.847208941367946, 54.444800632308684, 54.81102183981715, 55.012542834615445, 55.06356266514865, 54.88135766866701, 54.44612444179021, 54.773279907824005, 54.59173426290152, 54.23406251166757, 54.3549570009795, 54.14458345130954, 54.34244340416005, 54.43188079909501, 54.62850574226529],)\n\n\n\n\nMostrar / ocultar código\nusing Plots\ngr() # especificamos un \nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"CV estimate of RMSE error\")\n\n\n\n\n\nplot\n\n\nY esto es todo, en el próximo post contaré como crear el binario que nos va a permitir tener un motor de predicción para los modelos de un árbol de decisión y que funcione en cualquier linux. Casi listo para producción (o al menos una parte importante) sin tener que tener julia en dónde se vaya a utilizar."
  },
  {
    "objectID": "2021/08/28/dos-ejes-de-ordenadas-parte-2-n/index.html",
    "href": "2021/08/28/dos-ejes-de-ordenadas-parte-2-n/index.html",
    "title": "¿Dos ejes de ordenadas? (Parte 2/n)",
    "section": "",
    "text": "Siguiendo con el tema de los dos ejes de ordenadas, a mi no me gustan especialmente este tipo de gráficos, pero puedo entender que se use y, cómo dice mi amigo Raúl Vaquerizo, lo importante es que se entienda.\nVeamos un ejemplo que nos comentó Jesús Lagos dónde se suele aplicar este tipo de gráficos, se trata de los climogramas, dónde se presentan en el eje X los meses del año y en los dos ejes de ordenadas la precipitación y la temperatura.\nVeamos unos datos de Madrid en 2018, extraídos de aquí. Y veamos como queda en R.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nLeemos los datos , que son estos.\nMes,T,PP\nEnero,5.9,30.48\nFebrero,5.4,44.19\nMarzo,8.9,143.49\nAbril,13,57.67\nMayo,16.6,57.93\nJunio,22.2,32.75\nJulio,26.1,2.03\nAgosto,27.5,1.02\nSeptiembre,23.9,4.07\nOctubre,15.1,72.64\nNoviembre,9.9,54.07\nDiciembre,6,9.14\n\nmad <- read.csv(\"climograma_mad\")\nmad$Mes <- as_factor(mad$Mes)\n\nY pintamos un gráfico de dos ejes de ordenadas. Es importante elegir la transformación que le hacemos a la segunda variable “y” para que se pueda representar bien, al fin y al cabo se pinta en un sistema de coordenadas y tenemos que poner ambas variables en escala parecida.\n\n\nmulti <- 2\n\nggplot(mad, aes(x = Mes)) +\n  geom_col(aes(y = PP), fill = \"yellow\", alpha = 0.7) +\n  geom_line(aes(y = T * multi), group = 1, color = \"red\") +\n  geom_text(\n    aes(y = T * multi, label = paste(round(T, 1))),\n    vjust = 1.4,\n    color = \"darkred\"\n  ) +\n  scale_y_continuous(sec.axis = sec_axis(~ . / multi,\n    name = \"Temperatura\"\n  )) +\n  theme(\n    axis.title.y.right = element_text(\n      color = \"darkred\",\n      hjust = 0.01\n    ),\n    axis.text.y.right = element_text(\n      face = \"bold\",color = \"darkred\" )\n  )\n\n\n\n\n\n\n\n\nY la verdad es que eligiendo un factor de 2 podemos distorsionar el gráfico. Por convención lo que se suele hacer es considerar un factor que haga que al pintar en el sistema de referencia los máximos de ambas variables coincidan. Ya que este tipo de gráficos “mapea” dos variables al mismo “aesthetics”, se necesita hacer esa transformación para equiparar las variables.\n\nmulti <- max(mad$PP) / max(mad$T)\n\nggplot(mad, aes(x = Mes)) +\n  geom_col(aes(y = PP), fill = \"yellow\", alpha = 0.7) +\n  geom_line(aes(y = T * multi), group = 1, color = \"red\") +\n  geom_text(\n    aes(y = T * multi, label = paste(round(T, 1))),\n    vjust = 1.4,\n    color = \"darkred\"\n  ) +\n  scale_y_continuous(\n    name = \"Precipitaciones\",\n    # segundo eje\n    sec.axis = sec_axis(~ . / multi,\n      name = \"Temperatura\"\n    )\n  ) +\n  theme(\n    axis.title.y.right = element_text(\n      color = \"darkred\",\n      hjust = 0.01\n    ),\n    axis.text.y.right = element_text(\n      face = \"bold\", color = \"darkred\"\n    )\n  )\n\n\n\n\n\n\n\n\nY bueno, no queda mal del todo, aunque Tufte me mataría por esto.\nYo en realidad habría hecho algo como esto.\n\np1 <- mad %>% \n  ggplot(aes(x = Mes, y = T)) +\n  geom_point() + \n  geom_path(group=1) +\n  labs(title = \"Temperatura\")\n\np2 <- mad %>% \n  ggplot(aes(x = Mes, y = PP)) +\n  geom_col() +\n  labs(title = \"Precipitaciones\")\n\np1 / p2\n\n\n\n\n\n\n\n\nY si acaso, para ver la relación entre Precipitaciones y Temperatura, pues algo así.\n\nggplot(mad,aes(x = T, y = PP)) + \n  geom_point() +\n  ggrepel::geom_label_repel(aes(label = Mes), size = 3) +\n  geom_smooth() +\n  labs(title = \"Precipitaciones ~ Temperatura\")\n\n\n\n\n\n\n\n\nY bueno, poco más que decir, siguen sin gustarme los gráficos de dos ejes de ordenadas, puede que por cuestiones filosóficas de no asignar dos variables distintas a mismo “aesthetic”.\nHasta otro día."
  },
  {
    "objectID": "2021/08/18/eje-y-secundario-1-n/index.html",
    "href": "2021/08/18/eje-y-secundario-1-n/index.html",
    "title": "¿Dos ejes de ordenadas? (Parte 1/n)",
    "section": "",
    "text": "Anoche me iba a ir a la cama tras escuchar un podcast, pero al final estuve entretenido debatiendo con Raúl Vaquerizo, Alberto González Almuiña y Jesús Lagos , sobre los gráficos con dos ejes de ordenadas. Aquí os dejo el tweet que puso Raúl.\nPues yendo al post que puso Raúl construía el siguiente gráfico.\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(insuranceData)\nlibrary(ggplot2)\n\n\n# url='http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/acst_docs/glms_for_insurance_data/data/claimslong.csv'\n# df <- read.csv(url)\n\ndata(\"ClaimsLong\")\ndf <- ClaimsLong\n\nresumen <- df %>% group_by(period) %>%\n  summarise(pct_exposicion = n(),\n            frecuencia = sum(claim)/n())\n\n\ng2 <- ggplot(resumen,aes(x = period)) + \n  geom_col(aes(y = pct_exposicion), fill=\"yellow\",alpha=0.7) + \n  geom_line(aes(y=frecuencia * 500000 ), group = 1,color=\"red\") + \n  geom_text(aes(y = frecuencia * 500000, label = paste(round(frecuencia*100),'%')),\n            vjust = 1.4, color = \"red\", size = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~.* 500000, name = \"Frecuencia [%]\")) \n\ng2\n\n\n\n\n\n\n\n\nY bueno, no está mal, pero no me acaba de gustar, sobre todo porque al fin y al cabo todos sus períodos tienen el mismo valor\n\nresumen\n#> # A tibble: 3 × 3\n#>   period pct_exposicion frecuencia\n#>    <int>          <int>      <dbl>\n#> 1      1          40000      0.131\n#> 2      2          40000      0.141\n#> 3      3          40000      0.156\n\nAhora que lo pienso, casi que con la tabla valdría. Pero que pasaría si tenemos diferentes valores\n\nset.seed(155)\nresumen %<>% \n  mutate(pct_exposicion = pct_exposicion * 10*runif(3))\nresumen\n#> # A tibble: 3 × 3\n#>   period pct_exposicion frecuencia\n#>    <int>          <dbl>      <dbl>\n#> 1      1        315266.      0.131\n#> 2      2        173523.      0.141\n#> 3      3        304162.      0.156\n\n\nggplot(resumen,aes(x = period)) + \n  geom_col(aes(y = pct_exposicion), fill=\"yellow\",alpha=0.7) + \n  geom_line(aes(y=frecuencia * 500000 , group=), group = 1,color=\"red\") + \n  geom_text(aes(y = frecuencia * 500000, label = paste(round(frecuencia*100),'%')),\n            vjust = 1.4, color = \"red\", size = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~.* 500000, name = \"Frecuencia [%]\")) \n\n\n\n\n\n\n\n\nMe gusta aún menos este gráfico, yo propongo el siguiente en su lugar.\n\nresumen %>% \n  ggplot(aes(x = period, y = frecuencia)) +\n  geom_point(aes(size=pct_exposicion)) + \n  geom_line() +\n  scale_size_continuous(range=c(4, 10)) +\n  scale_y_continuous(labels = scales::percent, limits = c(0,0.2))\n\n\n\n\n\n\n\n\nDónde representamos la variable frecuencia en el eje de ordenadas y con el tamaño de los puntos representamos la otra variable."
  },
  {
    "objectID": "2021/11/01/a-dónde-va-vicente/index.html",
    "href": "2021/11/01/a-dónde-va-vicente/index.html",
    "title": "¿A dónde va Vicente?",
    "section": "",
    "text": "Cuando estamos haciendo un modelo y tratamos con variables categóricas como predictoras, hay que ser muy cuidadoso. Por ejemplo hay que tener en cuenta qué pasa cuándo tenemos un nuevo nivel en el conjunto de datos a predecir que no estaba en el de entrenamiento. Por ejemplo, si estoy utilizando un algoritmo moderno tipo xgboost, y tengo como variable predictora la provincia. ¿Qué pasa si en el conjunto de entrenamiento no tengo datos de “Granada”, pero en el de predicción si?\nEn el xgboost por defecto las categóricas se codifican con One-Hot encoder, por lo que al no tener datos de Granada en entrenamiento a la hora de predecir la fila de Granada siempre va a tirar hacia el 0, por ejemplo, un corte en uno de los árboles podría ser Almeria = 0 para la izquierda y Almeria = 1 para la derecha. Esto es lo que suelen hacer la mayoría de las implementaciones. Pero cabe preguntarse si es la mejor solución. Otra alternativa podría ser, dado que tengo que predecir para un nivel no visto en entrenamiento, podría asignarle el valor del target que había en el nodo superior. Esta decisión plantea el problema de como sigues el proceso de partición de datos del árbol. Otra posible decisión podría ser recorrer todos los caminos posibles y promediar. En el caso anterior sería ver qué predicción acaba teniendo cuando Almería = 0 y cuando Almería = 1 y promediar. Sería una solución más justa, aunque plantea el problema de tener que recorrer más ramas.\nOtra solución es en cada corte que implique a la variable categórica en cuestión, tirar hacia dónde van la mayoría de los caso “¿a dónde va Vicente? A dónde va la gente”. Esta es la solución que tiene la gente de h2o en su implementación de los Random Forest o de los Gradient Boosting. Dicen textualmente."
  },
  {
    "objectID": "2021/11/01/a-dónde-va-vicente/index.html#ejemplo",
    "href": "2021/11/01/a-dónde-va-vicente/index.html#ejemplo",
    "title": "¿A dónde va Vicente?",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nIniciamos h2o y cargamos datos\n\n## Not run: \nlibrary(h2o)\nh2o.init( max_mem_size = \"25G\")\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         4 minutes 48 seconds \n#>     H2O cluster timezone:       Europe/Madrid \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.38.0.1 \n#>     H2O cluster version age:    1 month and 28 days  \n#>     H2O cluster name:           H2O_started_from_R_jose_ltm884 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   21.82 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.2 Patched (2022-11-10 r83330)\n\nImportamos los datos del titanic. Ponemos como variables predictoras de la supervivencia, solo la clase y el sexo.\n\n\nf <- \"https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\"\ntitanic <- h2o.importFile(f)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======================================================================| 100%\n\ntitanic['survived'] <- as.factor(titanic['survived'])\npredictors <- c(\"pclass\",\"sex\")\nresponse <- \"survived\"\n\n# convertimos la clase a factor \ntitanic$pclass <- as.factor(titanic$pclass)\nh2o.getTypes(titanic$pclass)\n#> [[1]]\n#> [1] \"enum\"\n\nPartimos en train y test\n\nsplits <- h2o.splitFrame(data =  titanic, ratios = .8, seed = 1234)\ntrain <- splits[[1]]\nvalid <- splits[[2]]\n\nh2o.table(train$pclass)\n#>   pclass Count\n#> 1      1   260\n#> 2      2   223\n#> 3      3   571\n#> \n#> [3 rows x 2 columns]\nh2o.table(train$sex)\n#>      sex Count\n#> 1 female   387\n#> 2   male   667\n#> \n#> [2 rows x 2 columns]\n\nh2o.table(valid$pclass)\n#>   pclass Count\n#> 1      1    63\n#> 2      2    54\n#> 3      3   138\n#> \n#> [3 rows x 2 columns]\n\nY ahora cambio en test para que aparezcan valores en pclass y en sex que no están en train.\n\n\nvalid$pclass = h2o.ifelse(valid$pclass == \"3\", \"unknown\", valid$pclass)\nvalid$sex    = h2o.ifelse(valid$sex == \"male\", \"unknown\", valid$sex )\n\n\nh2o.table(valid$pclass)\n#>    pclass Count\n#> 1       1    63\n#> 2       2    54\n#> 3 unknown   138\n#> \n#> [3 rows x 2 columns]\nh2o.table(valid$sex)\n#>       sex Count\n#> 1  female    79\n#> 2 unknown   176\n#> \n#> [2 rows x 2 columns]\n\nPara ver bien qué sucede con los casos en que tenemos nivel nuevo en clase y sexo nos quedamos con el siguiente conjunto de datos a predecir\n\ntest <-  valid[valid$pclass== \"unknown\" & valid$sex == \"unknown\",]\ntest\n#>    pclass survived                          name     sex age sibsp parch ticket\n#> 1 unknown        0 Abbott  Master. Eugene Joseph unknown  13     0     2    NaN\n#> 2 unknown        1 Abelseth  Mr. Olaus Jorgensen unknown  25     0     0 348122\n#> 3 unknown        0                Ali  Mr. Ahmed unknown  24     0     0    NaN\n#> 4 unknown        0   Andersen  Mr. Albert Karvin unknown  32     0     0    NaN\n#> 5 unknown        0   Andersson  Mr. Anders Johan unknown  39     1     5 347082\n#> 6 unknown        0    Andreasson  Mr. Paul Edvin unknown  20     0     0 347466\n#>      fare cabin embarked boat body           home.dest\n#> 1 20.2500  <NA>        S  NaN  NaN East Providence  RI\n#> 2  7.6500 F G63        S  NaN  NaN  Perkins County  SD\n#> 3  7.0500  <NA>        S  NaN  NaN                <NA>\n#> 4 22.5250  <NA>        S  NaN  260      Bergen  Norway\n#> 5 31.2750  <NA>        S  NaN  NaN Sweden Winnipeg  MN\n#> 6  7.8542  <NA>        S  NaN  NaN  Sweden Chicago  IL\n#> \n#> [99 rows x 14 columns]\n\n\n\nModelo xgboost\nHacemos un sólo árbol\n\nmodeloxg<-  h2o.xgboost(\n  seed = 155,\n  x = predictors, \n  y = response,\n  max_depth = 3,\n  training_frame = train,\n  ntrees =1\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nY al predecir, nos da un warning que nos dice ¡¡ojo, tengo nuevos niveles que no estaban en train!! . Aún así , no casca y devuelve una predicción\n\nh2o.predict(modeloxg, test)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#>   predict        p0        p1\n#> 1       0 0.6016703 0.3983298\n#> 2       0 0.6016703 0.3983298\n#> 3       0 0.6016703 0.3983298\n#> 4       0 0.6016703 0.3983298\n#> 5       0 0.6016703 0.3983298\n#> 6       0 0.6016703 0.3983298\n#> \n#> [99 rows x 3 columns]\nh2o.predict_leaf_node_assignment(modeloxg, test)\n#>   T1.C1\n#> 1    LL\n#> 2    LL\n#> 3    LL\n#> 4    LL\n#> 5    LL\n#> 6    LL\n#> \n#> [99 rows x 1 column]\n\nPodemos extraer información del árbol con\n\narbol_ind_xg <- h2o.getModelTree(model = modeloxg, tree_number = 1)\n\nNodesInfo <-  function(arbol_ind){\n  for (i in 1:length(arbol_ind)) {\n    info <-\n      sprintf(\n        \"Node ID %s has left child node with index %s and right child node with index %s The split feature is %s. The NA direction is %s\",\n        arbol_ind@node_ids[i],\n        arbol_ind@left_children[i],\n        arbol_ind@right_children[i],\n        arbol_ind@features[i], \n        arbol_ind@nas[i]\n      )\n    print(info)\n  }}\n\nNodesInfo(arbol_ind_xg)\n#> [1] \"Node ID 0 has left child node with index 1 and right child node with index 2 The split feature is sex.female. The NA direction is LEFT\"\n#> [1] \"Node ID 1 has left child node with index 3 and right child node with index 4 The split feature is pclass.1. The NA direction is LEFT\"\n#> [1] \"Node ID 2 has left child node with index 5 and right child node with index 6 The split feature is pclass.3. The NA direction is LEFT\"\n#> [1] \"Node ID 3 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 4 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 5 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 6 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n\nY vemos que nos da información de hacia dónde van los NA (los niveles nuevos no vistos en train) y siempre van hacia la izquierda.\nPodemos pintar el árbol. Script Y vemos que los NA, siempre van hacia los 0’s.\n\n## importo funciones , encontradas por internet, como no, para pintar el árbol\nsource(\"plot_h2o_tree.R\")\n\ntitanicDataTree_XG = createDataTree(arbol_ind_xg)\n\n\nplotDataTree(titanicDataTree_XG, rankdir = \"TB\")\n\n\n\n\n\n\n\n\nModelo h2o.gbm\nVeamos qué hace la implementación de h2o.\n\n\ngbm_h2o <-  h2o.gbm(\n  seed = 155,\n  x = predictors, \n  y = response,\n  max_depth = 3,\n  distribution = \"bernoulli\",\n  training_frame = train,\n  ntrees = 1\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nh2o.predict(gbm_h2o, test)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#>   predict        p0        p1\n#> 1       0 0.6375663 0.3624337\n#> 2       0 0.6375663 0.3624337\n#> 3       0 0.6375663 0.3624337\n#> 4       0 0.6375663 0.3624337\n#> 5       0 0.6375663 0.3624337\n#> 6       0 0.6375663 0.3624337\n#> \n#> [99 rows x 3 columns]\nh2o.predict_leaf_node_assignment(gbm_h2o, test)\n#>   T1.C1\n#> 1   LLR\n#> 2   LLR\n#> 3   LLR\n#> 4   LLR\n#> 5   LLR\n#> 6   LLR\n#> \n#> [99 rows x 1 column]\n\nY pintando lo mismo\n\narbol_ind <- h2o.getModelTree(model = gbm_h2o, tree_number = 1)\nNodesInfo(arbol_ind )\n#> [1] \"Node ID 0 has left child node with index 1 and right child node with index 2 The split feature is sex. The NA direction is LEFT\"\n#> [1] \"Node ID 1 has left child node with index 3 and right child node with index 4 The split feature is pclass. The NA direction is LEFT\"\n#> [1] \"Node ID 2 has left child node with index 5 and right child node with index 6 The split feature is pclass. The NA direction is RIGHT\"\n#> [1] \"Node ID 3 has left child node with index 7 and right child node with index 8 The split feature is pclass. The NA direction is RIGHT\"\n#> [1] \"Node ID 11 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 12 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 6 has left child node with index 9 and right child node with index 10 The split feature is pclass. The NA direction is RIGHT\"\n#> [1] \"Node ID 13 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 14 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 15 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 16 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n\nAl pintar vemos un par de cosas curiosas, en primer lugar, h2o.gbm no ha codificado con one-hot las variables categóricas, esto permite por ejemplo que se pueden obtener reglas de corte como Izquierda:(Madrid, Barcelona, Valencia), Derecha: (resto de provincias), mientras que One Hot ese tipo de partición requiere más profundidad en el árbol. Y en segundo lugar vemos que por ejemplo los NA’s (y los nuevos niveles en test) de la variable pclass en un nodo van junto con p_class (2,3), en otro junto con p_class=1 y en otro junto p_class=3. El criterio elegido es en cada corte, los Nas y por ende los nuevos niveles no vistos en train van hacia dónde va la gente.\n\ntitanicDataTree = createDataTree(arbol_ind)\nplotDataTree(titanicDataTree, rankdir = \"TB\")\n\n Y nada más. hasta otra.\nNota: Los valores de prediction que saca plotDataTree no son las predicciones del modelo, sino las raw que saca ese árbol en particular. Como en los modelos de gradient boosting se va construyendo cada árbol sobre los errores del anterior, ni siquiera es la probabilidad en escala logit. He buscado en la docu de h2o pero no está claro qué es este valor. Eso sí, las ramas en el árbol están bien."
  },
  {
    "objectID": "2021/09/27/a-b-que/index.html",
    "href": "2021/09/27/a-b-que/index.html",
    "title": "¿A/B qué?",
    "section": "",
    "text": "Recuerdo siendo yo más bisoño cuando escuché a los marketinianos hablar del A/B testing para acá , A/B testing para allá. En mi ingenuidad pensaba que era alguna clase de rito que sólo ellos conocían, y encima lo veía como requisito en las ofertas de empleo que miraba.\nMi decepción fue mayúscula cuando me enteré que esto del A/B testing no es más que un nombre marketiniano para hacer un contraste de proporciones o contrastes de medias, vamos, un prop.test o un t.test, ya que ni siquera trataban el caso de tener varios grupos o la existencia de covariables. Ains, esas dos asignaturas en la carrera de diseño de experimentos y de ver fórmulas y sumas de cuadrados a diestro y siniestro, esos anovas, y ancovas.\nTotal que hoy vengo a contar alguna forma diferente a la de la fórmula para hacer este tipo de contrastes.\nSupongamos que tenemos los siguientes datos, inventados\n\ndf <-  data.frame( \n  exitos         = c(2, 200,  10, 20,  4,  200, 300,  20,  90,  90),\n  fracasos       = c(8, 1000, 35, 80, 20,  400, 900, 400, 230, 150) ,\n  gcontrol       = factor(c(1,0,1,1,1,0,0,0,0,0)))\n\ndf$n = df$exitos + df$fracasos\ndf\n#>    exitos fracasos gcontrol    n\n#> 1       2        8        1   10\n#> 2     200     1000        0 1200\n#> 3      10       35        1   45\n#> 4      20       80        1  100\n#> 5       4       20        1   24\n#> 6     200      400        0  600\n#> 7     300      900        0 1200\n#> 8      20      400        0  420\n#> 9      90      230        0  320\n#> 10     90      150        0  240\n\nTenemos 10 experimentos binomiales y hemos obtenido esos resultados, (podría ser por ejemplo la proporción de clientes que han contratado un producto A en 10 meses)\nPodriamos ver la proporción en cada fila\n\nlibrary(tidyverse)\n\ndf$prop <- df$exitos/df$n\ndf\n#>    exitos fracasos gcontrol    n       prop\n#> 1       2        8        1   10 0.20000000\n#> 2     200     1000        0 1200 0.16666667\n#> 3      10       35        1   45 0.22222222\n#> 4      20       80        1  100 0.20000000\n#> 5       4       20        1   24 0.16666667\n#> 6     200      400        0  600 0.33333333\n#> 7     300      900        0 1200 0.25000000\n#> 8      20      400        0  420 0.04761905\n#> 9      90      230        0  320 0.28125000\n#> 10     90      150        0  240 0.37500000\n\nggplot(df, aes(prop, fill = gcontrol)) +\n  geom_density(alpha = 0.3) +\n  labs(fill = \"Gcontrol\")\n\n\n\n\n\n\n\n\nPues como ahora me estoy volviendo bayesiano, ¿por qué no ajustar un modelo bayesiano a estos datos y obtener la posteriori de cada uno de las proporciones y de su diferencia. Vamos a ajustar lo que a veces se denomina una regresión binomial, dónde tenemos éxitos y ensayos. Normalmente la gente está acostumbrada a ajustar regresiones logísticas dónde la variable dependiente es 1 o 0, en este caso, la información está agregada, pero es equivalente.\nAl tener el número de “ensayos” de cada experimento, se va a tener en cuenta, de forma que no va a ser lo mismo un experimento con 20 ensayos que uno con 200, aun cuando tengan la misma proporción.\nUsando la librería brms y stan sería así de sencillo\n\nlibrary(brms)\nlibrary(cmdstanr)\nset_cmdstan_path(\"~/cmdstan/\")\n\nprior <-  get_prior(exitos | trials(n) ~  0 + gcontrol , \n                    data = df, \n                    family = binomial)\n\nmod_brm <-\n    brm(data = df, family = binomial,\n        exitos | trials(n) ~ 0 + gcontrol,\n        prior = prior,\n        iter = 2500, warmup = 500, cores = 6, chains = 6, \n        seed = 10, \n        backend = \"cmdstanr\")\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 5 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 5 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 6 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 6 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 1 finished in 0.1 seconds.\n#> Chain 2 finished in 0.1 seconds.\n#> Chain 3 finished in 0.1 seconds.\n#> Chain 4 finished in 0.1 seconds.\n#> Chain 5 finished in 0.1 seconds.\n#> Chain 6 finished in 0.1 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 0.1 seconds.\n#> Total execution time: 0.8 seconds.\n\n\nfixef(mod_brm) %>% \n    round(digits = 2)\n#>           Estimate Est.Error  Q2.5 Q97.5\n#> gcontrol0    -1.23      0.04 -1.31 -1.16\n#> gcontrol1    -1.39      0.19 -1.77 -1.03\n\nY ya tengo la estimación de cada proporción sin más que hacer el invlogit\n\nfixef(mod_brm) %>% \n    round(digits = 2) %>% \n  inv_logit_scaled()\n#>            Estimate Est.Error      Q2.5     Q97.5\n#> gcontrol0 0.2261814 0.5099987 0.2124868 0.2386673\n#> gcontrol1 0.1994078 0.5473576 0.1455423 0.2630841\n\nUna cosa buena de la estimación bayesiana es que tengo la posteriori completa de ambas proporciones\n\npost <- as_tibble(mod_brm)\npost\n#> # A tibble: 12,000 × 4\n#>    b_gcontrol0 b_gcontrol1 lprior  lp__\n#>          <dbl>       <dbl>  <dbl> <dbl>\n#>  1       -1.17       -1.39      0 -128.\n#>  2       -1.18       -1.38      0 -128.\n#>  3       -1.20       -1.10      0 -128.\n#>  4       -1.24       -1.45      0 -127.\n#>  5       -1.23       -1.79      0 -129.\n#>  6       -1.21       -1.60      0 -128.\n#>  7       -1.21       -1.62      0 -128.\n#>  8       -1.19       -1.68      0 -129.\n#>  9       -1.19       -1.59      0 -128.\n#> 10       -1.30       -1.54      0 -129.\n#> # … with 11,990 more rows\n\nY tenemos 2000 muestras por 6 cadenas, 12000 muestras aleatorias de cada proporción.\nAhora puedo hacer cosas como ver la distribución a posteriori de la diferencia\n\npost$diferencia = inv_logit_scaled(post$b_gcontrol1) - inv_logit_scaled(post$b_gcontrol0)\n\nggplot(post, aes(diferencia)) +\n  geom_density()\n\n\n\n\n\n\n\n\nIntervalo de credibilidad al 80%\n\nquantile(post$diferencia, probs = c(0.1,0.9))\n#>         10%         90% \n#> -0.06377131  0.01499445\n\nY si sospechamos que hay más estructura en nuestros datos podemos modelarla igulmente, por ejemplo las proporciones podrían tener relación con el mes del año o con cualquier otra cosa.\nEn fin, un método alternativo para hacer A/B testing o como se llame ahora."
  },
  {
    "objectID": "2021/09/10/los-viejos-rockeros-model-matrix/index.html",
    "href": "2021/09/10/los-viejos-rockeros-model-matrix/index.html",
    "title": "Los viejos [R]ockeros. model.matrix",
    "section": "",
    "text": "Nota: He cambiado la parte final para que hiciera lo mismo que el código de python, gracias a mi tocayo José Luis Hidalgo\nEl otro día por linkedin, mi jefe compartió el siguiente artículo recomendable por otro lado. El repo con el código y datos está aquí.\nEn el artículo hacen referencia a que una forma de ver el CATE (Conditional Average Treatmen Effect) cuando hay variables categóricas puede ser construirse los términos de interacción de alto orden entre las variables categóricas y calcular la diferencia entre la media de la variable de interés antes del tratamiento y después del tratamiento, en cada una de las variables de interacción consideradas.\nPara eso el autor del artículo hace lo siguiente\nY crea una función para crear las interacciones de orden n.\nY crea las variables, al medir cuánta tarda vemos que es en torno al minuto.\nY aquí es dónde vienen los viejos [R]ockeros. Cada vez que oigo hablar de interacciones pienso en R y en nuestras queridas fórmulas. En R podemos hacer lo mismo tirando de nuestro viejo amigo model.matrix\nConvertimos las variables que nos interesan a tipo factor\nY al utilizar model matrix R hace por defecto codificación parcial de las variables (One Hot quitando la que sobra para los modernos), así que para tener lo mismo hay que tocar un argumento de model matrix. el truco es definir para cada variable el contrasts = FALSE. Por ejemplo\nPor defecto el contrasts para una variable categórica elimina la categoría redundante.\nPero podemos decir que no, y así nos construirá tantas variables dicotómicas como categorías tenga nuestra variable.\nYa podemos crear nuestra función binarize\nPara crear interacciones de orden n en R basta con definir la fórmula ~ 0 + ( var1 + var2 + var3)^n\nY podemos medir cuanto tarda nuestra función sobre el mismo conjunto de datos. Y vemos, que en crear las variables tarda unos pocos segundos.\nY ya estaría ."
  },
  {
    "objectID": "2021/09/10/los-viejos-rockeros-model-matrix/index.html#cate",
    "href": "2021/09/10/los-viejos-rockeros-model-matrix/index.html#cate",
    "title": "Los viejos [R]ockeros. model.matrix",
    "section": "CATE",
    "text": "CATE\nLa parte interesante del artículo es la de calcular el CATE como la diferencia de medias de la variable order_value en cada uno de los segmentos antes de una determinada fecha y después.\nEn el artículo lo hacen así\n\n\nstart_time = time.time()\n\ndf_before = df[df[time_axis] <= '2019-09-11']\ndf_after  = df[df[time_axis] > '2019-09-11']\nfeatures = copy(df.drop([time_axis,kpi_axis], axis=1).columns)\n\nK = 10 \nsubgroups=[]\nscore=[]\nfor k in range(0,K):\n    CATE = []\n    y_before = df_before[kpi_axis]\n    y_after= df_after[kpi_axis]\n    \n    #compute CATEs for all subgroups\n    for d in features:\n        g = df_before[d] == True\n        m_before = np.mean(y_before[g])\n        g = df_after[d] == True\n        m_after = np.mean(y_after[g])\n        CATE.append(m_after-m_before)\n    \n    #find subgroup with biggest CATE\n    index = np.argsort(-abs(np.array(CATE)))\n    subgroups.append(features[index[0]])\n    score.append(abs( CATE [index[0]]))\n    \n    #remove found subgroups from dataset\n    df_before = df_before[df_before[features[index[0]]] == False]\n    df_after = df_after[df_after[features[index[0]]] == False] \n    features = features.drop(features[index[0]])\n    \n\ndf_nuevo = pd.DataFrame(np.array([score,subgroups]).T, columns=['CATE','features'])\n\nelapsed_time = time.time() - start_time\n\nprint(elapsed_time)\n\n39.18984651565552\n\n\n\ndf_nuevo\n\n                 CATE                                           features\n0   289.4008630608073  customer_age == 46+ AND first_order_made == ye...\n1   8.979524530417706  customer_age == 30-35 AND customer_country == ...\n2   8.690151515151513  customer_age == 36-45 AND customer_country == ...\n3   8.567118700265269  customer_age == 36-45 AND customer_country == ...\n4   7.811875000000015  customer_age == 30-35 AND customer_country == ...\n5   7.510393162393143  customer_age == 36-45 AND customer_country == ...\n6    8.40514915254235  customer_age == 36-45 AND customer_country == ...\n7   7.597928321678324  customer_age == 36-45 AND customer_country == ...\n8  7.4170337760987906  customer_age == 46+ AND customer_country == ca...\n9  7.2043861024033475  customer_age == 21-24 AND customer_country == ...\n\n\nY tarda su ratillo, pero no está mal\nEn R lo podemos hacer utilizando nuestro viejo amigo el R base para poner las condiciones\n\nCalcularCate_old <-  function(f, df){\n  \n  filtro_antes   = df[[f]] == 1 & df$corte_fecha == \"antes\"\n  filtro_despues = df[[f]] == 1 & df$corte_fecha != \"antes\"\n  \n  media_antes   = mean(df$order_value[filtro_antes])\n  media_despues = mean(df$order_value[filtro_despues])\n  \n  cate = media_despues - media_antes\n  \n  return(cate)\n  \n  \n}\n\n# usando fmean de collapse\n\nCalcularCate <-  function(f, df){\n  \n  filtro_antes   = df[[f]] == 1 & df$corte_fecha == \"antes\"\n  filtro_despues = df[[f]] == 1 & df$corte_fecha != \"antes\"\n  \n  media_antes   = fmean(df$order_value[filtro_antes])\n  media_despues = fmean(df$order_value[filtro_despues])\n  \n  cate = media_despues - media_antes\n  \n  return(cate)\n  \n  \n}\n\n\ntictoc::tic()\nK = 10\ncate = c()\ntmp <-  df_final\n\nfor ( k in 1:K) {\n  \n  features <- colnames(tmp)[3:ncol(tmp)]\n  res <-  unlist(lapply(features, function(x) CalcularCate(x, df = tmp)))\n  names(res) <- features\n  ordenado <-  sort(abs(res), decreasing = TRUE)[1]\n  f <-  names(ordenado)\n  cate <- c(cate, ordenado)\n  tmp <-  tmp[tmp[[f]]== 0, c(\"corte_fecha\", \"order_value\", setdiff(features, f))]\n}\n\n \n \ntictoc::toc()\n\n40.084 sec elapsed\n\ncate\n\n              systemandroid-tv:first_order_madeyes:customer_age46+ \n                                                        289.400863 \n                systemios-mob:customer_age30-35:customer_countryuk \n                                                          8.979525 \n     household_incomelow:customer_age36-45:customer_countrygermany \n                                                          8.690152 \n            systemwin-pc:customer_age36-45:customer_countrygermany \n                                                          8.567119 \n     household_incomehigh:customer_age30-35:customer_countrycanada \n                                                          7.811875 \n   product_categorybooks:customer_age36-45:customer_countrygermany \n                                                          7.510393 \n        systemandroid-tv:customer_age36-45:customer_countrygermany \n                                                          8.405149 \n            genderfemale:customer_age36-45:customer_countrygermany \n                                                          7.597928 \nproduct_categoryelectronics:customer_age46+:customer_countrycanada \n                                                          7.417034 \n                 systemios-pc:customer_age21-24:customer_countryuk \n                                                          7.204386 \n\n\nY bueno, parece que en este caso, los viejos [R]ockeros no lo hacen mal del todo, sobre todo la parte de model.matrix es muy rápida, y usando collapse para calcular la media es aún más rápido\nEn resumen, model.matrix de rbase es muy rápido, y usar fmean de collapse en vez del mean de R-base mejor, con lo que con esta implementación en R es mucho más rápida que la vista en python (que seguramente se puede mejorar hasta igualar)"
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html",
    "title": "Análisis de correspondencias “old_style”",
    "section": "",
    "text": "Quién me conoce sabe que siento debilidad por el análisis de datos categóricos, en particular por técnicas como el análisis de correspondencias simple o múltiple o por las cosas más modernas que hay. No en vano se me dió especialmente bien en la universidad, en parte debido a que por fin me centré después de unos años locos, y en parte debido a algún buen profesor. El caso es que en el curro utilizamos este tipo de técnicas para encontrar relaciones entre variables categóricas que quizá hayan pasado desapercibidas en un primer análisis.\nAntes de nada voy a dar un par de referencias en castellano, bastante útiles.\nDe hecho el ejemplo que voy a contar y la notación que voy a usar viene en el libro de Daniel. Es un ejemplo de Fisher (si, ese al que todo el mundo odia hoy en día) de 1940, sobre la relación entre el color de los ojos (en filas) y el color del pelo (en columnas). Se trata de una simple tabla de contingencia.\nCuyos totales por filas y columnas son"
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-filas.",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-filas.",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Proyección de las filas.",
    "text": "Proyección de las filas.\nPodríamos plantearnos la relación entre las filas (color de ojos) , ¿cómo de similares son los que tienen los ojos claros con los que tienen los ojos azules, respecto a su color del pelo?\nParece claro que deberíamos centrarnos en los porcentajes por filas (perfiles fila)\n\nprop.table(df_tabla, 1)\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646\n#> azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273\n#> castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144\n#> oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374\n\nA partir de ahora vamos a usar la tabla de frecuencias relativas, cuyos elementos llamaremos \\(f_{ij}\\)\n\n(tabla_frecuencias <-  prop.table(df_tabla))\n#>               rubio   pelirrojo    castaño     oscuro        negro\n#> claros   0.12922615 0.021788129 0.10969196 0.03531180 0.0007513148\n#> azules   0.06123216 0.007137491 0.04526672 0.02066116 0.0005634861\n#> castaños 0.06442524 0.015777611 0.17073629 0.07738542 0.0048835462\n#> oscuros  0.01840721 0.009015778 0.07569497 0.11607814 0.0159654395\n\nQue tiene los mismos porcentajes por filas.\n\n(perfiles_fila <- prop.table(tabla_frecuencias, 1))\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646\n#> azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273\n#> castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144\n#> oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374\n\nSe podría considerar utilizar la distancia euclídea para ver como de parecidas son las filas ojos claros y ojos azules, pero eso presenta un problema, que es la distribución del color del pelo en la tabla, dónde por ejemplo el porcentaje de rubios es mayor que el de pelirrojos. Así que usar esa distancia no sería justo. Podríamos definir una distancia ponderada que fuera \\(d(i, i') = \\sum_j ((f_{ij} - f_{i'j})^2/f_{.j})\\) dónde \\(f_{.j}\\) es la distribución de las columnas en la población, vamos, qué % de rubios, pelirrojos, etc hay en mis datos.\nEn forma matricial, en notación de Daniel sería \nEsta distancia es equivalente a la euclídea en la matriz Y definida como dividir cada elemento de los perfiles fila por la raíz cuadrada del peso de la columna.\n\n# peso de loas columnas\n(f.j <- colSums(tabla_frecuencias))\n#>      rubio  pelirrojo    castaño     oscuro      negro \n#> 0.27329076 0.05371901 0.40138993 0.24943651 0.02216379\n\n# peso de las filas\n(fi. <- rowSums(tabla_frecuencias))\n#>    claros    azules  castaños   oscuros \n#> 0.2967693 0.1348610 0.3332081 0.2351615\n\nPara hacerlo lo hacemos usando matrices\n\n# matriz diagonal con la raíz de los porcentjes totales de las columnas( col masses)\n(DC_12 <- diag(1/sqrt(f.j)))\n#>          [,1]     [,2]     [,3]     [,4]     [,5]\n#> [1,] 1.912879 0.000000 0.000000 0.000000 0.000000\n#> [2,] 0.000000 4.314555 0.000000 0.000000 0.000000\n#> [3,] 0.000000 0.000000 1.578399 0.000000 0.000000\n#> [4,] 0.000000 0.000000 0.000000 2.002258 0.000000\n#> [5,] 0.000000 0.000000 0.000000 0.000000 6.717041\n\n# Matriz diagonal con 1/fi. \n(DF_1 <- diag(1/fi.))\n#>         [,1]     [,2]     [,3]     [,4]\n#> [1,] 3.36962 0.000000 0.000000 0.000000\n#> [2,] 0.00000 7.415042 0.000000 0.000000\n#> [3,] 0.00000 0.000000 3.001127 0.000000\n#> [4,] 0.00000 0.000000 0.000000 4.252396\n\nMatriz Y\n\n# se puede hacer usando los perfiles fila o directamente utilizando DF_1 y DC_12\n# Y <- perfiles_fila %*% DC_12\n\nY <- DF_1 %*% as.matrix(tabla_frecuencias) %*% DC_12\nrownames(Y) <- rownames(tabla_frecuencias)\ncolnames(Y) <- colnames(tabla_frecuencias)\nY\n#>              rubio pelirrojo   castaño    oscuro      negro\n#> claros   0.8329499 0.3167648 0.5834082 0.2382433 0.01700517\n#> azules   0.8685217 0.2283469 0.5297968 0.3067526 0.02806563\n#> castaños 0.3698521 0.2042969 0.8087737 0.4650114 0.09844593\n#> oscuros  0.1497302 0.1654142 0.5080629 0.9883349 0.45602916\n\nEn esta tabla Y, la distancia euclídea entre filas coincide con la distancia ponderada que habíamos definido dónde la distancia entre dos filas venía ponderada por el peso de cada columna.\nPodríamos ahora plantearnos utilizar una descomposición en valores y vectores propios sobre esta tabla, pero tendríamos el problema de que el peso de cada fila sería el mismo, por eso se hace necesario tener en cuenta el peso de cada fila.\nPodemos construir ahora una matriz Z dónde se pondere por el peso de las filas y el de las columnas.\n\n# cremaos matriz diagonal con 1/sqrt(fi.)\n(DF_12 <- diag(1 / sqrt(fi.)))\n#>          [,1]     [,2]     [,3]     [,4]\n#> [1,] 1.835653 0.000000 0.000000 0.000000\n#> [2,] 0.000000 2.723057 0.000000 0.000000\n#> [3,] 0.000000 0.000000 1.732376 0.000000\n#> [4,] 0.000000 0.000000 0.000000 2.062134\n\nCreamos matriz Z como\n\nZ <- DF_12 %*% as.matrix(tabla_frecuencias) %*% DC_12\n\nrownames(Z) <- rownames(tabla_frecuencias)\ncolnames(Z) <- colnames(tabla_frecuencias)\nZ\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.45376229 0.17256250 0.3178206 0.1297867 0.009263827\n#> azules   0.31895094 0.08385681 0.1945596 0.1126501 0.010306662\n#> castaños 0.21349407 0.11792869 0.4668580 0.2684240 0.056827106\n#> oscuros  0.07260933 0.08021509 0.2463773 0.4792778 0.221144304\n\nSobre esta matriz Z que no es más que la tabla de frecuencias relativas estandarizada por el peso de las filas y el de las columnas podemos diagonalizar la matriz Z’Z. Esta matriz tiene un valor propio igual a 1, pero los importantes son los siguientes.\n\nres_diag <- eigen(t(Z) %*% Z)\nres_diag$values\n#> [1]  1.000000e+00  1.873957e-01  2.847581e-02  9.026139e-04 -5.424744e-18\nres_diag$vectors\n#>            [,1]        [,2]        [,3]        [,4]        [,5]\n#> [1,] -0.5227722 -0.64488569  0.50331668 -0.21984084  0.09578107\n#> [2,] -0.2317736 -0.11671207  0.06710395  0.91248080 -0.30908759\n#> [3,] -0.6335534 -0.02776729 -0.74923836 -0.04663792  0.18521831\n#> [4,] -0.4994362  0.65005451  0.30376531 -0.21371886 -0.43593979\n#> [5,] -0.1488751  0.38361288  0.29755318  0.26682945  0.81911020\n\nTeniendo la matriz Y, que son los perfiles (porcentajes) fila ponderados por el peso de las columnas, podemos proyectar esas filas sobre las dimensiones obtenidas por los vectores propios obtenidos asociados a los autovalores menores que 1. Esa será la mejor representación de las filas en un subespacio de las columnas.\n\n\n# proyeccion 1. con valor menor que 1 \nvector_propio1 <- res_diag$vectors[,2]\nvector_propio2 <- res_diag$vectors[,3]\n\n(coord1 <- Y %*% vector_propio1 )\n#>                 [,1]\n#> claros   -0.42893286\n#> azules   -0.39128686\n#> castaños  0.05523421\n#> oscuros   0.68743802\n(coord2 <- Y %*% vector_propio2 )\n#>                 [,1]\n#> claros    0.08081194\n#> azules    0.15705214\n#> castaños -0.23555524\n#> oscuros   0.14171621\n\nY ya lo puedo pintar\n\nlibrary(tidyverse)\n\n(to_plot <-  data.frame(Dim1 = coord1[,1], Dim2 = coord2[,1], color_ojos= rownames(Y)))\n#>                 Dim1        Dim2 color_ojos\n#> claros   -0.42893286  0.08081194     claros\n#> azules   -0.39128686  0.15705214     azules\n#> castaños  0.05523421 -0.23555524   castaños\n#> oscuros   0.68743802  0.14171621    oscuros\n\n\nto_plot %>% \n  ggplot(aes(x=Dim1, y=Dim2)) +\n  geom_label(aes(label= color_ojos)) +\n  scale_x_continuous(limits = c(-0.8,0.8))"
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-columnas",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-columnas",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Proyección de las columnas",
    "text": "Proyección de las columnas\nLa proyección de las columnas en un subespacio de las filas se hace de manera análoga , solo que en vez de diagonalizar Z’Z se hace con Z Z’, que tiene los mismos valores propios que los obtenidos.\nDe aquí viene la relación baricéntrica entre las filas y las columnas."
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#relación-con-la-distancia-chi-cuadrado",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#relación-con-la-distancia-chi-cuadrado",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Relación con la distancia Chi-cuadrado",
    "text": "Relación con la distancia Chi-cuadrado\nSi se desarrolla la expresión de la distancia Chi-cuadrado , tal como se hace en el libro de Daniel Peña se llega a que se corresponde con la distancia euclídea en la matriz Y que son los perfiles filas ponderados por el peso de cada columna.\nEsta implicación es importante, puesto que al descomponer el estadístico Chi-cuadrado que nos mide la asociación entre variables categóricas (filas y columnas), estamos descubriendo qué filas están asociados con determinadas columnas."
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#uso-con-factominer",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#uso-con-factominer",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Uso con FactoMineR",
    "text": "Uso con FactoMineR\nSólo trataba de dar una pequeña explicación de la relación entre el análisis de correspondencias y la diagonalización de matrices. Hay mucha más explicación en los libros que he enlazado al principio. En el día a día, podemos usar librerías específicas para calcular este análisis, como FactoMineR en R o prince en python.\nVeamos como se usa con FactoMiner\n\nlibrary(FactoMineR)\nlibrary(factoextra) # pa los dibujitos\n\n\nres_ca <- CA(df_tabla, graph = FALSE)\n\nY podemos pintar las filas en el espacio de las columnas\n\nfviz_ca_row(res_ca) +\n   scale_x_continuous(limits = c(-0.8,0.8))\n\n\n\n\n\n\n\n\nY vemos que las coordenadas son las mismas que hemos obtenido nosotros antes\n\nres_ca$row$coord\n#>                Dim 1       Dim 2        Dim 3\n#> claros   -0.42893286  0.08081194  0.032336831\n#> azules   -0.39128686  0.15705214 -0.065353061\n#> castaños  0.05523421 -0.23555524 -0.005724586\n#> oscuros   0.68743802  0.14171621  0.004781725\n\n\nto_plot\n#>                 Dim1        Dim2 color_ojos\n#> claros   -0.42893286  0.08081194     claros\n#> azules   -0.39128686  0.15705214     azules\n#> castaños  0.05523421 -0.23555524   castaños\n#> oscuros   0.68743802  0.14171621    oscuros\n\nLa representación conjunta.\n\nfviz_ca(res_ca) \n\n\n\n\n\n\n\n\nLa librería FactoMineR junto con factoextra devuelven también múltiples ayudas a la interpretación como la contribución de cada fila o columna a la estructura factorial etc. Por otro lado, la librería FactoInvestigate que toma como input un análisis factorial (pca, ca o mca), devuelve un informe en inglés (en Rmd) describiendo lo que significa cada dimensión obtenida."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Series\n\nCachitos nochevieja\nPost relacionados con extracción de imágenes y análisis de subtítulos de “cachitos nochevieja”\n\n\nJulia\nPost relacionados Julia\n\n\n\nTodos los posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nConsejos para dejar spss\n\n\n\n\n\n\n\nestadística\n\n\nsociología\n\n\n2022\n\n\nspss\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\nJulia\n\n\nproduccion\n\n\nlinux\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\ndocker\n\n\nR\n\n\napi\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n36 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\nestadística\n\n\nInvestigación operativa\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n47 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\nJulia\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\nestadística\n\n\npython\n\n\nR\n\n\nInvestigación operativa\n\n\nJulia\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPluralista\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\ntextmining\n\n\nocr\n\n\n2022\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nCocinando\n\n\n\n\n\n\n\nmuestreo\n\n\n2022\n\n\nencuestas electorales\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\n\n\n\nestadística\n\n\nspark\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecturas para el finde\n\n\n\n\n\n\n\nestadística\n\n\nanálisis bayesiano\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n¿A dónde va Vicente?\n\n\n\n\n\n\n\nárboles\n\n\nciencia de datos\n\n\nh2o\n\n\nestadísticca\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\ncorrespondencias\n\n\nfactorización\n\n\nfactorial\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿A/B qué?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\n\n\n\nR\n\n\npython\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\nsoftware\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputando datos. La estructura importa\n\n\n\n\n\n\n\nestadística\n\n\nimputación\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\n\n\n\nestadística\n\n\nbig data\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\n\n\n\nbayesian\n\n\n2021\n\n\nR\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\n\n\n\nciencia de datos\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2021\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nUna colina\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\nlinux\n\n\npolémica\n\n\nocr\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTendencias\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte II\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte I\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplillo con NMF\n\n\n\n\n\n\n\nestadística\n\n\ncorrespondencias\n\n\nfactorización\n\n\nnmf\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "julia.html",
    "href": "julia.html",
    "title": "Julia",
    "section": "",
    "text": "Sigo trasteando con julia\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "en_rmd.html",
    "href": "en_rmd.html",
    "title": "Correspondence Analysis",
    "section": "",
    "text": "This dataset contains 4 rows and 5 columns."
  },
  {
    "objectID": "en_rmd.html#annexes",
    "href": "en_rmd.html#annexes",
    "title": "Correspondence Analysis",
    "section": "Annexes",
    "text": "Annexes\n\nres.hcpc$desc.var\n\n$`1`\n            Intern %    glob % Intern freq Glob freq        p.value     v.test\nrubio     44.1253264 27.329076        1014       1455 2.052502e-127  24.012628\npelirrojo  6.7014795  5.371901         154        286  2.432399e-04   3.669274\ncastaño   35.9007833 40.138993         825       2137  4.252364e-08  -5.480037\nnegro      0.3046127  2.216379           7        118  8.802089e-20  -9.102814\noscuro    12.9677981 24.943651         298       1328  8.110151e-73 -18.048473\n\n$`2`\n         Intern %    glob % Intern freq Glob freq       p.value    v.test\ncastaño 51.240135 40.138993         909       2137 3.926728e-31 11.604127\noscuro  23.224352 24.943651         412       1328 4.317198e-02 -2.022042\nnegro    1.465614  2.216379          26        118 9.197007e-03 -2.604643\nrubio   19.334837 27.329076         343       1455 5.460448e-21 -9.399920\n\n$`3`\n           Intern %    glob % Intern freq Glob freq        p.value     v.test\noscuro    49.361022 24.943651         618       1328 1.792994e-105  21.811796\nnegro      6.789137  2.216379          85        118  3.994847e-29  11.201810\npelirrojo  3.833866  5.371901          48        286  5.584247e-03  -2.771245\ncastaño   32.188498 40.138993         403       2137  4.381154e-11  -6.590579\nrubio      7.827476 27.329076          98       1455  5.597942e-83 -19.297861\n\nattr(,\"class\")\n[1] \"descfreq\" \"list\"    \n\n\nFigure 5 - List of variables characterizing the clusters of the classification."
  },
  {
    "objectID": "cachitos.html",
    "href": "cachitos.html",
    "title": "Cachitos nochevieja",
    "section": "",
    "text": "Cachitos. Tercera parte\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 4, 2022\n\n\nConsejos para dejar spss\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 29, 2022\n\n\nLeaflet example\n\n\n\n\nOct 27, 2022\n\n\nAquí estoy de nuevo\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Hola a todos\nSoy José Luis Cañadas Reche, científico de datos (o estadístico que sabe algo de programación )\nMe puedes encontrar en Twitter o GitHub y en Fosstodon.\n\n\n\nBlogs antiguo y nuevo\n\nBlog nuevo\n\n\nBlog antiguo\n\n\n\nInvestigación operativa\n\nEjemplo simple\nEjemplo simple de como usar Julia, R y python para investigación operativa\n\n\nLocalización: Indios y jefes\nAsignar localizaciones según demanda y diferentes restricciones. Un problema usual"
  },
  {
    "objectID": "2021/06/13/imputando-datos-la-estructura-importa/index.html",
    "href": "2021/06/13/imputando-datos-la-estructura-importa/index.html",
    "title": "Imputando datos. La estructura importa",
    "section": "",
    "text": "Voy a empezar este post con un par de citas.\nLa primera no recuerdo dónde la leí, pero es de la escuela francesa de estadística, la segunda es del blog hermano datanalytics.\nY bueno, ambas tienen parte de razón. En esta entrada voy a comentar brevemente una forma de imputación de datos que quizá a alguien le sea útil.\nLa idea básica es:\nEste procedimiento iterativo suele usar Expectation Maximization y se conoce como EM-PCA. (también hay versión usando regularización)\nMás información en este artículo de François Husson"
  },
  {
    "objectID": "2021/06/13/imputando-datos-la-estructura-importa/index.html#ejemplo",
    "href": "2021/06/13/imputando-datos-la-estructura-importa/index.html#ejemplo",
    "title": "Imputando datos. La estructura importa",
    "section": "Ejemplo",
    "text": "Ejemplo\nY ahora vamos a ver un ejemplito, para eso vamos a usar la librería missMDA de François Husson y Julie Josse, que incorpora estos métodos.\n\nlibrary(missMDA)\n\nUsamos el conjunto de datos orange en cuya ayuda leeemos\nDescription Sensory description of 12 orange juices by 8 attributes. Some values are missing.\nUsage data(orange) Format A data frame with 12 rows and 8 columns. Rows represent the different orange juices, columns represent the attributes.\n\ndata(orange)\n\nPor ejemplo, tenemos un valor perdido para la variable Attack.intensity en la primera fila\n\nDT::datatable(orange,\n               options = list(scrollX = TRUE))\n\n\n\n\n\n\nBien, pues ahora podemos usar el EM-PCA para imputar los valores perdidos teniendo en cuenta la estructura factorial. Usamos el ejemplo que viene en la función.\nEstimamos el número de componentes a extraer y usamos la función imputePCA que\n\nestim_ncpPCA(orange)\n#> $ncp\n#> [1] 2\n#> \n#> $criterion\n#>         0         1         2         3         4         5 \n#> 1.0388714 0.9279948 0.5976727 0.7855501 2.0250470 2.6741735\n\n\nres_impute <-  imputePCA(orange, ncp=2)\n\nLa función devuelve el conjunto de datos ya imputado\n\nres_impute$completeObs\n#>    Color.intensity Odor.intensity Attack.intensity    Sweet     Acid   Bitter\n#> 1         4.791667       5.291667         4.077034 5.527352 4.177564 2.833333\n#> 2         4.583333       6.041667         4.416667 5.458333 4.125000 3.541667\n#> 3         4.708333       5.333333         4.158054 5.442936 4.291667 3.166667\n#> 4         6.583333       6.000000         7.416667 4.166667 6.750000 4.702509\n#> 5         6.271605       6.166667         5.333333 4.083333 5.455805 4.375000\n#> 6         6.333333       5.000000         5.375000 5.000000 5.500000 3.625000\n#> 7         4.291667       4.916667         5.291667 5.541667 5.250000 3.214232\n#> 8         4.460613       4.541667         4.833333 5.479128 4.958333 2.916667\n#> 9         4.416667       5.136550         5.166667 4.625000 5.041667 3.666667\n#> 10        4.541667       4.291667         4.176991 5.791667 4.375000 2.735255\n#> 11        4.083333       5.125000         3.916667 5.703297 3.900164 2.815857\n#> 12        6.500000       5.875000         6.125000 4.875000 5.291667 4.166667\n#>        Pulp Typicity\n#> 1  5.711715 5.208333\n#> 2  4.625000 4.458333\n#> 3  6.250000 5.166667\n#> 4  1.416667 3.416667\n#> 5  3.416667 4.416667\n#> 6  4.208333 4.875000\n#> 7  1.291667 4.333333\n#> 8  1.541667 3.958333\n#> 9  1.541667 3.958333\n#> 10 4.026062 5.000000\n#> 11 7.333333 5.250000\n#> 12 1.500000 3.500000"
  },
  {
    "objectID": "2021/06/13/imputando-datos-la-estructura-importa/index.html#extensiones",
    "href": "2021/06/13/imputando-datos-la-estructura-importa/index.html#extensiones",
    "title": "Imputando datos. La estructura importa",
    "section": "Extensiones",
    "text": "Extensiones\nSe puede hacer imputación múltiple.\n\n# creamos 100 datasets imputados, \n# por defecto usa método bootstrap pero también puede usar una versión bayesiana\nres_mi_impute <- MIPCA(orange, ncp = 2, nboot = 100)\n\nEn el Slot res.MI tenemos los datasets imputados\n\n# vemos por ejemplo la imputación 3\nres_mi_impute$res.MI[[3]]\n#>    Color.intensity Odor.intensity Attack.intensity    Sweet     Acid   Bitter\n#> 1         4.791667       5.291667         2.918305 6.869451 3.625573 2.833333\n#> 2         4.583333       6.041667         4.416667 5.458333 4.125000 3.541667\n#> 3         4.708333       5.333333         2.403693 6.861525 4.291667 3.166667\n#> 4         6.583333       6.000000         7.416667 4.166667 6.750000 4.701455\n#> 5         6.146539       6.166667         5.333333 4.083333 5.097418 4.375000\n#> 6         6.333333       5.000000         5.375000 5.000000 5.500000 3.625000\n#> 7         4.291667       4.916667         5.291667 5.541667 5.250000 2.974466\n#> 8         4.777501       4.541667         4.833333 4.940791 4.958333 2.916667\n#> 9         4.416667       4.999918         5.166667 4.625000 5.041667 3.666667\n#> 10        4.541667       4.291667         4.673680 5.791667 4.375000 2.185506\n#> 11        4.083333       5.125000         3.916667 6.264218 4.439501 3.627578\n#> 12        6.500000       5.875000         6.125000 4.875000 5.291667 4.166667\n#>        Pulp Typicity\n#> 1  2.870165 5.208333\n#> 2  4.625000 4.458333\n#> 3  6.250000 5.166667\n#> 4  1.416667 3.416667\n#> 5  3.416667 4.416667\n#> 6  4.208333 4.875000\n#> 7  1.291667 4.333333\n#> 8  1.541667 3.958333\n#> 9  1.541667 3.958333\n#> 10 4.090779 5.000000\n#> 11 7.333333 5.250000\n#> 12 1.500000 3.500000\n\nCuando el conjunto de datos es pequeño, como en este caso (12 filas), podemos ver gráficamente la incertidumbre asociada a la imputación. En la ayuda de la función plot.MIPCA se puede consultar lo que significa cada gráfico\n\nplot(res_mi_impute)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#> $PlotIndProc\n\n\n\n\n\n\n\n#> \n#> $PlotDim\n\n\n\n\n\n\n\n#> \n#> $PlotIndSupp\n\n\n\n\n\n\n\n#> \n#> $PlotVar"
  },
  {
    "objectID": "2021/06/13/imputando-datos-la-estructura-importa/index.html#mas-extensiones",
    "href": "2021/06/13/imputando-datos-la-estructura-importa/index.html#mas-extensiones",
    "title": "Imputando datos. La estructura importa",
    "section": "Mas extensiones",
    "text": "Mas extensiones\nLa librería que subyace bajo missMDA es la conocida FactoMineR que permite hacer métodos factoriales (otros nombres para lo mismo serían métodos de componentes principales, reducción de dimensiones, projection pursuit) teniendo en cuenta que las variables sean categóricas (Análisis de correspondencias simple y múltiple) o que haya una mezcla de continuas y categóricas (Análisis factorial para datos mixtos).\nUna de las extensiones más útiles a mi modo de ver es la que permite imputar teniendo en cuenta la estructura factorial y también que las observaciones estén asociadas, por ejemplo que tenga una clasificación previa, vía segmentación o un cluster previo.\nPor ejemplo podría tener datos de clientes de diferentes provincias de España, unirlo todo en un conjunto de datos dónde tenga la variable que indica de qué provincia es cada cliente y poder obtener una estructura factorial global general y una estructura factorial específica para cada provincia y poder utilizar ambas estructuras para imputar los valores perdidos. Sería una parte general y una parte específica. ¿No os recuerda a los modelos mixtos bayesianos o a la estrategia de de modelo global y particular de Carlos ?\nBueno, pues todo eso se puede hacer con la librería missMDA. Como ejemplo podemos ver el dataset ozone\nDescription This dataset contains 112 daily measurements of meteorological variables (wind speed, temperature, rainfall, etc.) and ozone concentration recorded in Rennes (France) during the summer 2001. There are 11 continuous variables and 2 categorical variables with 2 or 4 levels. Some values are missing.\n\ndata(ozone)\nhead(ozone,6)\n#>          maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v\n#> 20010601    87 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946     84\n#> 20010602    NA 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87\n#> 20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82\n#> 20010604   114 16.2 19.7 22.5   1   NA    0  0.9848      NA      NA     92\n#> 20010605    94 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301    114\n#> 20010606    80 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000     NA\n#>           vent pluie\n#> 20010601  Nord   Sec\n#> 20010602  Nord   Sec\n#> 20010603   Est  <NA>\n#> 20010604  <NA>   Sec\n#> 20010605 Ouest   Sec\n#> 20010606 Ouest Pluie\n\nY vamos a usar como variable de grupo vent y vemos que tenemos por ejemplo paara la segunda fila valores perdidos en la variable numérica maxO3 o para la tercer fila tenemos un valor perdido para la variable categórica pluie\n\nozone[c('20010602','20010603'), ]\n#>          maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v vent\n#> 20010602    NA 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87 Nord\n#> 20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82  Est\n#>          pluie\n#> 20010602   Sec\n#> 20010603  <NA>\n\nPara aplicar el método de imputación tenemos que decidir qué número de componentes queremos para la estructura general y cuántos para la particular.\n\n# en ifac tenemos que poner el índice de la variable de grupo (de tipo factor), vent es la 12\nncp_estim <- estim_ncpMultilevel(ozone, ifac = 12)\n\nY nos devuelve el número de componentes estimados entre los grupos ncpB y el número de componentes estimados dentro de los grupos.\n\nncp_estim$ncpB\n#> [1] 1\nncp_estim$ncpW\n#> [1] 3\n\nY con esto se lo pasamos a la función imputeMultilevel y hacemos la imputación\n\nozone_multilevel_imp <-  imputeMultilevel(ozone, ifac = 12, ncpB = 1, ncpW= 3)\n\nY podemos ver la imputación que ha realizado\n\nhead(ozone_multilevel_imp$completeObs)\n#>             maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15\n#> 20010601 87.00000 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946\n#> 20010602 79.51042 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000\n#> 20010603 92.00000 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209\n#> 20010605 94.00000 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301\n#> 20010606 80.00000 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000\n#> 20010607 68.07716 16.8 15.6 14.9   7    8    8 -4.3301 -1.8794 -3.7588\n#>             maxO3v  vent pluie\n#> 20010601  84.00000  Nord   Sec\n#> 20010602  87.00000  Nord   Sec\n#> 20010603  82.00000   Est   Sec\n#> 20010605 114.00000 Ouest   Sec\n#> 20010606  75.52026 Ouest Pluie\n#> 20010607  80.00000 Ouest   Sec\n\nY vemos que ha realizado tanto imputación de la variable numérica maxO3 para el dato de 20010602, como imputación de la variable categórica pluie para el datos de 20010603\n\nozone_multilevel_imp$completeObs[c('20010602','20010603'), c(\"maxO3\", \"pluie\") ]\n#>             maxO3 pluie\n#> 20010602 79.51042   Sec\n#> 20010603 92.00000   Sec\n\nLo dicho, me parece una técnica interesante cuándo se tiene por ejemplo una variable que te clasifica los clientes (ya sea una clasificación previa o dada por negocio) y tenemos datos faltantes tanto para variables numéricas como categóricas. Me parece una mejor solución que imputar por la media, mediana o simplemente asignar un valor pseudo aleatorio tipo -9999. Además de que el enfoque geométrico-algebraico de las técnicas de componentes principales siempre me ha gustado.\nY como decía al principio Quien renuncia a la estructura, deja dinero encima de la mesa.\nFeliz semana."
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "",
    "text": "Y seguimos dando vueltas a los datos de post anteriores. Siempre hay quien dice que el bayesiano no sirve para big data y qué se acaba el universo antes de que termine de ajustar tu modelo (esto último creo que se lo he dicho yo alguna vez a Carlos).\nPero ya hemos visto en los dos post anteriores que podemos condensar los datos en menos filas sin perder información, así que , ¿por qué no utilizar un modelo bayesiano?\nDel post anterior\nY tenemos nuestros conjuntos de train y de test en local"
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "Modelo bayesiano.",
    "text": "Modelo bayesiano.\nPues ahora vamos a probar a hacer un modelo bayesiano jerárquico, podríamos hacer el equivalente a glmer usando la librería rstanarm y ajustar varias regresiones logísticas independientes, pero en vez de eso vamos a ver como ajustar directamente la distribución multinomial usando brms.\nLos modelos serían algo así como\n\\[\n\\begin{equation} ans \\sim Multinomial(\\boldsymbol{\\theta}) \\end{equation}\n\\] Dónde \\[\n\\begin{equation}\n\\boldsymbol{\\theta} = \\{\\theta_{Rec}, \\theta_{Best}, \\theta_{Neut}, \\theta_{\\text{No_way}}\\}\n\\end{equation}\n\\]\nLo bueno de stan y de brms es que se puede modelar directamente la Multinomial, es decir, el número de “éxitos” en cada categoría dado un número de intentos. En brms podemos usar trials para especificarlo. Sería el equivalente al weights en glmer. De esta forma podemos trabajar con los datos agregados en vez de tenerlos individuales. Si tengo, 1000 clientes con edad < 21 y valor_cliente = 8, en vez de poner 1000 filas, pongo una columna de frecuencias, que es lo que hemos hecho.\n\nLibrerías\nYo uso cmdstan como backend para brms en vez de rstan, está más actualizado y tarda menos en muestrear.\n\n# Core libraries\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(cmdstanr)\n\n# For beauty plots\nlibrary(ggridges)\n\n## Using all cores. 12 in my machine\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n\n\n\nAdecuando los datos\nPara poder ajustar el modelo de regresión multinomial se necesita tener los datos de una determinada forma, básicamente tener una columna de tipo matriz. Para eso vamos a pivotar los datos y usar cbind\nPivotamos\n\ntrain_wider <-   train_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)), \n    total = Rec + Neut + Best + No_way\n  ) \n\ntest_wider <- test_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)),\n    total = Rec + Neut + Best + No_way\n  )\n\n\nDT::datatable(train_wider)\n\n\n\n\n\n\nY ahora unimos las columnas que indican el conteo en cada perfil de Rec, Best, Neut y NoWay en un columna que es una matriz\n\n# lo hacemos solo para el train, para el test no hace falta\n\ntrain_wider$cell_counts <- with(train_wider, cbind(Rec, Best, Neut, No_way))\nclass(train_wider$cell_counts)\n#> [1] \"matrix\" \"array\"\n\n\nDT::datatable( train_wider %>% \n                 select(tipo, valor_cliente,\n                        cell_counts, everything()\n))\n\n\n\n\n\n\nPues ya podemos ajustar el modelo. Brms tiene una función get_prior para poner las priors por defecto.\nVoy a usar un modelo con efectos aleatorios que tarda unos pocos minutos, pero si usamos cell_counts | trials(total) ~ edad_cat + valor_cliente el modelo se ajusta en menos de 60 segundos. Bueno, vamos a verlo\n\n\nAjuste de los modelos\nModelo efectos fijos\n\nformula_efectos_fijos <- brmsformula(\n  cell_counts | trials(total) ~ edad_cat + valor_cliente\n)\n\n# get priors\npriors <- get_prior(formula_efectos_fijos, train_wider, family = multinomial())\n\ntictoc::tic(\"Modelo efectos fijos\")\nmodel_multinomial1 <- brm(formula_efectos_fijos, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\",\n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 28.7 seconds.\n#> Chain 4 finished in 29.9 seconds.\n#> Chain 2 finished in 30.1 seconds.\n#> Chain 3 finished in 32.8 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 30.4 seconds.\n#> Total execution time: 33.0 seconds.\ntictoc::toc()\n#> Modelo efectos fijos: 51.868 sec elapsed\n\nModelo con efectos aleatorios\nY tarda unos 9 minutos o así\n\nformula <- brmsformula(\n  cell_counts | trials(total) ~ (1|edad_cat) + (1|valor_cliente\n))\n\n# get priors\npriors <- get_prior(formula, train_wider, family = multinomial())\n\nPodemos ver las priors que ha considerado por defecto. Y vemos las priors que ha tomado para modelar la distribución de las \\(\\sigma\\) asociadas a edad_cat y valor_cliente\n\npriors\n#>                 prior     class      coef         group resp    dpar nlpar lb\n#>                (flat) Intercept                                              \n#>  student_t(3, 0, 2.5) Intercept                               muBest         \n#>  student_t(3, 0, 2.5)        sd                               muBest        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5) Intercept                               muNeut         \n#>  student_t(3, 0, 2.5)        sd                               muNeut        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5) Intercept                              muNoway         \n#>  student_t(3, 0, 2.5)        sd                              muNoway        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente      muNoway        0\n#>  ub       source\n#>          default\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n\n\ntictoc::tic(\"modelo mixto\")\nmodel_multinomial2 <- brm(formula, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\", \n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 474.7 seconds.\n#> Chain 2 finished in 478.7 seconds.\n#> Chain 3 finished in 479.2 seconds.\n#> Chain 4 finished in 480.1 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 478.2 seconds.\n#> Total execution time: 480.2 seconds.\ntictoc::toc()\n#> modelo mixto: 502.623 sec elapsed\n\nPodemos ver el modelo con\n\nsummary(model_multinomial2)\n#>  Family: multinomial \n#>   Links: muBest = logit; muNeut = logit; muNoway = logit \n#> Formula: cell_counts | trials(total) ~ (1 | edad_cat) + (1 | valor_cliente) \n#>    Data: train_wider (Number of observations: 182) \n#>   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 12000\n#> \n#> Group-Level Effects: \n#> ~edad_cat (Number of levels: 5) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      1.03      0.49     0.48     2.31 1.00     2096\n#> sd(muNeut_Intercept)      0.61      0.34     0.26     1.50 1.00     2440\n#> sd(muNoway_Intercept)     0.56      0.31     0.24     1.40 1.00     2182\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      3529\n#> sd(muNeut_Intercept)      3759\n#> sd(muNoway_Intercept)     3057\n#> \n#> ~valor_cliente (Number of levels: 10) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.98      0.30     0.58     1.74 1.00     1254\n#> sd(muNeut_Intercept)      0.52      0.16     0.31     0.91 1.00     1514\n#> sd(muNoway_Intercept)     1.74      0.47     1.09     2.88 1.00     1276\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      2916\n#> sd(muNeut_Intercept)      3329\n#> sd(muNoway_Intercept)     2712\n#> \n#> Population-Level Effects: \n#>                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> muBest_Intercept     -0.02      0.56    -1.14     1.14 1.00      935     2317\n#> muNeut_Intercept      1.08      0.35     0.38     1.74 1.01      999     2278\n#> muNoway_Intercept     0.77      0.62    -0.46     2.00 1.00      530     1377\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPintarlo\n\nplot(model_multinomial2, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE incluso ver el modelo en stan\n\nmodel_multinomial2$model\n#> // generated with brms 2.18.0\n#> functions {\n#>   /* multinomial-logit log-PMF\n#>    * Args:\n#>    *   y: array of integer response values\n#>    *   mu: vector of category logit probabilities\n#>    * Returns:\n#>    *   a scalar to be added to the log posterior\n#>    */\n#>    real multinomial_logit2_lpmf(int[] y, vector mu) {\n#>      return multinomial_lpmf(y | softmax(mu));\n#>    }\n#> }\n#> data {\n#>   int<lower=1> N;  // total number of observations\n#>   int<lower=2> ncat;  // number of categories\n#>   int Y[N, ncat];  // response array\n#>   int trials[N];  // number of trials\n#>   // data for group-level effects of ID 1\n#>   int<lower=1> N_1;  // number of grouping levels\n#>   int<lower=1> M_1;  // number of coefficients per level\n#>   int<lower=1> J_1[N];  // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_1_muBest_1;\n#>   // data for group-level effects of ID 2\n#>   int<lower=1> N_2;  // number of grouping levels\n#>   int<lower=1> M_2;  // number of coefficients per level\n#>   int<lower=1> J_2[N];  // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_2_muBest_1;\n#>   // data for group-level effects of ID 3\n#>   int<lower=1> N_3;  // number of grouping levels\n#>   int<lower=1> M_3;  // number of coefficients per level\n#>   int<lower=1> J_3[N];  // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_3_muNeut_1;\n#>   // data for group-level effects of ID 4\n#>   int<lower=1> N_4;  // number of grouping levels\n#>   int<lower=1> M_4;  // number of coefficients per level\n#>   int<lower=1> J_4[N];  // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_4_muNeut_1;\n#>   // data for group-level effects of ID 5\n#>   int<lower=1> N_5;  // number of grouping levels\n#>   int<lower=1> M_5;  // number of coefficients per level\n#>   int<lower=1> J_5[N];  // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_5_muNoway_1;\n#>   // data for group-level effects of ID 6\n#>   int<lower=1> N_6;  // number of grouping levels\n#>   int<lower=1> M_6;  // number of coefficients per level\n#>   int<lower=1> J_6[N];  // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_6_muNoway_1;\n#>   int prior_only;  // should the likelihood be ignored?\n#> }\n#> transformed data {\n#> }\n#> parameters {\n#>   real Intercept_muBest;  // temporary intercept for centered predictors\n#>   real Intercept_muNeut;  // temporary intercept for centered predictors\n#>   real Intercept_muNoway;  // temporary intercept for centered predictors\n#>   vector<lower=0>[M_1] sd_1;  // group-level standard deviations\n#>   vector[N_1] z_1[M_1];  // standardized group-level effects\n#>   vector<lower=0>[M_2] sd_2;  // group-level standard deviations\n#>   vector[N_2] z_2[M_2];  // standardized group-level effects\n#>   vector<lower=0>[M_3] sd_3;  // group-level standard deviations\n#>   vector[N_3] z_3[M_3];  // standardized group-level effects\n#>   vector<lower=0>[M_4] sd_4;  // group-level standard deviations\n#>   vector[N_4] z_4[M_4];  // standardized group-level effects\n#>   vector<lower=0>[M_5] sd_5;  // group-level standard deviations\n#>   vector[N_5] z_5[M_5];  // standardized group-level effects\n#>   vector<lower=0>[M_6] sd_6;  // group-level standard deviations\n#>   vector[N_6] z_6[M_6];  // standardized group-level effects\n#> }\n#> transformed parameters {\n#>   vector[N_1] r_1_muBest_1;  // actual group-level effects\n#>   vector[N_2] r_2_muBest_1;  // actual group-level effects\n#>   vector[N_3] r_3_muNeut_1;  // actual group-level effects\n#>   vector[N_4] r_4_muNeut_1;  // actual group-level effects\n#>   vector[N_5] r_5_muNoway_1;  // actual group-level effects\n#>   vector[N_6] r_6_muNoway_1;  // actual group-level effects\n#>   real lprior = 0;  // prior contributions to the log posterior\n#>   r_1_muBest_1 = (sd_1[1] * (z_1[1]));\n#>   r_2_muBest_1 = (sd_2[1] * (z_2[1]));\n#>   r_3_muNeut_1 = (sd_3[1] * (z_3[1]));\n#>   r_4_muNeut_1 = (sd_4[1] * (z_4[1]));\n#>   r_5_muNoway_1 = (sd_5[1] * (z_5[1]));\n#>   r_6_muNoway_1 = (sd_6[1] * (z_6[1]));\n#>   lprior += student_t_lpdf(Intercept_muBest | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNeut | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNoway | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#>     - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#>     - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#>     - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#>     - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_5 | 3, 0, 2.5)\n#>     - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_6 | 3, 0, 2.5)\n#>     - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] muBest = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNeut = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNoway = rep_vector(0.0, N);\n#>     // linear predictor matrix\n#>     vector[ncat] mu[N];\n#>     muBest += Intercept_muBest;\n#>     muNeut += Intercept_muNeut;\n#>     muNoway += Intercept_muNoway;\n#>     for (n in 1:N) {\n#>       // add more terms to the linear predictor\n#>       muBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n] + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\n#>     }\n#>     for (n in 1:N) {\n#>       // add more terms to the linear predictor\n#>       muNeut[n] += r_3_muNeut_1[J_3[n]] * Z_3_muNeut_1[n] + r_4_muNeut_1[J_4[n]] * Z_4_muNeut_1[n];\n#>     }\n#>     for (n in 1:N) {\n#>       // add more terms to the linear predictor\n#>       muNoway[n] += r_5_muNoway_1[J_5[n]] * Z_5_muNoway_1[n] + r_6_muNoway_1[J_6[n]] * Z_6_muNoway_1[n];\n#>     }\n#>     for (n in 1:N) {\n#>       mu[n] = transpose([0, muBest[n], muNeut[n], muNoway[n]]);\n#>     }\n#>     for (n in 1:N) {\n#>       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#>     }\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#>   target += std_normal_lpdf(z_1[1]);\n#>   target += std_normal_lpdf(z_2[1]);\n#>   target += std_normal_lpdf(z_3[1]);\n#>   target += std_normal_lpdf(z_4[1]);\n#>   target += std_normal_lpdf(z_5[1]);\n#>   target += std_normal_lpdf(z_6[1]);\n#> }\n#> generated quantities {\n#>   // actual population-level intercept\n#>   real b_muBest_Intercept = Intercept_muBest;\n#>   // actual population-level intercept\n#>   real b_muNeut_Intercept = Intercept_muNeut;\n#>   // actual population-level intercept\n#>   real b_muNoway_Intercept = Intercept_muNoway;\n#> }\n\nViendo el código en stan que genera brms utiliza parametrización con multinomial_lpmf que toma el log de la probabilidad de la multinomial y usa softmax sobre el predictor lineal. multivariate_discrete_stan\nEn la parte de functions tiene\n\nreal multinomial_logit2_lpmf(int[] y, vector mu) {\n      return multinomial_lpmf(y | softmax(mu));\n  }\nY en la de model\n   for (n in 1:N) {\n      target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n     }\nY en la parte del predictor lineal mu[n] es dónde ha ido añadiendo los group levels effects.\nPor ejemplo la parte de la edad_cat para la categoría Best está en la parte de transformed parameters dónde z_1[1] se modela como normal y sd_1 como una t de student\nr_1_muBest_1 = (sd_1[1] * (z_1[1]));\nY en la parte de model va añadiendo términos al muBest que es al final el que entra en la parte de la verosimilitud.\nmuBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n] + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\nAquí añade el efecto de la edad r_1_muBest_1[J_1[n]] lo multiplica por Z_1_mubest_1[n] que es el indicador en los datos de la matriz Z para los efectos aleatorios (todo igual a 1) y luego añade el efecto de la variable valor_cliente.\nLa verdad es que eel bloque model que genera brms es un poco complicado. Imagino que genera código optimizado. Para los que quieran verlo todo con stan directamente este libro tiene un ejemplo básico\nEn brms tenemos la función make_standata que nos genera los datos tal y como se los pasa a Stan.\n\ndatos_stan <- make_standata(formula, data = train_wider, \n              family = multinomial(),\n              prior =  priors)\n\n\nnames(datos_stan)\n#>  [1] \"N\"             \"Y\"             \"trials\"        \"ncat\"         \n#>  [5] \"K_muBest\"      \"X_muBest\"      \"Z_1_muBest_1\"  \"Z_2_muBest_1\" \n#>  [9] \"K_muNeut\"      \"X_muNeut\"      \"Z_3_muNeut_1\"  \"Z_4_muNeut_1\" \n#> [13] \"K_muNoway\"     \"X_muNoway\"     \"Z_5_muNoway_1\" \"Z_6_muNoway_1\"\n#> [17] \"J_1\"           \"J_2\"           \"J_3\"           \"J_4\"          \n#> [21] \"J_5\"           \"J_6\"           \"N_1\"           \"M_1\"          \n#> [25] \"NC_1\"          \"N_2\"           \"M_2\"           \"NC_2\"         \n#> [29] \"N_3\"           \"M_3\"           \"NC_3\"          \"N_4\"          \n#> [33] \"M_4\"           \"NC_4\"          \"N_5\"           \"M_5\"          \n#> [37] \"NC_5\"          \"N_6\"           \"M_6\"           \"NC_6\"         \n#> [41] \"prior_only\"\n\n\n# datos\ndatos_stan$N\n#> [1] 182\n\n# numero de niveles edad\ndatos_stan$N_1\n#> [1] 5\n\n# numero niveles valor_cliente\ndatos_stan$N_2\n#> [1] 10\n\nEn los J_1, J_2, está codificado a que nivel de edad y valor_cliente perteneces esa fila. J_3 y J_4 es igual a J_1 y J_2. Lo repite para cada categoría de respuesta.\n\ndatos_stan$J_1\n#>   [1] 1 2 2 3 4 2 4 5 1 5 3 3 4 2 2 2 3 4 1 3 4 4 5 1 2 2 3 4 5 1 1 2 3 3 4 1 3\n#>  [38] 4 4 1 1 1 2 5 1 2 4 4 5 1 1 2 4 4 5 5 2 3 4 5 2 2 4 4 4 5 5 3 4 1 4 1 3 4\n#>  [75] 1 1 2 2 3 4 5 4 5 1 3 3 4 5 1 1 3 4 5 1 1 3 5 1 2 3 3 1 4 5 3 1 1 3 1 1 2\n#> [112] 3 3 1 2 3 1 2 2 5 5 3 3 2 3 1 4 5 3 5 5 2 4 3 5 1 2 2 5 1 3 2 1 2 4 2 1 3\n#> [149] 4 4 1 3 4 5 3 4 5 2 4 5 3 3 2 4 5 1 4 1 2 2 1 2 3 5 5 2 5 2 3 2 5 3\n\n\ndatos_stan$J_2\n#>   [1]  1  1  1  1  1  2  2  3  3  3  3  3  3  4  5  5  5  5  6  6  6  6  7  7  7\n#>  [26]  7  7  7  8  8  8  8  8  8  8  9  9  9  9 10 10  1  1  2  2  2  2  2  4  3\n#>  [51]  3  3  3  3  4  4  4  4  4  5  5  5  5  5  5  6  6  6  6  7  7  8  8  8  9\n#>  [76]  9  9  9  9  9  1  1  2  2  2  2  3  4  4  4  4  4  5  5  5  5  6  6  6  6\n#> [101]  6  7  7  8 10  1  1  1  2  2  2  2  3  3  3  3  5  6  6  7  7  7  7  8  8\n#> [126]  9  9  1  1  2  2  2  2  5  3  4  4  4  5  5  5  6  8  8  8  9 10  1  1  1\n#> [151]  4  4  4  7  7  7  8  8  8  9  9  2  3  4  5  6  6  7  7  1  6  9  9  1  3\n#> [176]  3  6  7  4 10  8 10\n\nPero yo estoy interesado en ver 2 cosas, como de bien predice sobre test y cuál es la probabilidad de cada clase condicionada a cada perfil\nPredicción\nPodemos obtener o bien todas las estimaciones o resumirlas\n\npredicciones_test <-  posterior_predict(model_multinomial2, newdata = test_wider)\n\nAquí lo que tenemos es un array de dimensiones 12000, 180, 4 . Que se corresponde a tener las 12000 estimaciones ( 4 cadenas x 3000 muestras efectivas) , para las 180 filas del conjunto de test\n\ndim(predicciones_test)\n#> [1] 12000   180     4\n\nPor ejemplo para la fila 35 de test que sería\n\ntest_wider[1,]\n#> # A tibble: 1 × 8\n#>   tipo  valor_cliente edad_cat   Rec  Neut  Best No_way total\n#>   <fct>         <dbl> <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl>\n#> 1 C                 0 21- 40     133   318    96     71   618\n\nY las predicciones (de la 1 a la 20) de las 1200\n\npredicciones_test[1:20, 1, ]\n#>       Rec Best Neut No_way\n#>  [1,]  73  131  241    173\n#>  [2,]  72  135  242    169\n#>  [3,]  76  127  254    161\n#>  [4,]  70  155  217    176\n#>  [5,]  67  134  248    169\n#>  [6,]  71  131  243    173\n#>  [7,]  71  132  219    196\n#>  [8,]  55  139  225    199\n#>  [9,]  68  143  237    170\n#> [10,]  54  119  255    190\n#> [11,]  61  116  260    181\n#> [12,]  78  134  206    200\n#> [13,]  73  135  229    181\n#> [14,]  81  141  210    186\n#> [15,]  72  137  264    145\n#> [16,]  80  130  241    167\n#> [17,]  72  142  226    178\n#> [18,]  83  134  209    192\n#> [19,]  59  136  244    179\n#> [20,]  72  157  244    145\n\nComo ahora todo es tidy voy a usar tidybayespara tener esa predicción.\n\npredicciones_tidy <- test_wider %>% \n  add_epred_draws(model_multinomial2) \n\nY se nos ha quedado un dataset muy muy grande\n\ndim(predicciones_tidy)\n#> [1] 8640000      14\n\n\nDT::datatable(predicciones_tidy %>% \n                ungroup() %>% \n                sample_n(30) %>% \n                select(edad_cat, valor_cliente,.category, .epred))\n\n\n\n\n\n\nPero si quisiéramos pintar las probabilidades estimadas tendríamos que dividir el valor predicho de cada categoría por el total de clientes en cada fila del conjunto de datos de test. Hay una forma más sencilla construyendo un conjunto de datos que tenga todas las combinaciones de edad_cat y valor_cliente y añadiendo columna totalcon valor 1.\n\n\nfake_data <- test_wider %>% \n  tidyr::expand(edad_cat, valor_cliente) %>% \n  mutate(total = 1)\n\n\ndf_pintar <-  fake_data %>% \n  add_epred_draws(model_multinomial2) %>% \n  mutate(valor_cliente = as_factor(valor_cliente))\n\nDe esta forma, al tener total = 1, el modelo devuelve la probabilidad de cada clase, si total = 13, hubiera devuelto “el reparto” de esos 13 individuos en los 4 grupos\n\nDT::datatable(df_pintar %>% \n  sample_n(30) %>% \n  select(edad_cat, valor_cliente, .category, .epred))\n\n\n\n\n\n\nAñadir las 12000 predicciones por fila ya “sólo” nos deja unos 2 millones de filas\n\ndim(df_pintar)\n#> [1] 2400000       9\n\nPintemos\nPor ejemplo si queremos ver las estimaciones que le da según la edad podemos ver la distribución posteriori de la probabilidad de cada segmento condicionada a cada grupo de edad. Salen distribuciones con varias modas debido a la variable valor_cliente que no estamos representando\n\ndf_pintar %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n             ) +\n  geom_density_ridges(scale = 0.8, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() \n\n\n\n\n\n\n\n\nSi vemos la posteriori para los clientes de mayor valor. Se ve claramente que a menor edad mayor probabilidad de pertenecer al segmento “Best” , mientras que a mayor edad es mucho más probabilidad del segmento “No_way”.\n\ndf_pintar %>%  \n  filter(valor_cliente == 0) %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Cliente valor: 0\")\n\n\n\n\n\n\n\n\nTeniendo toda la distribución podemos ver los resultados desde otro punto de vista. Por ejemplo, ver las probabilidades para los menores de 21.\n\ndf_pintar %>%  \n  filter(edad_cat %in% c(\"<21\")) %>% \n  ggplot(aes(x=.epred, y = valor_cliente, fill = .category) \n  ) +\n  geom_density_ridges(scale = 3, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Clientes menores de 21\\n Probabilidades estimadas\")\n\n\n\n\n\n\n\n\nEn fin, que se puede hacer estadística bayesiana aún con grandes volúmenes de datos, si te conviertes en lo que mi amigo Antonio llama un “artesano del dato”.\nFeliz semana"
  },
  {
    "objectID": "to-delete.html",
    "href": "to-delete.html",
    "title": "Untitled",
    "section": "",
    "text": "println(\"hola\")"
  },
  {
    "objectID": "2021/05/21/big-data-pobres-ii-auc/index.html",
    "href": "2021/05/21/big-data-pobres-ii-auc/index.html",
    "title": "Big data para pobres II. ¿AUC?",
    "section": "",
    "text": "Bueno, pues voy a ampliar el ejemplo del último día, como es viernes, estoy cansado y me iré a tomar una birra pronto, intentaré ser breve.\nLevantamos una sesión de spark y leemos los mismos datos del otro día. Ya de paso voy a probar el operador pipe nativo en R base |>. Si tienes la nueva versión de R instalada y la versión de Rstudio preview, en global options puedes poner para que al hacer Ctrl + Shift +M aparezca el nuevo operador o el antiguo.\nDiscretizamos la edad.\nY ahora vamos a crear conjunto de train y de test\nY ahora procedemos a agregar los datos y traerlos a local. Y seguro que alguno se pregunta ¿por qué no haces el modelo en spark?. Podría hacerlo, ya he contado en este blog como hacer modelos usando sparkling water por ejemplo, pero podría querer ajustar un tipo de modelo que no esté en distribuido, no sé, un modelo mixto con glmer o con stan . De hecho es eso lo que voy a hacer, un glmer.\nTenemos 669 filas con la info de 1.45699^{6} observaciones\nAgregamos y bajamos el test\nTenemos 653 con la info de 9.71396^{5} observaciones"
  },
  {
    "objectID": "2021/05/21/big-data-pobres-ii-auc/index.html#glmer",
    "href": "2021/05/21/big-data-pobres-ii-auc/index.html#glmer",
    "title": "Big data para pobres II. ¿AUC?",
    "section": "glmer",
    "text": "glmer\nHacemos un par de modelos mixtos como en el post anterior, pero en los datos de train\n\nlibrary(lme4)\n\nmodA <- glmer(segmento == \"Best\" ~ (1 | edad_cat) + (1 | valor_cliente) + (1 | tipo),\n              data = train_local, family= binomial, weights= train_local$n)  \n\nmodB <-  glmer(segmento == \"No_way\" ~(1 | edad_cat) + (1  |valor_cliente) + (1 | tipo),\n               data = train_local, family= binomial, weights= train_local$n)  \n\nPodemos ver el modelo A por ejemplo\n\nsummary(modA)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: segmento == \"Best\" ~ (1 | edad_cat) + (1 | valor_cliente) + (1 |  \n#>     tipo)\n#>    Data: train_local\n#> Weights: train_local$n\n#> \n#>       AIC       BIC    logLik  deviance  df.resid \n#>  795102.8  795120.8 -397547.4  795094.8       665 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -71.223  -6.481  -2.028   1.796 279.606 \n#> \n#> Random effects:\n#>  Groups        Name        Variance Std.Dev.\n#>  valor_cliente (Intercept) 0.06105  0.2471  \n#>  edad_cat      (Intercept) 0.36133  0.6011  \n#>  tipo          (Intercept) 0.01304  0.1142  \n#> Number of obs: 669, groups:  valor_cliente, 10; edad_cat, 5; tipo, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -2.1089     0.2875  -7.336  2.2e-13 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nsjPlot::plot_model(modA, type = \"re\")\n#> [[1]]\n\n\n\n\n\n\n\n#> \n#> [[2]]\n\n\n\n\n\n\n\n#> \n#> [[3]]\n\n\n\n\n\n\n\n\n\nPredicción del glmer\nAhora hacemos predicción sobre el conjunto de test, que recordemos también está en formato de datos agregados.\n\ntest_local$Apredict  <- predict(modA, newdata = test_local,\n                    allow.new.levels= TRUE,\n                    type= \"response\")\n\ntest_local$Bpredict <-  predict(modB, newdata = test_local,\n                    allow.new.levels= TRUE,\n                    type= \"response\")\n\n \n\n\ntest_local |> \n  select(segmento, n, Apredict, Bpredict) |> \n  DT::datatable()\n\n\n\n\n\n\n\n\nAUC\nSi calculamos el AUC con las librerías normales no se va a tener en cuenta que tenemos datos agrupados, sino que considera cada fila como una observación. En este caso los AUC’s son como si fuera un modelo aleatorio.\n\npROC::auc(test_local$segmento==\"Best\", test_local$Apredict)\n#> Area under the curve: 0.5132\npROC::auc(test_local$segmento==\"No_way\", test_local$Bpredict)\n#> Area under the curve: 0.5108\n\nCon los datos agregados se tiene por ejemplo que si en una fila n vale 1000 y la probabilidad de A es 0.2, la estimación de gente en segmento “Best” sería de 200, y podríamos calcular un test de bondad de ajuste de la Chi de Pearson, para comparar la frecuencia observada con la esperada.\n\\[\\chi^2 = \\sum_i\\dfrac{(observada_i- estimada_i)^2}{estimada_i}\\]\n\ntest_local |> \n  filter(segmento == \"Best\" & n > 100) |> \n  select(segmento, n, Apredict) |> \n  arrange( - Apredict) |> \n  mutate(A_estimate_num = Apredict * n) |> \n  DT::datatable()\n\n\n\n\n\n\nPero seguramente, muchos están más acostumbrados a tener un AUC. Podemos tener en cuenta las frecuencias de cada fila usando la librería WeightedROC\n\nlibrary(WeightedROC)\n# requiere que la etiqueta esté en 1 y 0 \nrocA <- WeightedROC::WeightedROC(test_local$Apredict,\n                               ifelse(test_local$segmento== \"Best\",1,0),\n                               weight = test_local$n)\n\nWeightedROC::WeightedAUC(rocA)\n#> [1] 0.6424066\n\nY vemos que el AUC teniendo en cuenta los pesos ya no es tan malo (con tan pocas variables tampoco era esperable un auc de 0.83)\nY para el modeloB\n\nrocB <- WeightedROC::WeightedROC(test_local$Bpredict,\n                               ifelse(test_local$segmento == \"No_way\",1,0),\n                               weight = test_local$n)\n\nWeightedROC::WeightedAUC(rocB)\n#> [1] 0.8503445\n\nY vemos que para el segmento “No_way” nuestro modelo mixto no está mal del todo.\nEl próximo día, quiza lo haga con Stan usando brms que la sintaxis es bastante sencilla."
  },
  {
    "objectID": "2021/05/14/cosas-viejunas-o-big-data-para-pobres/index.html",
    "href": "2021/05/14/cosas-viejunas-o-big-data-para-pobres/index.html",
    "title": "Cosas viejunas. O big data para pobres",
    "section": "",
    "text": "Antes, cuándo no había tanta capacidad de cálculo ni esa obsesión por cuántas más variables mejor, se trabajaban los datos, se seleccionaban las variables, se muestreaba o se trabajaba con los datos agregados. De esto último sabe bastante el gran Emilio Torres, autor entre otros del paquete xkcd\nTrabajar con datos agregados y con sus frecuencias es lo que hemos hecho toda la vida. Veamos un ejemplo tonto.\n\nejemplo <- data.frame(x1 = rep(1:3, 5),\n                      x2 = rep(1:3, length.out=5))\nejemplo\n#>    x1 x2\n#> 1   1  1\n#> 2   2  2\n#> 3   3  3\n#> 4   1  1\n#> 5   2  2\n#> 6   3  1\n#> 7   1  2\n#> 8   2  3\n#> 9   3  1\n#> 10  1  2\n#> 11  2  1\n#> 12  3  2\n#> 13  1  3\n#> 14  2  1\n#> 15  3  2\n\nTenemos 15 filas, pero en realidad podemos condensar la información en 9, que serían las combinaciones únicas\n\nlibrary(tidyverse)\ncondensado <- ejemplo %>% \n  group_by(x1, x2 ) %>% \n  count()\n\ncondensado\n#> # A tibble: 9 × 3\n#> # Groups:   x1, x2 [9]\n#>      x1    x2     n\n#>   <int> <int> <int>\n#> 1     1     1     2\n#> 2     1     2     2\n#> 3     1     3     1\n#> 4     2     1     2\n#> 5     2     2     2\n#> 6     2     3     1\n#> 7     3     1     2\n#> 8     3     2     2\n#> 9     3     3     1\n\nY es exactamente la misma info, pero en menos filas. Obvio.\nVeamos un ejemplo más claro con datos simulados que tengo en parquet.\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", spark_home = \"~/spark/spark-3.0.0-bin-hadoop2.7\")\n\ntmp <- sc %>% \n  spark_read_parquet(path = here::here(\"data/bd_pobres.parquet\" ))\n\nEs un conjnto de datos con 4 variables indicando valor del cliente (1 es más valor), edad, segmento y tipo de equipamiento\n\ntmp\n#> # Source: spark<bd_pobres_ded5dda4_6f30_45a0_ad4a_d3bc4eb0c655> [?? x 4]\n#>    valor_cliente  edad segmento tipo \n#>            <dbl> <dbl> <chr>    <chr>\n#>  1             3    79 No_way   B    \n#>  2             3    79 No_way   B    \n#>  3             3    79 No_way   B    \n#>  4             3    79 No_way   B    \n#>  5             3    79 No_way   B    \n#>  6             3    79 No_way   B    \n#>  7             3    79 No_way   B    \n#>  8             3    79 No_way   B    \n#>  9             3    79 No_way   B    \n#> 10             3    79 No_way   B    \n#> # … with more rows\n\nTenemos más de 2 millones de filas. Es sólo un ejemplo, esto en un pc moderno no es un problema, pero podrían ser 200 millones\n\ntmp %>% \n  count()\n#> # Source: spark<?> [?? x 1]\n#>         n\n#>     <dbl>\n#> 1 2428386\n\nSi quisiéramos por ejemplo modelar el segmento en función del resto de variables, podríamos hacer un árbol de decisión en spark o con h2o o con otra cosa moderna. Sin embargo también podríamos pensar en ver cuántos clientes de cada tipo hay y trabajar con la tabla que tenga las distintas combinaciones y una columna que indica las veces que se repite. También podríamos trabajar con una muestra de los datos.\nY vemos que tenemos combinaciones desde 13379 casos hasta combinaciones con 1 solo caso, La combinación segmento = “No_way”, tipo = “B”, valor cliente= 8 y edad = 70 solo tiene un caso.\n\ndf_info_completa <-\n  tmp %>%\n  group_by(segmento,\n           tipo,\n           valor_cliente,\n           edad) %>%\n  count()  %>%\n  ungroup\ndf_info_completa %>% arrange(desc(n))\n#> # Source:     spark<?> [?? x 5]\n#> # Ordered by: desc(n)\n#>    segmento tipo  valor_cliente  edad     n\n#>    <chr>    <chr>         <dbl> <dbl> <dbl>\n#>  1 No_way   SF                2    81 13379\n#>  2 No_way   SF                2    85 12356\n#>  3 No_way   SF                2    84 11657\n#>  4 No_way   SF                2    86 11274\n#>  5 No_way   SF                2    78 10943\n#>  6 No_way   SF                2    87 10232\n#>  7 No_way   SF                2    83 10158\n#>  8 No_way   SF                2    79 10021\n#>  9 No_way   SF                2    77  9869\n#> 10 No_way   SF                2    80  9738\n#> # … with more rows\ndf_info_completa %>% arrange(n)\n#> # Source:     spark<?> [?? x 5]\n#> # Ordered by: n\n#>    segmento tipo  valor_cliente  edad     n\n#>    <chr>    <chr>         <dbl> <dbl> <dbl>\n#>  1 Rec      SF                7    72     1\n#>  2 Rec      SF                0    49     1\n#>  3 Rec      C                 3   107     1\n#>  4 Neut     C                 4   120     1\n#>  5 Rec      B                 6    63     1\n#>  6 Rec      SM                4    24     1\n#>  7 Neut     SF                0    82     1\n#>  8 Rec      B                 8    65     1\n#>  9 No_way   C                 4   105     1\n#> 10 Rec      SM                6    27     1\n#> # … with more rows\n\nY tendríamos un total de 9598 filas. Oye, no está mal pasar de una tabla de 2 millones a una de 9598 representando exactamente la misma información.\n\ndf_info_completa %>% count()\n#> # Source: spark<?> [?? x 1]\n#>       n\n#>   <dbl>\n#> 1  9598\n\nPues ya podríamos traernos la información a local y trabajar con ella\n\ndf1 <-  collect(df_info_completa)\nDT::datatable(df1)\n\n\n\n\n\n\nY podríamos hacer nuestra segmentación utilizando la variable n como variable de frecuencia de los casos\nHagamos un segmentación sencilla\n\nlibrary(party)\n# convertimos a factor las variables de tipo character\n\n# The old school\n# df1[sapply(df1, is.character)] <- lapply(df1[sapply(df1, is.character)], \n#                                                            as.factor)\n\n# Tidyverse\n\ndf1 <- df1 %>% \n  mutate(across(where(is.character),\n                as.factor))  \n\ndf1 \n#> # A tibble: 9,598 × 5\n#>    segmento tipo  valor_cliente  edad     n\n#>    <fct>    <fct>         <dbl> <dbl> <dbl>\n#>  1 No_way   B                 3    79  1612\n#>  2 Best     SF                3    62    52\n#>  3 No_way   SF                2    29   139\n#>  4 Rec      C                 1    43    67\n#>  5 Neut     C                 0    25    16\n#>  6 Neut     C                 4   120     1\n#>  7 No_way   SM                2    24   800\n#>  8 No_way   SF                5    91     8\n#>  9 Neut     SM                1    68    51\n#> 10 No_way   SF                5    87    11\n#> # … with 9,588 more rows\n\narbol <- ctree(segmento ~ edad + valor_cliente + tipo,\n               data = df1,\n               weights = df1$n ,  #\n               controls = ctree_control(maxdepth = 3))\narbol\n#> \n#>   Conditional inference tree with 8 terminal nodes\n#> \n#> Response:  segmento \n#> Inputs:  edad, valor_cliente, tipo \n#> Number of observations:  9598 \n#> \n#> 1) tipo == {C}; criterion = 1, statistic = 848548.534\n#>   2) edad <= 62; criterion = 1, statistic = 53861.863\n#>     3) valor_cliente <= 4; criterion = 1, statistic = 25453.701\n#>       4)*  weights = 614820 \n#>     3) valor_cliente > 4\n#>       5)*  weights = 411834 \n#>   2) edad > 62\n#>     6) valor_cliente <= 4; criterion = 1, statistic = 13091.4\n#>       7)*  weights = 183723 \n#>     6) valor_cliente > 4\n#>       8)*  weights = 77754 \n#> 1) tipo == {B, SF, SM}\n#>   9) edad <= 61; criterion = 1, statistic = 144732.687\n#>     10) tipo == {B, SF}; criterion = 1, statistic = 96052.927\n#>       11)*  weights = 107390 \n#>     10) tipo == {SM}\n#>       12)*  weights = 501076 \n#>   9) edad > 61\n#>     13) valor_cliente <= 2; criterion = 1, statistic = 32631.891\n#>       14)*  weights = 397895 \n#>     13) valor_cliente > 2\n#>       15)*  weights = 133894\n\nPara pintar el árbol\n\n# una función que tengo en otro lado para modificar un\n# poco como lo pinta ctree\n\nsource(\"utils_plot_ctree.R\")\nplot(arbol,terminal_panel=altbp(arbol,ylines=1, gap=0,rot= -60))\n\n\n\n\n\n\n\n\nY voilá. ya hemos hecho un modelo sobre los más de 2 millones de clientes, utilizando toda la info pero en menos de 10 mil filas. Y vemos como el tipo de equipamiento es la variable más importante, seguida de la edad. La interpretación de los segmentos la dejamos para otro día.\nTambién podríamos considerar la variable valor cliente como categórica o discretizar la edad.\n\ndf2 <-  df1 %>% \n  mutate(\n         valor_cliente = as_factor(valor_cliente),\n         edad_cat = as_factor(case_when(\n           edad <= 20 ~ \"<21\",\n           edad <= 40 ~ \"21- 40\",\n           edad <= 50 ~ \"41-50\", \n           edad <= 60 ~ \"40-60\",\n           edad > 60 ~ \">60\"\n         ))\n         )\n\narbol2 <- ctree(segmento ~ edad_cat + valor_cliente + tipo,\n               data = df2,\n               weights = df2$n ,  #\n               controls = ctree_control(maxdepth = 3))\n\nplot(arbol2,terminal_panel=altbp(arbol,ylines=1, gap=0,rot= -60))\n\n\n\n\n\n\n\n\nYa que tenemos estos datos así, quizá estemos interesados en modelar la probabilidad de un segmento, quizá incluso usando Stan o lme4.\nCon lme4 sería algo así.\n\nlibrary(lme4)\n\nmodA <- glmer(segmento == \"Best\" ~ (1 | edad_cat) + (1|valor_cliente) + (1 | tipo),\n              data = df2, family= binomial, weights= df2$n)  \n\nmodB <-  glmer(segmento == \"No_way\" ~(1 | edad_cat) + (1|valor_cliente) + (1 | tipo),\n               data = df2, family= binomial, weights= df2$n)  \n\nY ver el modelo y efectos aleatorios\n\nsummary(modA) \n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: binomial  ( logit )\n#> Formula: segmento == \"Best\" ~ (1 | edad_cat) + (1 | valor_cliente) + (1 |  \n#>     tipo)\n#>    Data: df2\n#> Weights: df2$n\n#> \n#>       AIC       BIC    logLik  deviance  df.resid \n#> 1323329.0 1323357.6 -661660.5 1323321.0      9594 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -25.667  -2.415  -0.814   2.158 112.275 \n#> \n#> Random effects:\n#>  Groups        Name        Variance Std.Dev.\n#>  valor_cliente (Intercept) 0.06518  0.2553  \n#>  edad_cat      (Intercept) 0.38928  0.6239  \n#>  tipo          (Intercept) 0.01492  0.1222  \n#> Number of obs: 9598, groups:  valor_cliente, 10; edad_cat, 5; tipo, 4\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  -2.0959     0.2964   -7.07 1.55e-12 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nranef(modA)\n#> $valor_cliente\n#>   (Intercept)\n#> 0  0.40814923\n#> 1  0.08097686\n#> 2 -0.25006629\n#> 3 -0.22331665\n#> 4 -0.30648806\n#> 5 -0.15176045\n#> 6  0.05926187\n#> 7  0.20797052\n#> 8  0.26778715\n#> 9 -0.09214611\n#> \n#> $edad_cat\n#>         (Intercept)\n#> >60    -0.904158878\n#> 21- 40  0.307419213\n#> 41-50  -0.001556964\n#> 40-60  -0.334085973\n#> <21     0.934580924\n#> \n#> $tipo\n#>    (Intercept)\n#> B  -0.15014784\n#> C   0.14785785\n#> SF -0.01783839\n#> SM  0.02021266\n#> \n#> with conditional variances for \"valor_cliente\" \"edad_cat\" \"tipo\"\n\n\nsjPlot::plot_model(modA, type = \"re\")\n#> [[1]]\n\n\n\n\n\n\n\n#> \n#> [[2]]\n\n\n\n\n\n\n\n#> \n#> [[3]]\n\n\n\n\n\n\n\n\nY hasta aquí, el próximo post en vez de trabajar con toda la información partiremos los datos en train y test antes de agregar y traer a local. Veremos el ajuste con un modelo mixto con glmer y calcularemos los AUC’s, pero teniendo en cuenta que tenemos los datos con variable de frecuencia."
  },
  {
    "objectID": "2022/12/04/cosas-para-dejar-spss/index.html",
    "href": "2022/12/04/cosas-para-dejar-spss/index.html",
    "title": "Consejos para dejar spss",
    "section": "",
    "text": "Estuve del 23 al 25 de Noviembre en las Jornadas de R en Córdoba, y como siempre, me lo pasé estupendamente. Resulta que a la misma vez se celebraba el congreso andaluz de sociología, y dado mi pasado trabajando con sociólogos pues me pasé el viernes a última hora a ver a los colegas.\nMe sigue sorprendiendo el uso que se sigue haciendo de spss en este ámbito, un software privativo y no barato precisamente. Tengo que decir que para aquellos que son amigos de los GUIs, en R tenemos radiant, BlueSky, jamovi, etc, aquí os dejo una comparación con algunos de ellos.\nPor otro lado, siempre me comentan que spss se usa mucho para hacer tabulaciones básicas, cosas como tablas de frecuencias de variables , tanto univariantes como bivariantes. En R tenemos muchas formas de poner tablas bonitas usando librerías como DT, reactable, formattable, gt,flextable o otras más, veáse esto por ejemplo.\nYo voy a poner un ejemplo del uso de librerías de Daniel strengejacke que son unas de mis favoritas a la hora de extraer por ejemplo los efectos marginales de modelos lineales (incluyendo los mixtos) y que tiene algunas utilidades para hacer tablas de frecuencias y que queden bastante bien."
  },
  {
    "objectID": "2022/12/04/cosas-para-dejar-spss/index.html#sjplots-and-tables",
    "href": "2022/12/04/cosas-para-dejar-spss/index.html#sjplots-and-tables",
    "title": "Consejos para dejar spss",
    "section": "Sjplots and tables",
    "text": "Sjplots and tables\n\nlibrary(tidyverse)\nlibrary(sjlabelled)\nlibrary(sjPlot)\nlibrary(sjmisc)\n\nbar_mayo_2022 <-  haven::read_sav(here::here(\"data/3363.sav\"))\n\nplot_frq(bar_mayo_2022$CCAA, weight.by = bar_mayo_2022$PESO)\n\n\n\n\n\n\n\n\n\nTablas de frecuencias\nFormato texto\n\nfrq(bar_mayo_2022$CCAA, out = \"txt\")\n#> Comunidad autónoma (x) <numeric> \n#> # total N=3865 valid N=3865 mean=6.94 sd=5.27\n#> \n#> Value |                        Label |    N | Raw % | Valid % | Cum. %\n#> ----------------------------------------------------------------------\n#>     1 |                    Andalucía | 1293 | 33.45 |   33.45 |  33.45\n#>     2 |                       Aragón |   96 |  2.48 |    2.48 |  35.94\n#>     3 |     Asturias (Principado de) |  101 |  2.61 |    2.61 |  38.55\n#>     4 |              Balears (Illes) |   94 |  2.43 |    2.43 |  40.98\n#>     5 |                     Canarias |  116 |  3.00 |    3.00 |  43.98\n#>     6 |                    Cantabria |   93 |  2.41 |    2.41 |  46.39\n#>     7 |           Castilla-La Mancha |  123 |  3.18 |    3.18 |  49.57\n#>     8 |              Castilla y León |  163 |  4.22 |    4.22 |  53.79\n#>     9 |                     Cataluña |  423 | 10.94 |   10.94 |  64.73\n#>    10 |         Comunitat Valenciana |  284 |  7.35 |    7.35 |  72.08\n#>    11 |                  Extremadura |   93 |  2.41 |    2.41 |  74.49\n#>    12 |                      Galicia |  186 |  4.81 |    4.81 |  79.30\n#>    13 |        Madrid (Comunidad de) |  392 | 10.14 |   10.14 |  89.44\n#>    14 |           Murcia (Región de) |   98 |  2.54 |    2.54 |  91.98\n#>    15 | Navarra (Comunidad Foral de) |   87 |  2.25 |    2.25 |  94.23\n#>    16 |                   País Vasco |  125 |  3.23 |    3.23 |  97.46\n#>    17 |                   Rioja (La) |   79 |  2.04 |    2.04 |  99.51\n#>    18 |   Ceuta (Ciudad Autónoma de) |   10 |  0.26 |    0.26 |  99.77\n#>    19 | Melilla (Ciudad Autónoma de) |    9 |  0.23 |    0.23 | 100.00\n#>  <NA> |                         <NA> |    0 |  0.00 |    <NA> |   <NA>\n\nAñadiendo ponderación y que se vea en html\n\n\nfrq(bar_mayo_2022$CCAA, weights = bar_mayo_2022$PESO, out = \"viewer\")\n\n\n\nComunidad autónoma (xw) <numeric>\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n1\nAndalucía\n700\n18.11\n18.11\n18.11\n\n\n2\nAragón\n109\n2.82\n2.82\n20.93\n\n\n3\nAsturias (Principado de)\n94\n2.43\n2.43\n23.36\n\n\n4\nBalears (Illes)\n86\n2.23\n2.23\n25.59\n\n\n5\nCanarias\n174\n4.50\n4.50\n30.09\n\n\n6\nCantabria\n52\n1.35\n1.35\n31.44\n\n\n7\nCastilla-La Mancha\n171\n4.42\n4.42\n35.86\n\n\n8\nCastilla y León\n216\n5.59\n5.59\n41.45\n\n\n9\nCataluña\n595\n15.39\n15.39\n56.84\n\n\n10\nComunitat Valenciana\n393\n10.17\n10.17\n67.01\n\n\n11\nExtremadura\n95\n2.46\n2.46\n69.47\n\n\n12\nGalicia\n247\n6.39\n6.39\n75.86\n\n\n13\nMadrid (Comunidad de)\n534\n13.82\n13.82\n89.68\n\n\n14\nMurcia (Región de)\n116\n3.00\n3.00\n92.68\n\n\n15\nNavarra (Comunidad Foral de)\n54\n1.40\n1.40\n94.08\n\n\n16\nPaís Vasco\n190\n4.92\n4.92\n98.99\n\n\n17\nRioja (La)\n26\n0.67\n0.67\n99.66\n\n\n18\nCeuta (Ciudad Autónoma de)\n7\n0.18\n0.18\n99.84\n\n\n19\nMelilla (Ciudad Autónoma de)\n6\n0.16\n0.16\n100.00\n\n\nNA\nNA\n0\n0.00\nNA\nNA\n\n\ntotal N=3865 · valid N=3865 · x̄=8.29 · σ=4.73\n\n\n\n\n\n\nQue agrupe automáticamente una variable continua\n\nfrq(bar_mayo_2022, EDAD, auto.grp = 5, weights = PESO, out = \"viewer\")\n\n\n\nEdad de la persona entrevistada (EDAD) <numeric>\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n1\n18-33\n746\n19.30\n19.30\n19.30\n\n\n2\n34-49\n1058\n27.37\n27.37\n46.66\n\n\n3\n50-65\n1181\n30.55\n30.55\n77.21\n\n\n4\n66-81\n794\n20.54\n20.54\n97.75\n\n\n5\n82-97\n85\n2.20\n2.20\n99.95\n\n\n6\n98-113\n2\n0.05\n0.05\n100.00\n\n\nNA\nNA\n0\n0.00\nNA\nNA\n\n\ntotal N=3866 · valid N=3866 · x̄=50.72 · σ=17.05\n\n\n\n\n\n\nque ordene en orden descendente\n\nfrq(bar_mayo_2022, EDAD, auto.grp = 7, sort.frq = \"desc\", \n    title = str_glue(\"{attributes(bar_mayo_2022$EDAD)$label} AGRUPADA\" ), \n    weights = PESO, out = \"viewer\")\n\n\n\nEdad de la persona entrevistada AGRUPADA\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n3\n42-53\n946\n24.47\n24.47\n24.47\n\n\n4\n54-65\n846\n21.88\n21.88\n46.35\n\n\n5\n66-77\n693\n17.93\n17.93\n64.28\n\n\n2\n30-41\n666\n17.23\n17.23\n81.51\n\n\n1\n18-29\n527\n13.63\n13.63\n95.14\n\n\n6\n78-89\n176\n4.55\n4.55\n99.69\n\n\n7\n90-101\n12\n0.31\n0.31\n100.00\n\n\nNA\nNA\n0\n0.00\nNA\nNA\n\n\ntotal N=3866 · valid N=3866 · x̄=50.72 · σ=17.05\n\n\n\n\n\n\n\n\n(edad_media <- bar_mayo_2022 %>% \n    group_by(CCAA = as_factor(CCAA) ) %>% \n    summarise(\n        edad_media = weighted.mean(EDAD, PESO) %>% round(2)\n        )\n ) \n#> # A tibble: 19 × 2\n#>    CCAA  edad_media\n#>    <fct>      <dbl>\n#>  1 1           52.0\n#>  2 2           54.6\n#>  3 3           52.8\n#>  4 4           51.5\n#>  5 5           49.3\n#>  6 6           52.7\n#>  7 7           48.6\n#>  8 8           52.4\n#>  9 9           49.8\n#> 10 10          49.2\n#> 11 11          51.5\n#> 12 12          51.4\n#> 13 13          48.6\n#> 14 14          49.7\n#> 15 15          54.8\n#> 16 16          52.5\n#> 17 17          53.8\n#> 18 18          53.9\n#> 19 19          52.4\n\n\n\ntab_xtab(bar_mayo_2022$CCAA, bar_mayo_2022$P1, weight.by = bar_mayo_2022$PESO)\n\n\n\n \n Comunidad autónoma\n Grado depreocupación ante lasituación delcoronavirus COVID-19\n Total\n \n \n\n Mucho\n Bastante\n (NO LEER) Regular\n Poco\n Nada\n N.S.\n N.C.\n \n \n \nAndalucía\n222\n276\n18\n141\n35\n2\n7\n701 \n\n \n \nAragón\n20\n49\n1\n31\n6\n1\n1\n109 \n\n \n \nAsturias (Principadode)\n18\n42\n1\n27\n4\n0\n2\n94 \n\n \n \nBalears (Illes)\n15\n29\n2\n28\n8\n0\n5\n87 \n\n \n \nCanarias\n51\n63\n0\n44\n13\n2\n1\n174 \n\n \n \nCantabria\n14\n24\n1\n11\n2\n0\n1\n53 \n\n \n \nCastilla-La Mancha\n34\n73\n4\n46\n13\n2\n0\n172 \n\n \n \nCastilla y León\n70\n88\n3\n43\n9\n3\n1\n217 \n\n \n \nCataluña\n130\n246\n11\n152\n47\n0\n9\n595 \n\n \n \nComunitat Valenciana\n82\n175\n8\n97\n28\n1\n2\n393 \n\n \n \nExtremadura\n27\n38\n4\n21\n3\n1\n1\n95 \n\n \n \nGalicia\n60\n103\n5\n65\n12\n1\n0\n246 \n\n \n \nMadrid (Comunidadde)\n126\n222\n12\n145\n28\n0\n1\n534 \n\n \n \nMurcia (Región de)\n30\n52\n3\n26\n4\n1\n0\n116 \n\n \n \nNavarra (ComunidadForal de)\n11\n24\n0\n17\n2\n0\n0\n54 \n\n \n \nPaís Vasco\n48\n65\n10\n49\n17\n0\n2\n191 \n\n \n \nRioja (La)\n8\n10\n1\n7\n0\n0\n0\n26 \n\n \n \nCeuta (CiudadAutónoma de)\n3\n3\n0\n1\n0\n0\n0\n7 \n\n \n \nMelilla (CiudadAutónoma de)\n4\n1\n0\n1\n1\n0\n0\n7 \n\n \n \nTotal\n973\n1583\n84\n952\n232\n14\n33\n3871 \n\nχ2=160.797 · df=108 · Cramer's V=0.083 · Fisher's p=0.000 \n\n \n\n\n\n\nvar_names_long <-  lapply(bar_mayo_2022, function(x) attributes(x)$label)\n\nhead(var_names_long, 20)\n#> $ESTU\n#> [1] \"Código del estudio\"\n#> \n#> $REGISTRO\n#> [1] \"Número de registro\"\n#> \n#> $CUES\n#> [1] \"Cuestionario\"\n#> \n#> $CCAA\n#> [1] \"Comunidad autónoma\"\n#> \n#> $PROV\n#> [1] \"Provincia\"\n#> \n#> $MUN\n#> [1] \"Municipio\"\n#> \n#> $CAPITAL\n#> [1] \"Capital\"\n#> \n#> $TAMUNI\n#> [1] \"Tamaño de municipio\"\n#> \n#> $ENTREV\n#> [1] \"Entrevistador/a\"\n#> \n#> $TIPO_TEL\n#> [1] \"Tipo de teléfono\"\n#> \n#> $SEXO\n#> [1] \"Sexo de la persona entrevistada\"\n#> \n#> $EDAD\n#> [1] \"Edad de la persona entrevistada\"\n#> \n#> $P0\n#> [1] \"Nacionalidad de la persona entrevistada\"\n#> \n#> $P1\n#> [1] \"Grado de preocupación ante la situación del coronavirus COVID-19\"\n#> \n#> $P2\n#> [1] \"Efectos de la crisis del COVID-19 que más preocupan personalmente\"\n#> \n#> $P3\n#> [1] \"Valoración sobre si ha pasado lo peor o no de la situación sanitaria generada por el coronavirus\"\n#> \n#> $P4\n#> [1] \"Grado de conocimiento por la invasión de Rusia a Ucrania\"\n#> \n#> $P5\n#> [1] \"Grado de preocupación por la invasión de Rusia a Ucrania\"\n#> \n#> $P6\n#> [1] \"Posibilidad de una invasión de Rusia a países del Este de Europa (su antigua área de influencia)\"\n#> \n#> $P7_1\n#> [1] \"Hay que enviar ayuda humanitaria a los/as ucranianos/as\"\n\nTabla cruzada por Comunidad autónoma y p1 a p6\n\n\n(var_to_cruce <-  paste0(\"P\", 1:6))\n#> [1] \"P1\" \"P2\" \"P3\" \"P4\" \"P5\" \"P6\"\n\n\n\n\ncontingency_table_print <-\n    function(df, var_fila, var_columna, var_peso,\n             show.cell.prc = FALSE,\n             show.row.prc  = FALSE,\n             show.col.prc  = FALSE\n             ) {\n             res <- tab_xtab(\n                 var.row = df[[var_fila]],\n                 var.col = df[[var_columna]],\n                 weight.by = df[[var_peso]], \n                 emph.total = TRUE,\n                 show.cell.prc = show.cell.prc,\n                 show.row.prc = show.row.prc, \n                 show.col.prc = show.col.prc,\n                 title = str_glue(\n                     \"Pregunta: {var_columna}  \",\n                     \" Por {var_fila}\")\n             )\n             return(res$knitr)\n    }\n\nCon porcentajes\n\n\ncruces <- map(var_to_cruce,\n     function(x)\n  \n             contingency_table_print(bar_mayo_2022, \"CCAA\", x, \"PESO\",\n                                     show.row.prc = TRUE,\n                                     show.col.prc = TRUE)\n     )\n\n\nknitr::raw_html(map_chr(cruces, paste))\n\n\n\nPregunta: P1   Por CCAA\n \n Comunidad autónoma\n Grado depreocupación ante lasituación delcoronavirus COVID-19\n Total\n \n \n\n Mucho\n Bastante\n (NO LEER) Regular\n Poco\n Nada\n N.S.\n N.C.\n \n \n \nAndalucía\n22231.7 %22.8 %\n27639.4 %17.4 %\n182.6 %21.4 %\n14120.1 %14.8 %\n355 %15.1 %\n20.3 %14.3 %\n71 %21.2 %\n701100 %18.1 % \n\n \n \nAragón\n2018.3 %2.1 %\n4945 %3.1 %\n10.9 %1.2 %\n3128.4 %3.3 %\n65.5 %2.6 %\n10.9 %7.1 %\n10.9 %3 %\n109100 %2.8 % \n\n \n \nAsturias (Principadode)\n1819.1 %1.8 %\n4244.7 %2.7 %\n11.1 %1.2 %\n2728.7 %2.8 %\n44.3 %1.7 %\n00 %0 %\n22.1 %6.1 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n1517.2 %1.5 %\n2933.3 %1.8 %\n22.3 %2.4 %\n2832.2 %2.9 %\n89.2 %3.4 %\n00 %0 %\n55.7 %15.2 %\n87100 %2.2 % \n\n \n \nCanarias\n5129.3 %5.2 %\n6336.2 %4 %\n00 %0 %\n4425.3 %4.6 %\n137.5 %5.6 %\n21.1 %14.3 %\n10.6 %3 %\n174100 %4.5 % \n\n \n \nCantabria\n1426.4 %1.4 %\n2445.3 %1.5 %\n11.9 %1.2 %\n1120.8 %1.2 %\n23.8 %0.9 %\n00 %0 %\n11.9 %3 %\n53100 %1.4 % \n\n \n \nCastilla-La Mancha\n3419.8 %3.5 %\n7342.4 %4.6 %\n42.3 %4.8 %\n4626.7 %4.8 %\n137.6 %5.6 %\n21.2 %14.3 %\n00 %0 %\n172100 %4.4 % \n\n \n \nCastilla y León\n7032.3 %7.2 %\n8840.6 %5.6 %\n31.4 %3.6 %\n4319.8 %4.5 %\n94.1 %3.9 %\n31.4 %21.4 %\n10.5 %3 %\n217100 %5.6 % \n\n \n \nCataluña\n13021.8 %13.4 %\n24641.3 %15.5 %\n111.8 %13.1 %\n15225.5 %16 %\n477.9 %20.3 %\n00 %0 %\n91.5 %27.3 %\n595100 %15.4 % \n\n \n \nComunitat Valenciana\n8220.9 %8.4 %\n17544.5 %11.1 %\n82 %9.5 %\n9724.7 %10.2 %\n287.1 %12.1 %\n10.3 %7.1 %\n20.5 %6.1 %\n393100 %10.2 % \n\n \n \nExtremadura\n2728.4 %2.8 %\n3840 %2.4 %\n44.2 %4.8 %\n2122.1 %2.2 %\n33.2 %1.3 %\n11.1 %7.1 %\n11.1 %3 %\n95100 %2.5 % \n\n \n \nGalicia\n6024.4 %6.2 %\n10341.9 %6.5 %\n52 %6 %\n6526.4 %6.8 %\n124.9 %5.2 %\n10.4 %7.1 %\n00 %0 %\n246100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n12623.6 %12.9 %\n22241.6 %14 %\n122.2 %14.3 %\n14527.2 %15.2 %\n285.2 %12.1 %\n00 %0 %\n10.2 %3 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n3025.9 %3.1 %\n5244.8 %3.3 %\n32.6 %3.6 %\n2622.4 %2.7 %\n43.4 %1.7 %\n10.9 %7.1 %\n00 %0 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1120.4 %1.1 %\n2444.4 %1.5 %\n00 %0 %\n1731.5 %1.8 %\n23.7 %0.9 %\n00 %0 %\n00 %0 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n4825.1 %4.9 %\n6534 %4.1 %\n105.2 %11.9 %\n4925.7 %5.1 %\n178.9 %7.3 %\n00 %0 %\n21 %6.1 %\n191100 %4.9 % \n\n \n \nRioja (La)\n830.8 %0.8 %\n1038.5 %0.6 %\n13.8 %1.2 %\n726.9 %0.7 %\n00 %0 %\n00 %0 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n342.9 %0.3 %\n342.9 %0.2 %\n00 %0 %\n114.3 %0.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n457.1 %0.4 %\n114.3 %0.1 %\n00 %0 %\n114.3 %0.1 %\n114.3 %0.4 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nTotal\n97325.1 %100 %\n158340.9 %100 %\n842.2 %100 %\n95224.6 %100 %\n2326 %100 %\n140.4 %100 %\n330.9 %100 %\n3871100 %100 % \n\nχ2=160.797 · df=108 · Cramer's V=0.083 · Fisher's p=0.000 \n\n \n\n\nPregunta: P2   Por CCAA\n \n Comunidad autónoma\n Efectos de la crisisdel COVID-19 que máspreocupanpersonalmente\n Total\n \n \n\n Los efectos sobre lasalud\n Los efectos sobre laeconomía y el empleo\n (NO LEER) Ambos porigual\n (NO LEER) Ni unos niotros\n N.S.\n N.C.\n \n \n \nAndalucía\n20028.6 %19.2 %\n33347.6 %16.9 %\n15922.7 %20.2 %\n30.4 %9.4 %\n10.1 %16.7 %\n30.4 %15.8 %\n699100 %18.1 % \n\n \n \nAragón\n2926.9 %2.8 %\n5853.7 %2.9 %\n2018.5 %2.5 %\n10.9 %3.1 %\n00 %0 %\n00 %0 %\n108100 %2.8 % \n\n \n \nAsturias (Principadode)\n2931.2 %2.8 %\n4144.1 %2.1 %\n2223.7 %2.8 %\n11.1 %3.1 %\n00 %0 %\n00 %0 %\n93100 %2.4 % \n\n \n \nBalears (Illes)\n1618.6 %1.5 %\n4552.3 %2.3 %\n2124.4 %2.7 %\n22.3 %6.2 %\n00 %0 %\n22.3 %10.5 %\n86100 %2.2 % \n\n \n \nCanarias\n5431 %5.2 %\n8649.4 %4.4 %\n2614.9 %3.3 %\n31.7 %9.4 %\n21.1 %33.3 %\n31.7 %15.8 %\n174100 %4.5 % \n\n \n \nCantabria\n1325.5 %1.3 %\n2854.9 %1.4 %\n1019.6 %1.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n51100 %1.3 % \n\n \n \nCastilla-La Mancha\n4224.7 %4 %\n10360.6 %5.2 %\n2112.4 %2.7 %\n31.8 %9.4 %\n00 %0 %\n10.6 %5.3 %\n170100 %4.4 % \n\n \n \nCastilla y León\n4922.8 %4.7 %\n11453 %5.8 %\n4621.4 %5.8 %\n52.3 %15.6 %\n10.5 %16.7 %\n00 %0 %\n215100 %5.6 % \n\n \n \nCataluña\n14824.9 %14.2 %\n31853.5 %16.1 %\n12320.7 %15.6 %\n40.7 %12.5 %\n00 %0 %\n10.2 %5.3 %\n594100 %15.4 % \n\n \n \nComunitat Valenciana\n9423.9 %9 %\n22156.2 %11.2 %\n7118.1 %9 %\n51.3 %15.6 %\n00 %0 %\n20.5 %10.5 %\n393100 %10.2 % \n\n \n \nExtremadura\n3334.7 %3.2 %\n4244.2 %2.1 %\n2021.1 %2.5 %\n00 %0 %\n00 %0 %\n00 %0 %\n95100 %2.5 % \n\n \n \nGalicia\n7530.4 %7.2 %\n11847.8 %6 %\n5120.6 %6.5 %\n00 %0 %\n00 %0 %\n31.2 %15.8 %\n247100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n15228.5 %14.6 %\n27551.6 %13.9 %\n9918.6 %12.6 %\n40.8 %12.5 %\n20.4 %33.3 %\n10.2 %5.3 %\n533100 %13.8 % \n\n \n \nMurcia (Región de)\n3631 %3.5 %\n5144 %2.6 %\n2824.1 %3.6 %\n00 %0 %\n00 %0 %\n10.9 %5.3 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1425.9 %1.3 %\n2750 %1.4 %\n1222.2 %1.5 %\n11.9 %3.1 %\n00 %0 %\n00 %0 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n4624.1 %4.4 %\n9348.7 %4.7 %\n5026.2 %6.4 %\n00 %0 %\n00 %0 %\n21 %10.5 %\n191100 %5 % \n\n \n \nRioja (La)\n623.1 %0.6 %\n1453.8 %0.7 %\n623.1 %0.8 %\n00 %0 %\n00 %0 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n342.9 %0.3 %\n342.9 %0.2 %\n114.3 %0.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n00 %0 %\n583.3 %0.3 %\n116.7 %0.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n103926.9 %100 %\n197551.2 %100 %\n78720.4 %100 %\n320.8 %100 %\n60.2 %100 %\n190.5 %100 %\n3858100 %100 % \n\nχ2=106.010 · df=90 · Cramer's V=0.074 · Fisher's p=0.042 \n\n \n\n\nPregunta: P3   Por CCAA\n \n Comunidad autónoma\n Valoración sobre siha pasado lo peor ono de la situaciónsanitaria generadapor el coronavirus\n Total\n \n \n\n Lo peor ha pasado ya\n Seguimos en el peormomento\n Lo peor está porllegar\n (NO LEER) No losabe, duda\n N.C.\n \n \n \nAndalucía\n48669.4 %17.7 %\n8612.3 %21.7 %\n659.3 %19.2 %\n557.9 %16.9 %\n81.1 %14.8 %\n700100 %18.1 % \n\n \n \nAragón\n7870.9 %2.8 %\n1715.5 %4.3 %\n65.5 %1.8 %\n76.4 %2.2 %\n21.8 %3.7 %\n110100 %2.8 % \n\n \n \nAsturias (Principadode)\n5659.6 %2 %\n1111.7 %2.8 %\n99.6 %2.7 %\n1617 %4.9 %\n22.1 %3.7 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n6272.1 %2.3 %\n67 %1.5 %\n67 %1.8 %\n1112.8 %3.4 %\n11.2 %1.9 %\n86100 %2.2 % \n\n \n \nCanarias\n11264.4 %4.1 %\n2011.5 %5 %\n169.2 %4.7 %\n2313.2 %7.1 %\n31.7 %5.6 %\n174100 %4.5 % \n\n \n \nCantabria\n3261.5 %1.2 %\n815.4 %2 %\n59.6 %1.5 %\n611.5 %1.8 %\n11.9 %1.9 %\n52100 %1.3 % \n\n \n \nCastilla-La Mancha\n13578.9 %4.9 %\n137.6 %3.3 %\n105.8 %2.9 %\n127 %3.7 %\n10.6 %1.9 %\n171100 %4.4 % \n\n \n \nCastilla y León\n14868.8 %5.4 %\n219.8 %5.3 %\n2813 %8.3 %\n167.4 %4.9 %\n20.9 %3.7 %\n215100 %5.6 % \n\n \n \nCataluña\n43472.8 %15.8 %\n457.6 %11.3 %\n6010.1 %17.7 %\n518.6 %15.7 %\n61 %11.1 %\n596100 %15.4 % \n\n \n \nComunitat Valenciana\n28171.3 %10.2 %\n379.4 %9.3 %\n348.6 %10 %\n328.1 %9.8 %\n102.5 %18.5 %\n394100 %10.2 % \n\n \n \nExtremadura\n6770.5 %2.4 %\n1212.6 %3 %\n88.4 %2.4 %\n55.3 %1.5 %\n33.2 %5.6 %\n95100 %2.5 % \n\n \n \nGalicia\n18072.9 %6.5 %\n228.9 %5.5 %\n187.3 %5.3 %\n249.7 %7.4 %\n31.2 %5.6 %\n247100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n39974.7 %14.5 %\n5510.3 %13.9 %\n407.5 %11.8 %\n336.2 %10.2 %\n71.3 %13 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n7867.2 %2.8 %\n1714.7 %4.3 %\n108.6 %2.9 %\n97.8 %2.8 %\n21.7 %3.7 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n3666.7 %1.3 %\n814.8 %2 %\n35.6 %0.9 %\n611.1 %1.8 %\n11.9 %1.9 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n14174.6 %5.1 %\n168.5 %4 %\n168.5 %4.7 %\n157.9 %4.6 %\n10.5 %1.9 %\n189100 %4.9 % \n\n \n \nRioja (La)\n1973.1 %0.7 %\n13.8 %0.3 %\n311.5 %0.9 %\n311.5 %0.9 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n457.1 %0.1 %\n114.3 %0.3 %\n114.3 %0.3 %\n00 %0 %\n114.3 %1.9 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n457.1 %0.1 %\n114.3 %0.3 %\n114.3 %0.3 %\n114.3 %0.3 %\n00 %0 %\n7100 %0.2 % \n\n \n \nTotal\n275271.2 %100 %\n39710.3 %100 %\n3398.8 %100 %\n3258.4 %100 %\n541.4 %100 %\n3867100 %100 % \n\nχ2=82.878 · df=72 · Cramer's V=0.073 · Fisher's p=0.133 \n\n \n\n\nPregunta: P4   Por CCAA\n \n Comunidad autónoma\n Grado deconocimiento por lainvasión de Rusia aUcrania\n Total\n \n \n\n Mucho\n Bastante\n Algo\n Poco\n Nada\n N.S.\n N.C.\n \n \n \nAndalucía\n13619.4 %20.2 %\n24334.7 %18.5 %\n19427.7 %16.8 %\n9213.1 %16.6 %\n223.1 %22 %\n60.9 %20.7 %\n71 %14.6 %\n700100 %18.1 % \n\n \n \nAragón\n2119.1 %3.1 %\n4339.1 %3.3 %\n3128.2 %2.7 %\n1311.8 %2.4 %\n10.9 %1 %\n10.9 %3.4 %\n00 %0 %\n110100 %2.8 % \n\n \n \nAsturias (Principadode)\n1010.6 %1.5 %\n3739.4 %2.8 %\n2627.7 %2.3 %\n1516 %2.7 %\n11.1 %1 %\n11.1 %3.4 %\n44.3 %8.3 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n78 %1 %\n2832.2 %2.1 %\n3236.8 %2.8 %\n1618.4 %2.9 %\n22.3 %2 %\n22.3 %6.9 %\n00 %0 %\n87100 %2.2 % \n\n \n \nCanarias\n2212.7 %3.3 %\n4324.9 %3.3 %\n6437 %5.5 %\n3319.1 %6 %\n52.9 %5 %\n00 %0 %\n63.5 %12.5 %\n173100 %4.5 % \n\n \n \nCantabria\n611.5 %0.9 %\n2140.4 %1.6 %\n1528.8 %1.3 %\n917.3 %1.6 %\n11.9 %1 %\n00 %0 %\n00 %0 %\n52100 %1.3 % \n\n \n \nCastilla-La Mancha\n2615.2 %3.9 %\n4928.7 %3.7 %\n6638.6 %5.7 %\n2514.6 %4.5 %\n52.9 %5 %\n00 %0 %\n00 %0 %\n171100 %4.4 % \n\n \n \nCastilla y León\n4319.9 %6.4 %\n6329.2 %4.8 %\n7434.3 %6.4 %\n2612 %4.7 %\n31.4 %3 %\n10.5 %3.4 %\n62.8 %12.5 %\n216100 %5.6 % \n\n \n \nCataluña\n11719.6 %17.4 %\n21636.2 %16.5 %\n16527.7 %14.3 %\n7512.6 %13.6 %\n142.3 %14 %\n50.8 %17.2 %\n40.7 %8.3 %\n596100 %15.4 % \n\n \n \nComunitat Valenciana\n7118 %10.5 %\n12030.5 %9.2 %\n12030.5 %10.4 %\n6215.7 %11.2 %\n112.8 %11 %\n51.3 %17.2 %\n51.3 %10.4 %\n394100 %10.2 % \n\n \n \nExtremadura\n1212.6 %1.8 %\n3031.6 %2.3 %\n2829.5 %2.4 %\n2122.1 %3.8 %\n33.2 %3 %\n00 %0 %\n11.1 %2.1 %\n95100 %2.5 % \n\n \n \nGalicia\n3915.9 %5.8 %\n7731.3 %5.9 %\n7731.3 %6.7 %\n3915.9 %7.1 %\n83.3 %8 %\n20.8 %6.9 %\n41.6 %8.3 %\n246100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n9818.4 %14.6 %\n19135.8 %14.6 %\n16330.5 %14.1 %\n6712.5 %12.1 %\n101.9 %10 %\n20.4 %6.9 %\n30.6 %6.2 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n2521.4 %3.7 %\n4235.9 %3.2 %\n3227.4 %2.8 %\n1412 %2.5 %\n32.6 %3 %\n00 %0 %\n10.9 %2.1 %\n117100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1120.4 %1.6 %\n2138.9 %1.6 %\n1425.9 %1.2 %\n611.1 %1.1 %\n11.9 %1 %\n00 %0 %\n11.9 %2.1 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n2312.1 %3.4 %\n7036.8 %5.3 %\n4523.7 %3.9 %\n3317.4 %6 %\n105.3 %10 %\n31.6 %10.3 %\n63.2 %12.5 %\n190100 %4.9 % \n\n \n \nRioja (La)\n415.4 %0.6 %\n1142.3 %0.8 %\n623.1 %0.5 %\n415.4 %0.7 %\n00 %0 %\n13.8 %3.4 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n116.7 %0.1 %\n233.3 %0.2 %\n116.7 %0.1 %\n233.3 %0.4 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n116.7 %0.1 %\n350 %0.2 %\n116.7 %0.1 %\n116.7 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n67317.4 %100 %\n131033.9 %100 %\n115429.8 %100 %\n55314.3 %100 %\n1002.6 %100 %\n290.7 %100 %\n481.2 %100 %\n3867100 %100 % \n\nχ2=132.477 · df=108 · Cramer's V=0.076 · Fisher's p=0.044 \n\n \n\n\nPregunta: P5   Por CCAA\n \n Comunidad autónoma\n Grado depreocupación por lainvasión de Rusia aUcrania\n Total\n \n \n\n Muy preocupado/a\n Bastantepreocupado/a\n Algo preocupado/a\n Poco preocupado/a\n Nada preocupado/a\n (NO LEER) No tienecriterio\n (NO LEER) Le esindiferente\n N.S.\n N.C.\n \n \n \nAndalucía\n21330.4 %21 %\n32946.9 %17.2 %\n10214.6 %17.4 %\n375.3 %17.1 %\n111.6 %12.6 %\n10.1 %50 %\n00 %0 %\n10.1 %20 %\n71 %15.9 %\n701100 %18.1 % \n\n \n \nAragón\n2321.1 %2.3 %\n5752.3 %3 %\n1715.6 %2.9 %\n87.3 %3.7 %\n21.8 %2.3 %\n10.9 %50 %\n00 %0 %\n00 %0 %\n10.9 %2.3 %\n109100 %2.8 % \n\n \n \nAsturias (Principadode)\n2425.5 %2.4 %\n4244.7 %2.2 %\n1718.1 %2.9 %\n55.3 %2.3 %\n44.3 %4.6 %\n00 %0 %\n00 %0 %\n00 %0 %\n22.1 %4.5 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n1820.7 %1.8 %\n3944.8 %2 %\n1921.8 %3.2 %\n66.9 %2.8 %\n44.6 %4.6 %\n00 %0 %\n00 %0 %\n00 %0 %\n11.1 %2.3 %\n87100 %2.2 % \n\n \n \nCanarias\n5531.6 %5.4 %\n7643.7 %4 %\n2916.7 %4.9 %\n52.9 %2.3 %\n63.4 %6.9 %\n00 %0 %\n00 %0 %\n00 %0 %\n31.7 %6.8 %\n174100 %4.5 % \n\n \n \nCantabria\n1223.5 %1.2 %\n2651 %1.4 %\n815.7 %1.4 %\n23.9 %0.9 %\n23.9 %2.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n12 %2.3 %\n51100 %1.3 % \n\n \n \nCastilla-La Mancha\n4023.5 %4 %\n10058.8 %5.2 %\n2112.4 %3.6 %\n42.4 %1.8 %\n42.4 %4.6 %\n00 %0 %\n00 %0 %\n10.6 %20 %\n00 %0 %\n170100 %4.4 % \n\n \n \nCastilla y León\n6329.2 %6.2 %\n10649.1 %5.5 %\n2913.4 %4.9 %\n104.6 %4.6 %\n62.8 %6.9 %\n00 %0 %\n00 %0 %\n10.5 %20 %\n10.5 %2.3 %\n216100 %5.6 % \n\n \n \nCataluña\n13923.3 %13.7 %\n28948.5 %15.1 %\n10016.8 %17 %\n457.6 %20.7 %\n122 %13.8 %\n00 %0 %\n00 %0 %\n10.2 %20 %\n101.7 %22.7 %\n596100 %15.4 % \n\n \n \nComunitat Valenciana\n9524.2 %9.4 %\n19048.3 %9.9 %\n7017.8 %11.9 %\n235.9 %10.6 %\n92.3 %10.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n61.5 %13.6 %\n393100 %10.2 % \n\n \n \nExtremadura\n2526.3 %2.5 %\n5052.6 %2.6 %\n1414.7 %2.4 %\n44.2 %1.8 %\n11.1 %1.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n11.1 %2.3 %\n95100 %2.5 % \n\n \n \nGalicia\n6124.8 %6 %\n12751.6 %6.6 %\n2911.8 %4.9 %\n176.9 %7.8 %\n62.4 %6.9 %\n00 %0 %\n00 %0 %\n00 %0 %\n62.4 %13.6 %\n246100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n14527.1 %14.3 %\n27150.7 %14.2 %\n7714.4 %13.1 %\n326 %14.7 %\n91.7 %10.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n10.2 %2.3 %\n535100 %13.8 % \n\n \n \nMurcia (Región de)\n3025.9 %3 %\n6556 %3.4 %\n1512.9 %2.6 %\n10.9 %0.5 %\n21.7 %2.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n32.6 %6.8 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1221.8 %1.2 %\n3156.4 %1.6 %\n47.3 %0.7 %\n23.6 %0.9 %\n35.5 %3.4 %\n00 %0 %\n11.8 %100 %\n11.8 %20 %\n11.8 %2.3 %\n55100 %1.4 % \n\n \n \nPaís Vasco\n4423.2 %4.3 %\n9751.1 %5.1 %\n3116.3 %5.3 %\n126.3 %5.5 %\n63.2 %6.9 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n190100 %4.9 % \n\n \n \nRioja (La)\n726.9 %0.7 %\n1350 %0.7 %\n311.5 %0.5 %\n311.5 %1.4 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n342.9 %0.3 %\n114.3 %0.1 %\n228.6 %0.3 %\n114.3 %0.5 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n350 %0.3 %\n350 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n101226.2 %100 %\n191249.4 %100 %\n58715.2 %100 %\n2175.6 %100 %\n872.2 %100 %\n20.1 %100 %\n10 %100 %\n50.1 %100 %\n441.1 %100 %\n3867100 %100 % \n\nχ2=201.480 · df=144 · Cramer's V=0.081 · Fisher's p=0.073 \n\n \n\n\nPregunta: P6   Por CCAA\n \n Comunidad autónoma\n Posibilidad de unainvasión de Rusia apaíses del Este deEuropa (su antiguaárea de influencia)\n Total\n \n \n\n Sí cree que esposible\n No cree que seaposible\n (NO LEER) No tienesuficiente criterioo información\n (NO LEER) Le esindiferente, no leimporta\n N.S., duda\n N.C.\n \n \n \nAndalucía\n53676.6 %18.3 %\n11516.4 %17.2 %\n71 %20 %\n10.1 %100 %\n385.4 %17.9 %\n30.4 %12.5 %\n700100 %18.1 % \n\n \n \nAragón\n7367 %2.5 %\n2724.8 %4 %\n21.8 %5.7 %\n00 %0 %\n76.4 %3.3 %\n00 %0 %\n109100 %2.8 % \n\n \n \nAsturias (Principadode)\n6974.2 %2.4 %\n1617.2 %2.4 %\n00 %0 %\n00 %0 %\n77.5 %3.3 %\n11.1 %4.2 %\n93100 %2.4 % \n\n \n \nBalears (Illes)\n6473.6 %2.2 %\n1314.9 %1.9 %\n11.1 %2.9 %\n00 %0 %\n910.3 %4.2 %\n00 %0 %\n87100 %2.3 % \n\n \n \nCanarias\n13678.2 %4.6 %\n2816.1 %4.2 %\n00 %0 %\n00 %0 %\n74 %3.3 %\n31.7 %12.5 %\n174100 %4.5 % \n\n \n \nCantabria\n4279.2 %1.4 %\n815.1 %1.2 %\n11.9 %2.9 %\n00 %0 %\n23.8 %0.9 %\n00 %0 %\n53100 %1.4 % \n\n \n \nCastilla-La Mancha\n13277.6 %4.5 %\n3017.6 %4.5 %\n31.8 %8.6 %\n00 %0 %\n42.4 %1.9 %\n10.6 %4.2 %\n170100 %4.4 % \n\n \n \nCastilla y León\n17078.7 %5.8 %\n3013.9 %4.5 %\n31.4 %8.6 %\n00 %0 %\n136 %6.1 %\n00 %0 %\n216100 %5.6 % \n\n \n \nCataluña\n42571.4 %14.5 %\n12020.2 %18 %\n40.7 %11.4 %\n00 %0 %\n406.7 %18.9 %\n61 %25 %\n595100 %15.4 % \n\n \n \nComunitat Valenciana\n29073.6 %9.9 %\n8220.8 %12.3 %\n30.8 %8.6 %\n00 %0 %\n143.6 %6.6 %\n51.3 %20.8 %\n394100 %10.2 % \n\n \n \nExtremadura\n7882.1 %2.7 %\n1414.7 %2.1 %\n11.1 %2.9 %\n00 %0 %\n22.1 %0.9 %\n00 %0 %\n95100 %2.5 % \n\n \n \nGalicia\n19378.1 %6.6 %\n3413.8 %5.1 %\n31.2 %8.6 %\n00 %0 %\n166.5 %7.5 %\n10.4 %4.2 %\n247100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n41177 %14.1 %\n9718.2 %14.5 %\n30.6 %8.6 %\n00 %0 %\n224.1 %10.4 %\n10.2 %4.2 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n8875.9 %3 %\n1412.1 %2.1 %\n10.9 %2.9 %\n00 %0 %\n119.5 %5.2 %\n21.7 %8.3 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n4074.1 %1.4 %\n713 %1 %\n11.9 %2.9 %\n00 %0 %\n611.1 %2.8 %\n00 %0 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n14777.4 %5 %\n2814.7 %4.2 %\n21.1 %5.7 %\n00 %0 %\n126.3 %5.7 %\n10.5 %4.2 %\n190100 %4.9 % \n\n \n \nRioja (La)\n1872 %0.6 %\n520 %0.7 %\n00 %0 %\n00 %0 %\n28 %0.9 %\n00 %0 %\n25100 %0.6 % \n\n \n \nCeuta (CiudadAutónoma de)\n7100 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n6100 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n292575.7 %100 %\n66817.3 %100 %\n350.9 %100 %\n10 %100 %\n2125.5 %100 %\n240.6 %100 %\n3865100 %100 % \n\nχ2=79.266 · df=90 · Cramer's V=0.064 · Fisher's p=0.249"
  },
  {
    "objectID": "2022/12/04/cosas-para-dejar-spss/index.html#datos.",
    "href": "2022/12/04/cosas-para-dejar-spss/index.html#datos.",
    "title": "Consejos para dejar spss",
    "section": "Datos.",
    "text": "Datos.\nPara ejemplificar, voy a bajarme el fichero del barómetro delCIS de mayo de 2022 en formato sav de spss y de paso vemos como se lee con R"
  },
  {
    "objectID": "2022/12/04/cosas-para-dejar-spss/index.html#librerías-y-lectura-de-datos",
    "href": "2022/12/04/cosas-para-dejar-spss/index.html#librerías-y-lectura-de-datos",
    "title": "Consejos para dejar spss",
    "section": "Librerías y lectura de datos",
    "text": "Librerías y lectura de datos\n\n# tidyverse por si acaso y haven para leer ficheros spss\nlibrary(tidyverse)\nlibrary(haven) \n\n# librerias de Daniel\n# library(sjlabelled)\nlibrary(sjPlot)\nlibrary(sjmisc)\n\nbar_mayo_2022 <-  read_sav(here::here(\"data/3363.sav\"))\n\nbar_mayo_2022\n#> # A tibble: 3,865 × 172\n#>     ESTU REGISTRO  CUES CCAA    PROV    MUN      CAPITAL TAMUNI  ENTREV  TIPO_…¹\n#>    <dbl>    <dbl> <dbl> <dbl+l> <dbl+l> <dbl+lb> <dbl+l> <dbl+l> <dbl+l> <dbl+l>\n#>  1  3363      552     1 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 3 [10.… 0 [Ano… 1 [Fij…\n#>  2  3363      585     2 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 3 [10.… 0 [Ano… 1 [Fij…\n#>  3  3363      670     3 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 3 [10.… 0 [Ano… 2 [Móv…\n#>  4  3363      882     4 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 3 [10.… 0 [Ano… 1 [Fij…\n#>  5  3363    14056     5 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 3 [10.… 0 [Ano… 1 [Fij…\n#>  6  3363    14787     6 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 1 [Men… 0 [Ano… 1 [Fij…\n#>  7  3363     1447     7 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 1 [Men… 0 [Ano… 1 [Fij…\n#>  8  3363     4590     8 1 [And… 4 [Alm…  0 [Mun… 3 [Otr… 3 [10.… 0 [Ano… 2 [Móv…\n#>  9  3363      223     9 1 [And… 4 [Alm… 13 [Alm… 2 [Cap… 5 [100… 0 [Ano… 2 [Móv…\n#> 10  3363      399    10 1 [And… 4 [Alm… 13 [Alm… 2 [Cap… 5 [100… 0 [Ano… 1 [Fij…\n#> # … with 3,855 more rows, 162 more variables: SEXO <dbl+lbl>, EDAD <dbl+lbl>,\n#> #   P0 <dbl+lbl>, P1 <dbl+lbl>, P2 <dbl+lbl>, P3 <dbl+lbl>, P4 <dbl+lbl>,\n#> #   P5 <dbl+lbl>, P6 <dbl+lbl>, P7_1 <dbl+lbl>, P7_2 <dbl+lbl>, P7_3 <dbl+lbl>,\n#> #   P7_4 <dbl+lbl>, P7_5 <dbl+lbl>, P7_6 <dbl+lbl>, P7_7 <dbl+lbl>,\n#> #   ECOESP <dbl+lbl>, ECOPER <dbl+lbl>, PESPANNA1 <dbl+lbl>,\n#> #   PESPANNA2 <dbl+lbl>, PESPANNA3 <dbl+lbl>, PPERSONAL1 <dbl+lbl>,\n#> #   PPERSONAL2 <dbl+lbl>, PPERSONAL3 <dbl+lbl>, PREFPTE <dbl+lbl>, …\n\nInteresante que al leer con haven tengo los datos con el valor y con la etiqueta.\n\nhead(bar_mayo_2022$CCAA)\n#> <labelled<double>[6]>: Comunidad autónoma\n#> [1] 1 1 1 1 1 1\n#> \n#> Labels:\n#>  value                        label\n#>      1                    Andalucía\n#>      2                       Aragón\n#>      3     Asturias (Principado de)\n#>      4              Balears (Illes)\n#>      5                     Canarias\n#>      6                    Cantabria\n#>      7           Castilla-La Mancha\n#>      8              Castilla y León\n#>      9                     Cataluña\n#>     10         Comunitat Valenciana\n#>     11                  Extremadura\n#>     12                      Galicia\n#>     13        Madrid (Comunidad de)\n#>     14           Murcia (Región de)\n#>     15 Navarra (Comunidad Foral de)\n#>     16                   País Vasco\n#>     17                   Rioja (La)\n#>     18   Ceuta (Ciudad Autónoma de)\n#>     19 Melilla (Ciudad Autónoma de)"
  },
  {
    "objectID": "2022/12/04/cosas-para-dejar-spss/index.html#usando-las-librerías-de-daniel",
    "href": "2022/12/04/cosas-para-dejar-spss/index.html#usando-las-librerías-de-daniel",
    "title": "Consejos para dejar spss",
    "section": "Usando las librerías de Daniel",
    "text": "Usando las librerías de Daniel\nPodemos ver las frecuencias de gente que ha contestado por Comunidad autónoma y teniendo en cuenta la ponderación\n\n\nplot_frq(bar_mayo_2022$CCAA, weight.by = bar_mayo_2022$PESO)\n\n\n\n\n\n\n\n\n\nTablas de frecuencias\nPodemos ver las tablas de frecuencias en formato texto y sin ponderación\n\nfrq(bar_mayo_2022$CCAA, out = \"txt\")\n#> Comunidad autónoma (x) <numeric> \n#> # total N=3865 valid N=3865 mean=6.94 sd=5.27\n#> \n#> Value |                        Label |    N | Raw % | Valid % | Cum. %\n#> ----------------------------------------------------------------------\n#>     1 |                    Andalucía | 1293 | 33.45 |   33.45 |  33.45\n#>     2 |                       Aragón |   96 |  2.48 |    2.48 |  35.94\n#>     3 |     Asturias (Principado de) |  101 |  2.61 |    2.61 |  38.55\n#>     4 |              Balears (Illes) |   94 |  2.43 |    2.43 |  40.98\n#>     5 |                     Canarias |  116 |  3.00 |    3.00 |  43.98\n#>     6 |                    Cantabria |   93 |  2.41 |    2.41 |  46.39\n#>     7 |           Castilla-La Mancha |  123 |  3.18 |    3.18 |  49.57\n#>     8 |              Castilla y León |  163 |  4.22 |    4.22 |  53.79\n#>     9 |                     Cataluña |  423 | 10.94 |   10.94 |  64.73\n#>    10 |         Comunitat Valenciana |  284 |  7.35 |    7.35 |  72.08\n#>    11 |                  Extremadura |   93 |  2.41 |    2.41 |  74.49\n#>    12 |                      Galicia |  186 |  4.81 |    4.81 |  79.30\n#>    13 |        Madrid (Comunidad de) |  392 | 10.14 |   10.14 |  89.44\n#>    14 |           Murcia (Región de) |   98 |  2.54 |    2.54 |  91.98\n#>    15 | Navarra (Comunidad Foral de) |   87 |  2.25 |    2.25 |  94.23\n#>    16 |                   País Vasco |  125 |  3.23 |    3.23 |  97.46\n#>    17 |                   Rioja (La) |   79 |  2.04 |    2.04 |  99.51\n#>    18 |   Ceuta (Ciudad Autónoma de) |   10 |  0.26 |    0.26 |  99.77\n#>    19 | Melilla (Ciudad Autónoma de) |    9 |  0.23 |    0.23 | 100.00\n#>  <NA> |                         <NA> |    0 |  0.00 |    <NA> |   <NA>\n\nO usar la ponderación y verlas en formato de html\n\n\nfrq(bar_mayo_2022$CCAA, weights = bar_mayo_2022$PESO, out = \"viewer\")\n\n\n\nComunidad autónoma (xw) <numeric>\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n1\nAndalucía\n700\n18.11\n18.11\n18.11\n\n\n2\nAragón\n109\n2.82\n2.82\n20.93\n\n\n3\nAsturias (Principado de)\n94\n2.43\n2.43\n23.36\n\n\n4\nBalears (Illes)\n86\n2.23\n2.23\n25.59\n\n\n5\nCanarias\n174\n4.50\n4.50\n30.09\n\n\n6\nCantabria\n52\n1.35\n1.35\n31.44\n\n\n7\nCastilla-La Mancha\n171\n4.42\n4.42\n35.86\n\n\n8\nCastilla y León\n216\n5.59\n5.59\n41.45\n\n\n9\nCataluña\n595\n15.39\n15.39\n56.84\n\n\n10\nComunitat Valenciana\n393\n10.17\n10.17\n67.01\n\n\n11\nExtremadura\n95\n2.46\n2.46\n69.47\n\n\n12\nGalicia\n247\n6.39\n6.39\n75.86\n\n\n13\nMadrid (Comunidad de)\n534\n13.82\n13.82\n89.68\n\n\n14\nMurcia (Región de)\n116\n3.00\n3.00\n92.68\n\n\n15\nNavarra (Comunidad Foral de)\n54\n1.40\n1.40\n94.08\n\n\n16\nPaís Vasco\n190\n4.92\n4.92\n98.99\n\n\n17\nRioja (La)\n26\n0.67\n0.67\n99.66\n\n\n18\nCeuta (Ciudad Autónoma de)\n7\n0.18\n0.18\n99.84\n\n\n19\nMelilla (Ciudad Autónoma de)\n6\n0.16\n0.16\n100.00\n\n\nNA\nNA\n0\n0.00\nNA\nNA\n\n\ntotal N=3865 · valid N=3865 · x̄=8.29 · σ=4.73\n\n\n\n\n\n\nTambién podemos decirle que si hay una variable continua nos la intente agrupar\n\nfrq(bar_mayo_2022, EDAD, auto.grp = 5, weights = PESO, out = \"viewer\")\n\n\n\nEdad de la persona entrevistada (EDAD) <numeric>\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n1\n18-33\n746\n19.30\n19.30\n19.30\n\n\n2\n34-49\n1058\n27.37\n27.37\n46.66\n\n\n3\n50-65\n1181\n30.55\n30.55\n77.21\n\n\n4\n66-81\n794\n20.54\n20.54\n97.75\n\n\n5\n82-97\n85\n2.20\n2.20\n99.95\n\n\n6\n98-113\n2\n0.05\n0.05\n100.00\n\n\nNA\nNA\n0\n0.00\nNA\nNA\n\n\ntotal N=3866 · valid N=3866 · x̄=50.72 · σ=17.05\n\n\n\n\n\n\no que ordene en orden descendente\n\nfrq(bar_mayo_2022, EDAD, auto.grp = 7, sort.frq = \"desc\", \n    title = str_glue(\"{attributes(bar_mayo_2022$EDAD)$label} AGRUPADA\" ), \n    weights = PESO, out = \"viewer\")\n\n\n\nEdad de la persona entrevistada AGRUPADA\n\nval\nlabel\nfrq\nraw.prc\nvalid.prc\ncum.prc\n\n\n3\n42-53\n946\n24.47\n24.47\n24.47\n\n\n4\n54-65\n846\n21.88\n21.88\n46.35\n\n\n5\n66-77\n693\n17.93\n17.93\n64.28\n\n\n2\n30-41\n666\n17.23\n17.23\n81.51\n\n\n1\n18-29\n527\n13.63\n13.63\n95.14\n\n\n6\n78-89\n176\n4.55\n4.55\n99.69\n\n\n7\n90-101\n12\n0.31\n0.31\n100.00\n\n\nNA\nNA\n0\n0.00\nNA\nNA\n\n\ntotal N=3866 · valid N=3866 · x̄=50.72 · σ=17.05\n\n\n\n\n\n\n¿No está mal verdad?\nHe estado buscando si también tenía como hacer tablas de medias y demás, pero aún no lo he encontrado, no obstante podemos hacer esto.\n\n\n(edad_media <- bar_mayo_2022 %>% \n    group_by(CCAA = forcats::as_factor(CCAA) ) %>%  # con el as_factor  (de forcats o de haven) nos traemos las etiquetas \n    summarise(\n        edad_media = weighted.mean(EDAD, PESO) %>% round(2)\n        )\n ) \n#> # A tibble: 19 × 2\n#>    CCAA                         edad_media\n#>    <fct>                             <dbl>\n#>  1 Andalucía                          52.0\n#>  2 Aragón                             54.6\n#>  3 Asturias (Principado de)           52.8\n#>  4 Balears (Illes)                    51.5\n#>  5 Canarias                           49.3\n#>  6 Cantabria                          52.7\n#>  7 Castilla-La Mancha                 48.6\n#>  8 Castilla y León                    52.4\n#>  9 Cataluña                           49.8\n#> 10 Comunitat Valenciana               49.2\n#> 11 Extremadura                        51.5\n#> 12 Galicia                            51.4\n#> 13 Madrid (Comunidad de)              48.6\n#> 14 Murcia (Región de)                 49.7\n#> 15 Navarra (Comunidad Foral de)       54.8\n#> 16 País Vasco                         52.5\n#> 17 Rioja (La)                         53.8\n#> 18 Ceuta (Ciudad Autónoma de)         53.9\n#> 19 Melilla (Ciudad Autónoma de)       52.4\n\n\n\nTablas de contingencia\nEs usual en las tabulaciones de spss, hacer tablas dónde en filas (o columnas) siempre dejamos la misma variable y vemos el cruce con otras variables de interés. Por ejemplo queremos ver las respuestas a la pregunta P1 por cada comunidad autónoma, y teniendo en cuenta la ponderación.\nPara eso usamos la función tab_xtab, que por defecto además nos devuelve la V de Cramer o el estadístico Chi-cuadrado\n\n\ntab_xtab(bar_mayo_2022$CCAA, bar_mayo_2022$P1, weight.by = bar_mayo_2022$PESO)\n\n\n\n \n Comunidad autónoma\n Grado depreocupación ante lasituación delcoronavirus COVID-19\n Total\n \n \n\n Mucho\n Bastante\n (NO LEER) Regular\n Poco\n Nada\n N.S.\n N.C.\n \n \n \nAndalucía\n222\n276\n18\n141\n35\n2\n7\n701 \n\n \n \nAragón\n20\n49\n1\n31\n6\n1\n1\n109 \n\n \n \nAsturias (Principadode)\n18\n42\n1\n27\n4\n0\n2\n94 \n\n \n \nBalears (Illes)\n15\n29\n2\n28\n8\n0\n5\n87 \n\n \n \nCanarias\n51\n63\n0\n44\n13\n2\n1\n174 \n\n \n \nCantabria\n14\n24\n1\n11\n2\n0\n1\n53 \n\n \n \nCastilla-La Mancha\n34\n73\n4\n46\n13\n2\n0\n172 \n\n \n \nCastilla y León\n70\n88\n3\n43\n9\n3\n1\n217 \n\n \n \nCataluña\n130\n246\n11\n152\n47\n0\n9\n595 \n\n \n \nComunitat Valenciana\n82\n175\n8\n97\n28\n1\n2\n393 \n\n \n \nExtremadura\n27\n38\n4\n21\n3\n1\n1\n95 \n\n \n \nGalicia\n60\n103\n5\n65\n12\n1\n0\n246 \n\n \n \nMadrid (Comunidadde)\n126\n222\n12\n145\n28\n0\n1\n534 \n\n \n \nMurcia (Región de)\n30\n52\n3\n26\n4\n1\n0\n116 \n\n \n \nNavarra (ComunidadForal de)\n11\n24\n0\n17\n2\n0\n0\n54 \n\n \n \nPaís Vasco\n48\n65\n10\n49\n17\n0\n2\n191 \n\n \n \nRioja (La)\n8\n10\n1\n7\n0\n0\n0\n26 \n\n \n \nCeuta (CiudadAutónoma de)\n3\n3\n0\n1\n0\n0\n0\n7 \n\n \n \nMelilla (CiudadAutónoma de)\n4\n1\n0\n1\n1\n0\n0\n7 \n\n \n \nTotal\n973\n1583\n84\n952\n232\n14\n33\n3871 \n\nχ2=160.797 · df=108 · Cramer's V=0.083 · Fisher's p=0.001 \n\n \n\n\n\nPero no queremos ir una a una , sino que queremos tener rápidamente el cruce de varias preguntas con la comunidad autónoma.\nTabla cruzada por Comunidad autónoma y p1 a p6\n\n\n(var_to_cruce <-  paste0(\"P\", 1:6))\n#> [1] \"P1\" \"P2\" \"P3\" \"P4\" \"P5\" \"P6\"\n\nNos creamos una función para llamar a tab_xtab con algunas opciones por defecto y que devuelva el código html necesario para pintar. Nota. Uso lo de res$knitr al final porque quiero juntar en uno solo todas las tablas, si no lo hago así hay problemas para sacar todas las tablas en mismo html de forma programática.\n\n\n\ncontingency_table_print <-\n    function(df, var_fila, var_columna, var_peso,\n             show.cell.prc = FALSE,\n             show.row.prc  = FALSE,\n             show.col.prc  = FALSE\n             ) {\n             res <- tab_xtab(\n                 var.row = df[[var_fila]],\n                 var.col = df[[var_columna]],\n                 weight.by = df[[var_peso]], \n                 emph.total = TRUE,\n                 show.cell.prc = show.cell.prc,\n                 show.row.prc = show.row.prc, \n                 show.col.prc = show.col.prc,\n                 title = str_glue(\n                     \"Pregunta: {var_columna}  \",\n                     \" Por {var_fila}\")\n             )\n             return(res$knitr)\n    }\n\nHacemos un map para obtener de cada una de las preguntas (P1 a P6) su cruce con CCAA , utilizando PESO como variable de ponderación y que nos muestre los porcentajes de filas y de columnas\n\n\ncruces <- map(var_to_cruce,\n     function(x)\n  \n             contingency_table_print(bar_mayo_2022, \"CCAA\", x, \"PESO\",\n                                     show.row.prc = TRUE,\n                                     show.col.prc = TRUE)\n     )\n\nPara juntar todas las tablas, concateno los resultados y con la función raw_html los puedo pintar. En el chunk de R he puesto que el resultado sea asis\n`{r, results= 'asis', error=FALSE, warning=FALSE, message=FALSE}\n\n\nknitr::raw_html(map_chr(cruces, paste))\n\n\n\nPregunta: P1   Por CCAA\n \n Comunidad autónoma\n Grado depreocupación ante lasituación delcoronavirus COVID-19\n Total\n \n \n\n Mucho\n Bastante\n (NO LEER) Regular\n Poco\n Nada\n N.S.\n N.C.\n \n \n \nAndalucía\n22231.7 %22.8 %\n27639.4 %17.4 %\n182.6 %21.4 %\n14120.1 %14.8 %\n355 %15.1 %\n20.3 %14.3 %\n71 %21.2 %\n701100 %18.1 % \n\n \n \nAragón\n2018.3 %2.1 %\n4945 %3.1 %\n10.9 %1.2 %\n3128.4 %3.3 %\n65.5 %2.6 %\n10.9 %7.1 %\n10.9 %3 %\n109100 %2.8 % \n\n \n \nAsturias (Principadode)\n1819.1 %1.8 %\n4244.7 %2.7 %\n11.1 %1.2 %\n2728.7 %2.8 %\n44.3 %1.7 %\n00 %0 %\n22.1 %6.1 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n1517.2 %1.5 %\n2933.3 %1.8 %\n22.3 %2.4 %\n2832.2 %2.9 %\n89.2 %3.4 %\n00 %0 %\n55.7 %15.2 %\n87100 %2.2 % \n\n \n \nCanarias\n5129.3 %5.2 %\n6336.2 %4 %\n00 %0 %\n4425.3 %4.6 %\n137.5 %5.6 %\n21.1 %14.3 %\n10.6 %3 %\n174100 %4.5 % \n\n \n \nCantabria\n1426.4 %1.4 %\n2445.3 %1.5 %\n11.9 %1.2 %\n1120.8 %1.2 %\n23.8 %0.9 %\n00 %0 %\n11.9 %3 %\n53100 %1.4 % \n\n \n \nCastilla-La Mancha\n3419.8 %3.5 %\n7342.4 %4.6 %\n42.3 %4.8 %\n4626.7 %4.8 %\n137.6 %5.6 %\n21.2 %14.3 %\n00 %0 %\n172100 %4.4 % \n\n \n \nCastilla y León\n7032.3 %7.2 %\n8840.6 %5.6 %\n31.4 %3.6 %\n4319.8 %4.5 %\n94.1 %3.9 %\n31.4 %21.4 %\n10.5 %3 %\n217100 %5.6 % \n\n \n \nCataluña\n13021.8 %13.4 %\n24641.3 %15.5 %\n111.8 %13.1 %\n15225.5 %16 %\n477.9 %20.3 %\n00 %0 %\n91.5 %27.3 %\n595100 %15.4 % \n\n \n \nComunitat Valenciana\n8220.9 %8.4 %\n17544.5 %11.1 %\n82 %9.5 %\n9724.7 %10.2 %\n287.1 %12.1 %\n10.3 %7.1 %\n20.5 %6.1 %\n393100 %10.2 % \n\n \n \nExtremadura\n2728.4 %2.8 %\n3840 %2.4 %\n44.2 %4.8 %\n2122.1 %2.2 %\n33.2 %1.3 %\n11.1 %7.1 %\n11.1 %3 %\n95100 %2.5 % \n\n \n \nGalicia\n6024.4 %6.2 %\n10341.9 %6.5 %\n52 %6 %\n6526.4 %6.8 %\n124.9 %5.2 %\n10.4 %7.1 %\n00 %0 %\n246100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n12623.6 %12.9 %\n22241.6 %14 %\n122.2 %14.3 %\n14527.2 %15.2 %\n285.2 %12.1 %\n00 %0 %\n10.2 %3 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n3025.9 %3.1 %\n5244.8 %3.3 %\n32.6 %3.6 %\n2622.4 %2.7 %\n43.4 %1.7 %\n10.9 %7.1 %\n00 %0 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1120.4 %1.1 %\n2444.4 %1.5 %\n00 %0 %\n1731.5 %1.8 %\n23.7 %0.9 %\n00 %0 %\n00 %0 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n4825.1 %4.9 %\n6534 %4.1 %\n105.2 %11.9 %\n4925.7 %5.1 %\n178.9 %7.3 %\n00 %0 %\n21 %6.1 %\n191100 %4.9 % \n\n \n \nRioja (La)\n830.8 %0.8 %\n1038.5 %0.6 %\n13.8 %1.2 %\n726.9 %0.7 %\n00 %0 %\n00 %0 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n342.9 %0.3 %\n342.9 %0.2 %\n00 %0 %\n114.3 %0.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n457.1 %0.4 %\n114.3 %0.1 %\n00 %0 %\n114.3 %0.1 %\n114.3 %0.4 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nTotal\n97325.1 %100 %\n158340.9 %100 %\n842.2 %100 %\n95224.6 %100 %\n2326 %100 %\n140.4 %100 %\n330.9 %100 %\n3871100 %100 % \n\nχ2=160.797 · df=108 · Cramer's V=0.083 · Fisher's p=0.001 \n\n \n\n\nPregunta: P2   Por CCAA\n \n Comunidad autónoma\n Efectos de la crisisdel COVID-19 que máspreocupanpersonalmente\n Total\n \n \n\n Los efectos sobre lasalud\n Los efectos sobre laeconomía y el empleo\n (NO LEER) Ambos porigual\n (NO LEER) Ni unos niotros\n N.S.\n N.C.\n \n \n \nAndalucía\n20028.6 %19.2 %\n33347.6 %16.9 %\n15922.7 %20.2 %\n30.4 %9.4 %\n10.1 %16.7 %\n30.4 %15.8 %\n699100 %18.1 % \n\n \n \nAragón\n2926.9 %2.8 %\n5853.7 %2.9 %\n2018.5 %2.5 %\n10.9 %3.1 %\n00 %0 %\n00 %0 %\n108100 %2.8 % \n\n \n \nAsturias (Principadode)\n2931.2 %2.8 %\n4144.1 %2.1 %\n2223.7 %2.8 %\n11.1 %3.1 %\n00 %0 %\n00 %0 %\n93100 %2.4 % \n\n \n \nBalears (Illes)\n1618.6 %1.5 %\n4552.3 %2.3 %\n2124.4 %2.7 %\n22.3 %6.2 %\n00 %0 %\n22.3 %10.5 %\n86100 %2.2 % \n\n \n \nCanarias\n5431 %5.2 %\n8649.4 %4.4 %\n2614.9 %3.3 %\n31.7 %9.4 %\n21.1 %33.3 %\n31.7 %15.8 %\n174100 %4.5 % \n\n \n \nCantabria\n1325.5 %1.3 %\n2854.9 %1.4 %\n1019.6 %1.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n51100 %1.3 % \n\n \n \nCastilla-La Mancha\n4224.7 %4 %\n10360.6 %5.2 %\n2112.4 %2.7 %\n31.8 %9.4 %\n00 %0 %\n10.6 %5.3 %\n170100 %4.4 % \n\n \n \nCastilla y León\n4922.8 %4.7 %\n11453 %5.8 %\n4621.4 %5.8 %\n52.3 %15.6 %\n10.5 %16.7 %\n00 %0 %\n215100 %5.6 % \n\n \n \nCataluña\n14824.9 %14.2 %\n31853.5 %16.1 %\n12320.7 %15.6 %\n40.7 %12.5 %\n00 %0 %\n10.2 %5.3 %\n594100 %15.4 % \n\n \n \nComunitat Valenciana\n9423.9 %9 %\n22156.2 %11.2 %\n7118.1 %9 %\n51.3 %15.6 %\n00 %0 %\n20.5 %10.5 %\n393100 %10.2 % \n\n \n \nExtremadura\n3334.7 %3.2 %\n4244.2 %2.1 %\n2021.1 %2.5 %\n00 %0 %\n00 %0 %\n00 %0 %\n95100 %2.5 % \n\n \n \nGalicia\n7530.4 %7.2 %\n11847.8 %6 %\n5120.6 %6.5 %\n00 %0 %\n00 %0 %\n31.2 %15.8 %\n247100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n15228.5 %14.6 %\n27551.6 %13.9 %\n9918.6 %12.6 %\n40.8 %12.5 %\n20.4 %33.3 %\n10.2 %5.3 %\n533100 %13.8 % \n\n \n \nMurcia (Región de)\n3631 %3.5 %\n5144 %2.6 %\n2824.1 %3.6 %\n00 %0 %\n00 %0 %\n10.9 %5.3 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1425.9 %1.3 %\n2750 %1.4 %\n1222.2 %1.5 %\n11.9 %3.1 %\n00 %0 %\n00 %0 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n4624.1 %4.4 %\n9348.7 %4.7 %\n5026.2 %6.4 %\n00 %0 %\n00 %0 %\n21 %10.5 %\n191100 %5 % \n\n \n \nRioja (La)\n623.1 %0.6 %\n1453.8 %0.7 %\n623.1 %0.8 %\n00 %0 %\n00 %0 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n342.9 %0.3 %\n342.9 %0.2 %\n114.3 %0.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n00 %0 %\n583.3 %0.3 %\n116.7 %0.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n103926.9 %100 %\n197551.2 %100 %\n78720.4 %100 %\n320.8 %100 %\n60.2 %100 %\n190.5 %100 %\n3858100 %100 % \n\nχ2=106.010 · df=90 · Cramer's V=0.074 · Fisher's p=0.041 \n\n \n\n\nPregunta: P3   Por CCAA\n \n Comunidad autónoma\n Valoración sobre siha pasado lo peor ono de la situaciónsanitaria generadapor el coronavirus\n Total\n \n \n\n Lo peor ha pasado ya\n Seguimos en el peormomento\n Lo peor está porllegar\n (NO LEER) No losabe, duda\n N.C.\n \n \n \nAndalucía\n48669.4 %17.7 %\n8612.3 %21.7 %\n659.3 %19.2 %\n557.9 %16.9 %\n81.1 %14.8 %\n700100 %18.1 % \n\n \n \nAragón\n7870.9 %2.8 %\n1715.5 %4.3 %\n65.5 %1.8 %\n76.4 %2.2 %\n21.8 %3.7 %\n110100 %2.8 % \n\n \n \nAsturias (Principadode)\n5659.6 %2 %\n1111.7 %2.8 %\n99.6 %2.7 %\n1617 %4.9 %\n22.1 %3.7 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n6272.1 %2.3 %\n67 %1.5 %\n67 %1.8 %\n1112.8 %3.4 %\n11.2 %1.9 %\n86100 %2.2 % \n\n \n \nCanarias\n11264.4 %4.1 %\n2011.5 %5 %\n169.2 %4.7 %\n2313.2 %7.1 %\n31.7 %5.6 %\n174100 %4.5 % \n\n \n \nCantabria\n3261.5 %1.2 %\n815.4 %2 %\n59.6 %1.5 %\n611.5 %1.8 %\n11.9 %1.9 %\n52100 %1.3 % \n\n \n \nCastilla-La Mancha\n13578.9 %4.9 %\n137.6 %3.3 %\n105.8 %2.9 %\n127 %3.7 %\n10.6 %1.9 %\n171100 %4.4 % \n\n \n \nCastilla y León\n14868.8 %5.4 %\n219.8 %5.3 %\n2813 %8.3 %\n167.4 %4.9 %\n20.9 %3.7 %\n215100 %5.6 % \n\n \n \nCataluña\n43472.8 %15.8 %\n457.6 %11.3 %\n6010.1 %17.7 %\n518.6 %15.7 %\n61 %11.1 %\n596100 %15.4 % \n\n \n \nComunitat Valenciana\n28171.3 %10.2 %\n379.4 %9.3 %\n348.6 %10 %\n328.1 %9.8 %\n102.5 %18.5 %\n394100 %10.2 % \n\n \n \nExtremadura\n6770.5 %2.4 %\n1212.6 %3 %\n88.4 %2.4 %\n55.3 %1.5 %\n33.2 %5.6 %\n95100 %2.5 % \n\n \n \nGalicia\n18072.9 %6.5 %\n228.9 %5.5 %\n187.3 %5.3 %\n249.7 %7.4 %\n31.2 %5.6 %\n247100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n39974.7 %14.5 %\n5510.3 %13.9 %\n407.5 %11.8 %\n336.2 %10.2 %\n71.3 %13 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n7867.2 %2.8 %\n1714.7 %4.3 %\n108.6 %2.9 %\n97.8 %2.8 %\n21.7 %3.7 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n3666.7 %1.3 %\n814.8 %2 %\n35.6 %0.9 %\n611.1 %1.8 %\n11.9 %1.9 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n14174.6 %5.1 %\n168.5 %4 %\n168.5 %4.7 %\n157.9 %4.6 %\n10.5 %1.9 %\n189100 %4.9 % \n\n \n \nRioja (La)\n1973.1 %0.7 %\n13.8 %0.3 %\n311.5 %0.9 %\n311.5 %0.9 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n457.1 %0.1 %\n114.3 %0.3 %\n114.3 %0.3 %\n00 %0 %\n114.3 %1.9 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n457.1 %0.1 %\n114.3 %0.3 %\n114.3 %0.3 %\n114.3 %0.3 %\n00 %0 %\n7100 %0.2 % \n\n \n \nTotal\n275271.2 %100 %\n39710.3 %100 %\n3398.8 %100 %\n3258.4 %100 %\n541.4 %100 %\n3867100 %100 % \n\nχ2=82.878 · df=72 · Cramer's V=0.073 · Fisher's p=0.152 \n\n \n\n\nPregunta: P4   Por CCAA\n \n Comunidad autónoma\n Grado deconocimiento por lainvasión de Rusia aUcrania\n Total\n \n \n\n Mucho\n Bastante\n Algo\n Poco\n Nada\n N.S.\n N.C.\n \n \n \nAndalucía\n13619.4 %20.2 %\n24334.7 %18.5 %\n19427.7 %16.8 %\n9213.1 %16.6 %\n223.1 %22 %\n60.9 %20.7 %\n71 %14.6 %\n700100 %18.1 % \n\n \n \nAragón\n2119.1 %3.1 %\n4339.1 %3.3 %\n3128.2 %2.7 %\n1311.8 %2.4 %\n10.9 %1 %\n10.9 %3.4 %\n00 %0 %\n110100 %2.8 % \n\n \n \nAsturias (Principadode)\n1010.6 %1.5 %\n3739.4 %2.8 %\n2627.7 %2.3 %\n1516 %2.7 %\n11.1 %1 %\n11.1 %3.4 %\n44.3 %8.3 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n78 %1 %\n2832.2 %2.1 %\n3236.8 %2.8 %\n1618.4 %2.9 %\n22.3 %2 %\n22.3 %6.9 %\n00 %0 %\n87100 %2.2 % \n\n \n \nCanarias\n2212.7 %3.3 %\n4324.9 %3.3 %\n6437 %5.5 %\n3319.1 %6 %\n52.9 %5 %\n00 %0 %\n63.5 %12.5 %\n173100 %4.5 % \n\n \n \nCantabria\n611.5 %0.9 %\n2140.4 %1.6 %\n1528.8 %1.3 %\n917.3 %1.6 %\n11.9 %1 %\n00 %0 %\n00 %0 %\n52100 %1.3 % \n\n \n \nCastilla-La Mancha\n2615.2 %3.9 %\n4928.7 %3.7 %\n6638.6 %5.7 %\n2514.6 %4.5 %\n52.9 %5 %\n00 %0 %\n00 %0 %\n171100 %4.4 % \n\n \n \nCastilla y León\n4319.9 %6.4 %\n6329.2 %4.8 %\n7434.3 %6.4 %\n2612 %4.7 %\n31.4 %3 %\n10.5 %3.4 %\n62.8 %12.5 %\n216100 %5.6 % \n\n \n \nCataluña\n11719.6 %17.4 %\n21636.2 %16.5 %\n16527.7 %14.3 %\n7512.6 %13.6 %\n142.3 %14 %\n50.8 %17.2 %\n40.7 %8.3 %\n596100 %15.4 % \n\n \n \nComunitat Valenciana\n7118 %10.5 %\n12030.5 %9.2 %\n12030.5 %10.4 %\n6215.7 %11.2 %\n112.8 %11 %\n51.3 %17.2 %\n51.3 %10.4 %\n394100 %10.2 % \n\n \n \nExtremadura\n1212.6 %1.8 %\n3031.6 %2.3 %\n2829.5 %2.4 %\n2122.1 %3.8 %\n33.2 %3 %\n00 %0 %\n11.1 %2.1 %\n95100 %2.5 % \n\n \n \nGalicia\n3915.9 %5.8 %\n7731.3 %5.9 %\n7731.3 %6.7 %\n3915.9 %7.1 %\n83.3 %8 %\n20.8 %6.9 %\n41.6 %8.3 %\n246100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n9818.4 %14.6 %\n19135.8 %14.6 %\n16330.5 %14.1 %\n6712.5 %12.1 %\n101.9 %10 %\n20.4 %6.9 %\n30.6 %6.2 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n2521.4 %3.7 %\n4235.9 %3.2 %\n3227.4 %2.8 %\n1412 %2.5 %\n32.6 %3 %\n00 %0 %\n10.9 %2.1 %\n117100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1120.4 %1.6 %\n2138.9 %1.6 %\n1425.9 %1.2 %\n611.1 %1.1 %\n11.9 %1 %\n00 %0 %\n11.9 %2.1 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n2312.1 %3.4 %\n7036.8 %5.3 %\n4523.7 %3.9 %\n3317.4 %6 %\n105.3 %10 %\n31.6 %10.3 %\n63.2 %12.5 %\n190100 %4.9 % \n\n \n \nRioja (La)\n415.4 %0.6 %\n1142.3 %0.8 %\n623.1 %0.5 %\n415.4 %0.7 %\n00 %0 %\n13.8 %3.4 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n116.7 %0.1 %\n233.3 %0.2 %\n116.7 %0.1 %\n233.3 %0.4 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n116.7 %0.1 %\n350 %0.2 %\n116.7 %0.1 %\n116.7 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n67317.4 %100 %\n131033.9 %100 %\n115429.8 %100 %\n55314.3 %100 %\n1002.6 %100 %\n290.7 %100 %\n481.2 %100 %\n3867100 %100 % \n\nχ2=132.477 · df=108 · Cramer's V=0.076 · Fisher's p=0.041 \n\n \n\n\nPregunta: P5   Por CCAA\n \n Comunidad autónoma\n Grado depreocupación por lainvasión de Rusia aUcrania\n Total\n \n \n\n Muy preocupado/a\n Bastantepreocupado/a\n Algo preocupado/a\n Poco preocupado/a\n Nada preocupado/a\n (NO LEER) No tienecriterio\n (NO LEER) Le esindiferente\n N.S.\n N.C.\n \n \n \nAndalucía\n21330.4 %21 %\n32946.9 %17.2 %\n10214.6 %17.4 %\n375.3 %17.1 %\n111.6 %12.6 %\n10.1 %50 %\n00 %0 %\n10.1 %20 %\n71 %15.9 %\n701100 %18.1 % \n\n \n \nAragón\n2321.1 %2.3 %\n5752.3 %3 %\n1715.6 %2.9 %\n87.3 %3.7 %\n21.8 %2.3 %\n10.9 %50 %\n00 %0 %\n00 %0 %\n10.9 %2.3 %\n109100 %2.8 % \n\n \n \nAsturias (Principadode)\n2425.5 %2.4 %\n4244.7 %2.2 %\n1718.1 %2.9 %\n55.3 %2.3 %\n44.3 %4.6 %\n00 %0 %\n00 %0 %\n00 %0 %\n22.1 %4.5 %\n94100 %2.4 % \n\n \n \nBalears (Illes)\n1820.7 %1.8 %\n3944.8 %2 %\n1921.8 %3.2 %\n66.9 %2.8 %\n44.6 %4.6 %\n00 %0 %\n00 %0 %\n00 %0 %\n11.1 %2.3 %\n87100 %2.2 % \n\n \n \nCanarias\n5531.6 %5.4 %\n7643.7 %4 %\n2916.7 %4.9 %\n52.9 %2.3 %\n63.4 %6.9 %\n00 %0 %\n00 %0 %\n00 %0 %\n31.7 %6.8 %\n174100 %4.5 % \n\n \n \nCantabria\n1223.5 %1.2 %\n2651 %1.4 %\n815.7 %1.4 %\n23.9 %0.9 %\n23.9 %2.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n12 %2.3 %\n51100 %1.3 % \n\n \n \nCastilla-La Mancha\n4023.5 %4 %\n10058.8 %5.2 %\n2112.4 %3.6 %\n42.4 %1.8 %\n42.4 %4.6 %\n00 %0 %\n00 %0 %\n10.6 %20 %\n00 %0 %\n170100 %4.4 % \n\n \n \nCastilla y León\n6329.2 %6.2 %\n10649.1 %5.5 %\n2913.4 %4.9 %\n104.6 %4.6 %\n62.8 %6.9 %\n00 %0 %\n00 %0 %\n10.5 %20 %\n10.5 %2.3 %\n216100 %5.6 % \n\n \n \nCataluña\n13923.3 %13.7 %\n28948.5 %15.1 %\n10016.8 %17 %\n457.6 %20.7 %\n122 %13.8 %\n00 %0 %\n00 %0 %\n10.2 %20 %\n101.7 %22.7 %\n596100 %15.4 % \n\n \n \nComunitat Valenciana\n9524.2 %9.4 %\n19048.3 %9.9 %\n7017.8 %11.9 %\n235.9 %10.6 %\n92.3 %10.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n61.5 %13.6 %\n393100 %10.2 % \n\n \n \nExtremadura\n2526.3 %2.5 %\n5052.6 %2.6 %\n1414.7 %2.4 %\n44.2 %1.8 %\n11.1 %1.1 %\n00 %0 %\n00 %0 %\n00 %0 %\n11.1 %2.3 %\n95100 %2.5 % \n\n \n \nGalicia\n6124.8 %6 %\n12751.6 %6.6 %\n2911.8 %4.9 %\n176.9 %7.8 %\n62.4 %6.9 %\n00 %0 %\n00 %0 %\n00 %0 %\n62.4 %13.6 %\n246100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n14527.1 %14.3 %\n27150.7 %14.2 %\n7714.4 %13.1 %\n326 %14.7 %\n91.7 %10.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n10.2 %2.3 %\n535100 %13.8 % \n\n \n \nMurcia (Región de)\n3025.9 %3 %\n6556 %3.4 %\n1512.9 %2.6 %\n10.9 %0.5 %\n21.7 %2.3 %\n00 %0 %\n00 %0 %\n00 %0 %\n32.6 %6.8 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n1221.8 %1.2 %\n3156.4 %1.6 %\n47.3 %0.7 %\n23.6 %0.9 %\n35.5 %3.4 %\n00 %0 %\n11.8 %100 %\n11.8 %20 %\n11.8 %2.3 %\n55100 %1.4 % \n\n \n \nPaís Vasco\n4423.2 %4.3 %\n9751.1 %5.1 %\n3116.3 %5.3 %\n126.3 %5.5 %\n63.2 %6.9 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n190100 %4.9 % \n\n \n \nRioja (La)\n726.9 %0.7 %\n1350 %0.7 %\n311.5 %0.5 %\n311.5 %1.4 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n26100 %0.7 % \n\n \n \nCeuta (CiudadAutónoma de)\n342.9 %0.3 %\n114.3 %0.1 %\n228.6 %0.3 %\n114.3 %0.5 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n350 %0.3 %\n350 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n101226.2 %100 %\n191249.4 %100 %\n58715.2 %100 %\n2175.6 %100 %\n872.2 %100 %\n20.1 %100 %\n10 %100 %\n50.1 %100 %\n441.1 %100 %\n3867100 %100 % \n\nχ2=201.480 · df=144 · Cramer's V=0.081 · Fisher's p=0.086 \n\n \n\n\nPregunta: P6   Por CCAA\n \n Comunidad autónoma\n Posibilidad de unainvasión de Rusia apaíses del Este deEuropa (su antiguaárea de influencia)\n Total\n \n \n\n Sí cree que esposible\n No cree que seaposible\n (NO LEER) No tienesuficiente criterioo información\n (NO LEER) Le esindiferente, no leimporta\n N.S., duda\n N.C.\n \n \n \nAndalucía\n53676.6 %18.3 %\n11516.4 %17.2 %\n71 %20 %\n10.1 %100 %\n385.4 %17.9 %\n30.4 %12.5 %\n700100 %18.1 % \n\n \n \nAragón\n7367 %2.5 %\n2724.8 %4 %\n21.8 %5.7 %\n00 %0 %\n76.4 %3.3 %\n00 %0 %\n109100 %2.8 % \n\n \n \nAsturias (Principadode)\n6974.2 %2.4 %\n1617.2 %2.4 %\n00 %0 %\n00 %0 %\n77.5 %3.3 %\n11.1 %4.2 %\n93100 %2.4 % \n\n \n \nBalears (Illes)\n6473.6 %2.2 %\n1314.9 %1.9 %\n11.1 %2.9 %\n00 %0 %\n910.3 %4.2 %\n00 %0 %\n87100 %2.3 % \n\n \n \nCanarias\n13678.2 %4.6 %\n2816.1 %4.2 %\n00 %0 %\n00 %0 %\n74 %3.3 %\n31.7 %12.5 %\n174100 %4.5 % \n\n \n \nCantabria\n4279.2 %1.4 %\n815.1 %1.2 %\n11.9 %2.9 %\n00 %0 %\n23.8 %0.9 %\n00 %0 %\n53100 %1.4 % \n\n \n \nCastilla-La Mancha\n13277.6 %4.5 %\n3017.6 %4.5 %\n31.8 %8.6 %\n00 %0 %\n42.4 %1.9 %\n10.6 %4.2 %\n170100 %4.4 % \n\n \n \nCastilla y León\n17078.7 %5.8 %\n3013.9 %4.5 %\n31.4 %8.6 %\n00 %0 %\n136 %6.1 %\n00 %0 %\n216100 %5.6 % \n\n \n \nCataluña\n42571.4 %14.5 %\n12020.2 %18 %\n40.7 %11.4 %\n00 %0 %\n406.7 %18.9 %\n61 %25 %\n595100 %15.4 % \n\n \n \nComunitat Valenciana\n29073.6 %9.9 %\n8220.8 %12.3 %\n30.8 %8.6 %\n00 %0 %\n143.6 %6.6 %\n51.3 %20.8 %\n394100 %10.2 % \n\n \n \nExtremadura\n7882.1 %2.7 %\n1414.7 %2.1 %\n11.1 %2.9 %\n00 %0 %\n22.1 %0.9 %\n00 %0 %\n95100 %2.5 % \n\n \n \nGalicia\n19378.1 %6.6 %\n3413.8 %5.1 %\n31.2 %8.6 %\n00 %0 %\n166.5 %7.5 %\n10.4 %4.2 %\n247100 %6.4 % \n\n \n \nMadrid (Comunidadde)\n41177 %14.1 %\n9718.2 %14.5 %\n30.6 %8.6 %\n00 %0 %\n224.1 %10.4 %\n10.2 %4.2 %\n534100 %13.8 % \n\n \n \nMurcia (Región de)\n8875.9 %3 %\n1412.1 %2.1 %\n10.9 %2.9 %\n00 %0 %\n119.5 %5.2 %\n21.7 %8.3 %\n116100 %3 % \n\n \n \nNavarra (ComunidadForal de)\n4074.1 %1.4 %\n713 %1 %\n11.9 %2.9 %\n00 %0 %\n611.1 %2.8 %\n00 %0 %\n54100 %1.4 % \n\n \n \nPaís Vasco\n14777.4 %5 %\n2814.7 %4.2 %\n21.1 %5.7 %\n00 %0 %\n126.3 %5.7 %\n10.5 %4.2 %\n190100 %4.9 % \n\n \n \nRioja (La)\n1872 %0.6 %\n520 %0.7 %\n00 %0 %\n00 %0 %\n28 %0.9 %\n00 %0 %\n25100 %0.6 % \n\n \n \nCeuta (CiudadAutónoma de)\n7100 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n7100 %0.2 % \n\n \n \nMelilla (CiudadAutónoma de)\n6100 %0.2 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n00 %0 %\n6100 %0.2 % \n\n \n \nTotal\n292575.7 %100 %\n66817.3 %100 %\n350.9 %100 %\n10 %100 %\n2125.5 %100 %\n240.6 %100 %\n3865100 %100 % \n\nχ2=79.266 · df=90 · Cramer's V=0.064 · Fisher's p=0.257 \n\n \n\n\nY bueno, esto era para mostrar que R además de ser un excelente sustituto de todo lo que se puede hacer con spss a nivel analítico (de hecho tiene muchas más cosas) también nos puede servir para sacar tabulaciones de estudios que vengan en formato de spss.\nOs recomiendo encarecidamente que echéis un vistazo a las librerías sjPlot sjmisc de Daniel.\nHasta más ver."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "",
    "text": "El título de la entrada, sobre todo lo de la parte de “estilo compadre” viene de mis tiempos en consultoría, y tiene que ver con la necesidad de dar soluciones subóptimas a problemas acuciantes. Otra de mis frases, de la que puede que se acuerden Boris, Laura y Lourdes fue la de “si me das madera te hago un troncomóvil, no un ferrari”, lo cual es el equivalente a GIGO de toda la vida, pero a mi estilo.\nVamos al lío, últimamente ando estudianddo estadística bayesiana con el excelente material que pone a disposición de todo el mundo, y gratis, Aki Vehtari en este sitio Curso BDA3. Aki Vehtari es uno de los autores junto con Gelman y otros del libro Bayesian Data Analysis.\nEn la página 48 y siguientes tienen un ejemplo de como realizar inferencia bayesiana para el ratio muertes por cáncer usando un modelo básico Poisson-Gamma. Pero lo interesante es que comentan como construir una priori a partir de los datos, y que la forma en que lo hacen en este ejemplo puede considerarse una aproximación a como se construye en los modelos jerárquicos.\nTotal, que dado que en mi pueblo han aumentado, por desgracia, los casos y nos han confinado perimetralmente, voy a hacer el ejercicio de utilizar los datos del área sanitaria granada nordeste y adaptar el ejemplo.\nAviso que mi conocimiento de estadística bayesiana es limitado y muy probablemente puede que haga algo mal. Estoy aprendiendo, jejej."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#datos",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#datos",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "Datos",
    "text": "Datos\nEn primer lugar los datos por municipios vienen aqui. Y bueno, estaría bien que estuvieran un poco mejor organizados, puesto que solo puedes bajarte los últimos datos actualizados, no hay serie histórica por municipio, o al menos yo no la he encontrado.\nDespués de bajarme el excel al final me quedo solo con los datos de los municipios del área sanitaria Granada Nordeste.\n\nlibrary(tidyverse)\ng_nordeste <- read_csv(here::here(\"data/g_nordeste_20210326.csv\"))\n\ng_nordeste\n#> # A tibble: 46 × 3\n#>    lugar_de_residencia poblacion_miles confirmados_pdia_14_dias\n#>    <chr>                         <dbl>                    <dbl>\n#>  1 Alamedilla                    0.569                        0\n#>  2 Albuñán                       0.409                        0\n#>  3 Aldeire                       0.63                         0\n#>  4 Alicún de Ortega              0.471                        0\n#>  5 Alquife                       0.58                         0\n#>  6 Baza                         20.4                         27\n#>  7 Beas de Guadix                0.329                        0\n#>  8 Benalúa                       3.31                        11\n#>  9 Benamaurel                    2.26                         4\n#> 10 Calahorra (La)                0.668                        0\n#> # … with 36 more rows\n\nY podríamos contruir las tasas brutas por cada mil habitantes.\n\ng_nordeste <- g_nordeste %>% \n  mutate(tasa_bruta = confirmados_pdia_14_dias / poblacion_miles)\n\ng_nordeste %>% \n  arrange(-tasa_bruta)\n#> # A tibble: 46 × 4\n#>    lugar_de_residencia poblacion_miles confirmados_pdia_14_dias tasa_bruta\n#>    <chr>                         <dbl>                    <dbl>      <dbl>\n#>  1 Cogollos de Guadix            0.642                       17      26.5 \n#>  2 Purullena                     2.31                        29      12.6 \n#>  3 Cortes de Baza                1.84                        22      11.9 \n#>  4 Peza (La)                     1.17                         8       6.86\n#>  5 Dólar                         0.628                        4       6.37\n#>  6 Zújar                         2.54                        15       5.90\n#>  7 Cúllar                        4.09                        23       5.62\n#>  8 Cuevas del Campo              1.74                         6       3.44\n#>  9 Benalúa                       3.31                        11       3.32\n#> 10 Huéneja                       1.17                         3       2.56\n#> # … with 36 more rows\n\nBueno, y vemos que mi pueblo, está el tercero con mayor tasa con 12.58 por 1000 habitantes o 1258 por cada 100 mil (si revisan la situación el próximo martes posiblemente cierren la activad esencial)."
  },
  {
    "objectID": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#inferencia",
    "href": "2021/03/27/estimación-bayesiana-estilo-compadre/index.html#inferencia",
    "title": "Estimación Bayesiana, estilo compadre",
    "section": "Inferencia",
    "text": "Inferencia\nBueno, pues podríamos considerar que los casos en un municipio \\(y_i\\) la verosimilitud sería de la forma\n\\[y_i \\sim Poisson(X_i\\cdot\\theta_i)\\] dónde \\(X_i\\) sería la población en miles y \\(\\theta_i\\) la tasa por cada 1000 habitantes.\nAhora el tema para hacer inferencia bayesiana es especificar la prior, y como comentan en el libro podríamos construir la prior utilizando los datos. El número de casos sigue una “predictive distribution” binomial negativa y con un poco de álgebra llegan a igualar la media y varianza de las tasas brutas y obtener los parámetros de esa binomial negativa. Aquí es cuándo llega lo de “estilo compadre”, en vez de considerar la binomial negativa, yo voy a ajustar una Gamma a las tasas brutas y calculo el \\(\\alpha\\) y \\(\\beta\\) de la gamma por el método de los momentos.\nElegir una Gamma como Prior es en parte porque es distribución conjugada de la Poisson y la posterior se obtiene directamente.\nSin más, sería resolver estas ecuaciones\n\\[E[\\text{tasas brutas}] = \\dfrac{\\alpha_{prior}}{\\beta_{prior}}\\] \\[Var[\\text{tasas brutas}] = \\dfrac{\\alpha_{prior}}{\\beta_{prior}^{2}}\\]\nDespejando\n\nmedia = mean(g_nordeste$tasa_bruta)\ncuasivarianza = var(g_nordeste$tasa_bruta)\n(beta = media/cuasivarianza)\n#> [1] 0.1028402\n(alpha = media* beta)\n#> [1] 0.2312364\n\nPor lo que usaremos como prior una \\(Gamma(0.10, 0.23)\\), que parece un poco débil, seguramente porque las tasas brutas son muy diferentes entre los municipios.\nComparando la densidad de las tasas brutas con la densidad de la priori no parece mala elección\n\n# repito 10 veces cada tasa para tner suficients puntos para ver la densidad  estimada \ndf <- data.frame(raw_thetas = rep(g_nordeste$tasa_bruta,10),   \n                 simulados = rgamma(nrow(g_nordeste)*10, alpha, beta) )\n\ndf %>% \n    ggplot(aes(x=raw_thetas)) + \n    geom_density(size = 1) +\n    geom_density(aes(x=simulados), col = \"darkred\", linetype=2, size = 1)\n\n\n\n\n\n\n\n\nY ya sólo quedaría calcular la posterior para cada municipio. Que sería de esta forma\n\\[P\\left( \\theta_i\\mid data \\right) \\sim \\text{Gamma}(\\alpha_{prior} + y_i, \\beta_{prior} + x_i)\\] Dónde \\(y_i\\) es el número de casos en los últimos 14 días en cada municipio y \\(x_i\\) los expuestos, es decir, la población (en miles) en cada municipio. Con esto ya podemos calcular, y añadimos también los intervalos de credibilidad\n\ng_nordeste <- g_nordeste %>% \n    mutate(\n        posterior_a = alpha + confirmados_pdia_14_dias,\n        posterior_b = beta + poblacion_miles,\n        posterior_mean = posterior_a/posterior_b, \n        lb = qgamma(.025, posterior_a, posterior_b), \n        ub = qgamma(.025, posterior_a, posterior_b, lower.tail = FALSE)\n    ) \n\nY si vemos los datos de mi pueblo y de alguno más.\n\ng_nordeste %>%\n    filter(lugar_de_residencia %in% c(\"Cortes de Baza\",\"Castilléjar\", \"Baza\",\"Castril\", \"Benamaurel\", \"Zújar\")) %>%\n    select(lugar_de_residencia, poblacion_miles,\n           confirmados_pdia_14_dias,\n           tasa_bruta,\n           posterior_mean) %>% \n  arrange(-posterior_mean)\n#> # A tibble: 6 × 5\n#>   lugar_de_residencia poblacion_miles confirmados_pdia_14_dias tasa_br…¹ poste…²\n#>   <chr>                         <dbl>                    <dbl>     <dbl>   <dbl>\n#> 1 Cortes de Baza                 1.84                       22    11.9    11.4  \n#> 2 Zújar                          2.54                       15     5.90    5.75 \n#> 3 Benamaurel                     2.26                        4     1.77    1.79 \n#> 4 Baza                          20.4                        27     1.32    1.33 \n#> 5 Castilléjar                    1.32                        1     0.757   0.865\n#> 6 Castril                        2.02                        0     0       0.109\n#> # … with abbreviated variable names ¹​tasa_bruta, ²​posterior_mean\n\nPues no varía mucho la posterior con respecto a a la bruta. Puede deberse a dos motivos, uno, que al tener gran variabilidad las tasas brutas en los municipios considerados la información que comparten es poca comparada con la información específica que aporta cada municipio y la verosimilitud se impone a la prior y por otro lado, al no haber hecho full bayesian para estimar la prior , hemos utilizado los datos de los muncipios dos veces, una para obtener los parámetros de la prior y otra para la posterior, lo que puede llevar a sobreajuste. En los modelos jerárquicos bien estimados (y no al estilo compadre), si se estima bien, pero esto es una aproximación para entender un poco la lógica que hay debajo.\nY ya solo falta pintar .\n\ng_nordeste %>% \n    ggplot(aes(x=reorder(lugar_de_residencia, posterior_mean), y = posterior_mean)) +\n    geom_point(color = \"darkred\", size = rel(2)) +\n    geom_errorbar(aes(ymin = lb , ymax = ub)) +\n    coord_flip() +\n    labs(x = \"municipio\", y = \"Tasa x 1000 habitantes\")\n\n\n\n\n\n\n\n\nLos intervalos de credibilidad más pequeños se corresponden con los municipios con mayor población. A la vista de estos datos, se deberían usar este tipo de estimadores (bien hechos) sobre todo para estimar en municipios con una población menor, y no tomar decisiones basadas en una estimación puntual.\nCoda. Utilizando un glmer con family poisson (o con binomial si se modela la tasa directamente) con efecto aleatorio el lugar de residencia se obtienen prácticamente los mismos resultados"
  },
  {
    "objectID": "2021/03/13/purrr-furrr-o-cómo-se-diga/index.html",
    "href": "2021/03/13/purrr-furrr-o-cómo-se-diga/index.html",
    "title": "Purrr, furrr, maps y future_maps",
    "section": "",
    "text": "Hace un par de días un amigo mío me preguntaba por temas de que quería paralelizar un proceso con R, y no acababa de ver claro cómo. A falta de que mande un ejemplo creí entender que tiene un dataframe dónde tiene un proceso que encuentra para cada fila un conjunto de n filas parecidas y sobre ese conjunto de n filas hace cosas, como estimar algo y tal. Y lo que quiere al final es tener tanto lo estimado como un dataframe con las n filas similares a cada fila original\nBueno, hasta que me mande el ejemplo y me entere bien, me acordé que en R y más concretamente en el mundo tidyverse tenemos los nested data, que nos permite tener columnas cuyos elementos pueden ser cualquier cosa, desde un json, un dataframe, un modelo , etcc..\nVeamos un ejemplo"
  },
  {
    "objectID": "2021/03/13/purrr-furrr-o-cómo-se-diga/index.html#purrr",
    "href": "2021/03/13/purrr-furrr-o-cómo-se-diga/index.html#purrr",
    "title": "Purrr, furrr, maps y future_maps",
    "section": "purrr",
    "text": "purrr\n\n\nShow me the code\nlibrary(tidyverse)\n\n\n\ndf_nest <-  tibble(\n  grupo = c(letters[1:5]),\n  dfs = list(\n    dfa = data.frame(x = rnorm(2000), y = rexp(2000)),\n    dfb = data.frame(x = rnorm(1000), y = rexp(1000)),\n    dfc = data.frame(x = rnorm(30), y = rexp(30)),\n    dfd = data.frame(x = rnorm(200), y = rexp(200)),\n    dfe = data.frame(x = rnorm(1e5), y = rexp(1e5))\n  )\n)\n\ndf_nest\n#> # A tibble: 5 × 2\n#>   grupo dfs               \n#>   <chr> <named list>      \n#> 1 a     <df [2,000 × 2]>  \n#> 2 b     <df [1,000 × 2]>  \n#> 3 c     <df [30 × 2]>     \n#> 4 d     <df [200 × 2]>    \n#> 5 e     <df [100,000 × 2]>\n\n\nY vemos que el primer elemento de la columna dfs es un dataframe de 2000 filas y 2 colmnas y que el quinto tiene 100 mil filas de filas y dos columnas. Esta forma de tener la información puede ser útil, o al menos a mi me lo parece.\n¿Podríamos ahora hacer, por ejemplo un modelo sobre cada elemento de dfs? Si, y de manera muy sencilla utilizando funciones de purrr ya incluida con tidyverse\n\n\nShow me the code\ndf_con_modelo <- df_nest %>%\n  mutate(modelo = map(dfs, function(df)\n    lm(y ~ x, df))) %>% \n  mutate(resumen_modelo = map(modelo, broom::tidy)) # añodo tambin tabla resumen\n\ndf_con_modelo\n#> # A tibble: 5 × 4\n#>   grupo dfs                modelo       resumen_modelo  \n#>   <chr> <named list>       <named list> <named list>    \n#> 1 a     <df [2,000 × 2]>   <lm>         <tibble [2 × 5]>\n#> 2 b     <df [1,000 × 2]>   <lm>         <tibble [2 × 5]>\n#> 3 c     <df [30 × 2]>      <lm>         <tibble [2 × 5]>\n#> 4 d     <df [200 × 2]>     <lm>         <tibble [2 × 5]>\n#> 5 e     <df [100,000 × 2]> <lm>         <tibble [2 × 5]>\n\n\nY podemos ver los modelos o los resúmenes\n\n\nShow me the code\ndf_con_modelo$modelo[[2]]\n#> \n#> Call:\n#> lm(formula = y ~ x, data = df)\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>    0.990119    -0.007575\n# o equivalentemente\ndf_con_modelo %>% \n  pull(modelo) %>% \n  .[[5]]\n#> \n#> Call:\n#> lm(formula = y ~ x, data = df)\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>    0.999731    -0.004999\n\n\n\n\nShow me the code\ndf_con_modelo$resumen_modelo[[2]]\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  0.990      0.0317    31.3   3.15e-150\n#> 2 x           -0.00757    0.0311    -0.244 8.08e-  1\n\n\nHagamos un ejemplo más elaborado, usando datos de la librería gapminder más info en la ayuda de la misma\n\n\nShow me the code\nlibrary(gapminder)\n\nDT::datatable(gapminder)\n\n\n\n\n\n\n\nPodemos construirnos un dataset con datos agregados por país o por continente usando la función nest\n\n\nShow me the code\ndf <- gapminder %>%\n  group_by(continent) %>%\n  nest()\n\ndf\n#> # A tibble: 5 × 2\n#> # Groups:   continent [5]\n#>   continent data              \n#>   <fct>     <list>            \n#> 1 Asia      <tibble [396 × 5]>\n#> 2 Europe    <tibble [360 × 5]>\n#> 3 Africa    <tibble [624 × 5]>\n#> 4 Americas  <tibble [300 × 5]>\n#> 5 Oceania   <tibble [24 × 5]>\n\n\nY ahora en data tenemos los datos de cada continente\nAhora el objetivo es hacer un modelo para cada continente un modelo que relacione la esperanza de vida y el año.\nPara eso nos creamos un par de funciones, que usaremos en el map. Una para hacer el modelo y otra un gráfico.\n\n\nShow me the code\n\nmake_modelo_simple <-  function(df){\n  \n  modelo_simple <- function(df) {\n    lm(lifeExp ~ year , df)\n  }\n  # el uso de possibly mola. \n  modelo_safe <- possibly(modelo_simple, otherwise = NA) \n  return(modelo_safe(df))\n}\n\nplot_lifeexp <-  function(df){\n df %>% \n    ggplot(aes(x= year, y=lifeExp, color = country, group= country)) + \n    geom_point() +\n    geom_line() +\n    labs(title = \"LifeExp evolution\")\n\n}\n\n\nY ahora hacemos un gráfico para cada continente, un modelo por continente y obtenemos las predicciones.\n\n\nShow me the code\ndf2 <-  df %>%\n  mutate(\n    # modelo\n    modelo_x_continente = map(data, make_modelo_simple),\n    # grafico\n    plots = map(data, plot_lifeexp),\n    \n    # uso de map2 para funcion con dos argumentos\n    predicciones = map2(modelo_x_continente,\n                        data,\n                        function(modelo, datos)\n                          predict(modelo, newdata = datos)\n                        )\n  )\n\ndf2\n#> # A tibble: 5 × 5\n#> # Groups:   continent [5]\n#>   continent data               modelo_x_continente plots  predicciones\n#>   <fct>     <list>             <list>              <list> <list>      \n#> 1 Asia      <tibble [396 × 5]> <lm>                <gg>   <dbl [396]> \n#> 2 Europe    <tibble [360 × 5]> <lm>                <gg>   <dbl [360]> \n#> 3 Africa    <tibble [624 × 5]> <lm>                <gg>   <dbl [624]> \n#> 4 Americas  <tibble [300 × 5]> <lm>                <gg>   <dbl [300]> \n#> 5 Oceania   <tibble [24 × 5]>  <lm>                <gg>   <dbl [24]>\n\n\nY por ejemplo, el plot para Oceanía\n\n\nShow me the code\ndf2$plots[[5]]\n\n\n\n\n\n\n\n\n\nY su modelo y predicciones\n\n\nShow me the code\ndf2$modelo_x_continente[[5]]\n#> \n#> Call:\n#> lm(formula = lifeExp ~ year, data = df)\n#> \n#> Coefficients:\n#> (Intercept)         year  \n#>   -341.9080       0.2103\ndf2$predicciones[[5]]\n#>        1        2        3        4        5        6        7        8 \n#> 68.54372 69.59508 70.64644 71.69780 72.74917 73.80053 74.85189 75.90325 \n#>        9       10       11       12       13       14       15       16 \n#> 76.95461 78.00597 79.05734 80.10870 68.54372 69.59508 70.64644 71.69780 \n#>       17       18       19       20       21       22       23       24 \n#> 72.74917 73.80053 74.85189 75.90325 76.95461 78.00597 79.05734 80.10870"
  },
  {
    "objectID": "2021/03/13/purrr-furrr-o-cómo-se-diga/index.html#furrr",
    "href": "2021/03/13/purrr-furrr-o-cómo-se-diga/index.html#furrr",
    "title": "Purrr, furrr, maps y future_maps",
    "section": "furrr",
    "text": "furrr\n¿Y podemos hacer todo esto en paralelo? Pues si, y muy fácil con la librería furrr que usa la fantástica future. Con future podemos usar múltiples procesadores, múltiples sesiones, o incluso montar un cluster sobre varias máquinas usando MPI (eso si era computación distribuida y no spark , que me lo ha contado mi amigo Rubén, que fue sysadmin en uno de esos sistemas).\nPara usar furrr aparte de instalarlo lo que tenemos que especificar es el tipo de plan, yo recomiendo usar plan(multisession) que no da problemas con Rstudio como si da plan(multiprocess)\n\n\nShow me the code\nlibrary(furrr\n        )\nplan(multisession, workers = 5) # utilizo 5 workers  porque tengo 5 contintentes\n\n\nY para hacer lo mismo que con purrr solo necesitamos añadir el prefijo future a los map del ejemplo anterior y ya se hace en paralelo.\n\n\nShow me the code\ndf3 <-  df %>% \n  mutate(\n    modelo_x_continente = future_map(data,make_modelo_simple),\n    plots = future_map(data, plot_lifeexp), \n    predicciones = future_map2( modelo_x_continente,data,\n                         function(modelo,datos)\n                           predict(modelo, newdata = datos))\n  )\n\ndf3\n#> # A tibble: 5 × 5\n#> # Groups:   continent [5]\n#>   continent data               modelo_x_continente plots  predicciones\n#>   <fct>     <list>             <list>              <list> <list>      \n#> 1 Asia      <tibble [396 × 5]> <lm>                <gg>   <dbl [396]> \n#> 2 Europe    <tibble [360 × 5]> <lm>                <gg>   <dbl [360]> \n#> 3 Africa    <tibble [624 × 5]> <lm>                <gg>   <dbl [624]> \n#> 4 Americas  <tibble [300 × 5]> <lm>                <gg>   <dbl [300]> \n#> 5 Oceania   <tibble [24 × 5]>  <lm>                <gg>   <dbl [24]>\n\n\n\n\nShow me the code\ndf3$plots[[2]]\n\n\n\n\n\n\n\n\n\nBueno, espero que le sirva a mi colega."
  },
  {
    "objectID": "2021/03/08/auc-wilcoxon-de-nuevo/index.html",
    "href": "2021/03/08/auc-wilcoxon-de-nuevo/index.html",
    "title": "AUC = Wilcoxon , de nuevo",
    "section": "",
    "text": "Anda la gente que si viendo a ver cómo calcular el AUC (roc), que si cómo se hace en spark o que si hay que tener en cuenta muchos puntos de corte y ver las tablas de clasificación asociadas. Eso está bien para contarlo, pero no para calcularlo.\nEl AUC se puede considerar como la probabilidad de que el score de un “sí” elegido al azar es mayor que el score de un “no” elegido al azar. No lo digo yo, lo cuenta mucho mejor que yo , Carlos en su entrada AUC = WILCOXON. De hecho, voy a coger sus mismos datos, salvo por la semilla, que no la indica.\n\n\nMostrar / ocultar código\nset.seed(45)\nn <- 30\n\nsi <- data.frame(res = \"si\", score = rnorm(n, 1, 1))\nno <- data.frame(res = \"no\", score = rnorm(n, 0, 1))\n\ndat <- rbind(si, no)\n\n\nY como bien comenta Carlos, tanto Wilcoxon como el cálculo usual del AUC dan esa probabilidad. Para esto da igual que los scores sean incluso negativo, se trata de algo que da un orden.\n\n\nMostrar / ocultar código\ncar::some(dat)\n#>    res      score\n#> 1   si  1.3407997\n#> 2   si  0.2966597\n#> 7   si  0.4986218\n#> 9   si  2.8090374\n#> 12  si  1.2159889\n#> 34  no  0.2854323\n#> 40  no  0.2459699\n#> 48  no -0.3504094\n#> 53  no  0.1114906\n#> 60  no -0.8165108\n\n\nY copiando de su blog\n\n\nMostrar / ocultar código\ntest <- wilcox.test(score ~ res, data = dat)$statistic\n# pongo el 1- pq salia menor que 0.5\n1- test / n^2\n#>         W \n#> 0.7122222\n\n\n\n\nMostrar / ocultar código\nlibrary(pROC)\nmy_roc <- roc(dat$res, dat$score)\nauc(my_roc)\n#> Area under the curve: 0.7122\n\n\nPero también podemos hacerlo muestreando.\nNos creamos función\n\n\nMostrar / ocultar código\netiquetas <- dat$res==\"si\"\ntable(etiquetas)\n#> etiquetas\n#> FALSE  TRUE \n#>    30    30\n\n\nAhora muestreamos con reemplazamiento los scores correspondientes a etiqueta = TRUE, es decir a los sis\n\n\nMostrar / ocultar código\npos <- sample(dat$score[etiquetas], size = 1e7, replace = TRUE)\nneg <- sample(dat$score[!etiquetas], size = 1e7, replace = TRUE)\n\n\nY simplemente calculamos la probabilidad que buscamos haciendo la proporción de la veces qeu el score de un positivo gana al de un negativo.\n\n\nMostrar / ocultar código\nmean(pos>neg)\n#> [1] 0.7122233\n\n\nY es una buena aproximación, creemos función y pongamos que en caso de empate de score, ni pa ti ni pa mi.\n\n\nMostrar / ocultar código\nauc_probability <- function(labels, scores, N=1e7){\n  pos <- sample(scores[labels], N, replace=TRUE)\n  neg <- sample(scores[!labels], N, replace=TRUE)\n  (sum(pos > neg) + sum(pos == neg)/2) / N\n}\n\n\nPues ya tenemos una forma “sencilla” de calcular auc’s, que se puede usar por ejemplo en sql.\n\n\nMostrar / ocultar código\nauc_probability(etiquetas, dat$score, N= 1e6)\n#> [1] 0.712108\n\n\nSalud y buena tarde"
  },
  {
    "objectID": "2021/02/14/una-colina/index.html",
    "href": "2021/02/14/una-colina/index.html",
    "title": "Una colina",
    "section": "",
    "text": "Esta entrada es una fe de erratas de esta de hace casi dos años.\nLo de la colina viene porque Carlos me comentaba el otro día, que en mi entrada había algo raro, ya que era muy extraño que al resumir información el modelo fuera peor. Y efectivamente, había algo raro, cuándo se lo comenté y que me tocaba hacer una fe de erratas, me dijo que si no lo hacía ya tenía una “colina dónde morir”, lo cual no está mal del todo. Le conminé a que se explicara, en torno burlón evidentemente.\nEl tema es que hay gente que se aferra a una idea aunque luego se demuestre que estaban equivocados o que cometieron un error y muere en su pequeña colina defendiendo esa idea. No es mi costumbre aferrarme a mis ideas una vez se demuestran equivocadas (no, no pienso hablar de mis ideas políticas, porque para eso debería abrirme otro blog). Y bueno, hace poco alguien a quien aprecio me dijo que era muy borde, pero que no era cabezón, así que voy a comentar un poco los errores de la entrada de la codificación de variables categóricas."
  },
  {
    "objectID": "2021/02/14/una-colina/index.html#niños-futuro-pinchad-aquí-amantes-de-los-simpsons",
    "href": "2021/02/14/una-colina/index.html#niños-futuro-pinchad-aquí-amantes-de-los-simpsons",
    "title": "Una colina",
    "section": "Niños, futuro (pinchad aquí amantes de los simpsons)",
    "text": "Niños, futuro (pinchad aquí amantes de los simpsons)\nEl primer gran error en el post es, cómo no podía ser de otra, usar info del futuro para evaluar como de buenas eran las codificaciones mediante MCA o con embedding respecto a la codificación parcial (one-hot para los de la logse).\nEl tema es que construyo los modelos de MCA y de embedding usando el conjunto de datos completos, pero luego evalúo como de bien funcionan en un modelo en el que parto en entrenamiento y test. Así que en el entrenamiento estoy usando info del futuro, ya que parte de la estructura factorial (o los embeddings) se ha obtenido utilizando la información del test. Así que toca cambiar la función testRun para que tanto el MCA como el embedding se calculen sólo con los datos de entrenamiento.\n\n\nMostrar / ocultar código\ntestRun <- function(x) {\n    sample <- caret::createDataPartition(df$weekDayF, list = FALSE, p = 0.8)\n    train <- df[sample,]\n    test <- df[-sample,]\n    \n    #TODO El MCA y el embedding deben calcularse en train\n    # y obtener las proeyecciones en test\n    \n    fit1 <- lm(Manhattan.Bridge  ~ weekDayF, data = train) \n    fit2 <- lm(Manhattan.Bridge ~ X1 + X2 + X3, data = train)\n    fit3 <- lm(Manhattan.Bridge ~ MCA_1  + MCA_2 + MCA_3, data = train)\n    \n    data.frame(\n      run = x,\n      Categ      = sqrt(mean((predict(fit1, test) - test$Manhattan.Bridge) ^ 2)),\n      Embedding  = sqrt(mean((predict(fit2, test) - test$Manhattan.Bridge) ^ 2)),\n      Corresp    = sqrt(mean((predict(fit3, test) - test$Manhattan.Bridge) ^ 2))\n    )\n}"
  },
  {
    "objectID": "2021/02/14/una-colina/index.html#no-maltratemos-al-modelo-base",
    "href": "2021/02/14/una-colina/index.html#no-maltratemos-al-modelo-base",
    "title": "Una colina",
    "section": "No maltratemos al modelo base",
    "text": "No maltratemos al modelo base\nEl segundo gran error tiene que ver con una trampa que le hacemos al modelo base. Para el modelo base la única info que usamos para modelar la variable de usuarios de bici que cruzan el puente de Manhattan es el día de la semana. En cambio para el MCA y para el embedding usamos la relación que hay entre los usuarios que pasan por el puente de Brooklyn y el día de la semana.\nPara ser justos, debería haber utilizado la relación de los usuarios con el puente de Manhattan o bien haber añadido la variable de los usuarios del puente de Brooklyn al modelo base.\n\n\nMostrar / ocultar código\ntestRun <- function(x) {\n    sample <- caret::createDataPartition(df$weekDayF, list = FALSE, p = 0.8)\n    train <- df[sample,]\n    test <- df[-sample,]\n    \n    #TODO El MCA y el embedding deben calcularse en train\n    # y obtener las proeyecciones en test\n    \n    # Seamos justos con el modelo base, añadiendo la variable del puente de Brooklyn a \n    # los 3 modelos\n    fit1 <- lm(Manhattan.Bridge  ~ weekDayF + Brooklyn.Bridge, data = train) \n    fit2 <- lm(Manhattan.Bridge ~ X1 + X2 + X3 + Brooklyn.Bridge, data = train)\n    fit3 <- lm(Manhattan.Bridge ~ MCA_1  + MCA_2 + MCA_3 + Brooklyn.Bridge, data = train)\n    data.frame(\n      run = x,\n      Categ      = sqrt(mean((predict(fit1, test) - test$Manhattan.Bridge) ^ 2)),\n      Embedding  = sqrt(mean((predict(fit2, test) - test$Manhattan.Bridge) ^ 2)),\n      Corresp    = sqrt(mean((predict(fit3, test) - test$Manhattan.Bridge) ^ 2))\n    )\n}"
  },
  {
    "objectID": "2021/02/14/una-colina/index.html#detallitos",
    "href": "2021/02/14/una-colina/index.html#detallitos",
    "title": "Una colina",
    "section": "Detallitos",
    "text": "Detallitos\nY como tercer error, un pequeño detalle. Las variables obtenidas en el MCA no son las coordenadas obtenidas de la variable weekday en la estructura factorial, sino la predicción de nuevas filas, que tienen tanto weekday como los usuarios del puente de Manhattan (categorizado). Esto en principio no es ningún problema, pero estaría bien probar asignando sólo el valor del día de la semana proyectado en las componentes, sobre todo porque puede que en el test no tenga todas las combinaciones de día de la semana y los usuarios del puente de Manhattan categorizado que tenga en el train. Esta apreciación se la debo a Diego Serrano, ex compañero de curro y sin embargo amigo, que me la hizo en su día y tenía pendiente ponerlo.\nTotal, que en un próximo post pondré el código para corregir esto, que es domingo y lo que me toca es corregir ejercicios de alumnos. Ya no tengo colina dónde morir y toca abordar otros cerros."
  },
  {
    "objectID": "2021/02/14/una-colina/index.html#pd",
    "href": "2021/02/14/una-colina/index.html#pd",
    "title": "Una colina",
    "section": "PD",
    "text": "PD\nPor el momento dejo la comparación con parte de los errores corregidos. El MCA si que se obtiene sólo en train, pero el embedding aún está entrenado con todo, y se ha añadido la variable Manhattan.Bridge en la comparación de los 3 modelos.\n\n\n\ncomparacion"
  },
  {
    "objectID": "2021/01/26/cachitos-tercera-parte/index.html",
    "href": "2021/01/26/cachitos-tercera-parte/index.html",
    "title": "Cachitos. Tercera parte",
    "section": "",
    "text": "Después del último post llega el momento de ver si se puede sacar algo interesante del texto. Ya aviso ( y avisé) de que no tengo mucha idea de análisis de texto, por lo que esto es sólo un pequeño ejercicio que he hecho. El csv con el texto de los subtítulos para 2020 lo tenéis en este enlace.\nVamos al lío\n\n\nMostrar / ocultar código\n\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno <- \"2020\"\n\n\nLeemos el csv. Uso DT y así podéis ver todos los datos o buscar cosas, por ejemplo Ayuso o pandemia en el cuadro de búsqueda.\n\n\nMostrar / ocultar código\nsubtitulos_proces <-  read_csv(str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\nsubtitulos_proces %>% \n  select(texto, n_fichero, n_caracteres) %>% \n  DT::datatable()\n\n\n\n\n\n\n\nOye, pues sólo con esto ya nos valdría ¿no?\nPero veamos un poco algunas cosas que podrían hacerse, por ejemplo quitar stopwords. Esto es tan sencillo como tener una lista de palabras que queremos quitar del texto, puede ser nuestra particular, que nos hayamos bajado de algún sitio o que estén disponibles en algún lado\n\n\nMostrar / ocultar código\nto_remove <- c(tm::stopwords(\"es\"),\n               \"110\", \"4\",\"1\",\"2\",\"7\",\"10\",\"0\",\"ñ\",\"of\",\n               \"5\",\"á\",\"i\",\"the\",\"3\", \"n\", \"p\",\n               \"ee\",\"uu\",\"mm\",\"ema\", \"zz\",\n               \"wr\",\"wop\",\"wy\",\"x\",\"xi\",\"xl\",\"xt\",\n               \"xte\",\"yí\", \"your\", \"si\")\n\nhead(to_remove, 40)\n#>  [1] \"de\"      \"la\"      \"que\"     \"el\"      \"en\"      \"y\"       \"a\"      \n#>  [8] \"los\"     \"del\"     \"se\"      \"las\"     \"por\"     \"un\"      \"para\"   \n#> [15] \"con\"     \"no\"      \"una\"     \"su\"      \"al\"      \"lo\"      \"como\"   \n#> [22] \"más\"     \"pero\"    \"sus\"     \"le\"      \"ya\"      \"o\"       \"este\"   \n#> [29] \"sí\"      \"porque\"  \"esta\"    \"entre\"   \"cuando\"  \"muy\"     \"sin\"    \n#> [36] \"sobre\"   \"también\" \"me\"      \"hasta\"   \"hay\"\n\n\nPero en nuestros datos, las palabras no están separadas, tendríamos que separarlas y luego quitar las que no queremos. Para eso voy a utilizar la librería tidytext de Julia Silge y David Robinson, que nos permite hacer varias cosas relacionadas con análisis de texto.\n\n\nMostrar / ocultar código\nlibrary(tidytext)\n\n# Con unnest token pasamos a un dataframe qeu tiene tantas filas como palabras\n\nprint(str_glue(\"Filas datos originales: {tally(subtitulos_proces)}\"))\n#> Filas datos originales: 541\n\nsubtitulos_proces_one_word <- subtitulos_proces %>% \n    unnest_tokens(input = texto,\n                  output = word) %>% \n    filter(! word %in% to_remove) %>% # quito palabras de la lista \n    filter(nchar(word)>1) # Nos quedamos con palabras que tengan más de un cáracter\n\n\nprint(str_glue(\"Filas datos tokenizado: {tally(subtitulos_proces_one_word)}\"))\n#> Filas datos tokenizado: 3688\n\nsubtitulos_proces_one_word %>% \n  select(name,n_fichero,word, n_caracteres)\n#> # A tibble: 3,688 × 4\n#>     name n_fichero                      word      n_caracteres\n#>    <dbl> <chr>                          <chr>            <dbl>\n#>  1    14 00000014.jpg.subtitulo.tif.txt después             92\n#>  2    14 00000014.jpg.subtitulo.tif.txt añito               92\n#>  3    14 00000014.jpg.subtitulo.tif.txt pasado              92\n#>  4    14 00000014.jpg.subtitulo.tif.txt aman                92\n#>  5    14 00000014.jpg.subtitulo.tif.txt consuela            92\n#>  6    14 00000014.jpg.subtitulo.tif.txt quiere              92\n#>  7    15 00000015.jpg.subtitulo.tif.txt viendo              62\n#>  8    15 00000015.jpg.subtitulo.tif.txt actitud             62\n#>  9    15 00000015.jpg.subtitulo.tif.txt público             62\n#> 10    15 00000015.jpg.subtitulo.tif.txt actuación           62\n#> # … with 3,678 more rows\n\n\nUna cosa simple que podemos hacer es contar palabras, y vemos que lo que más se repite es canción, obvio\n\n\nMostrar / ocultar código\npalabras_ordenadas <- subtitulos_proces_one_word %>% \n    group_by(word) %>% \n    summarise(veces = n()) %>% \n    arrange(desc(veces))\n\npalabras_ordenadas %>% \n    slice(1:20) %>% \n    ggplot(aes(x = reorder(word, veces), y = veces)) +\n    geom_col(show.legend = FALSE) +\n    ylab(\"veces\") +\n    xlab(\"\") +\n    coord_flip() +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nO pintarlas en plan nube de palabras.\n\n\nMostrar / ocultar código\nlibrary(wordcloud)\npal <- brewer.pal(8,\"Dark2\")\nsubtitulos_proces_one_word %>% \n    group_by(word) %>% \n    count() %>% \n    with(wordcloud(word, n, random.order = FALSE, max.words = 80, colors=pal))    \n\n\n\n\n\n\n\n\n\nPues una vez que tenemos las palabras de cada subtítulo separadas podemos buscar palabras polémicas, aunque antes al usar la librería DT ya podíamos buscar, veamos como sería con el código.\nCreamos lista de palabras a buscar.\n\n\nMostrar / ocultar código\npalabras_1 <- c(\"monarca\",\"pp\",\"vox\",\"rey\",\"coron\",\"zarzuela\",\n                \"prisión\", \"democracia\", \"abascal\",\"casado\",\n                \"ultra\",\"ciudada\", \"oposición\",\"derech\",\n                \"podem\",\"sanchez\",\"iglesias\",\"errejon\",\"izquier\",\n                \"gobierno\",\"illa\",\"redondo\",\"ivan\",\"celaa\",\n                \"guardia\",\"príncipe\",\"principe\",\"ayuso\",\n                \"tezanos\",\"cis\",\"republic\", \"simon\", \"pandem\",\"lazo\",\n                \"toled\",\"alber\",\"fach\", \"zarzu\", \"democr\",\"vicepre\", \"minist\",\n                \"irene\",\"montero\",\"almeida\")\n\n\nConstruimos una regex para que encuentre las palabras que empiecen así.\n\n\nMostrar / ocultar código\n(exp_regx <- paste0(\"^\",paste(palabras_1, collapse = \"|^\")))\n#> [1] \"^monarca|^pp|^vox|^rey|^coron|^zarzuela|^prisión|^democracia|^abascal|^casado|^ultra|^ciudada|^oposición|^derech|^podem|^sanchez|^iglesias|^errejon|^izquier|^gobierno|^illa|^redondo|^ivan|^celaa|^guardia|^príncipe|^principe|^ayuso|^tezanos|^cis|^republic|^simon|^pandem|^lazo|^toled|^alber|^fach|^zarzu|^democr|^vicepre|^minist|^irene|^montero|^almeida\"\n\n\nY nos creamos una variable que valga TRUE cuando suceda esto\n\n\nMostrar / ocultar código\n\nsubtitulos_proces_one_word <- subtitulos_proces_one_word %>% \n    mutate(polemica= str_detect(word, exp_regx))\n\nsubtitulos_proces_one_word %>% \n  filter(polemica) %>% \n  select(name, word, n_fichero) \n#> # A tibble: 38 × 3\n#>     name word     n_fichero                     \n#>    <dbl> <chr>    <chr>                         \n#>  1   193 gobierno 00000193.jpg.subtitulo.tif.txt\n#>  2   193 pandemia 00000193.jpg.subtitulo.tif.txt\n#>  3   222 montero  00000222.jpg.subtitulo.tif.txt\n#>  4   222 montero  00000222.jpg.subtitulo.tif.txt\n#>  5   300 illa     00000300.jpg.subtitulo.tif.txt\n#>  6   308 reyes    00000308.jpg.subtitulo.tif.txt\n#>  7   308 pandemia 00000308.jpg.subtitulo.tif.txt\n#>  8   343 prisión  00000343.jpg.subtitulo.tif.txt\n#>  9   357 zarzuela 00000357.jpg.subtitulo.tif.txt\n#> 10   363 abascal  00000363.jpg.subtitulo.tif.txt\n#> # … with 28 more rows\n\n\nPodríamos ver el texto de los subtítulos, para eso, nos quedamos con un identificador, como el nombre del fichero txt, que nos servirá luego para leer la imagen.\nPues en realidad tenemos sólo 32 subtítulos polémicos de los de alrededor de 540 que hay, no parecen muchos.\n\n\nMostrar / ocultar código\nsubtitulos_polemicos <- subtitulos_proces_one_word %>% \n    filter(polemica) %>% \n    pull(n_fichero) %>% \n    unique()\nsubtitulos_polemicos\n#>  [1] \"00000193.jpg.subtitulo.tif.txt\" \"00000222.jpg.subtitulo.tif.txt\"\n#>  [3] \"00000300.jpg.subtitulo.tif.txt\" \"00000308.jpg.subtitulo.tif.txt\"\n#>  [5] \"00000343.jpg.subtitulo.tif.txt\" \"00000357.jpg.subtitulo.tif.txt\"\n#>  [7] \"00000363.jpg.subtitulo.tif.txt\" \"00000471.jpg.subtitulo.tif.txt\"\n#>  [9] \"00000508.jpg.subtitulo.tif.txt\" \"00000510.jpg.subtitulo.tif.txt\"\n#> [11] \"00000531.jpg.subtitulo.tif.txt\" \"00000551.jpg.subtitulo.tif.txt\"\n#> [13] \"00000557.jpg.subtitulo.tif.txt\" \"00000598.jpg.subtitulo.tif.txt\"\n#> [15] \"00000632.jpg.subtitulo.tif.txt\" \"00000638.jpg.subtitulo.tif.txt\"\n#> [17] \"00000640.jpg.subtitulo.tif.txt\" \"00000670.jpg.subtitulo.tif.txt\"\n#> [19] \"00000702.jpg.subtitulo.tif.txt\" \"00000760.jpg.subtitulo.tif.txt\"\n#> [21] \"00000830.jpg.subtitulo.tif.txt\" \"00000893.jpg.subtitulo.tif.txt\"\n#> [23] \"00000896.jpg.subtitulo.tif.txt\" \"00000948.jpg.subtitulo.tif.txt\"\n#> [25] \"00001010.jpg.subtitulo.tif.txt\" \"00001037.jpg.subtitulo.tif.txt\"\n#> [27] \"00001057.jpg.subtitulo.tif.txt\" \"00001115.jpg.subtitulo.tif.txt\"\n#> [29] \"00001122.jpg.subtitulo.tif.txt\" \"00001142.jpg.subtitulo.tif.txt\"\n#> [31] \"00001143.jpg.subtitulo.tif.txt\" \"00001229.jpg.subtitulo.tif.txt\"\n\n\nVemos el texto mirando en el dataframe antes de separar las palabras. La verdad es que hay que reconocer que son bastante ingeniosos, jejje. Aunque hay algún falso positivo como el de “la carta a los reyes magos de la post pandemia 4 pan alegría y ertes” y alguno más. La verdad es que un pelín de sesgo se les nota, de meterse más con la oposición que con el gobierno comparado con lo del año pasado (probad)\n\n\nMostrar / ocultar código\n(texto_polemicos <- subtitulos_proces %>% \n    filter(n_fichero %in% subtitulos_polemicos) %>% \n    arrange(n_fichero) %>% \n    pull(texto))\n#>  [1] \"esta fue la lógica del gobierno al ceder la responsabilidad 4 del control de la pandemia a las comunidades autónomas\"    \n#>  [2] \"ante montero y post montero\"                                                                                             \n#>  [3] \"ahí parecían formales pero ya cerraban más bares que salvador illa\"                                                      \n#>  [4] \"la carta a los reyes magos de la post pandemia 4 pan alegría y ertes\"                                                    \n#>  [5] \"cintas amarillas gente en prisión pasó hace 3 años si eso ponéis vosotros el rótulo y ya os llama marchena\"              \n#>  [6] \"canción romántica rock zarzuela yeyé versátil como bowie pero más conservador en cuestión de peinados\"                   \n#>  [7] \"el único elemento rojo en un entorno blanco paco es el bote de pimentón en el despacho de santiago abascal\"              \n#>  [8] \"no has querido ver el vídeo de las vacaciones de tu cuñado y nosotros te estamos colando las de julio iglesias\"          \n#>  [9] \"príncipe gitano ditah\"                                                                                                   \n#> [10] \"es increíble la que se lía en barcelona cada vez que la pisa un príncipe\"                                                \n#> [11] \"según un informe de la guardia civil la culpa fue del cha cha chá y de la mani del 8 m\"                                  \n#> [12] \"suele decirse que una de las mejores cosas de nuestro país es su luz no podemos estar más de acuerdo\"                    \n#> [13] \"esta es la música que sonaba de fondo cuando casado se hizo la foto ante el espejo del baño\"                             \n#> [14] \"nos ahorraremos decir que era el rey de la rumba porque p este año todo lo que lleva corona ha ido regular\"              \n#> [15] \"o ortega e isabel montero cre la hiedra\"                                                                                 \n#> [16] \"aprovechamos para celebrar la llegada de la primera mujer a la vicepresidencia de los estados unidos camela harri\"       \n#> [17] \"su mayor éxito lo compuso miguel angel cabrera el teclista de la derecha que está currándose una tendi\"                  \n#> [18] \"esto les puso ayuso a los albañiles de su hospital por megafonía y ni aún así oye\"                                       \n#> [19] \"este es su mayor éxito y se basó en un libro sobre elvis tenemos fe en que esté vivo y reclame los derechos\"             \n#> [20] \"cuando pienses que la pandemia te ha afectado recuerda que a geo le ha robado la inspiración del 100 de sus temas\"       \n#> [21] \"la pandemia interrumpió su gira de 40 aniversario carlos segarra al rock and roll con las dos manos\"                     \n#> [22] \"en ella esos colores conjuntan mejor que en el gobierno de coalición\"                                                    \n#> [23] \"3 la fernando simón del pp pop internacional\"                                                                            \n#> [24] \"mucho antes de su coaching durante la pandemia p karina ya lanzaba mensajes de taza de desayuno\"                         \n#> [25] \"si aute no fue capaz de entender el mundo quiénes somos nosotros para creer que podemos\"                                 \n#> [26] \"hay que tener mucho cuidado con las fiestas te despistas un momento y la ultraderecha se te cuela en la de la democracia\"\n#> [27] \"iván redondo está apuntando todo para el próximo discurso sobre la nueva normalidad\"                                     \n#> [28] \"on ppy children\"                                                                                                         \n#> [29] \"el baile reproduce la rara habilidad de pp y vox darse la mano y la espalda al mismo tiempo ll\"                          \n#> [30] \"en la zarzuela ha sonado más la de se fue\"                                                                               \n#> [31] \"el currículum amoroso de laura pausini tiene más abandonos que las listas de ciudadanos\"                                 \n#> [32] \"seria la versión latina del compañero de mimos que propuso el gobierno belga en el confinamiento y qué mimos\"\n\n\nPodemos ver las imágenes\n\n\nMostrar / ocultar código\n(polemica_fotogramas <- unique(substr(subtitulos_polemicos, 1,12)))\n#>  [1] \"00000193.jpg\" \"00000222.jpg\" \"00000300.jpg\" \"00000308.jpg\" \"00000343.jpg\"\n#>  [6] \"00000357.jpg\" \"00000363.jpg\" \"00000471.jpg\" \"00000508.jpg\" \"00000510.jpg\"\n#> [11] \"00000531.jpg\" \"00000551.jpg\" \"00000557.jpg\" \"00000598.jpg\" \"00000632.jpg\"\n#> [16] \"00000638.jpg\" \"00000640.jpg\" \"00000670.jpg\" \"00000702.jpg\" \"00000760.jpg\"\n#> [21] \"00000830.jpg\" \"00000893.jpg\" \"00000896.jpg\" \"00000948.jpg\" \"00001010.jpg\"\n#> [26] \"00001037.jpg\" \"00001057.jpg\" \"00001115.jpg\" \"00001122.jpg\" \"00001142.jpg\"\n#> [31] \"00001143.jpg\" \"00001229.jpg\"\n\npolemica_fotogramas_full <- paste0(str_glue(\"{root_directory}video/{anno}_jpg/\"), polemica_fotogramas)\n\nsubtitulos_polemicos_full <- paste0(polemica_fotogramas_full,\".subtitulo.tif\")\n\n\nY ahora utilizando la librería magick en R y un poco de programación funcional (un simple map), tenemos la imagen leída\n\n\nMostrar / ocultar código\nlibrary(magick)\n\nfotogramas_polemicos_img <- map(polemica_fotogramas_full, image_read)\nsubtitulos_polemicos_img <- map(subtitulos_polemicos_full, image_read)\n\n\n\n\nMostrar / ocultar código\nsubtitulos_polemicos_img[[31]]\n\n\n\n\n\n\n\n\n\n\n\nMostrar / ocultar código\nfotogramas_polemicos_img[[31]]\n\n\n\n\n\n\n\n\n\nUhmm, la verdad es que podría montar un shiny que dada una palabra mostrara el fotograma, sería sencillo.\nO podriamos ponerlos todos juntos, la verdad es que magick mola\n\n\nMostrar / ocultar código\nlista_fotogram_polemicos <- map(fotogramas_polemicos_img, grid::rasterGrob)\ngridExtra::grid.arrange(grobs=lista_fotogram_polemicos)\n\n\n\n\n\n\n\n\n\nRealmente creo que falta mucha limpieza del texto, por lo que me cuentan los que saben el trabajo de verdad en texto es ese.\nMás cositas que se me ocurrieron hacer, por ejemplo ver ngramas. Para eso puedo recomponer los comentarios a partir de subtitulos_proces_one_word que ya tienen palabras quitadas.\nFijaros en este código\n\n\nMostrar / ocultar código\n\nn = 4\nsubtitulos_proces_one_word %>% \n    group_by(name, n_fichero) %>% \n    nest(data = word) %>% \n    mutate(texto = map(data, unlist), \n           texto = map_chr(texto, paste, collapse = \" \")) %>% \n    unnest_tokens(input = texto,\n                  output = ngramas,token = \"ngrams\", n = n) %>% \n    ungroup() %>% \n    select(n_fichero,ngramas) %>%\n    filter(nchar(ngramas)>2) %>% \n    group_by(ngramas) %>% \n    summarise(veces = n()) %>% \n    arrange(desc(veces)) %>% \n    top_n(20, veces)\n\n\nVamos por cachos, valga la redundancia.\nA partir de las palabras puedo recomponer el subtítulo original porque tengo el identificador, para eso la función nest es muy útil. Yo a veces utilizo esta función para almacenar en un elemento de una columna un dataframe enteror.\n\n\nMostrar / ocultar código\nsubtitulos_proces_one_word %>% \n    group_by(name, n_fichero) %>% \n    nest(data = word) %>% \n  select(name, data)\n#> # A tibble: 573 × 3\n#> # Groups:   name, n_fichero [541]\n#>    n_fichero                       name data             \n#>    <chr>                          <dbl> <list>           \n#>  1 00000014.jpg.subtitulo.tif.txt    14 <tibble [6 × 1]> \n#>  2 00000015.jpg.subtitulo.tif.txt    15 <tibble [4 × 1]> \n#>  3 00000016.jpg.subtitulo.tif.txt    16 <tibble [6 × 1]> \n#>  4 00000019.jpg.subtitulo.tif.txt    19 <tibble [4 × 1]> \n#>  5 00000020.jpg.subtitulo.tif.txt    20 <tibble [12 × 1]>\n#>  6 00000021.jpg.subtitulo.tif.txt    21 <tibble [10 × 1]>\n#>  7 00000025.jpg.subtitulo.tif.txt    25 <tibble [4 × 1]> \n#>  8 00000026.jpg.subtitulo.tif.txt    26 <tibble [6 × 1]> \n#>  9 00000027.jpg.subtitulo.tif.txt    27 <tibble [8 × 1]> \n#> 10 00000031.jpg.subtitulo.tif.txt    31 <tibble [9 × 1]> \n#> # … with 563 more rows\n\n\nEn este caso para cada name y n_fichero ha generado un tibble, de una sola columna y de tantas filas como palabras.\n\n\nMostrar / ocultar código\nsubtitulos_proces_one_word %>% \n    group_by(name, n_fichero) %>% \n    nest(data = word) %>% \n  ungroup() %>% \n  slice(1:2) %>% \n  pull(data)\n#> [[1]]\n#> # A tibble: 6 × 1\n#>   word    \n#>   <chr>   \n#> 1 después \n#> 2 añito   \n#> 3 pasado  \n#> 4 aman    \n#> 5 consuela\n#> 6 quiere  \n#> \n#> [[2]]\n#> # A tibble: 4 × 1\n#>   word     \n#>   <chr>    \n#> 1 viendo   \n#> 2 actitud  \n#> 3 público  \n#> 4 actuación\n\n\nEl resto de funciones es convertir esa lista en vector de caracteres, juntar las palabras y separar por espacios, extraer los n_gramas de tamaño 4 palabras, contar cuántas veces aparece cada n_grama y ver los 20 más frecuentes. Con esto lo que se puede detectar son subtítulos que aparezcan duplicados y se nos hayan escapado por la distancia de strings que usamos en el post anterior\n\n\nMostrar / ocultar código\n\nn = 4\nsubtitulos_proces_one_word %>% \n    group_by(name, n_fichero) %>% \n    nest(data = word) %>% \n    mutate(texto = map(data, unlist), \n           texto = map_chr(texto, paste, collapse = \" \")) %>% \n    unnest_tokens(input = texto,\n                  output = ngramas,token = \"ngrams\", n = n) %>% \n    ungroup() %>% \n    select(n_fichero,ngramas) %>%\n    filter(nchar(ngramas)>2) %>% \n    group_by(ngramas) %>% \n    summarise(veces = n()) %>% \n    arrange(desc(veces)) %>% \n    top_n(20, veces)\n#> # A tibble: 2,079 × 2\n#>    ngramas                           veces\n#>    <chr>                             <int>\n#>  1 alto nunca llegó ser                  2\n#>  2 aunque fama saturó supo               2\n#>  3 aute capaz entender mundo             2\n#>  4 después dos décadas elegido           2\n#>  5 fama saturó supo volver               2\n#>  6 justo reconocer submarino beatles     2\n#>  7 llegó ser remero harvard              2\n#>  8 mujer tan valiente después            2\n#>  9 nunca llegó ser remero                2\n#> 10 reconocer submarino beatles menos     2\n#> # … with 2,069 more rows\n\n\nEn el próximo post veremos algo más, que estoy “cansao de to el día”."
  },
  {
    "objectID": "2021/01/13/cachitos-segunda-parte/index.html",
    "href": "2021/01/13/cachitos-segunda-parte/index.html",
    "title": "Cachitos. Segunda parte",
    "section": "",
    "text": "En el post anterior vimos como extraer 1 de cada n fotogramas de un video, recortar una zona en concreto y pasarle un software de reconocimiento óptico de caracteres para tener el texto. En esta parte vamos a ver como leer esos ficheros de texto y también una de las formas de quitar subtítulos duplicados. Para eso vamos a utilizar R. Vamos al lío.\nEjecuto el script extract_subtitles.sh del post anterior de la siguiente forma.\n./extract_subtitles.sh 2020\n./extract_subtitles.sh 2019\nSe baja el video desde alacarta, recorta los subtítulos y obtiene el texto. La estructura de directorios que me crea en dónde le haya dicho que es el root_directory es\n\n╰─ $ ▶ tree -d\n.\n├── 2019_txt\n├── 2020_txt\n└── video\n    ├── 2019_jpg\n    └── 2020_jpg\nDónde en video tenemos los dos videos en mp4, y los directorios con los fotogramas originales junto con los subtítulos, y en los directorios anno_txt cada uno de los ficheros de texto correspondientes a los fotogramas.\n╰─ $ ▶ ll 2020_txt | head -n 20\ntotal 5456\ndrwxrwxr-x 2 jose jose 77824 ene 11 20:51 ./\ndrwxrwxr-x 8 jose jose  4096 ene 13 19:41 ../\n-rw-rw-r-- 1 jose jose     1 ene  4 13:07 00000001.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     1 ene  4 13:06 00000002.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     1 ene  4 13:07 00000003.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     1 ene  4 13:08 00000004.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     3 ene  4 13:07 00000005.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     3 ene  4 13:07 00000006.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     3 ene  4 13:07 00000007.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     3 ene  4 13:06 00000008.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     1 ene  4 13:07 00000009.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     3 ene  4 13:08 00000010.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     1 ene  4 13:08 00000011.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     6 ene  4 13:07 00000012.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose    24 ene  4 13:06 00000013.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose    94 ene  4 13:07 00000014.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose    65 ene  4 13:07 00000015.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose    93 ene  4 13:06 00000016.jpg.subtitulo.tif.txt\n-rw-rw-r-- 1 jose jose     1 ene  4 13:06 00000017.jpg.subtitulo.tif.txt\n\n\n╰─ $ ▶ ll 2020_txt | wc -l\n1347\n\nY vemos que hay 1347 ficheros txt, y algunos muy pequeños (los que no tienen texto)\nVeamos el 00000016.jpg.subtitulo.tif.txt\n╰─ $ ▶ cat 2020_txt/00000016.jpg.subtitulo.tif.txt\nViendo la actitud del público, más que una actuación\nesto es una sesión de coaching.\nPues vamos a leerlos todos usando R.\n\n\nMostrar / ocultar código\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno <- \"2020\"\n\n# Construims un data frame con los nombrs de los ficheros \n\nnombre_ficheros <- list.files(path = str_glue(\"{root_directory}{anno}_txt/\")) %>% \n    enframe() %>% \n    rename(n_fichero = value)\n\nnombre_ficheros\n#> # A tibble: 1,344 × 2\n#>     name n_fichero                     \n#>    <int> <chr>                         \n#>  1     1 00000001.jpg.subtitulo.tif.txt\n#>  2     2 00000002.jpg.subtitulo.tif.txt\n#>  3     3 00000003.jpg.subtitulo.tif.txt\n#>  4     4 00000004.jpg.subtitulo.tif.txt\n#>  5     5 00000005.jpg.subtitulo.tif.txt\n#>  6     6 00000006.jpg.subtitulo.tif.txt\n#>  7     7 00000007.jpg.subtitulo.tif.txt\n#>  8     8 00000008.jpg.subtitulo.tif.txt\n#>  9     9 00000009.jpg.subtitulo.tif.txt\n#> 10    10 00000010.jpg.subtitulo.tif.txt\n#> # … with 1,334 more rows\n\n\nAhora los podemos leer en orden\n\n\nMostrar / ocultar código\nsubtitulos <-  list.files(path = str_glue(\"{root_directory}{anno}_txt/\"), \n                        pattern = \"*.txt\", full.names = TRUE) %>% \n    map(~read_file(.)) %>% \n    enframe() %>%  \n  # hacemos el join con el dataframe anterior para tener el nombre del fichero original\n    left_join(nombre_ficheros)\n\nglimpse(subtitulos)\n#> Rows: 1,344\n#> Columns: 3\n#> $ name      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n#> $ value     <list> \"\\f\", \"\\f\", \"\\f\", \"\\f\", \" \\n\\f\", \" \\n\\f\", \" \\n\\f\", \" \\n\\f\",…\n#> $ n_fichero <chr> \"00000001.jpg.subtitulo.tif.txt\", \"00000002.jpg.subtitulo.ti…\nsubtitulos\n#> # A tibble: 1,344 × 3\n#>     name value     n_fichero                     \n#>    <int> <list>    <chr>                         \n#>  1     1 <chr [1]> 00000001.jpg.subtitulo.tif.txt\n#>  2     2 <chr [1]> 00000002.jpg.subtitulo.tif.txt\n#>  3     3 <chr [1]> 00000003.jpg.subtitulo.tif.txt\n#>  4     4 <chr [1]> 00000004.jpg.subtitulo.tif.txt\n#>  5     5 <chr [1]> 00000005.jpg.subtitulo.tif.txt\n#>  6     6 <chr [1]> 00000006.jpg.subtitulo.tif.txt\n#>  7     7 <chr [1]> 00000007.jpg.subtitulo.tif.txt\n#>  8     8 <chr [1]> 00000008.jpg.subtitulo.tif.txt\n#>  9     9 <chr [1]> 00000009.jpg.subtitulo.tif.txt\n#> 10    10 <chr [1]> 00000010.jpg.subtitulo.tif.txt\n#> # … with 1,334 more rows\n\n\nen n_fichero tenemos el nombre y en value el texto\n\n\nMostrar / ocultar código\n\nsubtitulos %>% \n  pull(value) %>%\n  ## usamos `[[` que es el operador para acceder a la lista el que normalemente se usa [[nombre_elemento]]\n  `[[`(16)\n#> [1] \"Viendo la actitud del público, más que una actuación\\nesto es una sesión de coaching.\\n\\n \\n\\f\"\n\n# equivalentemente\n\n# subtitulos %>% \n#     pull(value) %>% \n#     pluck(16)\n\n\nComo sabemos que hay muchos ficheros sin texto podemos contar letras.\n\n\nMostrar / ocultar código\nsubtitulos <- subtitulos %>% \n    mutate(n_caracteres = nchar(value)) \n\nsubtitulos %>% \n    group_by(n_caracteres) %>% \n    count()\n#> # A tibble: 127 × 2\n#> # Groups:   n_caracteres [127]\n#>    n_caracteres     n\n#>           <int> <int>\n#>  1            1   480\n#>  2            3   125\n#>  3            4    17\n#>  4            5     7\n#>  5            6    13\n#>  6            7     2\n#>  7            8     7\n#>  8            9     6\n#>  9           10     4\n#> 10           11     5\n#> # … with 117 more rows\n\nsubtitulos %>% \n    group_by(n_caracteres) %>% \n    count() %>% \n  ggplot(aes(x = n_caracteres, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nY vemos que hay muchos subtitulos con pocos caracteres. Si vemos por ejemplo los que tienen 8 caracteres\n\n\nMostrar / ocultar código\nsubtitulos %>% \n    filter(n_caracteres ==8) %>% \n    pull(value)\n#> [[1]]\n#> [1] \"LEN As\\n\\f\"\n#> \n#> [[2]]\n#> [1] \"pro\\n\\nÑ\\n\\f\"\n#> \n#> [[3]]\n#> [1] \"ñ Xd a\\n\\f\"\n#> \n#> [[4]]\n#> [1] \"/ EI\\nE\\n\\f\"\n#> \n#> [[5]]\n#> [1] \"TOY ES\\n\\f\"\n#> \n#> [[6]]\n#> [1] \"110\\n\\ny\\n\\f\"\n#> \n#> [[7]]\n#> [1] \"steria\\n\\f\"\n\n\nQue se corresponden con haber pillado parte no del subtítulo sino del nombre de la actuación\n\n\nMostrar / ocultar código\nsubtitulos %>% \n    filter(n_caracteres ==8)\n#> # A tibble: 7 × 4\n#>    name value     n_fichero                      n_caracteres\n#>   <int> <list>    <chr>                                 <int>\n#> 1   207 <chr [1]> 00000207.jpg.subtitulo.tif.txt            8\n#> 2   252 <chr [1]> 00000252.jpg.subtitulo.tif.txt            8\n#> 3   321 <chr [1]> 00000321.jpg.subtitulo.tif.txt            8\n#> 4   339 <chr [1]> 00000339.jpg.subtitulo.tif.txt            8\n#> 5   442 <chr [1]> 00000442.jpg.subtitulo.tif.txt            8\n#> 6   494 <chr [1]> 00000494.jpg.subtitulo.tif.txt            8\n#> 7   722 <chr [1]> 00000722.jpg.subtitulo.tif.txt            8\n\n\nUsando la librería magick en R que permite usar imagemagick en R, ver post de Raúl Vaquerizo y su homenaje a Sean Connery, podemos ver el fotgrama correspondiente\n\n\nMostrar / ocultar código\nlibrary(magick)\n(directorio_imagenes <- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#> /media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/video/2020_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00000207.jpg\"))\n\n\n\n\n\n\n\n\n\nTambién podemos ver hasta cuando pasa eso, por ejemplo si vemos subtítulos con 18 caracteres\n\n\nMostrar / ocultar código\nsubtitulos %>% \n    filter(n_caracteres ==18) %>% \n    pull(value)\n#> [[1]]\n#> [1] \" \\n\\nA BAEZA\\n\\n ———\\n\\f\"\n#> \n#> [[2]]\n#> [1] \"Descanse en Pau.\\n\\f\"\n#> \n#> [[3]]\n#> [1] \"VEL y BIMBA BOSÉ\\n\\f\"\n#> \n#> [[4]]\n#> [1] \"IIS >>\\n\\npd.\\ndd >\\n\\f\"\n\n\nVemos que también pasa, pero ya vamos pillando rótulos de verdad como el “Descanse en Pau” que pusieron ante una actuación de Pau Donés.\nComo vemos hay que hacer limpieza, pero por el momento vamos a quedarnos con los subtítulos con número de caracteres mayor de 17. Esta decisión hace que perdamos algunos subtítulos de verdad, como por ejemplo el conocido “Loco Vox”.\n\n\nMostrar / ocultar código\nsubtitulos <- subtitulos %>% \n    filter(n_caracteres > 17) \n\nglimpse(subtitulos)\n#> Rows: 664\n#> Columns: 4\n#> $ name         <int> 13, 14, 15, 16, 19, 20, 21, 22, 25, 26, 27, 31, 32, 33, 3…\n#> $ value        <list> \" \\n\\nA BAEZA\\n\\n ———\\n\\f\", \"Después del añito que hemos…\n#> $ n_fichero    <chr> \"00000013.jpg.subtitulo.tif.txt\", \"00000014.jpg.subtitulo…\n#> $ n_caracteres <int> 18, 92, 62, 89, 50, 112, 114, 114, 31, 91, 78, 117, 98, 9…\n\n\nPues ya hemos pasado de más de 1000 rótulos a 664. Pero sabemos, por el post anterior que hay algunos duplicados.\nCon el fin de detectar cuáles están duplicados y aprovechando que están en orden de aparición, podemos hacer utilizar distancias de texto para calcular la distancia de cada subtítulo con el anterior, y si la distancia es pequeña es que es el mismo rótulo.\nPrimero hacemos una minilimpieza.\n\n\nMostrar / ocultar código\nstring_mini_clean <-  function(string){\n    string <- gsub(\"?\\n|\\n\", \" \", string)\n    string <- gsub(\"\\r|?\\f|=\", \" \", string)\n    string <- gsub('“|”|—|>',\" \", string)\n    \n    string <- gsub(\"[[:punct:][:blank:]]+\", \" \", string)\n    string <- tolower(string)\n    string <- gsub(\"  \", \" \", string)\n    string <-  \n    \n    return(string)\n}\n\n# Haciendo uso de programacion funciona con purrr es muy fácil pasar esta función a cada elemento. y decirle que # el reultado es string con map_chr\n\nsubtitulos_proces <- subtitulos %>% \n    mutate(texto = map_chr(value, string_mini_clean)) %>% \n    select(-value)\n\nsubtitulos_proces %>% \n  select(texto)\n#> # A tibble: 664 × 1\n#>    texto                                                                        \n#>    <chr>                                                                        \n#>  1 \" a baeza \"                                                                  \n#>  2 \"después del añito que hemos pasado quien mm aman 110 se consuela es porque …\n#>  3 \"viendo la actitud del público más que una actuación esto es \"               \n#>  4 \"viendo la actitud del público más que una actuación esto es una sesión de c…\n#>  5 \" intura y su conjunto conga del jaruco 2 \"                                  \n#>  6 \"la última vez que hiciste algo parecido fue en el súper y llevabas 25 rollo…\n#>  7 \" a que produce nostalgia ver a un grupo de españoles p poniéndose de acuerd…\n#>  8 \" a que produce nostalgia ver a un grupo de españoles poniéndose de acuerdo …\n#>  9 \"jno lomas xte conmigo pi \"                                                  \n#> 10 \"í 7 igual lo que nos ha caído es una maldición india rel y este es el orige…\n#> # … with 654 more rows\n\n\nY ya vemos a simple vista que hay algun duplicado. Calculemos ahora la distancia de strings, utilizando la función stringdist de la librería del mismo nombre.\n\n\nMostrar / ocultar código\n\nsubtitulos_proces %>% \n    mutate(texto_anterior = lag(texto)) %>% \n    # calculamos distancias con método lcs (que no me he leído que hace exactamente)\n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\")) %>% \n  # veamos algunos elementos\n    filter(distancia < 10) %>% \n    arrange(desc(distancia) ) %>% \n    select(texto, texto_anterior, distancia) %>% \n    head()\n#> # A tibble: 6 × 3\n#>   texto                                                          texto…¹ dista…²\n#>   <chr>                                                          <chr>     <dbl>\n#> 1 \"mn por si no te lo ha dicho aún tu cuñado 98 6 se considerab… \"a si …       9\n#> 2 \"por alguna razón a beyoncé le sale bastante mejor el truco d… \" zz p…       6\n#> 3 \"asi es como se visten los daft punk lr ey para teletrabajar \" \"asi e…       6\n#> 4 \"la pandemia interrumpió su gira de 40 aniversario pasas carl… \"la pa…       6\n#> 5 \"2 xl en viajes al pasado los 80 con nostalgia de los 50 yy c… \"2 xl …       6\n#> 6 \"lp parece que a nek le ha pillado despistado 3 el cachito an… \"parec…       6\n#> # … with abbreviated variable names ¹​texto_anterior, ²​distancia\n\n\nY parece que funciona. Así que decido quitar las filas dónde la distancia sea menos que 19 y así eliminar muchos de los duplicados.\n\n\nMostrar / ocultar código\nsubtitulos_proces <- subtitulos_proces %>% \n    mutate(texto_anterior = lag(texto)) %>% \n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\")) %>% \n    filter(distancia > 19) %>% \n    select(-texto_anterior)\n\nsubtitulos_proces %>% \n  head()\n#> # A tibble: 6 × 5\n#>    name n_fichero                      n_caracteres texto                dista…¹\n#>   <int> <chr>                                 <int> <chr>                  <dbl>\n#> 1    14 00000014.jpg.subtitulo.tif.txt           92 \"después del añito …      84\n#> 2    15 00000015.jpg.subtitulo.tif.txt           62 \"viendo la actitud …      89\n#> 3    16 00000016.jpg.subtitulo.tif.txt           89 \"viendo la actitud …      23\n#> 4    19 00000019.jpg.subtitulo.tif.txt           50 \" intura y su conju…      76\n#> 5    20 00000020.jpg.subtitulo.tif.txt          112 \"la última vez que …     103\n#> 6    21 00000021.jpg.subtitulo.tif.txt          114 \" a que produce nos…     128\n#> # … with abbreviated variable name ¹​distancia\n\n\nY ahora escribimos este dataframe en un csv y será la materia prima para ver qué podemos hacer con esto (para eso requeriré ayuda de algún amigo más ducho en tales artes)\n\n\nMostrar / ocultar código\nwrite_csv(subtitulos_proces,\n          file = str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\n\nY os dejo este csv en este enlace"
  },
  {
    "objectID": "2021/01/11/cachitos-primera-parte/index.html",
    "href": "2021/01/11/cachitos-primera-parte/index.html",
    "title": "Cachitos. Primera parte",
    "section": "",
    "text": "En las ya pasadas navidades se generó algo de polémica con el especial de cachitos nochevieja. Qué si los rótulos se metían mucho con la oposición, el rey y ciudadanos y muy poco con el gobierno. Así que me entró la curiosidad y pensé, ¿por qué no analizar los rótulos del cachitos nochevieja de 2020 y de paso del 2019? Pues me pusé manos a la obra. Lo primero de todo, dar las gracias a Raúl Vaquerizo y a Carlos Gil Bellosta por darme consejos y pasarme el enlace al blog de Waldo Jaquith en el que se basa esta primera entrada.\nEsta primera parte va a consistir en varios pasos\n\nBajar el video del especial cachitos nochevieja\nExtraer fotogramas con subtítulos (imagemagick)\nRecortar los subtítulos (imagemagick)\nReconocimiento óptico de caracteres (tesseract)\n\nLo primero que tenemos que hacer es conseguir los vídeos de cachitos de 2019 y de 2020. Para el primero podemos encontrarlo en youtube, pero el segundo no estaba (al menos en el momento en que hice todo esto). Si está en youtube se puede descargar usando youtube-dl, pero cómo eso lo contará Raúl en uno de sus próximos post me voy a centrar en como descargar el cachitos 2020 desde la web oficial.\nPara eso, una vez que ya hemos encontrado el programa que queremos, tenemos que ir a inspeccionar código , sección XHR, filtar por mp4 y darle al play para así poder identificar a qué Url hace request y ver exactamente la dirección de dónde está alojado el mp4. Mejor pongo una imagen\n\n\n\nimagen\n\n\nDel request Url anterior, nos quedamos con la dirección hasta el .mp4 y ya podemos bajarlo. Antes de nada, para todo el proceso aconsejor usar un SO unix -friendly. En el caso de linux es importante instalar por consola imagemagick, mplayer, parallel y tesseract.\nNos bajamos el video con\n    wget http://mediavod-lvlt.rtve.es/resources/TE_GLUCA/mp4/2/4/1609487028742.mp4\n    mv 1609487028742.mp4 2020_cachitos.mp4\nUna vez ya tenemos el video tenemos que decidir cuántos fotogramas extraer para pillar los subtítulos, viendo que los subtítulos duran entre 5 y 10 segundos , podríamos extraer un fotograma cada 220. Yo he decidido extraer uno cada 200 para ilustrar posteriormente como con análisis de texto podemos identificar los rótulos duplicados y también para que no se me escape ni un sólo subtítulo.\nVamos a utilizar mplayer para extraer 1/200 fotogramas a formato jpg.\nmplayer -vf framestep=200 -framedrop -nosound 2020_cachitos.mp4 -speed 100 -vo jpeg:outdir=2020_jpg \nY después de un rato (tengo que probar si hacerlo con ffmpeg es más rápido), tenemos unos 1300 jpg. Hemos resumido 3 horas de video en 1300 imágenes. Pongo un ejemplo.\n\n\n\nimagen\n\n\nPara ahorrar espacio reescalamos la imagen usando mogrify de imagemagick . Y aquí ya empezamos a usar parallel para hacer el proceso en paralelo.\ncd 2020_jpg\n# Uso los 6 cores físicos de mi portátil\n# con {} le decimos que nos ponga el mismo nombre al fichero resultante, sustituyendo\n# al anterior\nfind . -name '*.jpg' |  parallel -j 6 mogrify -resize 642x480 {}\nY el resultado es una imagen más pequeña\n\n\n\n\n\nAl extraer uno de cada 200 frames, muchas veces extraemos fotogramas sin rótulos y otras (las menos) el mismo rótulo 2 veces. \nPara extraer sólo el rótulo hay que utilizar la herramienta crop también de imagemagick dónde le decimos la resolución de la imagen resultante y la coordenada x e y de la imagen de 642x480 original dónde empieza el corte. Aquí tuve que hacer bastantes pruebas hasta identificar la posición de los subtítulos, dado que el cuadro dónde aparecen es de tamaño variable.\nEn este caso decimos que nos cree ficheros tif que tengan el mismo nombr que el original y le añada el sufijo .subtitulo.tif. Por ejemplo tendremos ficheros con este patrón 00000186.jpg.subtitulo.tif\n# en paralelo de nuevo\nfind . -name '*.jpg' |  parallel -j 6 convert {} -crop 460x50+90+295 +repage -compress none -depth 8 {}.subtitulo.tif\n\n\n\n\n\nCómo me comentaba ayer alguien por twitter, tesseract es un poco “tiquismiquis”, así que para facilitarle el trabajo “negativizamos” las imágenes.\nfind . -name '*.tif' |  parallel -j 6 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n\n\n\n\nY ahora utilizamos tesseract para extraer el texto de estas imágenes. Recomiendo instalar tesseract con los modelos para el idioma español para que lo haga mejor (pille tildes y eñes). En ubuntu y derivados lo instalamos con apt\nsudo apt-get install tesseract-ocr tesseract-ocr-spa\nYo estoy usando la versión 4.1.1, la cual dicen en su github que utiliza modelos LSTM para mejorar el reconocimiento de texto. Sea como fuere, para extraer el texto sería con\nfind . -name '*.tif' |  parallel -j 6 tesseract -l spa {} {}\ny nos generaría un fichero txt por cada una de las imágenes .subtitulo.tif . La mayoría de esos txt no tienen texto (muchos fotogramas no tienen rótulo). Veamos cómo lo ha hecho con el fichero 00000186.jpg.subtitulo.tif.txt\n\n╰─ $ ▶ cat 00000186.jpg.subtitulo.tif.txt \nNuestro “We Are The World” cantado para el mundo\nen el mismo inglés que hablaba Emilio Botín\nPues al menos en este caso funciona bastante bien. En las siguientes entradas comentaremos brevemente como podríamos analizar los subtítulos.\nCon estos pasos hemos conseguido extraer el texto de los subtítulos de unas 3 horas de vídeo, evidentemente si los subtítulos estuvieran en una pista srt dentro del mp4 no habría sido necesario todo esto. Este tipo de análisis hecho enteramente en bash es fácilmente escalable y se puede utilizar por ejemplo para identificar matrículas o similar.\nOs dejo también un script extract_subtitles.sh que le pasas como argumento el año , 2020 o 2019 y te baja el video, te extrae los fotogramas, hace el ocr y te deja los ficheros de texto en un directorio.\nSaludos.\n#!/bin/bash\n\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n \nif [ \"$ANNO\" == \"2020\" ] ;\nthen\n    wget http://mediavod-lvlt.rtve.es/resources/TE_GLUCA/mp4/2/4/1609487028742.mp4\n    mv 1609487028742.mp4 $video_file\nfi\n\nif [ \"$ANNO\" == \"2019\" ] ;\nthen\n    wget https://rtvehlsvod2020a-fsly.vod-rtve.cross-media.es/resources/TE_GLUCA/mp4/0/9/1577860099590.mp4\n    mv 1577860099590.mp4 $video_file\nfi\n\n# Pasar a jpg uno de cada 220 fotogramas\n\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 6 mogrify -resize 642x480 {}\n\n# Seleccionar cacho dond estan subtitulos\nfind . -name '*.jpg' |  parallel -j 6 convert {} -crop 460x50+90+295 +repage -compress none -depth 8 {}.subtitulo.tif\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 6 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 6 tesseract -l spa {} {}\n\n# mover a directorio texto\nmkdir -p $root_directory/$ANNO$suffix_txt_dir\n\nmv *.txt $root_directory/$ANNO$suffix_txt_dir\n\ncd $root_directory"
  },
  {
    "objectID": "2021/01/07/tendencias/index.html",
    "href": "2021/01/07/tendencias/index.html",
    "title": "Tendencias",
    "section": "",
    "text": "Hoy, mi amigo Jesús Lagos ha retuiteado una entrevista que ambos consideramos bastante mala, tweet, y el caso es que me ha hecho reflexionar sobre un par de tendencias que veo en el sector.\n\nInferencia causal\nAlgoritmos éticos (“fairness”)\nOtras cosas\n\nNo se trata de bandos, pero si tuviera que elegir uno, me quedaría en el de la inferencia causal. Eso sí, ahora mismo está de moda y parece que antes de Pearl no había nada. Un par de reflexiones sobre esto, desde mi escaso conocimiento. Una, en inferencia causal tenemos el Rubin Causal Model desde hace ya unos añitos y según comentan quienes saben de esto es matemáticamente equivalente a los DAGs de Judea. Por otro lado tambień se tiene todo el aparataje de los economistas, (las diferencias en diferencias, variables instrumentales, etc) , y también lo relacionado con los modelos de ecuaciones estructurales.\nEn fin, todo este mundo de la inferencia causal tiene como objetivo poder detectar el efecto de un tratamiento o variable cuándo tenemos estudios observacionales, en experimentales todo lo anterior sobra. Dicho esto, es un campo interesante y con algunas aplicaciones, aunque creo que muchas veces nos complicamos la vida demasiado.\nRespecto a lo de los algoritmos éticos, limpios, no discriminatorios o como demonios los llamen, mejor me reservo mi opinión (el que la quiera que me invite a una birra en una terraza), simplemente leed el artículo del tweet con espíritu crítico (y sin que os salga la carjada ante tamañas barbaridades).\nY con respecto a otras cosas, creo que en el mundo de la biología y la genética hay bastante potencial en cuánto al análisis de datos, y digo creo porque no tengo ni idea de ese ámbito y sólo ahora lo estoy atisbando.\nNota1. Para los interesados en tener una visión introductoria de la inferencia causal topé el otro día con este libro online, que no tiene mala pinta. Causal inference: The Mixtape. Para profundizar más en el libro (que no he leído aún ) de Miguel Hernán Causal Inference. What if. En el caso del primero viene con código en stata y en R, y en el de segundo hay código en SAS, stata, R y python. Y librería en R para lo que llama parametric g-formula\nNota2. Respecto a la inferencia causal no hay que dejar de leer a Gelman, Reseña de Gelman sobrre book of why\nNota3. Iba a hablar más sobre los algoritmos éticos pero me da tanta pereza. Eso sí, estoy abierto a qué me recomendéis lecturas sobre el tema.\nY nada más."
  },
  {
    "objectID": "2020/12/30/y-si-parte-ii/index.html",
    "href": "2020/12/30/y-si-parte-ii/index.html",
    "title": "¿Y si … ? Parte II",
    "section": "",
    "text": "Volvamos a nuestro ejemplo tonto, dónde habíamos visto que el T-learner cuando el modelo base es un modelo lineal equivale a tener un modelo saturado (con interacciones).\nEn estos de los “metalearners” tenemos entre otros, los T-learners vistos en el post anterior , los S-learner y los X-learners.\nLos S-learners no es más que usar un solo modelo “Single” para estimar el Conditional Average Treatment Effect , CATE.\nUsando el mismo ejemplo sencillo, se tiene que."
  },
  {
    "objectID": "2020/12/30/y-si-parte-ii/index.html#extra-uso-de-causalml",
    "href": "2020/12/30/y-si-parte-ii/index.html#extra-uso-de-causalml",
    "title": "¿Y si … ? Parte II",
    "section": "Extra, uso de causalml",
    "text": "Extra, uso de causalml\nEn la librería causalml de Uber vienen implmentandos los metalearner entre otras cosas. Usando el mismo ejemplo veamos como se calcularía el CATE.\nNota: He sido incapaz de ver como predecir para mi nueva x, no hay o no he encontrado que funcione un método predict para aplicar el X learner a unos nuevos datos.\n\n\nMostrar / ocultar código\nfrom causalml.inference.meta import BaseXRegressor\n#> The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\nfrom sklearn.linear_model import LinearRegression\n# llamamos al df que está en R\ndf_python = r.df[['Y','W','X','pesos']]\ndf_python\n#>             Y    W          X     pesos\n#> 0   48.784384  1.0  10.800067  0.608752\n#> 1   25.286438  0.0  10.707605  0.612492\n#> 2   28.395375  0.0   9.925625  0.643553\n#> 3   47.602247  1.0  10.652555  0.614712\n#> 4   46.722247  1.0   9.992698  0.640932\n#> ..        ...  ...        ...       ...\n#> 95  47.309873  1.0  10.771928  0.609891\n#> 96  20.741797  0.0   9.829798  0.647284\n#> 97  24.393540  0.0  10.412418  0.624340\n#> 98  53.716540  1.0  11.506078  0.579804\n#> 99  51.222453  1.0  10.711256  0.612344\n#> \n#> [100 rows x 4 columns]\n\n\n\n\nMostrar / ocultar código\nlearner_x = BaseXRegressor(learner=LinearRegression())\n\nX = df_python.X.values.reshape(-1,1)\ny = df_python.Y.values\ntreatment = df_python.W.values\ne = df_python.pesos.values\nnueva_X = r.df_nueva_x['X'].values.reshape(-1,1)\n\n# estimamos\ncate_x = learner_x.fit_predict(X=X, treatment=treatment, y=y, p=e)\n\nprint(cate_x)\n\n#> [[24.18445071]\n#>  [23.96165333]\n#>  [22.07738827]\n#>  [23.8290041 ]\n#>  [22.23900667]\n#>  [25.90657902]\n#>  [20.07281545]\n#>  [20.22764413]\n#>  [21.93491718]\n#>  [24.90817005]\n#>  [22.94900375]\n#>  [20.55940033]\n#>  [20.22323467]\n#>  [18.67470923]\n#>  [22.41952385]\n#>  [22.01148778]\n#>  [23.65059347]\n#>  [20.4584526 ]\n#>  [23.13640524]\n#>  [24.76464061]\n#>  [29.17118624]\n#>  [20.61781408]\n#>  [21.84758166]\n#>  [21.37933037]\n#>  [20.36590009]\n#>  [18.29646419]\n#>  [19.56622485]\n#>  [21.73818594]\n#>  [20.82250793]\n#>  [23.25545554]\n#>  [19.9476568 ]\n#>  [23.32259103]\n#>  [17.77992951]\n#>  [19.94871811]\n#>  [21.59347327]\n#>  [19.87909366]\n#>  [21.37264782]\n#>  [24.36127525]\n#>  [21.65050016]\n#>  [23.72101678]\n#>  [18.30885076]\n#>  [22.98876753]\n#>  [19.85749887]\n#>  [23.65489924]\n#>  [23.8774463 ]\n#>  [26.52943858]\n#>  [21.9561297 ]\n#>  [22.06854152]\n#>  [19.65625568]\n#>  [19.09653297]\n#>  [23.61708013]\n#>  [20.31583121]\n#>  [20.90446626]\n#>  [21.98840222]\n#>  [21.98655993]\n#>  [24.7992766 ]\n#>  [18.2264522 ]\n#>  [23.02953222]\n#>  [23.66633862]\n#>  [20.65807062]\n#>  [24.55089614]\n#>  [23.61143985]\n#>  [27.18483957]\n#>  [17.65692503]\n#>  [22.83885017]\n#>  [24.24427817]\n#>  [24.21382276]\n#>  [18.30237035]\n#>  [21.08933438]\n#>  [26.59466223]\n#>  [21.76777266]\n#>  [23.39083619]\n#>  [21.28861592]\n#>  [18.22376141]\n#>  [22.17449599]\n#>  [24.35309297]\n#>  [19.94041482]\n#>  [24.90061955]\n#>  [23.60184813]\n#>  [25.3037691 ]\n#>  [25.71434126]\n#>  [28.01598546]\n#>  [21.10080014]\n#>  [22.45184837]\n#>  [19.16269587]\n#>  [18.77807334]\n#>  [20.1102733 ]\n#>  [24.1353215 ]\n#>  [22.23422848]\n#>  [25.87465564]\n#>  [24.81438579]\n#>  [16.33858204]\n#>  [19.50206724]\n#>  [24.67806615]\n#>  [20.5075564 ]\n#>  [24.11664769]\n#>  [21.84648138]\n#>  [23.25036905]\n#>  [25.88566055]\n#>  [23.97045205]]"
  },
  {
    "objectID": "2020/11/15/y-si-parte-i/index.html",
    "href": "2020/11/15/y-si-parte-i/index.html",
    "title": "¿Y si … ? Parte I",
    "section": "",
    "text": "Lo de la inferencia causal está de moda, y motivos hay, es una herramienta que intenta dar respuesta a preguntas cómo las siguientes.\n\n¿Qué habría pasado si en vez de poner este precio a este producto hubiera puesto otro?\n¿Se habría vendido más?\n¿He mandado a mi campaña a aquellos para los que justo al mandar a campaña su probabilidad de compra se incrementa?\n\nTradicionalmente a esta pregunta, los estadísticos respondían con una de sus herramientas más potentes, el diseño de experimentos. Pero muchas veces lo único que tenemos son datos observacionales y se trata de estimar el tamaño del efecto.\nLeyendo sobre cosas de este tipo llegué a los “metalearners” y en particular al “T-learner”.\nSe trata de estimar el efecto de una variable, típicamente un tratamiento con 2 categorías sobre una variable respuesta, y con presencia de otras variables, de forma que el efecto del tratamiento puede ser diferente según el valor de las covariables, vamos, que haya interacción.\nSupongamos que tenemos una variable respuesta Y, un tratamiento W (con dos niveles, 0 y 1) y una o varias covariables X. El T-learner (La T es de two models) lo que propone básicamente es estimar dos modelos. Uno que estime \\(E[Y | X]\\) en el grupo de control (W=0) y otro que estime lo mismo pero en el grupo del tratamiento (W=1) y luego restar esas dos esperanzas. A esto lo llaman una estimación del CATE (Conditional Average Treatment Effects) ¿Fácil, verdad?\nSi estamos en el marco de los modelos lineales esta forma de proceder es idéntica a estimar un sólo modelo dónde W es otra variable más y además pondríamos todas las posibles interacciones entre W y X, casi podríamos decir que es el modelo saturado. De hecho en un modelo lineal, podríamos sacar el CATE simplemente utilizando los coeficientes estimados.\nEjemplo tonto\n\n\nMostrar / ocultar código\nset.seed(155)\n\nX <- rnorm(100, 10,1)\nW <- rbinom(100, 1, 0.6)\n\n# Me construyo la Y de forma que haya efectos principales e interacción\nY <- 4 + 2 * X + 2 * W + 2 * W * X + rnorm(100, 0, sd = 2)\n\ndf <- as.data.frame(cbind(Y,W,X))\n\n\nSi hacemos un modelo sólo sobre los que son W = 0 y otro para los que son W = 1 (He obviado la parte de hacer train, test, validación, etc).\n\n\nMostrar / ocultar código\nmod0 <- lm(Y ~ X, data = df[W==0, ])\nmod1 <- lm(Y ~ X, data = df[W==1, ])\n\n\nY si suponemos una nueva observación dónde X = 14 entonces laa estimación del CATE mediante un T -learner.\n\n\nMostrar / ocultar código\ndf_nuevo <- data.frame(X = 14)\n(cate1 <- predict(mod1, newdata = df_nuevo) - predict(mod0, newdata = df_nuevo))\n#>        1 \n#> 31.89504\n\n\nHaciendo el modelo con interacción\n\n\nMostrar / ocultar código\n\nmod_saturado <-  lm(Y ~ W *X , data = df)\nsummary(mod_saturado)\n#> \n#> Call:\n#> lm(formula = Y ~ W * X, data = df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.6786 -1.2138  0.1903  1.5419  4.6289 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   6.9118     3.1354   2.204   0.0299 *  \n#> W            -1.8395     4.0511  -0.454   0.6508    \n#> X             1.6981     0.3085   5.504 3.08e-07 ***\n#> W:X           2.4096     0.4016   6.000 3.49e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.011 on 96 degrees of freedom\n#> Multiple R-squared:  0.9689, Adjusted R-squared:  0.9679 \n#> F-statistic: 995.9 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nPara ver el efecto de W sobre una hipotética población sería tener la misma observación con X=14, pero en un caso con W= 0 y en otro con W=1\nUtilizando los coeficientes, el CATE sería simplemente tener en cuenta cuando interviene W (los otros términos se cancelan).\n\n\nMostrar / ocultar código\n# \n(cate_2 <- coef(mod_saturado)[2] + coef(mod_saturado)[4] * 14 )\n#>        W \n#> 31.89504\n\n\nQue coincide con la estimación usando el “T - Learner”. Es decir, en este ejemplo sencillo, utilizando como modelo base del T-learner un modelo lineal, la estimación es la misma que considerar un solo modelo dónde tenemos las interacciones del tratamiento con las covariables.\nLa aproximación de T - Learner (y de otros metalearners ) cobra sentido cuando tenemos muchas covariables y un modelo lineal con interacciones se puede volver muy complicado. En el caso del T-learner se podría utilizar como modelo base cualquier modelo que estime la \\(E[Y|W=w_i,X=x]\\).\nSin meterme mucho en la parte de los “potential outcomes” , básicamente se trata de inferir con la población con W=0 lo que pasaría si todas las observaciones tuvieran \\(Y^{(0)}\\) y lo mismo con la población con W=1. Este tipo de estrategias funcionan bien mientras el grado de solape de tratamiento y control en los diferentes valores de X sea alto (en el diseño de experimentos se busca justo eso, jeje).\nEn fin, que creo que me he enrollado demasiado para algo que es muy simple. En próximos post a ver si explico mejor los S-learners, los X- learners, los causal tree y causal forest, modelos de uplift, y más cositas, y con algún ejemplo más claro."
  },
  {
    "objectID": "2020/10/21/nmf/index.html",
    "href": "2020/10/21/nmf/index.html",
    "title": "Ejemplillo con NMF",
    "section": "",
    "text": "Ando falto de ideas, no sé si es la pandemia, el teletrabajo ( o la esclavitud en tiempos modernos como me gusta llamarlo) u otra cosa. Total, que me he puesto a bichear un post antiguo de mi amigo Carlos Gil sobre NMF (factorización no negativa de matrices). Cómo siempre el lo cuenta mucho mejor que yo.\nTotal, que puede que en breve me toque tener algo a lo que quizá se pueda aplicar este tipo de técnicas, a saber, tener clientes y productos.\nDe hecho voy a usar su mismo ejemplo.\nNota: La librería es mejor si se instala desde BioConductor con BiocManager.\nLa librería NMF está bastante bien, utiliza paralelización, por debajo está escrita en C, pero tiene el incoveniente de que aún no está implementado un método predict para nuevos datos\n\n\nMostrar / ocultar código\nlibrary(MASS)\nlibrary(NMF) # BiocManager::install(\"NMF\")\n\na <- as.matrix(caith)\nres <- nmf(a, rank = 2)\n \na\n#>        fair red medium dark black\n#> blue    326  38    241  110     3\n#> light   688 116    584  188     4\n#> medium  343  84    909  412    26\n#> dark     98  48    403  681    85\n\n\nEl caso es factorizar esta matriz en dos matrices no negativas. Otras formas de factorizar esta matriz podría ser con análisis de correspondencias simples. Pero vamos a la descomposición por nmf\n\n\nMostrar / ocultar código\nw <-  res@fit@W\nh <-  res@fit@H\n\n\n\n\nMostrar / ocultar código\nw \n#>             [,1]      [,2]\n#> blue    34.41271 386.48565\n#> light   32.29480 896.90510\n#> medium 438.78664 576.80290\n#> dark   720.24703   5.38822\n\n\n\n\nMostrar / ocultar código\nh\n#>           fair        red    medium      dark        black\n#> [1,] 0.1141435 0.06226416 0.6667132 0.8736829 9.626828e-02\n#> [2,] 0.7049219 0.11239402 0.7074373 0.1715769 1.971208e-13\n\n\nReconstruimos a\n\n\nMostrar / ocultar código\nw %*% h\n#>             fair       red   medium      dark     black\n#> blue   276.37021  45.58136 296.3578  96.37782  3.312852\n#> light  635.93433 102.81758 656.0355 182.10365  3.108965\n#> medium 456.68567  92.14988 700.5967 482.32648 42.241237\n#> dark    86.00979  45.45118 484.0100 630.19204 69.336946\n\n\nY si comparamos con a\n\n\nMostrar / ocultar código\na - (w %*% h)\n#>              fair       red    medium       dark       black\n#> blue     49.62979 -7.581356 -55.35776  13.622177  -0.3128523\n#> light    52.06567 13.182418 -72.03547   5.896346   0.8910349\n#> medium -113.68567 -8.149881 208.40327 -70.326485 -16.2412371\n#> dark     11.99021  2.548819 -81.01004  50.807961  15.6630545\n\n\nBueno, la reconstrucción no es perfecta, pero bueno, no está tan mal.\nBien, tal y como cuenta Carlos en su entrada ahora podemos normalizar las filas de W y de H, de forma que tengamos probabilidades. Dónde entonces H sería funciones de probabilidad sobre las filas de la matriz original y W serán ponderaciones. O como dice él, H es un menú de preferencias (imaginemos que tenemos usuarios en filas y productos en columnas), en este caso hemos hecho una reducción de dimensión para quedarnos en 2 preferencias, (sería el equivalente conceptual al número de componentes en un PCA o en un CA), y W serían las ponderaciones que cada usuario da a cada una de las preferencias (sus coordenadas en un correspondencias siguiendo el símil)\nNormalicemos\n\n\nMostrar / ocultar código\nw_hat <- w / rowSums(w)\nw_hat\n#>              [,1]        [,2]\n#> blue   0.08176014 0.918239864\n#> light  0.03475549 0.965244506\n#> medium 0.43205116 0.567948839\n#> dark   0.99257448 0.007425522\n\n\n\n\nMostrar / ocultar código\nh_hat <-  h / rowSums(h)\nh_hat\n#>            fair        red    medium      dark        black\n#> [1,] 0.06295585 0.03434180 0.3677257 0.4818799 5.309678e-02\n#> [2,] 0.41555704 0.06625716 0.4170398 0.1011460 1.162043e-13\n\n\nAsí, el primer “menú” está compuesto por los “productos” fair, red, etc, en proporción a como indica la primera fila de h_hat. Y el individuo “blue” prefiere el primer menú en casi un 0.9 de probabilidad vs un alrededor de 0.1 de preferencia del menú 2. En un PCA diríamos que esos son los “loadings”.\nLas filas de W a veces se asocian con arquetipos o individuos promedio. Los individuos “blue” tienen esos pesos los dos factores latentes.\nEn este caso dónde tenemos color de ojos (fila) y color del pelo (columnas), vemos que en las dos distribuciones multinomiales que hay en sendas filas de h_hat (si, eso es lo que son, dos distribuciones multinomiales), la probabilidad de tener el pelo negro es bastante pequeña (tiene que ver con la tabla de contingencia original, hay muy pocos con el pelo negro). Pero vemos que hay un arquetipo, (el de os ojos oscuros) para el cual el peso que da al menú de preferencias dónde la probabilidad de tener el pelo negro sea mayor. Es decir, al final es una mixtura de distribuciones multinomiales.\nEn realidad lo que hace NMF es descubrir la estructura subyacente de los datos en un espacio de menor dimensión que el original. Bueno, pues con W y H normalizadas podemos construir una matriz diagonal D que simplemente nos genere muestras de individuos y en qué columnas caen.\nPodemos utilizar como matriz diagonal la suma de las filas de a, y así obtener\n\n\nMostrar / ocultar código\n(d <- diag(rowSums(a)))\n#>      [,1] [,2] [,3] [,4]\n#> [1,]  718    0    0    0\n#> [2,]    0 1580    0    0\n#> [3,]    0    0 1774    0\n#> [4,]    0    0    0 1315\n\n\nY podemos hacer \\(A \\approx D \\cdot W \\cdot H\\)\n\n\nMostrar / ocultar código\nd %*% w_hat %*% h_hat\n#>           fair       red   medium      dark     black\n#> [1,] 277.67093  45.69909 296.5397  94.97332  3.116981\n#> [2,] 637.21749 102.93373 656.2149 180.71812  2.915739\n#> [3,] 466.94392  93.07840 702.0314 471.24977 40.696489\n#> [4,]  86.22994  45.47111 484.0408 629.95432 69.303794\n\n\nQue se parece bastante a w %*% h , o podríamos usar otra D, en este caso para obtener qué matriz se obtendría para 10 casos de cada fila.\n\n\nMostrar / ocultar código\nd <-  diag(c(10,10, 10, 10))\nd %*% w_hat %*% h_hat\n#>          fair       red   medium     dark      black\n#> [1,] 3.867283 0.6364776 4.130079 1.322748 0.04341200\n#> [2,] 4.033022 0.6514793 4.153259 1.143786 0.01845405\n#> [3,] 2.632153 0.5246809 3.957336 2.656425 0.22940524\n#> [4,] 0.655741 0.3457879 3.680919 4.790527 0.52702505\n\n\ny bueno, la verdad es que me pregunto si esto se parece o no a un análisis de correspondencias. Veamos\n\n\nMostrar / ocultar código\nlibrary(FactoMineR)\nres_ca <-  CA (a, ncp = 2, graph = FALSE)\nfactoextra::fviz_ca(res_ca)\n\n\n\n\n\n\n\n\n\nLo primero que hay que darse cuenta es que ambas técnicas no son del todo comparables, el correspondencias busca encontrar dimensiones que expliquen la mayor cantidad de inercia (distancia Chi-cuadrado) y es parecido al PCA en el sentido de que la primera dimensión es la que más explica, etc.. De hecho el CA, diagonaliza 2 matrices derivadas de la tabla de contingencia, una la de los perfiles filas y otra la de los perfiles columna. Y las pinta junta de acuerdo a algún teorema baricéntrico que tuve que demostrar en algún examen allá por los lejanos 90’s.\nPero en realidad si nos fijamos en las coordenadas de las filas en el CA\n\n\nMostrar / ocultar código\nres_ca$row$coord\n#>              Dim 1       Dim 2\n#> blue   -0.40029985  0.16541100\n#> light  -0.44070764  0.08846303\n#> medium  0.03361434 -0.24500190\n#> dark    0.70273880  0.13391383\n\n\nNo es más que ver las filas en un subespacio (el calculado por el CA) del espacio definido por las columnas y de forma análoga pasa con las columnas. Estas coordenadas podrían ser una forma de codificar la variable categórica. Cabe preguntarse si tienen relación con la estructura obtenida por el NMF.\n\n\nMostrar / ocultar código\ncoordenadas_filas <-  cbind(res_ca$row$coord, w_hat)\ncolnames(coordenadas_filas)[3:4] <-  paste0(\"nmf_\", 1:2)\ncoordenadas_filas\n#>              Dim 1       Dim 2      nmf_1       nmf_2\n#> blue   -0.40029985  0.16541100 0.08176014 0.918239864\n#> light  -0.44070764  0.08846303 0.03475549 0.965244506\n#> medium  0.03361434 -0.24500190 0.43205116 0.567948839\n#> dark    0.70273880  0.13391383 0.99257448 0.007425522\n\n\ny\n\n\nMostrar / ocultar código\ncor(coordenadas_filas)\n#>             Dim 1       Dim 2       nmf_1       nmf_2\n#> Dim 1  1.00000000 -0.05155554  0.99991352 -0.99991352\n#> Dim 2 -0.05155554  1.00000000 -0.04510151  0.04510151\n#> nmf_1  0.99991352 -0.04510151  1.00000000 -1.00000000\n#> nmf_2 -0.99991352  0.04510151 -1.00000000  1.00000000\n\n\nResultado coherente, ¿no? . En este ejemplo de juguete una única dimensión del correspondencias explica el 86,5% de la inercia.\nCosas buenas del nmf.\n\nNos da una interpretación más natural de ciertas cosas\nLas dimensiones encontradas al no estar ordenadas por importancia, no sufren del efecto tamaño de otras técnicas que buscan el mejor primer resumen y luego el segundo mejor resumen de los datos. La verdad que no estoy seguro de esto que acabo de escribir.\nEs equivalente a un LDA cuando se utiliza como función objetivo la divergencia de Kullback-Leibler.\n\nAh, se me olvidaba. ¿qué pasa si tengo una nueva fila/usario?, la librería NMF no permite predecir, y aunque se podría implementar, buscando un poco se encuentra la forma\n\n\nMostrar / ocultar código\nlibrary(NNLM)\n\nres_nmf2 <-  nnmf(a, k = 2, loss = \"mkl\")\n\n\n\n\nMostrar / ocultar código\nres_nmf2$W\n#>              [,1]       [,2]\n#> blue    0.6928050 13.3441587\n#> light   0.1273311 30.9726568\n#> medium 13.0923170 19.8721597\n#> dark   22.4117740  0.1060281\nres_nmf2$H\n#>           fair      red   medium      dark      black\n#> [1,]  3.740643 2.012339 21.49645 28.091524 3.09335848\n#> [2,] 20.516743 3.311351 21.09274  5.764011 0.08766068\n\n\nNuevas filas\n\n\nMostrar / ocultar código\n\nb <- matrix( data = c(20, 30, 40, 0,20, 10, 10, 30,10, 90), nrow=2, byrow = TRUE)\ncolnames(b) <-  colnames(a)\nrownames(b) <-  c(\"tipo_n1\", \"tipo_n2\")\nb\n#>         fair red medium dark black\n#> tipo_n1   20  30     40    0    20\n#> tipo_n2   10  10     30   10    90\n\n\nY tiene un método predict\n\n\nMostrar / ocultar código\npredict(res_nmf2, newdata = b, which = \"W\")\n#> $coefficients\n#>             [,1]    [,2]\n#> tipo_n1 0.734150 1.32159\n#> tipo_n2 2.566985 0.00000\n#> \n#> $n.iteration\n#> [1] 30\n#> \n#> $error\n#>          MSE          MKL target.error \n#>   1305.05201     26.74997     26.74997 \n#> \n#> $options\n#> $options$method\n#> [1] \"scd\"\n#> \n#> $options$loss\n#> [1] \"mkl\"\n#> \n#> $options$max.iter\n#> [1] 10000\n#> \n#> $options$rel.tol\n#> [1] 1e-12\n#> \n#> \n#> $call\n#> nnlm(x = t(object$H), y = t(newdata), method = method, loss = loss)\n#> \n#> attr(,\"class\")\n#> [1] \"nnlm\"\n\n\nLo dicho, una técnica muy interesante y útil."
  },
  {
    "objectID": "2020/10/18/lo-del-pca/index.html",
    "href": "2020/10/18/lo-del-pca/index.html",
    "title": "PCA I. El álgebra es tu amiga",
    "section": "",
    "text": "Me pide mi amigo Jesús Lagos que hagamos un vídeo hablando del análisis de componentes principales para un canal que tiene junto a Miguel Angel.\nEl caso es que llevo muchos años usándolo y lo estudié en la carrera, haciendo varios a mano, como no podía ser de otra manera, pero desde que empecé a usar software estadístico se me habían olvidado los detalles de la matemática subyacente.\nBien, dejando un poco todo eso, el análisis de componentes principales es una técnica de interdependencia que busca describir la relación entre las variables de un conjunto de datos e incluso obtener una reducción de su dimensión (si nos quedamos con k componentes principales siendo k menor que el número de variables del conjunto de datos).\nLas componentes son una combinación lineal de las variables tales que, la primera recoge la dirección de la máxima varianza de los datos, la segunda es aquella que siendo ortogonal a la primera recoge la máxima varianza que no ha recogido la primera y así hasta llegar a tantas componentes como variables originales tienes. Visto de esta forma, no se trata más que de un cambio de base elegida de cierta forma y ahí es dónde interviene la diagonalización de matrices.\nEn el blog de Joaquín Amat, el cual recomiendo encarecidamente, se cuenta todo esto de manera mucho más clara, aquí os lo dejo\nUna explicación más algebraica la podemos encontrar en el blog de Carlos\nLas 2 primeras componentes serían algo así como\n\\[ \\begin{aligned}\nPC1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p \\\\\nPC2 = \\phi_{12}X_1 + \\phi_{22}X_2 + \\ldots + \\phi_{p2}X_p \\\\\n\\end{aligned}\n\\]\nY para encontrar los \\(\\phi_{ij}\\) es cuando se recurre a la diagonalización de la matriz de covarianzas.\nVeamos un ejemplo simple con datos de fbref que me ha pasado Jesús\nNos quedamos sólo con 4 variables para el ejemplo, escalamos los datos y obtenemos la matriz de correlaciones\nEsta matriz simétrica es justo la materia prima del PCA, diagonalizándola es como encontramos las direcciones de mayor variabilidad y se hace el cambio de base\nCon la función eigen obtenemos tanto los valores propios como los vectores propios\nAhora vamos a calcular para cada club su valor en las 4 componentes principales para eso necesitamos multiplicar los vectores propios por los datos.\nEstas son las coordenadas de los clubes en esta nueva base, pero podemos recuperar la info original."
  },
  {
    "objectID": "2020/10/18/lo-del-pca/index.html#reducción-de-dimensionalidad",
    "href": "2020/10/18/lo-del-pca/index.html#reducción-de-dimensionalidad",
    "title": "PCA I. El álgebra es tu amiga",
    "section": "Reducción de dimensionalidad",
    "text": "Reducción de dimensionalidad\nAhora si simplemente queremos quedarnos con el mejor resumen de las 4 variables originales nos quedamos solo con primer PC\n\n\nMostrar / ocultar código\nsort(t(pc_scores)[,1])\n#>       Barcelona           Betis     Real Madrid   Real Sociedad        Espanyol \n#>     -3.46460082     -2.69090398     -2.19175378     -1.63031251     -0.96444334 \n#>      Celta Vigo          Girona         Sevilla  Rayo Vallecano      Villarreal \n#>     -0.77766939     -0.68429649     -0.20760011      0.01532123      0.13996320 \n#>        Valencia Atlético Madrid      Valladolid   Athletic Club         Levante \n#>      0.25217154      0.31990099      0.42063611      0.69997361      1.20774846 \n#>          Huesca          Alavés         Leganés           Eibar          Getafe \n#>      1.42757573      1.43582553      1.60505341      1.81794277      3.26946783\n\n\nDónde vemos que en un extremo está el Barcelona y en el otro el Getafe, ¿pero como pondera cada variable en esa componente principal? Pues simplemente viendo el primer vector propio (primera columna) se puede ver lo que pesa cada variable.\n\n\nMostrar / ocultar código\npesos_variables <-  v_propios$vectors\nrownames(pesos_variables) <-  colnames(df_centrado)\ncolnames(pesos_variables) <-  paste0(\"PC\",1:4)\npesos_variables\n#>                           PC1        PC2        PC3         PC4\n#> Creacion_Toques    -0.5353245  0.4191853 -0.1263221  0.72232542\n#> Creacion_Toques_AP -0.3766687 -0.6970094 -0.6098767  0.01868295\n#> Creacion_Toques_Z1 -0.5469743 -0.3635967  0.7514344 -0.06295168\n#> Creacion_Toques_Z2 -0.5218883  0.4541575 -0.2178062 -0.68842867\n\n\nAsi vemos, que en la primera componente todas las variables tienen coordenada del mismo signo, es decir, se trata de una variable que pondrá en un extremo todos los equipos con alto valor en todas esas variables y en el otro los que tienen bajo valor en ese conjunto de variables.\nLa segunda PC ya si diferencia entre Creacion_toques y Creacion_toques_z2 con peso positivo vs Creacion_toques en AP y Z1.\nEste comportamiento de un PCA es usual y se suele decir que la primera componente recoge el tamaño (equipos con alto valor en la mayoría de variables vs los que tienen bajo valor) y la segunda y siguientes componentes añaden matices.\nPara facilitar la interpretación a veces se utiliza otro cambio de base haciendo rotaciones, la más usual es la rotación varimax, para facilitar la interpretación"
  },
  {
    "objectID": "2020/10/18/lo-del-pca/index.html#pca-en-r",
    "href": "2020/10/18/lo-del-pca/index.html#pca-en-r",
    "title": "PCA I. El álgebra es tu amiga",
    "section": "PCA en R",
    "text": "PCA en R\nEn R ya hay multitud de funciones y librerías que hacen el pca directamente, por ejemplo princomp y obtenemos lo mismo que antes salvo el signo en alguna componente, lo cual no varía su interpretación\n\n\nMostrar / ocultar código\nres <- princomp(df_numerico[,2:5], cor=TRUE)\nres$loadings\n#> \n#> Loadings:\n#>                    Comp.1 Comp.2 Comp.3 Comp.4\n#> Creacion_Toques     0.535  0.419  0.126  0.722\n#> Creacion_Toques_AP  0.377 -0.697  0.610       \n#> Creacion_Toques_Z1  0.547 -0.364 -0.751       \n#> Creacion_Toques_Z2  0.522  0.454  0.218 -0.688\n#> \n#>                Comp.1 Comp.2 Comp.3 Comp.4\n#> SS loadings      1.00   1.00   1.00   1.00\n#> Proportion Var   0.25   0.25   0.25   0.25\n#> Cumulative Var   0.25   0.50   0.75   1.00\n\n\nY aquí los scores\n\n\nMostrar / ocultar código\nres$scores\n#>            Comp.1       Comp.2      Comp.3       Comp.4\n#>  [1,] -1.47312591 -0.894656624  0.21784771 -0.020179866\n#>  [2,] -0.71815778  0.694310797 -0.08512334 -0.083695875\n#>  [3,] -0.32821149  1.149327966 -0.15063193  0.048192518\n#>  [4,]  3.55460544  1.639197045  0.33285528  0.058442105\n#>  [5,]  2.76080923 -0.115429590  0.21655835  0.013945759\n#>  [6,]  0.79787196 -0.148376931  0.02686889 -0.156011369\n#>  [7,] -1.86516992  2.661589241  0.20657318 -0.006300067\n#>  [8,]  0.98949799 -1.561151804  0.17739137  0.088920636\n#>  [9,] -3.35440322  0.528103818  0.16066112  0.034570627\n#> [10,]  0.70207339 -1.298996700 -0.29418642 -0.033318049\n#> [11,] -1.46466180 -0.249386278  0.02551176 -0.022539838\n#> [12,] -1.64675005 -0.024226568 -0.06234356 -0.061721168\n#> [13,] -1.23912377 -0.918827637  0.18189450  0.072835609\n#> [14,] -0.01571925 -0.996347013 -0.16727041  0.026577141\n#> [15,]  2.24869193  1.744009038 -0.32301661 -0.035840606\n#> [16,]  1.67266534 -1.476954610  0.14195906 -0.081997377\n#> [17,]  0.21299322  0.031819187 -0.29816043  0.146668682\n#> [18,] -0.25872255  0.082636708 -0.30022953  0.046260068\n#> [19,] -0.43156355 -0.844484815  0.01989843  0.023146309\n#> [20,] -0.14359922 -0.002155231 -0.02705741 -0.057955239\n\n\nEsta entrada era solo para recordar que podemos obtener un PCA simplemente obteniendo los vectores propios de la matriz de correlaciones, y que aquello que vimos en los primeros cursos de la universidad sirve para algo."
  }
]
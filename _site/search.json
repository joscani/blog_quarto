[
  {
    "objectID": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "href": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "title": "EPA, muestreo y partial pooling",
    "section": "",
    "text": "Sale la EPA a a finales de Abril, con datos de Enero a Marzo. Es proverbial el retraso en la publicación de resultados por parte de las administraciones públicas. En intercambio de tweets con Carlos Gil, comentaba la posibilidad de ir actualizando datos poco a poco, en plan como las elecciones: - Al 20% del escrutinio de la EPA, el número de parados y ocupados en España es de X y cosas así.\nGracias a que la EPA tiene un buen diseño muestral no sería tan difícil hacerlo, e incluso realizar buensa estimaciones con poco escrutado, al fin y al cabo la epa es un panel,(los sujetos permanecen en la EPA varias oleadas) y es de suponer que hay cierta relación entre la variable latente “estar en paro” y que se trate del mismo individuo, y relación de esa variable con los de determinado grupo de edad al que pertenece, y que la estimación en otros grupos de edad ayude a estimar la tasa de paro en otro grupo, etc…. En fin, que me lío.\nPues de toda esa estructura e información compartida es de lo que van, grosso modo, los modelos mixtos y el partial pooling. En este mismo blog los he comentado alguna vez y he puesto algún ejemplo. Con la EPA hice hace unos años un ejercicio para ver precisamente cómo, con poca muestra, se pueden tener buenas estimaciones. Aquí os lo dejo, al final hay algunas referencias, justo las que usé (no me gusta poner referencias de cosas que no he leído solo por rellenar).\nPues nada, buen confinamiento, yo voy a ver si instalo la nueva versión de R en mi linux, dudo entre arriesgarme y hacerlo a pelo o usar un docker."
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "",
    "text": "Y seguimos dando vueltas a los datos de post anteriores. Siempre hay quien dice que el bayesiano no sirve para big data y qué se acaba el universo antes de que termine de ajustar tu modelo (esto último creo que se lo he dicho yo alguna vez a Carlos).\nPero ya hemos visto en los dos post anteriores que podemos condensar los datos en menos filas sin perder información, así que , ¿por qué no utilizar un modelo bayesiano?\nDel post anterior\nY tenemos nuestros conjuntos de train y de test en local"
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "Modelo bayesiano.",
    "text": "Modelo bayesiano.\nPues ahora vamos a probar a hacer un modelo bayesiano jerárquico, podríamos hacer el equivalente a glmer usando la librería rstanarm y ajustar varias regresiones logísticas independientes, pero en vez de eso vamos a ver como ajustar directamente la distribución multinomial usando brms.\nLos modelos serían algo así como\n\\[\n\\begin{equation} ans \\sim Multinomial(\\boldsymbol{\\theta}) \\end{equation}\n\\]\nDónde\n\\[\n\\begin{equation}\n\\boldsymbol{\\theta} = \\{\\theta_{Rec}, \\theta_{Best}, \\theta_{Neut}, \\theta_{\\text{No_way}}\\}\n\\end{equation}\n\\]\nLo bueno de stan y de brms es que se puede modelar directamente la Multinomial, es decir, el número de “éxitos” en cada categoría dado un número de intentos. En brms podemos usar trials para especificarlo. Sería el equivalente al weights en glmer. De esta forma podemos trabajar con los datos agregados en vez de tenerlos individuales. Si tengo, 1000 clientes con edad < 21 y valor_cliente = 8, en vez de poner 1000 filas, pongo una columna de frecuencias, que es lo que hemos hecho.\n\nLibrerías\nYo uso cmdstan como backend para brms en vez de rstan, está más actualizado y tarda menos en muestrear.\n\n# Core libraries\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(cmdstanr)\n\n# For beauty plots\nlibrary(ggridges)\n\n## Using all cores. 12 in my machine\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n\n\n\nAdecuando los datos\nPara poder ajustar el modelo de regresión multinomial se necesita tener los datos de una determinada forma, básicamente tener una columna de tipo matriz. Para eso vamos a pivotar los datos y usar cbind\nPivotamos\n\ntrain_wider <-   train_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)), \n    total = Rec + Neut + Best + No_way\n  ) \n\ntest_wider <- test_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)),\n    total = Rec + Neut + Best + No_way\n  )\n\n\nDT::datatable(train_wider)\n\n\n\n\n\n\nY ahora unimos las columnas que indican el conteo en cada perfil de Rec, Best, Neut y NoWay en un columna que es una matriz\n\n# lo hacemos solo para el train, para el test no hace falta\n\ntrain_wider$cell_counts <- with(train_wider, cbind(Rec, Best, Neut, No_way))\nclass(train_wider$cell_counts)\n#> [1] \"matrix\" \"array\"\n\n\nDT::datatable( train_wider %>% \n                 select(tipo, valor_cliente,\n                        cell_counts, everything()\n))\n\n\n\n\n\n\nPues ya podemos ajustar el modelo. Brms tiene una función get_prior para poner las priors por defecto.\nVoy a usar un modelo con efectos aleatorios que tarda unos pocos minutos, pero si usamos cell_counts | trials(total) ~ edad_cat + valor_cliente el modelo se ajusta en menos de 60 segundos. Bueno, vamos a verlo\n\n\nAjuste de los modelos\nModelo efectos fijos\n\nformula_efectos_fijos <- brmsformula(\n  cell_counts | trials(total) ~ edad_cat + valor_cliente\n)\n\n# get priors\npriors <- get_prior(formula_efectos_fijos, train_wider, family = multinomial())\n\ntictoc::tic(\"Modelo efectos fijos\")\nmodel_multinomial1 <- brm(formula_efectos_fijos, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\",\n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 2 finished in 47.9 seconds.\n#> Chain 3 finished in 48.8 seconds.\n#> Chain 1 finished in 49.5 seconds.\n#> Chain 4 finished in 53.0 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 49.8 seconds.\n#> Total execution time: 53.1 seconds.\ntictoc::toc()\n#> Modelo efectos fijos: 70.554 sec elapsed\n\nModelo con efectos aleatorios\nY tarda unos 9 minutos o así\n\nformula <- brmsformula(\n  cell_counts | trials(total) ~ (1|edad_cat) + (1|valor_cliente\n))\n\n# get priors\npriors <- get_prior(formula, train_wider, family = multinomial())\n\nPodemos ver las priors que ha considerado por defecto. Y vemos las priors que ha tomado para modelar la distribución de las \\(\\sigma\\) asociadas a edad_cat y valor_cliente\n\npriors\n#>                 prior     class      coef         group resp    dpar nlpar lb\n#>                (flat) Intercept                                              \n#>  student_t(3, 0, 2.5) Intercept                               muBest         \n#>  student_t(3, 0, 2.5)        sd                               muBest        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5) Intercept                               muNeut         \n#>  student_t(3, 0, 2.5)        sd                               muNeut        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5) Intercept                              muNoway         \n#>  student_t(3, 0, 2.5)        sd                              muNoway        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente      muNoway        0\n#>  ub       source\n#>          default\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n\n\ntictoc::tic(\"modelo mixto\")\nmodel_multinomial2 <- brm(formula, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\", \n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 715.6 seconds.\n#> Chain 4 finished in 728.4 seconds.\n#> Chain 2 finished in 728.8 seconds.\n#> Chain 3 finished in 732.7 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 726.4 seconds.\n#> Total execution time: 732.8 seconds.\ntictoc::toc()\n#> modelo mixto: 755.055 sec elapsed\n\nPodemos ver el modelo con\n\nsummary(model_multinomial2)\n#>  Family: multinomial \n#>   Links: muBest = logit; muNeut = logit; muNoway = logit \n#> Formula: cell_counts | trials(total) ~ (1 | edad_cat) + (1 | valor_cliente) \n#>    Data: train_wider (Number of observations: 184) \n#>   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 12000\n#> \n#> Group-Level Effects: \n#> ~edad_cat (Number of levels: 5) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.95      0.45     0.43     2.14 1.00     2460\n#> sd(muNeut_Intercept)      0.55      0.31     0.23     1.40 1.00     2566\n#> sd(muNoway_Intercept)     0.55      0.29     0.24     1.38 1.00     2237\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      4370\n#> sd(muNeut_Intercept)      4061\n#> sd(muNoway_Intercept)     3810\n#> \n#> ~valor_cliente (Number of levels: 10) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.98      0.30     0.58     1.74 1.00     1792\n#> sd(muNeut_Intercept)      0.53      0.16     0.31     0.94 1.00     1841\n#> sd(muNoway_Intercept)     1.71      0.44     1.07     2.78 1.00     1681\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      3351\n#> sd(muNeut_Intercept)      3528\n#> sd(muNoway_Intercept)     2940\n#> \n#> Population-Level Effects: \n#>                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> muBest_Intercept     -0.05      0.55    -1.15     1.01 1.00     1209     2238\n#> muNeut_Intercept      1.02      0.33     0.35     1.67 1.01     1036     2136\n#> muNoway_Intercept     0.68      0.60    -0.55     1.85 1.00      907     1612\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPintarlo\n\nplot(model_multinomial2, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE incluso ver el modelo en stan\n\nmodel_multinomial2$model\n#> // generated with brms 2.18.0\n#> functions {\n#>   /* multinomial-logit log-PMF\n#>    * Args:\n#>    *   y: array of integer response values\n#>    *   mu: vector of category logit probabilities\n#>    * Returns:\n#>    *   a scalar to be added to the log posterior\n#>    */\n#>   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#>     return multinomial_lpmf(y | softmax(mu));\n#>   }\n#> }\n#> data {\n#>   int<lower=1> N; // total number of observations\n#>   int<lower=2> ncat; // number of categories\n#>   array[N, ncat] int Y; // response array\n#>   array[N] int trials; // number of trials\n#>   // data for group-level effects of ID 1\n#>   int<lower=1> N_1; // number of grouping levels\n#>   int<lower=1> M_1; // number of coefficients per level\n#>   array[N] int<lower=1> J_1; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_1_muBest_1;\n#>   // data for group-level effects of ID 2\n#>   int<lower=1> N_2; // number of grouping levels\n#>   int<lower=1> M_2; // number of coefficients per level\n#>   array[N] int<lower=1> J_2; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_2_muBest_1;\n#>   // data for group-level effects of ID 3\n#>   int<lower=1> N_3; // number of grouping levels\n#>   int<lower=1> M_3; // number of coefficients per level\n#>   array[N] int<lower=1> J_3; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_3_muNeut_1;\n#>   // data for group-level effects of ID 4\n#>   int<lower=1> N_4; // number of grouping levels\n#>   int<lower=1> M_4; // number of coefficients per level\n#>   array[N] int<lower=1> J_4; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_4_muNeut_1;\n#>   // data for group-level effects of ID 5\n#>   int<lower=1> N_5; // number of grouping levels\n#>   int<lower=1> M_5; // number of coefficients per level\n#>   array[N] int<lower=1> J_5; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_5_muNoway_1;\n#>   // data for group-level effects of ID 6\n#>   int<lower=1> N_6; // number of grouping levels\n#>   int<lower=1> M_6; // number of coefficients per level\n#>   array[N] int<lower=1> J_6; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_6_muNoway_1;\n#>   int prior_only; // should the likelihood be ignored?\n#> }\n#> transformed data {\n#>   \n#> }\n#> parameters {\n#>   real Intercept_muBest; // temporary intercept for centered predictors\n#>   real Intercept_muNeut; // temporary intercept for centered predictors\n#>   real Intercept_muNoway; // temporary intercept for centered predictors\n#>   vector<lower=0>[M_1] sd_1; // group-level standard deviations\n#>   array[M_1] vector[N_1] z_1; // standardized group-level effects\n#>   vector<lower=0>[M_2] sd_2; // group-level standard deviations\n#>   array[M_2] vector[N_2] z_2; // standardized group-level effects\n#>   vector<lower=0>[M_3] sd_3; // group-level standard deviations\n#>   array[M_3] vector[N_3] z_3; // standardized group-level effects\n#>   vector<lower=0>[M_4] sd_4; // group-level standard deviations\n#>   array[M_4] vector[N_4] z_4; // standardized group-level effects\n#>   vector<lower=0>[M_5] sd_5; // group-level standard deviations\n#>   array[M_5] vector[N_5] z_5; // standardized group-level effects\n#>   vector<lower=0>[M_6] sd_6; // group-level standard deviations\n#>   array[M_6] vector[N_6] z_6; // standardized group-level effects\n#> }\n#> transformed parameters {\n#>   vector[N_1] r_1_muBest_1; // actual group-level effects\n#>   vector[N_2] r_2_muBest_1; // actual group-level effects\n#>   vector[N_3] r_3_muNeut_1; // actual group-level effects\n#>   vector[N_4] r_4_muNeut_1; // actual group-level effects\n#>   vector[N_5] r_5_muNoway_1; // actual group-level effects\n#>   vector[N_6] r_6_muNoway_1; // actual group-level effects\n#>   real lprior = 0; // prior contributions to the log posterior\n#>   r_1_muBest_1 = sd_1[1] * z_1[1];\n#>   r_2_muBest_1 = sd_2[1] * z_2[1];\n#>   r_3_muNeut_1 = sd_3[1] * z_3[1];\n#>   r_4_muNeut_1 = sd_4[1] * z_4[1];\n#>   r_5_muNoway_1 = sd_5[1] * z_5[1];\n#>   r_6_muNoway_1 = sd_6[1] * z_6[1];\n#>   lprior += student_t_lpdf(Intercept_muBest | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNeut | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNoway | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_5 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_6 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] muBest = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNeut = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNoway = rep_vector(0.0, N);\n#>     // linear predictor matrix\n#>     array[N] vector[ncat] mu;\n#>     muBest += Intercept_muBest;\n#>     muNeut += Intercept_muNeut;\n#>     muNoway += Intercept_muNoway;\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n]\n#>                    + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNeut[n] += r_3_muNeut_1[J_3[n]] * Z_3_muNeut_1[n]\n#>                    + r_4_muNeut_1[J_4[n]] * Z_4_muNeut_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNoway[n] += r_5_muNoway_1[J_5[n]] * Z_5_muNoway_1[n]\n#>                     + r_6_muNoway_1[J_6[n]] * Z_6_muNoway_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       mu[n] = transpose([0, muBest[n], muNeut[n], muNoway[n]]);\n#>     }\n#>     for (n in 1 : N) {\n#>       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#>     }\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#>   target += std_normal_lpdf(z_1[1]);\n#>   target += std_normal_lpdf(z_2[1]);\n#>   target += std_normal_lpdf(z_3[1]);\n#>   target += std_normal_lpdf(z_4[1]);\n#>   target += std_normal_lpdf(z_5[1]);\n#>   target += std_normal_lpdf(z_6[1]);\n#> }\n#> generated quantities {\n#>   // actual population-level intercept\n#>   real b_muBest_Intercept = Intercept_muBest;\n#>   // actual population-level intercept\n#>   real b_muNeut_Intercept = Intercept_muNeut;\n#>   // actual population-level intercept\n#>   real b_muNoway_Intercept = Intercept_muNoway;\n#> }\n\nViendo el código en stan que genera brms utiliza parametrización con multinomial_lpmf que toma el log de la probabilidad de la multinomial y usa softmax sobre el predictor lineal. multivariate_discrete_stan\nEn la parte de functions tiene\nreal multinomial_logit2_lpmf(int[] y, vector mu) {\n      return multinomial_lpmf(y | softmax(mu));\n  }\nY en la de model\n   for (n in 1:N) {\n      target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n     }\nY en la parte del predictor lineal mu[n] es dónde ha ido añadiendo los group levels effects.\nPor ejemplo la parte de la edad_cat para la categoría Best está en la parte de transformed parameters dónde z_1[1] se modela como normal y sd_1 como una t de student\nr_1_muBest_1 = (sd_1[1] * (z_1[1]));\nY en la parte de model va añadiendo términos al muBest que es al final el que entra en la parte de la verosimilitud.\nmuBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n] + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\nAquí añade el efecto de la edad r_1_muBest_1[J_1[n]] lo multiplica por Z_1_mubest_1[n] que es el indicador en los datos de la matriz Z para los efectos aleatorios (todo igual a 1) y luego añade el efecto de la variable valor_cliente.\nLa verdad es que eel bloque model que genera brms es un poco complicado. Imagino que genera código optimizado. Para los que quieran verlo todo con stan directamente este libro tiene un ejemplo básico\nEn brms tenemos la función make_standata que nos genera los datos tal y como se los pasa a Stan.\n\ndatos_stan <- make_standata(formula, data = train_wider, \n              family = multinomial(),\n              prior =  priors)\n\n\nnames(datos_stan)\n#>  [1] \"N\"             \"Y\"             \"trials\"        \"ncat\"         \n#>  [5] \"K_muBest\"      \"X_muBest\"      \"Z_1_muBest_1\"  \"Z_2_muBest_1\" \n#>  [9] \"K_muNeut\"      \"X_muNeut\"      \"Z_3_muNeut_1\"  \"Z_4_muNeut_1\" \n#> [13] \"K_muNoway\"     \"X_muNoway\"     \"Z_5_muNoway_1\" \"Z_6_muNoway_1\"\n#> [17] \"J_1\"           \"J_2\"           \"J_3\"           \"J_4\"          \n#> [21] \"J_5\"           \"J_6\"           \"N_1\"           \"M_1\"          \n#> [25] \"NC_1\"          \"N_2\"           \"M_2\"           \"NC_2\"         \n#> [29] \"N_3\"           \"M_3\"           \"NC_3\"          \"N_4\"          \n#> [33] \"M_4\"           \"NC_4\"          \"N_5\"           \"M_5\"          \n#> [37] \"NC_5\"          \"N_6\"           \"M_6\"           \"NC_6\"         \n#> [41] \"prior_only\"\n\n\n# datos\ndatos_stan$N\n#> [1] 184\n\n# numero de niveles edad\ndatos_stan$N_1\n#> [1] 5\n\n# numero niveles valor_cliente\ndatos_stan$N_2\n#> [1] 10\n\nEn los J_1, J_2, está codificado a que nivel de edad y valor_cliente perteneces esa fila. J_3 y J_4 es igual a J_1 y J_2. Lo repite para cada categoría de respuesta.\n\ndatos_stan$J_1\n#>   [1] 1 2 2 3 4 2 4 5 1 5 3 3 4 2 2 2 3 4 1 3 4 4 5 1 2 2 3 4 5 1 1 2 3 3 4 1 3\n#>  [38] 4 4 1 1 1 2 5 1 2 4 4 5 1 1 2 4 4 5 5 2 3 4 5 2 2 4 4 4 5 5 3 4 1 4 1 3 4\n#>  [75] 1 1 2 2 3 4 5 5 5 4 5 1 3 3 4 5 1 1 3 4 5 1 1 3 5 1 2 3 3 1 4 5 3 1 1 3 1\n#> [112] 1 2 3 3 1 2 3 1 2 2 5 3 3 2 3 5 1 4 5 3 5 5 2 4 5 1 2 2 5 1 3 3 2 1 2 4 2\n#> [149] 3 1 3 4 4 1 3 4 5 5 3 4 5 2 4 5 3 4 2 5 1 4 1 2 2 1 2 3 5 2 2 4 3 2 5 3\n\n\ndatos_stan$J_2\n#>   [1]  1  1  1  1  1  2  2  3  3  3  3  3  3  4  5  5  5  5  6  6  6  6  7  7  7\n#>  [26]  7  7  7  8  8  8  8  8  8  8  9  9  9  9 10 10  1  1  2  2  2  2  2  4  3\n#>  [51]  3  3  3  3  4  4  4  4  4  5  5  5  5  5  5  6  6  6  6  7  7  8  8  8  9\n#>  [76]  9  9  9  9  9  1  1  1  1  2  2  2  2  3  4  4  4  4  4  5  5  5  5  6  6\n#> [101]  6  6  6  7  7  8 10  1  1  1  2  2  2  2  3  3  3  3  5  6  6  7  7  7  8\n#> [126]  8  9  9  9  1  1  2  2  2  2  3  4  4  4  5  5  5  5  6  8  8  8  9  9 10\n#> [151]  1  1  1  4  4  4  6  7  7  7  8  8  8  9  2  4  3  5  6  6  7  7  1  6  9\n#> [176]  9  3  3  7 10  4 10  8 10\n\nPero yo estoy interesado en ver 2 cosas, como de bien predice sobre test y cuál es la probabilidad de cada clase condicionada a cada perfil\nPredicción\nPodemos obtener o bien todas las estimaciones o resumirlas\n\npredicciones_test <-  posterior_predict(model_multinomial2, newdata = test_wider)\n\nAquí lo que tenemos es un array de dimensiones 12000, 180, 4 . Que se corresponde a tener las 12000 estimaciones ( 4 cadenas x 3000 muestras efectivas) , para las 180 filas del conjunto de test\n\ndim(predicciones_test)\n#> [1] 12000   181     4\n\nPor ejemplo para la fila 35 de test que sería\n\ntest_wider[1,]\n#> # A tibble: 1 × 8\n#>   tipo  valor_cliente edad_cat   Rec  Best  Neut No_way total\n#>   <fct>         <dbl> <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl>\n#> 1 C                 0 21- 40     141   110   336     80   667\n\nY las predicciones (de la 1 a la 20) de las 1200\n\npredicciones_test[1:20, 1, ]\n#>       Rec Best Neut No_way\n#>  [1,]  75  153  255    184\n#>  [2,]  74  151  259    183\n#>  [3,]  78  126  239    224\n#>  [4,]  88  157  224    198\n#>  [5,]  79  148  260    180\n#>  [6,]  82  134  257    194\n#>  [7,]  61  145  250    211\n#>  [8,]  60  152  257    198\n#>  [9,]  80  132  242    213\n#> [10,]  62  136  263    206\n#> [11,]  72  151  272    172\n#> [12,]  75  133  259    200\n#> [13,]  78  150  240    199\n#> [14,]  78  129  244    216\n#> [15,]  78  137  249    203\n#> [16,]  73  136  265    193\n#> [17,]  68  142  251    206\n#> [18,]  68  157  234    208\n#> [19,]  67  143  248    209\n#> [20,]  62  180  227    198\n\nComo ahora todo es tidy voy a usar tidybayespara tener esa predicción.\n\npredicciones_tidy <- test_wider %>% \n  add_epred_draws(model_multinomial2) \n\nY se nos ha quedado un dataset muy muy grande\n\ndim(predicciones_tidy)\n#> [1] 8688000      14\n\n\nDT::datatable(predicciones_tidy %>% \n                ungroup() %>% \n                sample_n(30) %>% \n                select(edad_cat, valor_cliente,.category, .epred))\n\n\n\n\n\n\nPero si quisiéramos pintar las probabilidades estimadas tendríamos que dividir el valor predicho de cada categoría por el total de clientes en cada fila del conjunto de datos de test. Hay una forma más sencilla construyendo un conjunto de datos que tenga todas las combinaciones de edad_cat y valor_cliente y añadiendo columna totalcon valor 1.\n\n\nfake_data <- test_wider %>% \n  tidyr::expand(edad_cat, valor_cliente) %>% \n  mutate(total = 1)\n\n\ndf_pintar <-  fake_data %>% \n  add_epred_draws(model_multinomial2) %>% \n  mutate(valor_cliente = as_factor(valor_cliente))\n\nDe esta forma, al tener total = 1, el modelo devuelve la probabilidad de cada clase, si total = 13, hubiera devuelto “el reparto” de esos 13 individuos en los 4 grupos\n\nDT::datatable(df_pintar %>% \n  sample_n(30) %>% \n  select(edad_cat, valor_cliente, .category, .epred))\n\n\n\n\n\n\nAñadir las 12000 predicciones por fila ya “sólo” nos deja unos 2 millones de filas\n\ndim(df_pintar)\n#> [1] 2400000       9\n\nPintemos\nPor ejemplo si queremos ver las estimaciones que le da según la edad podemos ver la distribución posteriori de la probabilidad de cada segmento condicionada a cada grupo de edad. Salen distribuciones con varias modas debido a la variable valor_cliente que no estamos representando\n\ndf_pintar %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n             ) +\n  geom_density_ridges(scale = 0.8, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() \n\n\n\n\n\n\n\n\nSi vemos la posteriori para los clientes de mayor valor. Se ve claramente que a menor edad mayor probabilidad de pertenecer al segmento “Best” , mientras que a mayor edad es mucho más probabilidad del segmento “No_way”.\n\ndf_pintar %>%  \n  filter(valor_cliente == 0) %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Cliente valor: 0\")\n\n\n\n\n\n\n\n\nTeniendo toda la distribución podemos ver los resultados desde otro punto de vista. Por ejemplo, ver las probabilidades para los menores de 21.\n\ndf_pintar %>%  \n  filter(edad_cat %in% c(\"<21\")) %>% \n  ggplot(aes(x=.epred, y = valor_cliente, fill = .category) \n  ) +\n  geom_density_ridges(scale = 3, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Clientes menores de 21\\n Probabilidades estimadas\")\n\n\n\n\n\n\n\n\nEn fin, que se puede hacer estadística bayesiana aún con grandes volúmenes de datos, si te conviertes en lo que mi amigo Antonio llama un “artesano del dato”.\nFeliz semana"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Series\n\nCachitos nochevieja\nPost relacionados con extracción de imágenes y análisis de subtítulos de “cachitos nochevieja”\n\n\nJulia\nPost relacionados Julia\n\n\n\nTodos los posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nExplicatividad no usual\n\n\n\n\n\n\n\nestadística\n\n\nranger\n\n\nExplicatividad\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\n\n\n\nestadística\n\n\nbrms\n\n\nanálisis bayesiano\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsejos para dejar spss\n\n\n\n\n\n\n\nestadística\n\n\nsociología\n\n\n2022\n\n\nspss\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\nJulia\n\n\nproduccion\n\n\nlinux\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\ndocker\n\n\nR\n\n\napi\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n36 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\nestadística\n\n\nInvestigación operativa\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n47 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\nJulia\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\nestadística\n\n\npython\n\n\nR\n\n\nInvestigación operativa\n\n\nJulia\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPluralista\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\ntextmining\n\n\nocr\n\n\n2022\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nCocinando\n\n\n\n\n\n\n\nmuestreo\n\n\n2022\n\n\nencuestas electorales\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\n\n\n\nestadística\n\n\nspark\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecturas para el finde\n\n\n\n\n\n\n\nestadística\n\n\nanálisis bayesiano\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n¿A dónde va Vicente?\n\n\n\n\n\n\n\nárboles\n\n\nciencia de datos\n\n\nh2o\n\n\nestadísticca\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\ncorrespondencias\n\n\nfactorización\n\n\nfactorial\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿A/B qué?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\n\n\n\nR\n\n\npython\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\nsoftware\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputando datos. La estructura importa\n\n\n\n\n\n\n\nestadística\n\n\nimputación\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\n\n\n\nestadística\n\n\nbig data\n\n\nanálisis bayesiano\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\n\n\n\n2021\n\n\nR\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\n\n\n\nciencia de datos\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2021\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nUna colina\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\nlinux\n\n\npolémica\n\n\nocr\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTendencias\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte II\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte I\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplillo con NMF\n\n\n\n\n\n\n\nestadística\n\n\ncorrespondencias\n\n\nfactorización\n\n\nnmf\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\n\n\n\nestadística\n\n\nempresas\n\n\nbig data\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 4.0.2 en amazon linux\n\n\n\n\n\n\n\nR\n\n\nlinux\n\n\naws\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicción, Estimación y Atribución\n\n\n\n\n\n\n\nestadística\n\n\nciencia de datos\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactoriales….\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPA, muestreo y partial pooling\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncuesta\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmuestreo\n\n\nencuestas\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\nDec 4, 2022\n\n\nConsejos para dejar spss\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 29, 2022\n\n\nLeaflet example\n\n\n\n\nOct 27, 2022\n\n\nAquí estoy de nuevo\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\nDec 30, 2020\n\n\n¿Y si … ? Parte II\n\n\n\n\nNov 15, 2020\n\n\n¿Y si … ? Parte I\n\n\n\n\nOct 21, 2020\n\n\nEjemplillo con NMF\n\n\n\n\nOct 18, 2020\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\nOct 15, 2020\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\nAug 20, 2020\n\n\nR 4.0.2 en amazon linux\n\n\n\n\nJun 11, 2020\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\nJun 7, 2020\n\n\nPredicción, Estimación y Atribución\n\n\n\n\nJun 4, 2020\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\nJun 2, 2020\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\nMay 24, 2020\n\n\nFactoriales….\n\n\n\n\nApr 28, 2020\n\n\nEPA, muestreo y partial pooling\n\n\n\n\nApr 8, 2020\n\n\nEncuesta\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2021.html",
    "href": "2021.html",
    "title": "2021",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2023.html",
    "href": "2023.html",
    "title": "2023",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2020/04/08/encuesta/index.html",
    "href": "2020/04/08/encuesta/index.html",
    "title": "Encuesta",
    "section": "",
    "text": "Estudio serológico covid19\nEl muestreo, ese gran olvidado. Se trata de una herramienta muy útil cuando se quiere saber una o varias características de una población pero, por lo que sea, no es factible abordar a toda la población o porque se necesita tener una idea aproximada de dicha característica de forma rápida.\nBueno, pues si queremos saber el porcentaje y el número de personas que han tenido el coronavirus por provincia la herramienta adecuada es el muestreo. Se trata de elegir una muestra representativa a nivel provincial y hacerles test serológicos a todos los incluidos en esa muestra y así poder extrapolar los resultados al conjunto de la provincia. Por fin, el Instituto de Salud Carlos III junto con el INE van a realizar dicho estudio.\nEn todo muestreo hay una fase crucial, que es la del diseño muestral, tengo que decir que después de leer el documento técnico que me parece un muy buen diseño. Se trata de un diseño bietápico estratificado con un tamaño mínimo de 600 personas por provincia y dónde por ejemplo se van a hacer 5000 encuestas en Madrid.\nEl tamaño muestral total elegido, 30 mil hogares (unos 62 mil personas), y la implicación del INE garantizan la rigurosidad y la representatividad de la encuesta. Yo por mi parte, solo comentar que una vez se tengan los microdatos, existen formas de mejorar algo las estimaciones en áreas pequeñas, entendiendo áreas pequeñas a combinaciones de variables con poca representación en la muestra, por ejemplo, si quisieramos saber la proporción de mujeres contagiadas en Cádiz cuya edad esté entre 20 y 25 años. En ese caso, es probable que una estimación directa\n\\[ \\hat{Prop} = \\dfrac{\\text{Positivos en ese grupo}}{\\text{Total personas encuestadas en ese grupo}} \\] sea poco precisa debido a que haya caído poca muestra en ese grupo.\nPara estos casos puede ser útil la utilización de estimaciones con partial pooling, entrada blog. A colación de esto, hice un estudio hace unos años sobre como incluso con poca muestra las estimaciones de este tipo suelen arrojar mejores estimaciones, aquí"
  }
]
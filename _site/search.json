[
  {
    "objectID": "2022.html",
    "href": "2022.html",
    "title": "2022",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html",
    "href": "2022/06/21/io-parte-1/index.html",
    "title": "IO Parte 1",
    "section": "",
    "text": "Allá por el año 1997 más o menos andaba yo estudiando Investigación Operativa en la Universidad de Granada. Recuerdo aprender el archiconocido algoritmo del simplex y algo también sobre programación entera (dónde el dominio de las variables está en \\(\\mathcal{Z}\\) ). No se me daba muy bien al principio, pero si recuerdo que luego me acabó gustando y el día que encuentre mis apuntes os pondré una demostración que desarrollé para un teorema que tenía algo que ver con la relación entre espacio primal y el dual.\nBueno, dejando de lado las batallitas de final del siglo pasado y debido a que estuve hace poco en la SEIO 2022 en Granada y coincidí con grandes profesionales de este tema, como por ejemplo el gran Alberto Torrejón Valenzuela, estoy convencido de que la investigación operativa es una de las grandes áreas que aún queda por explotar en las empresas.\nSe lleva haciendo Investigación Operativa desde hace tiempo, véase esto. Por otro lado creo que va a ser el próximo hype por las señales que estoy viendo, la primera de ellas es el cambio de nombre de la materia, ahora estoy empezando escuchar Analítica prescriptiva en vez de Investigación Operativa. Y como es norma en este mundillo, el cambio de nombre precede a la ¿burbuja?.\nPues vamos al grano, en esto de la investigación operativa se ha desarrollado mucho software para resolver este tipo de problemas, a partir de ahora los llamaremos solvers. Dentro de estos podemos destacar solvers comerciales como CPlex y Gurobi, pero también hay software libre como GLPK dentro del proyecto GNU, o sin olvidarnos de la fundación COIN-OR dónde se han desarrollado muchos solvers de manera opensource.\nDentro de nuestros 3 lenguajes favoritos (R, Julia y Python) hay librerías que permiten hacer de API para los diferentes solvers, tanto los de software libre como los comerciales. Paso a enumerar 3 proyectos, uno para cada lenguaje y pongo enlace.\nEn realidad, en principio uno podría decir que se trata sólo de sintaxis y que da igual cual uses pues al final todos utilizan los mismos solvers en el backend . No obstante, hay problemas en los que la misma construcción del mismo para pasárselo al solver puede tardar bastante y, según esto, JuMP parece estar a la altura con respecto a los softwares comerciales como GAMS en la generación del modelo y envío al solver.\nVeamos ahora un ejemplillo tonto de programación lineal y cual es la sintaxis (al fin y al cabo es sólo eso) en cada uno de los lenguajes.\nTenemos el siguiente problema de programación lineal\nY el objetivo es encontrar los valores de x e y que cumpliendo las restricciones minimicen la función 7x + 8y. Para los 3 lenguajes vamos a usar GLPK como solver"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#r",
    "href": "2022/06/21/io-parte-1/index.html#r",
    "title": "IO Parte 1",
    "section": "R",
    "text": "R\nlibrary(ROI)\n## ROI: R Optimization Infrastructure\n## Registered solver plugins: nlminb, alabama, glpk.\n## Default solver: auto.\nlibrary(ROI.plugin.glpk)\nHay que definir el objetivo, las restricciones y los límites de la variable. Para las restricciones en R hay que poner los coeficientes en forma de matriz , de ahí el uso de rbind\nobjetivo          = L_objective(c(7, 8), names = c(\"x\", \"y\"))\nrestricciones     = L_constraint(L = rbind(c(3, 4), c(2, 1)),\n                                   dir = c(\"==\", \">=\"),\n                                   rhs = c(9, 3))\nlimites_variables = V_bound(\n  li = 1:2,\n  ui = 1:2,\n  lb = c(-100, -Inf),\n  ub = c(Inf, 100)\n)\nAhora usamos la función OP que es el constructor del problema, por defecto considera que se trata de un problema de maximización OP(objective, constraints, types, bounds, maximum = FALSE)\nlp  <- OP(\n  objective = objetivo,\n  constraints = restricciones,\n  bounds = limites_variables\n)\nROI_applicable_solvers(lp)\n## [1] \"alabama\" \"glpk\"\nResolvemos con glpk\n(sol <- ROI_solve(lp, solver = \"glpk\"))\n## Optimal solution found.\n## The objective value is: 1.860000e+01\nY vemos cuáles son los valores de x e y que minimizan la función objetivo\nsolution(sol)\n##   x   y \n## 0.6 1.8"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#julia",
    "href": "2022/06/21/io-parte-1/index.html#julia",
    "title": "IO Parte 1",
    "section": "Julia",
    "text": "Julia\nEn Julia tenemos este librito online que va contando estupendamente como usar JuMP y las diferentes formas de utilizarlo.\n\n\nusing JuMP, GLPK\n\nm = Model(GLPK.Optimizer)\n## A JuMP Model\n## Feasibility problem with:\n## Variables: 0\n## Model mode: AUTOMATIC\n## CachingOptimizer state: EMPTY_OPTIMIZER\n## Solver name: GLPK\n\n# Variables y límites\n@variable(m, -100 <= x)\n## x\n@variable(m, y <= 100)\n## y\n\n# Objtivo\n@objective(m, Min, 7x + 8y)\n## 7 x + 8 y\n\n# restricciones\n@constraint(m, constraint1, 3x +  4y == 9)\n## constraint1 : 3 x + 4 y = 9.0\n@constraint(m, constraint2,  2x + 1y  >= 3)\n## constraint2 : 2 x + y ≥ 3.0\n\n# Solving the optimization problem\nJuMP.optimize!(m)\n\n# Printing the optimal solutions obtained\nprintln(\"Optimal Solutions:\")\n## Optimal Solutions:\nprintln(\"x = \", JuMP.value(x))\n## x = 0.5999999999999943\nprintln(\"y = \", JuMP.value(y))\n## y = 1.8000000000000114\nLa verdad que me ha gustado la sintaxis de JuMP , es casi un calco de como lo escribirías a mano. Como curiosidad fijaros en que se puede poner 3x +  4y == 9 , y no hace falta poner 3 * x  + 4 * y\nOtra curiosidad es que desde Julia puedes obtener el modelo en latex\nlatex_formulation(m)\n## $$ \\begin{aligned}\n## \\min\\quad & 7 x + 8 y\\\\\n## \\text{Subject to} \\quad & 3 x + 4 y = 9.0\\\\\n##  & 2 x + y \\geq 3.0\\\\\n##  & x \\geq -100.0\\\\\n##  & y \\leq 100.0\\\\\n## \\end{aligned} $$"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#python",
    "href": "2022/06/21/io-parte-1/index.html#python",
    "title": "IO Parte 1",
    "section": "Python",
    "text": "Python\nEn python usaremos pyomo, el cual también tiene una sintaxis clara.\nfrom pyomo.environ import *\n\nmodel = ConcreteModel()\nmodel.x = Var(domain=NonNegativeReals)\nmodel.y = Var(domain=NonNegativeReals)\n\nmodel.objetivo = Objective(expr = 7*model.x + 8*model.y, sense=minimize)\n\nmodel.constraint1 = Constraint(expr = 3* model.x +  4*model.y == 9)\nmodel.constraint2 = Constraint(expr = 2*model.x + 1*model.y  >= 3)\nresults = SolverFactory('glpk').solve(model)\nif results.solver.status == 'ok':\n    model.pprint()\n## 2 Var Declarations\n##     x : Size=1, Index=None\n##         Key  : Lower : Value : Upper : Fixed : Stale : Domain\n##         None :     0 :   0.6 :  None : False : False : NonNegativeReals\n##     y : Size=1, Index=None\n##         Key  : Lower : Value : Upper : Fixed : Stale : Domain\n##         None :     0 :   1.8 :  None : False : False : NonNegativeReals\n## \n## 1 Objective Declarations\n##     objetivo : Size=1, Index=None, Active=True\n##         Key  : Active : Sense    : Expression\n##         None :   True : minimize : 7*x + 8*y\n## \n## 2 Constraint Declarations\n##     constraint1 : Size=1, Index=None, Active=True\n##         Key  : Lower : Body      : Upper : Active\n##         None :   9.0 : 3*x + 4*y :   9.0 :   True\n##     constraint2 : Size=1, Index=None, Active=True\n##         Key  : Lower : Body    : Upper : Active\n##         None :   3.0 : 2*x + y :  +Inf :   True\n## \n## 5 Declarations: x y objetivo constraint1 constraint2\nprint('Objetivo = ', model.objetivo())\n## Objetivo =  18.6\nprint('\\nDecision Variables')\n## \n## Decision Variables\nprint('x = ', model.x())\n## x =  0.6\nprint('y = ', model.y())\n## y =  1.8"
  },
  {
    "objectID": "2022/06/21/io-parte-1/index.html#miscelánea",
    "href": "2022/06/21/io-parte-1/index.html#miscelánea",
    "title": "IO Parte 1",
    "section": "Miscelánea",
    "text": "Miscelánea\nLa investigación operativa es un campo muy amplio, desde problemas de asignación de turnos, optimizar contenedores en grandes buques que pasan por varios puertos, optimización de gasto en medios para maximizar ventas (Marketing Mix Modelling), problemas de localización (dónde poner una tienda o gasolinera nueva dada una demanda existente y competencia), problemas de grafos, etc.\nHay muchos investigadores que se están dedicando a resolver este tipo de cosas, y dónde no se trata tanto de usar tal o cual solver, sino de formular bien el problema, ya que diferentes formulaciones del mismo problema igualmente válidas dan lugar a tiempos de cómputo muy diferentes. No es de extrañar que se tarden muchas horas en encontrar soluciones a problemas determinados.\nPor otro lado, no puedo dejar de señalar el problema de los incentivos perversos. Me explico, a los investigadores que están en estos temas se les valora por paper publicado en revista de alto impacto, por lo que una vez le han publicado un artículo pasan al siguiente, olvidando la necesaria transferencia entre universidad y empresa. No es culpa suya, el sistema funciona así. Así que animo a todos aquellos que se dedican en serio a estos temas a meter la patita en el mundo de la empresa. Hay mucho trabajo que hacer y dinero que ganar."
  },
  {
    "objectID": "2022/02/06/pluralista/index.html",
    "href": "2022/02/06/pluralista/index.html",
    "title": "Pluralista",
    "section": "",
    "text": "Ando viendo los vídeos de Richard McElreath , Statistical Rethinking 2022 y ciertamente me están gustando mucho. En la segunda edición de su libro hace hincapié en temas de inferencia causal. Cuenta bastante bien todo el tema de los “confounders”, “forks”, “colliders” y demás. Además lo hace simulando datos, por lo que entiende todo de forma muy sencilla. Un par de conceptos que me han llamado la atención son por ejemplo cuando dice que condicionar por una variable no significa lo mismo en un modelo de regresión al uso que en uno bayesiano, en el segundo caso significa incluir esa variable en la distribución conjunta. Esto permite por ejemplo que bajo el marco de un modelo bayesiano se pueda condicionar incluso por un “collider” cosa que los entendidos de la inferencia causal prohíben expresamente pues eso abre un camino no causal en el DAG definido.\nSegún la RAE , pluralismo significa\ny en los videos se toma dicha postura, por ejemplo, se especifica el modelo teórico utilizando los diagramas causales y el Back door criterio para ver sobre qué variables hay que condicionar o no , para ver el efecto total de X sobre Y o para estimar el efecto directo.\nHay un ejemplo muy bueno en este post de Richard.\nNota: Este post es simplemente para entender un poco el post de Richard, el mérito es totalmente de él.\nBásicamente es una situación dónde se quiere estimar el efecto que tiene sobre el número de hijos que tiene una mujer, el número de hijos que tuvo su madre. En el diagrama causal también se indica la influencia que tiene el orden de nacimiento de de la madre y de la hija.\nDiagrama causal:\nSi queremos estimar el efecto global o el directo de M sobre D, habría que condicionar por U (siguiendo el backdoor criterio), y al ser no observable, no se puede estimar.\n¿Cómo podemos “estimar” el efecto de M sobre D dado que no podemos condicionar (en el sentido clásico) sobre U? .\nRichard propone lo que el llama “full luxury bayesian” que consiste en estimar a la vez todo el DAG y luego generar simulaciones usando la distribución conjunta obtenida para medir el efecto de la “intervención” y poder obtener el efecto causal.\nNótese que cuando en el DAG las relaciones se pueden expresar como modelos lineales, se puede estimar todo el DAG usando técnicas como los modelos de ecuaciones estructurales o el path analysis."
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#simulación",
    "href": "2022/02/06/pluralista/index.html#simulación",
    "title": "Pluralista",
    "section": "Simulación",
    "text": "Simulación\nSimulamos unos datos de forma qué vamos a conocer la “verdad” de la relaciones entre variables, que para eso simulamos.\n\n\nset.seed(1908)\nN <- 1000 # número de pares, 1000 madres y 1000 hijas\n\n\nU <- rnorm(N,0,1) # Simulamos el confounder\n\n# orden de nacimiento y \nB1 <- rbinom(N,size=1,prob=0.5)  # 50% de madres nacieeron en primer lugar\nM <- rnorm( N , 2 * B1 + U )\n\nB2 <- rbinom(N,size=1,prob=0.5) # 50% son las primogénitas\nD <- rnorm( N , 2  *B2 + U + 0 * M )\n\nEn esta simulación se ha forzado que el efecto del número de hijos de la madre sobre el núemro de hijos de la hija sea nulo. Por tanto sabemos que el efecto de M sobre D es 0..\nSi hacemos un modelo sin condicionar, tenemos sesgo\n\nlm(D ~ M)\n#> \n#> Call:\n#> lm(formula = D ~ M)\n#> \n#> Coefficients:\n#> (Intercept)            M  \n#>      0.7108       0.2930\n\nCondicionando por B1 también, de hecho tenemos la situación de amplificación del sesgo\n\nlm(D ~ M + B1)\n#> \n#> Call:\n#> lm(formula = D ~ M + B1)\n#> \n#> Coefficients:\n#> (Intercept)            M           B1  \n#>      1.0356       0.4606      -1.0441\n\n\nlm(D ~ M + B1 +B2 )\n#> \n#> Call:\n#> lm(formula = D ~ M + B1 + B2)\n#> \n#> Coefficients:\n#> (Intercept)            M           B1           B2  \n#>    -0.01621      0.46913     -0.91307      2.01487\n\n\nlm(D ~ M + B2)\n#> \n#> Call:\n#> lm(formula = D ~ M + B2)\n#> \n#> Coefficients:\n#> (Intercept)            M           B2  \n#>     -0.3204       0.3231       2.0550\n\nEn esta situación, no podemos estimar el efecto de M sobre D utilizando un solo modelo.\nUna forma de estimar el efecto de M sobre D es tirar de path analysis, que en este caso se puede al ser las relaciones lineales.\nSea:\n\nb: Efecto de B1 sobre M\nm: Efecto de M sobre D\n\nSe tiene que\n\\[Cov(B1, D ) = b\\cdot m \\cdot Var(B1)\\] Y como\n\\[b = \\dfrac{Cov(B1,M)}{Var(B1)} \\]\nPodemos estimar \\(m\\) como\n\\[m = \\dfrac{Cov(B1,D)}{b \\cdot Var(B1)} = \\dfrac{Cov(B1,D)}{Cov(B1,M)} \\] Y\n\n(m_hat = cov(B1,D) / cov(B1,M))\n#> [1] -0.0563039\n\ny esta estimación está menos sesgada, antes era del orden de 0.1 o 0.2 y ahora la estimación es del orden 0.01. Pero con esta estimación no tenemos información de su distribución sino sólo de esta estimación puntual. Y si las relaciones no fueran lineales no podría usarse, en cambio la siguiente aproximación si funciona"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#full-luxury-bayes",
    "href": "2022/02/06/pluralista/index.html#full-luxury-bayes",
    "title": "Pluralista",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nUtilizamos la librería de Richard rethinking y también cmdstanr para expresar el modelo causal completo y ajustarlo con Stan.\nAhora estimamos el DAG completo, aquí es dónde es diferente de la aproximación causal de Pearl, de esta forma podemos “condicionar” incluso por los colliders, porque condicionar en este marco significa meter esa información dentro de la distribución conjunta.\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\n# No metemos U al ser no observable\ndat <- list(\n  N = N,\n  M = M,\n  D = D,\n  B1 = B1,\n  B2 = B2\n)\nset.seed(1908)\n\nflbi <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 + k*U[i],\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M + k*U[i],\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n    # unmeasured confound\n    vector[N]:U ~ normal(0,1),\n    # priors\n    c(a1,a2,b,m) ~ normal( 0 , 0.5 ),\n    c(k,sigma,tau) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n#> Running MCMC with 4 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 4 finished in 17.1 seconds.\n#> Chain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 3 finished in 17.5 seconds.\n#> Chain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 1 finished in 17.6 seconds.\n#> Chain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 2 finished in 17.9 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 17.6 seconds.\n#> Total execution time: 18.0 seconds.\n\n\npost <- extract.samples(flbi)\nprecis(flbi)\n#>             mean         sd        5.5%      94.5%      n_eff     Rhat4\n#> m     0.01029393 0.04047150 -0.05430275 0.07409054   890.4789 1.0018319\n#> b     1.98864256 0.05811342  1.89487725 2.08168275  3042.1496 1.0007825\n#> a2    0.02836364 0.07310509 -0.08729441 0.14659932  1335.7564 1.0027553\n#> a1    0.06834714 0.05394291 -0.01785119 0.15621615  3747.6036 1.0007151\n#> tau   0.98041340 0.03617180  0.92352906 1.03914055  2745.6394 1.0007016\n#> sigma 1.07286195 0.05249312  0.98902178 1.15557275   909.1510 1.0015771\n#> k     0.98545837 0.05638324  0.89516212 1.07461055   828.2759 1.0013545\n#> p     0.48012384 0.01111044  0.46233213 0.49808700 16117.4910 0.9996995\n\nVemos que no aparece la estimación de U, pero en la posterior se ha estimado un valor de U para cada uno de las observaciones. 1000 observaciones y\n\n\ndim(post$U)\n#> [1] 8000 1000\npost$U[1:4, 1:5]\n#>          [,1]       [,2]       [,3]      [,4]        [,5]\n#> [1,] 0.382053 -0.4087260 -1.5231200  0.486262  0.00381229\n#> [2,] 0.733459  0.2971870  0.0441252 -1.424160 -0.98469300\n#> [3,] 0.254011 -0.0201819 -0.7499000 -1.444340 -0.51772400\n#> [4,] 0.468004  0.1923180 -1.8694400 -1.314570 -0.43902700"
  },
  {
    "objectID": "2022/02/06/pluralista/index.html#efecto-de-m-sobre-d.",
    "href": "2022/02/06/pluralista/index.html#efecto-de-m-sobre-d.",
    "title": "Pluralista",
    "section": "Efecto de M sobre D.",
    "text": "Efecto de M sobre D.\nEste era el efecto que queríamos obtener y el cuál no podíamos estimar al no poder condicionar sobre U. Aquí es tan sencillo como ver su distribución a posteriori.\n\n\nquantile(post$m)\n#>          0%         25%         50%         75%        100% \n#> -0.14804300 -0.01706778  0.01080760  0.03813360  0.15027000\n\nggplot() +\n  geom_density(aes(post$m)) + \n  labs(title = \"Efecto directo de M sobre D\", \n       x = \"m\")\n\n\n\n\n\n\n\n\n\nEfecto de B1 sobre D\nComo ya sabíamos, al haber simulado los datos de forma que las relaciones entre las variables sean lineales, el efecto de B1 sobre D no es más que el efecto de B1 sobre M multiplicado por el efecto de M sobre D.\nUtilizando la distribución a posteriori.\n\n# Efecto de B1 sobre D \nquantile( with(post,b*m) )\n#>          0%         25%         50%         75%        100% \n#> -0.27728158 -0.03379382  0.02148739  0.07658919  0.30614657\n\nggplot() +\n  geom_density(aes(post$b * post$m))+\n  labs(title = \"Efecto de B1 sobre D\", \n       x = \"b1 x m\")\n\n\n\n\n\n\n\n\n\n\nEfecto de B1 sobre D, simulando\nTal y como dice en su curso, el efecto causal puede ser visto como hacer una intervención supuesto cierto el modelo causal.\nSimplemente utilizamos las posterioris obtenidas y vamos simulando , en primer lugar B1 = 0 y simulamos qué M se obtendría, y lo hacemos también para B1 = 1 y restamos para obtener el efecto causal, que coindice con b * m\n\n# \n\n# B1 = 0\n# B1 -> M\nM_B1_0 <- with( post , a1 + b*0 + k*0 )\n# M -> D\nD_B1_0 <- with( post , a2 + b*0 + m*M_B1_0 + k*0 )\n\n# now same but with B1 = 1\nM_B1_1 <- with( post , a1 + b*1 + k*0 )\nD_B1_1 <- with( post , a2 + b*0 + m*M_B1_1 + k*0 )\n\n# difference to get causal effect\nd_D_B1 <- D_B1_1 - D_B1_0\nquantile(d_D_B1)\n#>          0%         25%         50%         75%        100% \n#> -0.27728158 -0.03379382  0.02148739  0.07658919  0.30614657\n\nPues como dice el título , ser pluralista no está tan mal, puedes usar el DAG y el backdoor criterio para entender qué variables ha de tener en cuenta para estimar tu efecto causal, y a partir de ahí podrías usar el “full luxury bayesian” en situaciones más complicadas."
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html",
    "title": "Mediator. Full luxury bayes",
    "section": "",
    "text": "Continuando con la serie sobre cosas de inferencia causal y full luxury bayes, antes de que empiece mi amigo Carlos Gil, y dónde seguramente se aprenderá más.\nEste ejemplo viene motivado precisamente por una charla que tuve el otro día con él.\nSea el siguiente diagrama causal\nSe tiene que z es un mediador entre x e y, y la teoría nos dice que si quiero obtener el efecto directo de x sobre y he de condicionar por z , y efectivamente, así nos lo dice el backdoor criterio. Mientras que si quiero el efecto total de x sobre y no he de condicionar por z."
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#datos-simulados",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#datos-simulados",
    "title": "Mediator. Full luxury bayes",
    "section": "Datos simulados",
    "text": "Datos simulados\n\n\nset.seed(155)\nN <- 1000 \n\nx <- rnorm(N, 2, 1) \nz <- rnorm(N, 4+ 4*x , 2 ) # a z le ponemos más variabilidad, pero daría igual\ny <- rnorm(N, 2+ 3*x + z, 1)\n\nEfecto total de x sobre y \nTal y como hemos simulado los datos, se esperaría que el efecto total de x sobre y fuera\n\\[ \\dfrac{cov(x,y)} {var(x)} \\approx 7 \\]\nY qué el efecto directo fuera de 3\nEfectivamente\nEfecto total\n\n# efecto total\nlm(y~x)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Coefficients:\n#> (Intercept)            x  \n#>       5.881        7.112\n\n\n# efecto directo\nlm(y~x+z)\n#> \n#> Call:\n#> lm(formula = y ~ x + z)\n#> \n#> Coefficients:\n#> (Intercept)            x            z  \n#>      2.0318       3.0339       0.9945"
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#full-luxury-bayes",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#full-luxury-bayes",
    "title": "Mediator. Full luxury bayes",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nHagamos lo mismo pero estimando el dag completo\n\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\ndat <- list(\n  N = N,\n  x = x,\n  y = y,\n  z = z\n)\nset.seed(1908)\n\nflbi <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a0,\n    z ~ normal( mu , sigma ),\n    \n    mu <- a1 + bx * x,\n   \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx2 * x + bz * z,\n\n    # priors\n    \n    c(a0,a1,a2,bx,bx2, bz) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n#> Running MCMC with 10 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 finished in 10.6 seconds.\n#> Chain 3 finished in 10.6 seconds.\n#> Chain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 8 finished in 11.0 seconds.\n#> Chain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 6 finished in 11.5 seconds.\n#> Chain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 5 finished in 12.4 seconds.\n#> Chain 9 finished in 12.4 seconds.\n#> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 finished in 12.9 seconds.\n#> Chain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 7 finished in 13.3 seconds.\n#> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 finished in 13.6 seconds.\n#> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 4 finished in 13.9 seconds.\n#> \n#> All 10 chains finished successfully.\n#> Mean chain execution time: 12.2 seconds.\n#> Total execution time: 14.0 seconds.\n\nY recuperamos los coeficientes y varianzas\n\n\nprecis(flbi)\n#>            mean         sd      5.5%    94.5%     n_eff     Rhat4\n#> bz    1.0131931 0.01557219 0.9883466 1.038241  8226.042 1.0005919\n#> bx2   2.9610334 0.07098661 2.8447278 3.073501  9640.122 1.0004040\n#> bx    4.1535508 0.06180592 4.0552767 4.252771  8753.661 1.0001456\n#> a2    1.9440850 0.09315785 1.7952495 2.093027 11266.267 1.0001373\n#> a1    3.7089914 0.13578637 3.4916007 3.926010  8709.448 1.0002893\n#> a0    1.9857103 0.03108199 1.9357773 2.035712 16252.310 0.9997523\n#> k     0.9719412 0.02166791 0.9378896 1.007410 15276.158 0.9999036\n#> tau   0.9859183 0.02216068 0.9512944 1.021932 15735.140 1.0001540\n#> sigma 1.9636543 0.04382554 1.8950489 2.034130 15855.098 1.0000898\n\n\n# extraemos 10000 muestras de la posteriori \npost <- extract.samples(flbi, n = 10000) \n\nY el efecto directo de x sobre y sería\n\nquantile(post$bx2, probs = c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 2.823659 2.962330 3.099855\n\nEn este ejemplo sencillo, podríamos estimar el efecto causal de x sobre y simplemente sumando las posterioris\n\nquantile(post$bx + post$bx2, c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 6.928046 7.114180 7.299252\n\nTambién podríamos obtener el efecto causal total de x sobre y simulando una intervención. En este caso, al ser la variable continua, lo que queremos obtener como de diferente es y cuando \\(X = x_i\\) versus cuando \\(X = x_i+1\\).\nSiempre podríamos ajustar otro modelo bayesiano dónde no tuviéramos en cuenta a z y obtendríamos la estimación de ese efecto total de x sobre y, pero siguiendo a Rubin y Gelman, la idea es incluir en nuestro modelo toda la información disponible. Y tal y como dice Richard McElreath , Statistical Rethinking 2022, el efecto causal se puede estimar simulando la intervención.\nAsí que el objetivo es dado nuestro modelo que incluye la variable z, simular la intervención cuando \\(X = x_i\\) y cuando \\(X = x_i+1\\) y una estimación del efecto directo es la resta de ambas distribuciones a posteriori de y.\nCreamos función para calcular el efecto de la intervención y_do_x\n\nget_total_effect <- function(x_value = 0, incremento = 0.5) {\n  n <- length(post$bx)\n  with(post, {\n    # simulate z, y  for x= x_value\n    z_x0 <- rnorm(n, a1 + bx * x_value  , sigma)\n    y_x0 <- rnorm(n, a2  + bz * z_x0 + bx * x_value , tau)\n    \n    # simulate z,y for x= x_value +1 \n    z_x1 <- rnorm(n, a1 + bx * (x_value + incremento)  , sigma)\n    y_x1 <- rnorm(n, a2  + bz * z_x1 + bx2 * (x_value + incremento) , tau)\n    \n    \n    # compute contrast\n    y_do_x <- (y_x1 - y_x0) / incremento\n    return(y_do_x)\n  })\n  \n}\n\nDado un valor de x, podemos calcular el efecto total\n\ny_do_x_0_2 <- get_total_effect(x_value = 0.2) \n\nquantile(y_do_x_0_2)\n#>         0%        25%        50%        75%       100% \n#> -15.324628   2.395551   6.702379  10.987520  28.909002\n\nPodríamos hacer lo mismo para varios valores de x\n\nx_seq <- seq(-0.5, 0.5, length = 1000)\ny_do_x <-\n  mclapply(x_seq,  function(lambda)\n    get_total_effect(x_value = lambda))\n\nY para cada uno de estos 1000 valores tendría 10000 valores de su efecto total de x sobre y.\n\nlength(y_do_x[[500]])\n#> [1] 10000\n\nhead(y_do_x[[500]])\n#> [1]  3.055107  4.988595  5.030397 12.469616 14.944735 22.881773\n\nCalculamos los intervalos de credibilidad del efecto total de x sobre y para cada valor de x.\n\n# lo hacemos simplemente por cuantiles, aunque podríamos calcular el hdi también, \n\nintervalos_credibilidad <-  mclapply( y_do_x, function(x) quantile(x, probs = c(0.025, 0.5, 0.975)))\n\n# Media e intervalor de credibilidad para el valor de x_seq en la posición 500 \nmean(y_do_x[[500]])\n#> [1] 7.278184\nintervalos_credibilidad[[500]]\n#>      2.5%       50%     97.5% \n#> -4.867513  7.337732 19.761850\n\nintervalo de predicción clásico con el lm\nHabría que calcular la predicción de y para un valor de x y para el de x + 1, podemos calcular los intervalos de predicción clásicos parea cada valor de x, pero no para la diferencia ( al menos con la función predict)\n\n\nmt_lm <- lm(y~x)\npredict(mt_lm, newdata = list(x= x_seq[[500]]), interval  = \"prediction\")\n#>        fit      lwr     upr\n#> 1 5.877439 1.578777 10.1761\npredict(mt_lm, newdata = list(x= x_seq[[500]] +1), interval  = \"prediction\")\n#>        fit      lwr     upr\n#> 1 12.98993 8.698051 17.2818\n\nPero como sería obtener el intervalo de credibilidad para la media de los efectos totales?\nCalculando para cada valor de x la media de la posteriori del efecto global y juntando todas las medias.\n\nslopes_mean <- lapply(y_do_x, mean)\n\nquantile(unlist(slopes_mean), c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 6.023455 7.180465 8.326128\n\nQue tiene mucha menos variabilidad que el efecto global en un valor concreto de x, o si juntamos todas las estimaciones\n\nquantile(unlist(y_do_x),  c(0.025, 0.5, 0.975))\n#>      2.5%       50%     97.5% \n#> -5.219611  7.168985 19.562729\n\nEvidentemente, podríamos simplemente no haber tenido en cuenta la variable z en nuestro modelo bayesiano.\n\nflbi_2 <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a1,\n    \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx * x ,\n    \n    # priors\n    \n    c(a1,a2,bx) ~ normal( 0 , 0.5 ),\n    c(tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n#> Running MCMC with 10 parallel chains, with 1 thread(s) per chain...\n#> \n#> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \n#> Chain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \n#> Chain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \n#> Chain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \n#> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \n#> Chain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \n#> Chain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \n#> Chain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \n#> Chain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \n#> Chain 5 finished in 1.1 seconds.\n#> Chain 6 finished in 1.1 seconds.\n#> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \n#> Chain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \n#> Chain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \n#> Chain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 1 finished in 1.4 seconds.\n#> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \n#> Chain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 2 finished in 1.4 seconds.\n#> Chain 4 finished in 1.5 seconds.\n#> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \n#> Chain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \n#> Chain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \n#> Chain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 3 finished in 1.6 seconds.\n#> Chain 8 finished in 1.5 seconds.\n#> Chain 9 finished in 1.5 seconds.\n#> Chain 10 finished in 1.4 seconds.\n#> Chain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \n#> Chain 7 finished in 1.6 seconds.\n#> \n#> All 10 chains finished successfully.\n#> Mean chain execution time: 1.4 seconds.\n#> Total execution time: 1.8 seconds.\n\nY obtenemos directamente el efecto total de x sobre y.\n\nprecis(flbi_2)\n#>          mean         sd      5.5%    94.5%    n_eff     Rhat4\n#> bx  7.1948083 0.06787458 7.0863389 7.302933 10446.14 1.0001211\n#> a2  5.6085615 0.14948341 5.3685467 5.848723 10598.69 1.0001680\n#> a1  1.9860307 0.03058033 1.9368689 2.034631 14116.13 1.0004122\n#> k   0.9721054 0.02159773 0.9382407 1.006761 15044.78 0.9995902\n#> tau 2.1898100 0.04953736 2.1120600 2.269831 14995.33 0.9999084\n\n\npost2 <- extract.samples(flbi_2,  n = 10000)\n\nquantile(post2$bx, probs = c(0.025, 0.5, 0.975))\n#>     2.5%      50%    97.5% \n#> 7.061389 7.194525 7.327361"
  },
  {
    "objectID": "2022/02/12/mediator-full-luxury-bayes/index.html#pensamientos-finales",
    "href": "2022/02/12/mediator-full-luxury-bayes/index.html#pensamientos-finales",
    "title": "Mediator. Full luxury bayes",
    "section": "Pensamientos finales",
    "text": "Pensamientos finales\n\nParece que no es tan mala idea incluir en tu modelo bayesiano toda la información disponible.\nSer pluralista es una buena idea, usando el backdoor criterio dado que nuestro dag sea correcto, nos puede llevar a un modelo más simple y fácil de estimar.\nComo dije en el último post, estimar todo el dag de forma conjunta puede ser útil en varias situaciones.\nYa en 2009 se hablaba de esto aquí"
  },
  {
    "objectID": "2022/02/09/collider-bias/index.html",
    "href": "2022/02/09/collider-bias/index.html",
    "title": "Collider Bias?",
    "section": "",
    "text": "Continuando con temas del post anterior. Dice Pearl, con buen criterio, que si condicionas por un collider abres ese camino causal y creas una relación espuria entre las dos variables “Tratamiento” y “Respuesta” y por lo tanto si condicionas por el collider, aparece un sesgo.\nHablando estilo compadre. Si Tratamiento -> Collider y Respuesta -> Collider, si condiciono en el Collider, es decir, calculo la relación entre Tratamiento y Respuesta para cada valor de C, se introduce un sesgo.\nSi \\[C = 2\\cdot Tratamiento + 3 \\cdot respuesta\\]\nSi sé que C = 3, y que Tratamiento = 4 , ya hay relación entre Tratamiento y respuesta aunque sean independientes, porque ambos son causa de C.\nSimulemos, que es una buena forma de ver qué pasa si condiciono por el collider, siendo el tratamiento y la respuesta independientes.\nSi no ajusto por el collider, obtengo que no hay efecto del tratamiento , correcto\nCondicionando, aparece el sesgo\nRetomando el ejemplo del último post, pero en vez de tener una variable de confusión no observable, tenemos un collider.\nUsando la función adjustmentSets de dagitty nos dice sobre qué variables hay que condicionar si quiero el efecto causal total y directo de M sobre D, siguiendo las reglas de Pearl, ver por ejemplo (J. Pearl (2009), Causality: Models, Reasoning and Inference. Cambridge University Press.)\nSimulo unos datos dónde fuerzo a que no haya efecto causal de M a D.\nEn grafo sería\nY vemos lo de antes,\nVemos si hay efecto causal de M sobre D (uso modelos lineales por simplicidad).\nEl modelo correcto sería sin condicionar por el collider. Y bien, hace lo que debe, no hay efecto de M sobre D, tal y como sabemos que pasa en la realidad\nCondicionando ahora por el collider, tenemos sesgo.\nQueda como curiosidad que si condicionas por B1 en vez de por el collider también hay sesgo, pero si condicionas solo por B2, no hay."
  },
  {
    "objectID": "2022/02/09/collider-bias/index.html#full-luxury-bayes",
    "href": "2022/02/09/collider-bias/index.html#full-luxury-bayes",
    "title": "Collider Bias?",
    "section": "Full luxury bayes",
    "text": "Full luxury bayes\nNo todos los DAg’s son tan sencillos como el que he puesto, hay veces en los que una misma variable puede ser a la vez un collider y una variable de confusión, porque puede haber varios path causales y tenga diferente rol. En esos casos, condicionar por el collider te abre un path, y si no condicionas te abre otro. Ante esas situaciones, y suponiendo que el dag es correcto, no se podría estimar el efecto causal.\nSin embargo, condicionar en la red bayesiana no significa lo mismo que condicionar en un sólo modelo, sino que significa que introduzco la información que me proporciona el collider en la distribución conjunta y que me obtenga la posteriori.\nAl estimar el DAG completo, usando Stan por ejemplo, se estima tanto el modelo para M, como para D de forma conjunta.\n\nModelo bayesiano sin condicionar por el collider\nFormulamos el modelo usando la librería rethinking y lo ajustamos usando la función ulam que por debajo llama a Stan\n\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\n\ndat <- list(\n  N = N,\n  M = M,\n  D = D,\n  B1 = B1,\n  B2 = B2, \n  C = C\n)\n\nset.seed(155)\n\nflbi <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 ,\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M ,\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n\n    # priors\n    c(a1,a2,b,m) ~ normal( 0 , 0.5 ),\n    c(k,sigma,tau) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n\nWarning in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 6, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 7, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 39, column 4 to column 29)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 39, column 4 to column 29)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/Rtmpsdysi4/model-8eef3881083c.stan', line 35, column 4 to column 27)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 finished in 2.2 seconds.\nChain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 finished in 2.3 seconds.\nChain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 finished in 2.4 seconds.\nChain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 finished in 2.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.4 seconds.\nTotal execution time: 2.6 seconds.\n\n\nVemos los parámetros estimados y sus intervalos de credibilidad y extraemos la posteriori\n\nprecis(flbi)\n\n              mean         sd         5.5%      94.5%    n_eff     Rhat4\nm     -0.008692293 0.02258036 -0.044472871 0.02734239 6673.459 1.0005257\nb      1.961391786 0.04356684  1.891207250 2.03241110 5380.781 0.9998034\na2     0.059745778 0.04370034 -0.009909845 0.12931849 5500.657 1.0000543\na1     0.026656585 0.03702030 -0.032118024 0.08601562 6410.710 0.9997482\ntau    0.984889552 0.02213892  0.949803835 1.02046055 7752.122 1.0000469\nsigma  0.967785086 0.02155725  0.933665955 1.00275110 8907.866 1.0001535\nk      0.998568923 1.01036660  0.056543283 2.88207040 9005.060 1.0000657\np      0.482860133 0.01114257  0.465039000 0.50049459 8082.865 1.0000378\n\npost <- extract.samples(flbi)\n\nPintamos la distribución a posteriori del efecto y cómo ya sabíamos, al no condicionar por el collider, se estima sin sesgo que no hay efecto causal de M a D.\n\nplot(bayestestR::hdi(post$m, ci = c( 0.80, 0.95))) +\n  labs(title = \"Estimación sin collider\")\n\n\n\n\n\n\n\n\n\n\nModelo condicionando por el collider\nYa sabemos que no es necesario de hecho condicionar por el collider, más aún, que hacerlo induce un sesgo en la estimación del efecto, ¿pero qué pasa si estimamos el dag al completo?\n\nset.seed(155)\n\nflbi_collider <- ulam(\n  alist(\n    # mom model\n    M ~ normal( mu , sigma ),\n    mu <- a1 + b*B1 ,\n    # daughter model\n    D ~ normal( nu , tau ),\n    nu <- a2 + b*B2 + m*M ,\n    # B1 and B2\n    B1 ~ bernoulli(p),\n    B2 ~ bernoulli(p),\n    \n    # Collider\n    \n     C ~ normal( cmu , csigma ),\n     cmu <- a3 + cm * M  + cd * D,\n\n    # priors\n    c(a1,a2,a3,b,m, cm, cd) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, csigma) ~ exponential( 1 ),\n    p ~ beta(2,2)\n  ), data=dat , chains=4 , cores=4 , warmup = 500, iter=2500 , cmdstan=TRUE )\n\nWarning in '/tmp/Rtmpsdysi4/model-8eef4e4ce260.stan', line 3, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\nWarning in '/tmp/Rtmpsdysi4/model-8eef4e4ce260.stan', line 4, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \nChain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \nChain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \nChain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \nChain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \nChain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \nChain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \nChain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \nChain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \nChain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \nChain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \nChain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \nChain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \nChain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \nChain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \nChain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \nChain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \nChain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \nChain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \nChain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \nChain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \nChain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \nChain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \nChain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \nChain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \nChain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 4 finished in 4.4 seconds.\nChain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 1 finished in 4.5 seconds.\nChain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \nChain 2 finished in 4.5 seconds.\nChain 3 finished in 4.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.5 seconds.\nTotal execution time: 4.6 seconds.\n\n\nViendo la distribución posterior de los parámetros resulta que hemos podido estimar el verdadero efecto causal de M sobre D (que sabemos que es 0), incluso aunque hayamos “condicionado” por el collider.\n\nprecis(flbi_collider)\n\n               mean         sd        5.5%      94.5%     n_eff     Rhat4\ncd      3.968243739 0.04611450  3.89560945 4.04327110  8248.105 1.0004150\ncm      2.934145831 0.04687289  2.85989505 3.00919165  8890.321 1.0001715\nm      -0.009309562 0.02249104 -0.04553421 0.02674973  8503.878 1.0000069\nb       1.962677924 0.04345328  1.89429945 2.03282660  7107.343 0.9999206\na3      0.160077627 0.09126337  0.01252729 0.30475624  7756.140 1.0006626\na2      0.060140941 0.04359355 -0.01003761 0.13036986  7499.209 0.9996584\na1      0.025057624 0.03711509 -0.03519740 0.08448685  7396.713 1.0005212\ncsigma  2.064163930 0.04708394  1.99060945 2.14121220 10185.107 1.0002583\ntau     0.984744502 0.02189981  0.95050602 1.01991110  8571.627 0.9998603\nsigma   0.968314337 0.02180198  0.93405345 1.00380110  9457.782 1.0001085\np       0.482931190 0.01117531  0.46538884 0.50083727 10342.731 0.9998451\n\npost_with_collider <- extract.samples(flbi_collider)\n\n\nquantile(post_with_collider$m)\n\n          0%          25%          50%          75%         100% \n-0.095140500 -0.024429525 -0.009353655  0.005972277  0.072001900 \n\nplot(bayestestR::hdi(post_with_collider$m, ci = c( 0.80, 0.95))) +\n  labs(title = \"Estimación con collider\")\n\n\n\n\n\n\n\n\nAsí, que siendo “pluralista”, estimar el dag completo nos puede ayudar en situaciones dónde el backdoor criterio nos diga que no se puede estimar el efecto causal."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html",
    "href": "2022/01/01/cocinando/index.html",
    "title": "Cocinando",
    "section": "",
    "text": "Lo primero, feliz año a todos (no me da la gana de poner todas y todes), y espero que este año sea mejor que el pasado.\nHoy voy a hablar un poco de la “cocina” electoral en los barómetros de opinión, pero de forma muy simplificada.\nUna de las primeras cosas que se hacía era comparar el recuerdo de voto declarado en la encuesta con el resultado real de las elecciones a las que hacía referencia.\nCuándo no coinciden una de las cosas que se hacían era imputar el recuerdo de voto para aquellos que no contestaron a la pregunta. Esto se hacía utilizando variables de la encuesta, típicamente variables de autoposición idelógica y similares.\nUna vez imputado el recuerdo de voto se comparaba de nuevo con el resultado real de las elecciones y si variaba se recurría a la ponderación por el recuerdo de voto real. Esto es, se estimaban unos pesos de forma que la distribución del recuerdo de voto en la encuesta fuera lo más similar posible a los resultados reales.\nEsta “reponderación” corre el riesgo de descalibrar la encuesta en otras variables, tales como sexo y edad, por poner un ejemplo. La solución podría ser postestratificar la muestra, pero para eso deberíamos saber los valores poblaciones en cada combinación de sexo, edad (posiblemente agrupada) y recuerdo de voto. Es decir, tener la distribución conjunta, lo cual implica tener por ejemplo todas las combinaciones de edad y partido al que votó en la muestra y también saber la distribución poblacional (en las elecciones consideradas). Evidentemente no siempre es posible tener tanta información, por lo que se opta por al menos ajustar las distribuciones marginales.\nPara obtener esos pasos se utiliza un procedimiento iterativo llamado raking\nPara ver como se haría esa parte de la “cocina” (lo de imputar los nulos en recuerdo de voto usando un modelo no lo voy a hacer), utilizando la librería survey de Thomas Lumley."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#fuentes-de-datos.",
    "href": "2022/01/01/cocinando/index.html#fuentes-de-datos.",
    "title": "Cocinando",
    "section": "Fuentes de datos.",
    "text": "Fuentes de datos.\n\nBarómetro del CIS, noviembre de 2021.\nResultados electorales oficiales utilizando la fantástica librería infoelectoral\nPadrón oficial de habitantes a 1 de Enero de 2020."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#obtención-datos",
    "href": "2022/01/01/cocinando/index.html#obtención-datos",
    "title": "Cocinando",
    "section": "Obtención datos",
    "text": "Obtención datos\n\nEncuesta CIS.\n\nlibrary(tidyverse)\nlibrary(infoelectoral)\nlibrary(magrittr) # \nlibrary(patchwork) #\n \ndf <- haven::read_sav(\"/home/jose/Rstudio_projects/raking_ejemplo/MD3340/3340.sav\")\ndf %<>%  ## change in place\n  rename_all(tolower)\n\n\n# Convierto a factor algunas variables para que pillen el label que \n# viene del fichero de spss. \n\ndf <- df %>% \n  mutate(across(.cols = c(ccaa, sexo, recuerdo, recuvotogr, \n                          intenciongalter, intenciong, intenciongr, \n                          intenciongalterr), .fns = as_factor))\n\n# categorizmaos la edad\ndf <- df %>%\n  mutate(\n    gedad =\n      case_when(\n        edad >= 100 ~ \"100 años y más\",\n        edad >= 18 & edad <= 24 ~ \"18-24 años\",\n        edad >= 25 & edad <= 29 ~ \"25-29 años\",\n        edad >= 30 & edad <= 34 ~ \"30-34 años\",\n        edad >= 35 & edad <= 39 ~ \"35-39 años\",\n        edad >= 40 & edad <= 44 ~ \"40-44 años\",\n        edad >= 45 & edad <= 49 ~ \"45-49 años\",\n        edad >= 50 & edad <= 54 ~ \"50-54 años\",\n        edad >= 55 & edad <= 59 ~ \"55-59 años\",\n        edad >= 60 & edad <= 64 ~ \"60-64 años\",\n        edad >= 65 & edad <= 69 ~ \"65-69 años\",\n        edad >= 70 & edad <= 74 ~ \"70-74 años\",\n        edad >= 75 & edad <= 79 ~ \"75-79 años\",\n        edad >= 80 & edad <= 84 ~ \"80-84 años\",\n        edad >= 85 & edad <= 89 ~ \"85-89 años\",\n        edad >= 90 & edad <= 94 ~ \"90-94 años\",\n        edad >= 95 & edad <= 99 ~ \"95-99 años\",\n       \n      )\n  )\n\n\ndf$gedad <- as.factor(df$gedad)\nhead(df)\n#> # A tibble: 6 × 254\n#>   estudio     registro  cues tipo_tel  ccaa     prov    mun      tamuni  entrev \n#>   <dbl+lbl>      <dbl> <dbl> <dbl+lbl> <fct>    <dbl+l> <dbl+lb> <dbl+l> <dbl+l>\n#> 1 3340 [3340]     3982     1 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 1 [Men… 0 [Ano…\n#> 2 3340 [3340]     8979     2 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 3 [10.… 0 [Ano…\n#> 3 3340 [3340]    11104     3 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 3 [10.… 0 [Ano…\n#> 4 3340 [3340]     4205     4 2 [Móvil] Andaluc… 4 [Alm…  0 [Mun… 3 [10.… 0 [Ano…\n#> 5 3340 [3340]     1191     5 1 [Fijo]  Andaluc… 4 [Alm… 13 [Alm… 5 [100… 0 [Ano…\n#> 6 3340 [3340]     1203     6 2 [Móvil] Andaluc… 4 [Alm… 13 [Alm… 5 [100… 0 [Ano…\n#> # … with 245 more variables: capital <dbl+lbl>, sexo <fct>, edad <dbl+lbl>,\n#> #   p0 <dbl+lbl>, p1 <dbl+lbl>, p2 <dbl+lbl>, p3 <dbl+lbl>, p3_1_1 <dbl+lbl>,\n#> #   p3_1_2 <dbl+lbl>, p3_1_3 <dbl+lbl>, p3_1_4 <dbl+lbl>, p3_1_5 <dbl+lbl>,\n#> #   p3_1_6 <dbl+lbl>, p4 <dbl+lbl>, sersanientre <dbl+lbl>, p4anno <dbl+lbl>,\n#> #   p4mes <dbl+lbl>, p4a0_1_20 <dbl+lbl>, p4a0_2_21 <dbl+lbl>,\n#> #   servientre_1 <dbl+lbl>, servientre_2 <dbl+lbl>, servientre_3 <dbl+lbl>,\n#> #   servientre_4 <dbl+lbl>, servientre_5 <dbl+lbl>, servientre_7 <dbl+lbl>, …\n\nPodemos ver cuántos encuestados hay por cada sexo,edad, y recuerdo de voto, contando los datos brutos o los datos utilizando la ponderación que ha calculado el CIS.\n\ndf %>% \n  group_by(sexo, gedad, recuerdo) %>% \n  summarise(total = n(),\n            peso_tot = sum(peso))\n#> # A tibble: 509 × 5\n#> # Groups:   sexo, gedad [31]\n#>    sexo   gedad      recuerdo       total peso_tot\n#>    <fct>  <fct>      <fct>          <int>    <dbl>\n#>  1 Hombre 18-24 años PP                 7    6.82 \n#>  2 Hombre 18-24 años PSOE              17   18.1  \n#>  3 Hombre 18-24 años C's                2    2.11 \n#>  4 Hombre 18-24 años En Comú Podem      4    4.10 \n#>  5 Hombre 18-24 años Més Compromís      1    1.03 \n#>  6 Hombre 18-24 años EAJ-PNV            2    2.06 \n#>  7 Hombre 18-24 años Na+                1    0.563\n#>  8 Hombre 18-24 años VOX                4    4.13 \n#>  9 Hombre 18-24 años Unidas Podemos    13   13.7  \n#> 10 Hombre 18-24 años BNG                2    2.05 \n#> # … with 499 more rows\n\nPara normalizar las siglas usamos otra tabla de lookup\n\nsiglas_cis <-  read_csv(\"/home/jose/Rstudio_projects/raking_ejemplo/siglas_cis.csv\")\n\nsiglas_cis\n#> # A tibble: 28 × 2\n#>    recuerdo       key       \n#>    <chr>          <chr>     \n#>  1 No recuerda    abstencion\n#>  2 No votó        abstencion\n#>  3 C's            CIUDADANOS\n#>  4 PSOE           PSOE      \n#>  5 PP             PP        \n#>  6 N.C.           abstencion\n#>  7 VOX            VOX       \n#>  8 En blanco      abstencion\n#>  9 Unidas Podemos PODEMOS-IU\n#> 10 PACMA          PACMA     \n#> # … with 18 more rows\n\n\n\n# le pegamos las siglas normalizadas\ndf <- df %>% \n  left_join(siglas_cis) \n\n\n\n# Vemos los totales por recuerdo de voto , usando la ponderacińo del cis\ndf %>% \n  group_by(key) %>% \n  summarise(frq = sum(peso))\n#> # A tibble: 16 × 2\n#>    key           frq\n#>    <chr>       <dbl>\n#>  1 abstencion 1064. \n#>  2 BILDU        21.5\n#>  3 BNG          23.6\n#>  4 CIUDADANOS  239. \n#>  5 COMPROMIS    22.1\n#>  6 CUP          17.4\n#>  7 ERC          81.9\n#>  8 JXCAT        40.0\n#>  9 MAS PAIS     28.7\n#> 10 OTROS        62.4\n#> 11 PACMA        26.4\n#> 12 PNV          40.5\n#> 13 PODEMOS-IU  426. \n#> 14 PP          525. \n#> 15 PSOE        917. \n#> 16 VOX         244.\n\n# Vemos total de datos en la encuesta y comprobamos que la suma de las \n# ponderaciones coincide con el total de encuestados. \n\ndf %>% \n  summarise(n(), \n            sum(peso))\n#> # A tibble: 1 × 2\n#>   `n()` `sum(peso)`\n#>   <int>       <dbl>\n#> 1  3779       3779.\n\n# Convertimos a factor la variable con el recuerdo de voto normalizado\ndf$key <- as.factor(df$key)\n\nAhora lo que nos hace falta es saber los totales de recuerdo de voto, sexo y edad que debería haber tenido la encuesta.\n\n\nResultados electorales\n\n\n\ncongress_2019 <- municipios(tipo_eleccion = \"congreso\", anno = 2019, mes = \"11\")\n\n(votos_summary <-  congress_2019 %>%\n  group_by(codigo_ccaa, codigo_provincia,\n           codigo_municipio, municipio) %>%\n  summarise(\n    abstencion = first(censo_ine) -\n      first(votos_blancos) -\n      first(votos_nulos) -\n      first(votos_candidaturas),\n    censo_ine = first(censo_ine), \n    votos_blancos = first(votos_blancos),\n    votos_nulos = first(votos_nulos),\n    votos_candidaturas = first(votos_candidaturas)  ) %>%\n    ungroup() %>% \n  summarise(\n    abstencion = sum(abstencion, na.rm = TRUE) +\n      sum(votos_blancos, na.rm = TRUE) + \n      sum(votos_nulos, na.rm = TRUE),\n    censo_ine = sum(censo_ine, na.rm = TRUE), \n    votos_blancos = sum(votos_blancos, na.rm = TRUE),\n    votos_nulos = sum(votos_nulos, na.rm = TRUE),\n    votos_candidaturas = sum(votos_candidaturas, na.rm = TRUE)\n  ))\n#> # A tibble: 1 × 5\n#>   abstencion censo_ine votos_blancos votos_nulos votos_candidaturas\n#>        <dbl>     <dbl>         <dbl>       <dbl>              <dbl>\n#> 1   10973466  34870481        216249      248543           23897015\n\n# ponemos abstencion como partido\nabstencion <-  votos_summary %>%\n  select( abstencion) %>%\n  mutate(siglas = \"abstencion\", \n         denominacion = \"abstencion\", \n         codigo_partido = \"abstencion\") %>%  \n  rename(votos = abstencion)\n\n\n\n\nvotos_partidos <-  congress_2019 %>% \n  group_by(codigo_partido, siglas, denominacion) %>% \n  summarise(votos = sum(votos))\n\n\nvotos_final <- votos_partidos %>% \n  bind_rows(abstencion) %>% \n  bind_cols(votos_summary %>% \n               select( censo_ine)) %>% # debe ser pob > = 18 \n  ungroup() %>%\n    mutate(prop_voto = votos/censo_ine) %>% \n  arrange(-prop_voto) \n  \n\nDT::datatable(votos_final)\n\n\n\n\n\n\nComo las siglas de los partidos en la info oficial y las que vienen en la encuesta no están normalizadas, me construí una tabla de “lookup” para eso.\n\nsiglas_infoelectoral <- read_csv(\"/home/jose/Rstudio_projects/raking_ejemplo/siglas_infoelectoral.csv\")\n\nDT::datatable(siglas_infoelectoral)\n\n\n\n\n\n\n\n(votos_final_summary <- votos_final %>% \n  left_join(siglas_infoelectoral) %>% \n  group_by(key) %>% \n  summarise(prop_voto = sum(prop_voto, na.rm=TRUE)))\n#> # A tibble: 16 × 2\n#>    key        prop_voto\n#>    <chr>          <dbl>\n#>  1 abstencion   0.315  \n#>  2 BILDU        0.00793\n#>  3 BNG          0.00343\n#>  4 CIUDADANOS   0.0470 \n#>  5 COMPROMIS    0.00502\n#>  6 CUP          0.00702\n#>  7 ERC          0.0249 \n#>  8 JXCAT        0.0151 \n#>  9 MAS PAIS     0.0115 \n#> 10 OTROS        0.0309 \n#> 11 PACMA        0.00649\n#> 12 PNV          0.0108 \n#> 13 PODEMOS-IU   0.0732 \n#> 14 PP           0.144  \n#> 15 PSOE         0.194  \n#> 16 VOX          0.104\n\n\nvotos_final_summary$key <- as.factor(votos_final_summary$key)\n\n(pop_revoto <-  votos_final_summary %>% \n  mutate(Freq = prop_voto * sum(df$peso)) %>% \n  select(key, Freq ) )\n#> # A tibble: 16 × 2\n#>    key          Freq\n#>    <fct>       <dbl>\n#>  1 abstencion 1189. \n#>  2 BILDU        30.0\n#>  3 BNG          13.0\n#>  4 CIUDADANOS  177. \n#>  5 COMPROMIS    19.0\n#>  6 CUP          26.5\n#>  7 ERC          94.3\n#>  8 JXCAT        57.1\n#>  9 MAS PAIS     43.5\n#> 10 OTROS       117. \n#> 11 PACMA        24.5\n#> 12 PNV          40.9\n#> 13 PODEMOS-IU  276. \n#> 14 PP          544. \n#> 15 PSOE        732. \n#> 16 VOX         395.\n\nY vemos que no coincide mucho con la que hay en la encuesta\n\ndf %>% \n  group_by(key) %>% \n  summarise(Freq = sum(peso))\n#> # A tibble: 16 × 2\n#>    key          Freq\n#>    <fct>       <dbl>\n#>  1 abstencion 1064. \n#>  2 BILDU        21.5\n#>  3 BNG          23.6\n#>  4 CIUDADANOS  239. \n#>  5 COMPROMIS    22.1\n#>  6 CUP          17.4\n#>  7 ERC          81.9\n#>  8 JXCAT        40.0\n#>  9 MAS PAIS     28.7\n#> 10 OTROS        62.4\n#> 11 PACMA        26.4\n#> 12 PNV          40.5\n#> 13 PODEMOS-IU  426. \n#> 14 PP          525. \n#> 15 PSOE        917. \n#> 16 VOX         244.\n\nEn la encuesta hay más personas que recuerdan haber votado al psoe que las que debería haber, así como también hay menos que recuerdan haber votado a Mas País o a Vox. Había una hipótesis por ahi que decía que el votante de derechas está infrarrepresentado en las encuestas.\n\n\nPadrón\nEsto es solo un csv con la info de población por sexo y edad exacta (quité los menores de 18 años)\nLeemos el csv, que lamentablemente viene en encoding de windows y con separador decimal y tal.\n\npadron <- read_delim(\n  \"/home/jose/Rstudio_projects/raking_ejemplo/pad_2021.csv\",\n  delim = \";\",\n  escape_double = FALSE,\n  locale = locale(\n    date_names = \"es\",\n    decimal_mark = \",\",\n    grouping_mark = \".\",\n    encoding = \"WINDOWS-1252\"\n  ),\n  trim_ws = TRUE\n)\n\nCategorizamos la edad y vemos cuales habrían sido las frecuencias de edad en la encuestas si fueran representativas de la estructura de población del padrón de 2020\n\npop_edad <- padron %>% \n  mutate(\n    gedad =\n      case_when(\n        edad >= 100 ~ \"100 años y más\",\n        edad >= 18 & edad <= 24 ~ \"18-24 años\",\n        edad >= 25 & edad <= 29 ~ \"25-29 años\",\n        edad >= 30 & edad <= 34 ~ \"30-34 años\",\n        edad >= 35 & edad <= 39 ~ \"35-39 años\",\n        edad >= 40 & edad <= 44 ~ \"40-44 años\",\n        edad >= 45 & edad <= 49 ~ \"45-49 años\",\n        edad >= 50 & edad <= 54 ~ \"50-54 años\",\n        edad >= 55 & edad <= 59 ~ \"55-59 años\",\n        edad >= 60 & edad <= 64 ~ \"60-64 años\",\n        edad >= 65 & edad <= 69 ~ \"65-69 años\",\n        edad >= 70 & edad <= 74 ~ \"70-74 años\",\n        edad >= 75 & edad <= 79 ~ \"75-79 años\",\n        edad >= 80 & edad <= 84 ~ \"80-84 años\",\n        edad >= 85 & edad <= 89 ~ \"85-89 años\",\n        edad >= 90 & edad <= 94 ~ \"90-94 años\",\n        edad >= 95 & edad <= 99 ~ \"95-99 años\",\n        \n      )\n  ) %>% \n  group_by(gedad) %>% \n  summarise(pob = sum(total)) %>% \n  ungroup() %>%\n  mutate(pct = pob/sum(pob)) %>% \n  mutate(Freq = pct* sum(df$peso)) %>% \n  select(gedad, Freq) %>% \n  filter(gedad!=\"100 años y más\")\n\npop_sexo <-   padron %>% \n  mutate(sexo = ifelse(sexo == \"Hombres\", \"Hombre\", \"Mujer\")) %>% \n  group_by(sexo) %>% \n  summarise(pob = sum(total)) %>% \n  ungroup() %>%\n  mutate(pct = pob/sum(pob)) %>% \n  mutate(Freq = pct* sum(df$peso)) %>% \n  select(sexo, Freq)\n\npop_sexo$sexo <- as.factor(pop_sexo$sexo)\n\npop_edad$gedad <- as.factor(pop_edad$gedad)\n\n\npop_edad\n#> # A tibble: 16 × 2\n#>    gedad       Freq\n#>    <fct>      <dbl>\n#>  1 18-24 años 352. \n#>  2 25-29 años 269. \n#>  3 30-34 años 213. \n#>  4 35-39 años 313. \n#>  5 40-44 años 377. \n#>  6 45-49 años 381. \n#>  7 50-54 años 356. \n#>  8 55-59 años 362. \n#>  9 60-64 años 286. \n#> 10 65-69 años 240. \n#> 11 70-74 años 191. \n#> 12 75-79 años 187. \n#> 13 80-84 años 123. \n#> 14 85-89 años  77.9\n#> 15 90-94 años  41.5\n#> 16 95-99 años  11.1\n\nY veamos si se parece a lo que hay en la encuesta\n\ndf %>% \n  group_by(gedad) %>% \n  summarise(Freq = sum(peso))\n#> # A tibble: 16 × 2\n#>    gedad        Freq\n#>    <fct>       <dbl>\n#>  1 18-24 años 235.  \n#>  2 25-29 años 214.  \n#>  3 30-34 años 214.  \n#>  4 35-39 años 268.  \n#>  5 40-44 años 402.  \n#>  6 45-49 años 382.  \n#>  7 50-54 años 392.  \n#>  8 55-59 años 336.  \n#>  9 60-64 años 334.  \n#> 10 65-69 años 375.  \n#> 11 70-74 años 282.  \n#> 12 75-79 años 177.  \n#> 13 80-84 años 114.  \n#> 14 85-89 años  40.0 \n#> 15 90-94 años  12.8 \n#> 16 95-99 años   1.05\n\nPues en la encuesta ha caído más gente joven de la que debería, cosas que pasan."
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#raking",
    "href": "2022/01/01/cocinando/index.html#raking",
    "title": "Cocinando",
    "section": "Raking",
    "text": "Raking\nPues ya tenemos todo para hacer el ejercicio simple de raking.\nImportamos la librería survery, comprobamos que los niveles de las variables que vamos a considerar en el raking son los mismos en los datos de las encuestas y en los dataframes auxiliares\n\n\n# Comprobamos niveles\nall.equal(levels(pop_revoto$key) , levels(df$key))\n#> [1] TRUE\nall.equal(levels(pop_edad$gedad) , levels(df$gedad))\n#> [1] TRUE\nall.equal(levels(pop_sexo$sexo),  levels(df$sexo))\n#> [1] TRUE\n\nConstruimos un diseño muestral inicial utilizando los pesos que facilita el CIS.\n\nlibrary(survey)\n\ndisenno <- svydesign(id=~1, weight=~peso,data=df)\n\nVemos los totales por recuerdo de voto por ejemplo, con la estimación de su error estándar\n\nsvytotal(~key, disenno)\n#>                  total      SE\n#> keyabstencion 1064.301 28.1710\n#> keyBILDU        21.475  4.4926\n#> keyBNG          23.630  4.9128\n#> keyCIUDADANOS  238.593 15.1738\n#> keyCOMPROMIS    22.099  4.8096\n#> keyCUP          17.429  4.2181\n#> keyERC          81.939  9.0654\n#> keyJXCAT        39.984  6.3702\n#> keyMAS PAIS     28.734  5.4132\n#> keyOTROS        62.353  7.5990\n#> keyPACMA        26.359  5.2625\n#> keyPNV          40.515  6.4151\n#> keyPODEMOS-IU  425.598 19.7388\n#> keyPP          525.044 21.6608\n#> keyPSOE        917.318 26.7850\n#> keyVOX         244.059 15.4552\n\nPara hacer el raking utilizamos la funcion rake que toma argumentos el diseño muestral original, una lista con el nombre de las variables (en formula) en la encuesta , y una lista con los dataframes auxiliares cada uno con dos columnas, la variable que se corresponde con la de la encuesta y una columna numérica con el valor de cuántos individuos habría de haber en la muestra para que la distribución fuera igual a la de la población. Otros parámetros, serían el número de iteraciones máximas y el criterio de parada (epsilon) del procedimiento iterativo.\n\n\nponderacion_1 <- \n  rake (\n  design             = disenno,\n  sample.margins     = list(~gedad, ~key, ~sexo), \n  population.margins = list(pop_edad, pop_revoto, pop_sexo)\n  ) \n\nY ahora podemos comprobar qué tal lo ha hecho\nEdad\n\npop_edad # dist poblacional\n#> # A tibble: 16 × 2\n#>    gedad       Freq\n#>    <fct>      <dbl>\n#>  1 18-24 años 352. \n#>  2 25-29 años 269. \n#>  3 30-34 años 213. \n#>  4 35-39 años 313. \n#>  5 40-44 años 377. \n#>  6 45-49 años 381. \n#>  7 50-54 años 356. \n#>  8 55-59 años 362. \n#>  9 60-64 años 286. \n#> 10 65-69 años 240. \n#> 11 70-74 años 191. \n#> 12 75-79 años 187. \n#> 13 80-84 años 123. \n#> 14 85-89 años  77.9\n#> 15 90-94 años  41.5\n#> 16 95-99 años  11.1\n\n\nsvytotal(~gedad, disenno) # usando ponderaciones cis\n#>                    total      SE\n#> gedad18-24 años 234.8523 15.0942\n#> gedad25-29 años 214.4397 14.4131\n#> gedad30-34 años 214.3337 14.3916\n#> gedad35-39 años 267.7623 16.0524\n#> gedad40-44 años 401.8541 19.2163\n#> gedad45-49 años 382.1934 18.8006\n#> gedad50-54 años 392.2205 19.0815\n#> gedad55-59 años 335.9379 17.7668\n#> gedad60-64 años 334.4781 17.7006\n#> gedad65-69 años 374.9334 18.6334\n#> gedad70-74 años 281.8288 16.3995\n#> gedad75-79 años 176.6998 13.0458\n#> gedad80-84 años 114.0455 10.6935\n#> gedad85-89 años  40.0390  6.3841\n#> gedad90-94 años  12.7567  3.6124\n#> gedad95-99 años   1.0536  1.0536\n\n\nsvytotal(~gedad, ponderacion_1)\n#>                   total     SE\n#> gedad18-24 años 351.958 0.0022\n#> gedad25-29 años 269.150 0.0022\n#> gedad30-34 años 213.023 0.0019\n#> gedad35-39 años 312.727 0.0023\n#> gedad40-44 años 377.061 0.0024\n#> gedad45-49 años 380.727 0.0024\n#> gedad50-54 años 355.685 0.0022\n#> gedad55-59 años 361.652 0.0023\n#> gedad60-64 años 286.424 0.0018\n#> gedad65-69 años 239.837 0.0016\n#> gedad70-74 años 190.638 0.0013\n#> gedad75-79 años 186.745 0.0016\n#> gedad80-84 años 123.233 0.0013\n#> gedad85-89 años  77.958 0.0010\n#> gedad90-94 años  41.526 0.0011\n#> gedad95-99 años  11.083 0.0000\n\nsexo\n\npop_sexo\n#> # A tibble: 2 × 2\n#>   sexo    Freq\n#>   <fct>  <dbl>\n#> 1 Hombre 1889.\n#> 2 Mujer  1891.\n\nEn la encuesta original hay sobrepresentacion de mujeres\n\nsvytotal(~sexo, disenno)\n#>             total     SE\n#> sexoHombre 1820.6 31.548\n#> sexoMujer  1958.8 31.519\n\n\nsvytotal(~sexo, ponderacion_1)\n#>             total SE\n#> sexoHombre 1888.6  0\n#> sexoMujer  1890.8  0\n\nRecuerdo de voto\n\npop_revoto\n#> # A tibble: 16 × 2\n#>    key          Freq\n#>    <fct>       <dbl>\n#>  1 abstencion 1189. \n#>  2 BILDU        30.0\n#>  3 BNG          13.0\n#>  4 CIUDADANOS  177. \n#>  5 COMPROMIS    19.0\n#>  6 CUP          26.5\n#>  7 ERC          94.3\n#>  8 JXCAT        57.1\n#>  9 MAS PAIS     43.5\n#> 10 OTROS       117. \n#> 11 PACMA        24.5\n#> 12 PNV          40.9\n#> 13 PODEMOS-IU  276. \n#> 14 PP          544. \n#> 15 PSOE        732. \n#> 16 VOX         395.\n\n\nsvytotal(~key, disenno)\n#>                  total      SE\n#> keyabstencion 1064.301 28.1710\n#> keyBILDU        21.475  4.4926\n#> keyBNG          23.630  4.9128\n#> keyCIUDADANOS  238.593 15.1738\n#> keyCOMPROMIS    22.099  4.8096\n#> keyCUP          17.429  4.2181\n#> keyERC          81.939  9.0654\n#> keyJXCAT        39.984  6.3702\n#> keyMAS PAIS     28.734  5.4132\n#> keyOTROS        62.353  7.5990\n#> keyPACMA        26.359  5.2625\n#> keyPNV          40.515  6.4151\n#> keyPODEMOS-IU  425.598 19.7388\n#> keyPP          525.044 21.6608\n#> keyPSOE        917.318 26.7850\n#> keyVOX         244.059 15.4552\n\n\nsvytotal(~key, ponderacion_1)\n#>                  total    SE\n#> keyabstencion 1189.358 5e-04\n#> keyBILDU        29.972 1e-04\n#> keyBNG          12.962 0e+00\n#> keyCIUDADANOS  177.463 2e-04\n#> keyCOMPROMIS    18.969 1e-04\n#> keyCUP          26.547 1e-04\n#> keyERC          94.271 2e-04\n#> keyJXCAT        57.097 1e-04\n#> keyMAS PAIS     43.529 1e-04\n#> keyOTROS       116.681 2e-04\n#> keyPACMA        24.515 1e-04\n#> keyPNV          40.915 1e-04\n#> keyPODEMOS-IU  276.473 2e-04\n#> keyPP          544.268 3e-04\n#> keyPSOE        731.849 4e-04\n#> keyVOX         394.559 4e-04\n\nPues podríamos dar por buena la calibración alcanzada\nLos pesos los podemos extraer usando la función weights\n\nweights(ponderacion_1)[1:10]\n#>         1         2         3         4         5         6         7         8 \n#> 1.8324806 1.1606015 0.5372897 1.4168519 0.6115565 0.9731935 0.8024724 0.7280362 \n#>         9        10 \n#> 1.4168519 1.2670284"
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#estimación-simple-de-la-intención-de-voto.",
    "href": "2022/01/01/cocinando/index.html#estimación-simple-de-la-intención-de-voto.",
    "title": "Cocinando",
    "section": "Estimación simple de la intención de voto.",
    "text": "Estimación simple de la intención de voto.\nPara realizar una “buena” estimación de voto tendria que haber hecho algo más aparte del “raking”, tal vez un modelo para tener voto probable de los indecisos etcétera.\nNo obstante vamos a ver qué estimación saldría simplemente utilizando los pesos originales y los pesos calibrados.\n\n(estim_cis <- svytotal(~intenciongr, disenno))\n#>                                            total      SE\n#> intenciongrPP                           531.4821 21.8379\n#> intenciongrPSOE                         745.3097 24.8698\n#> intenciongrCiudadanos                   111.9348 10.5990\n#> intenciongrMés Compromís                 23.1522  4.9224\n#> intenciongrERC                           61.6780  7.9014\n#> intenciongrJxCat                         31.7818  5.6855\n#> intenciongrEAJ-PNV                       39.4836  6.3334\n#> intenciongrEH Bildu                      13.0444  3.5539\n#> intenciongrCCa-NC                        10.4548  3.4812\n#> intenciongrNA+                            9.4790  2.4073\n#> intenciongrPACMA                         29.3263  5.4769\n#> intenciongrVOX                          320.9019 17.4761\n#> intenciongrCUP                           13.3279  3.6906\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               402.1001 19.2592\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           18.4930  4.3490\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            3.1434  1.6474\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      60.9837  7.8825\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  2.1852  1.5450\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.9441  6.7160\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  74.6794  8.5264\n#> intenciongrEn blanco                    172.4132 13.0502\n#> intenciongrNo votaría                   352.4158 18.0842\n#> intenciongrNo sabe todavía              548.6834 21.9338\n#> intenciongrN.C.                         158.0310 12.4293\n\n\n(estim_calibrada <- svytotal(~intenciongr, ponderacion_1))\n#>                                            total      SE\n#> intenciongrPP                           566.2767 19.2580\n#> intenciongrPSOE                         635.4325 18.5286\n#> intenciongrCiudadanos                    96.7651  8.9299\n#> intenciongrMés Compromís                 21.7971  3.8757\n#> intenciongrERC                           69.4503  6.1644\n#> intenciongrJxCat                         43.1817  5.2533\n#> intenciongrEAJ-PNV                       37.1996  4.1088\n#> intenciongrEH Bildu                      17.3237  4.0213\n#> intenciongrCCa-NC                        14.3188  4.9252\n#> intenciongrNA+                           16.8526  4.4475\n#> intenciongrPACMA                         31.5831  5.6846\n#> intenciongrVOX                          434.0434 17.2445\n#> intenciongrCUP                           18.8067  4.1303\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               302.0278 12.7507\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           11.5789  2.4683\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            4.1735  2.2022\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      69.1656  7.8136\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  3.6379  2.6304\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.0260  6.7859\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  84.9910  9.7962\n#> intenciongrEn blanco                    186.4136 14.3569\n#> intenciongrNo votaría                   376.8699 19.3696\n#> intenciongrNo sabe todavía              524.4669 21.7009\n#> intenciongrN.C.                         169.0461 13.5892\n\nVamos a pintarlas. El objeto devuelto por svytotal no es muy manejable, pero podemos utilizar lo que devuelve el print.\n\n\n\n\nestim_simple1 <- print(svytotal(~intenciongr, disenno))\n#>                                            total      SE\n#> intenciongrPP                           531.4821 21.8379\n#> intenciongrPSOE                         745.3097 24.8698\n#> intenciongrCiudadanos                   111.9348 10.5990\n#> intenciongrMés Compromís                 23.1522  4.9224\n#> intenciongrERC                           61.6780  7.9014\n#> intenciongrJxCat                         31.7818  5.6855\n#> intenciongrEAJ-PNV                       39.4836  6.3334\n#> intenciongrEH Bildu                      13.0444  3.5539\n#> intenciongrCCa-NC                        10.4548  3.4812\n#> intenciongrNA+                            9.4790  2.4073\n#> intenciongrPACMA                         29.3263  5.4769\n#> intenciongrVOX                          320.9019 17.4761\n#> intenciongrCUP                           13.3279  3.6906\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               402.1001 19.2592\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           18.4930  4.3490\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            3.1434  1.6474\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      60.9837  7.8825\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  2.1852  1.5450\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.9441  6.7160\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  74.6794  8.5264\n#> intenciongrEn blanco                    172.4132 13.0502\n#> intenciongrNo votaría                   352.4158 18.0842\n#> intenciongrNo sabe todavía              548.6834 21.9338\n#> intenciongrN.C.                         158.0310 12.4293\nestim_simple2 <- print(svytotal(~intenciongr, ponderacion_1))\n#>                                            total      SE\n#> intenciongrPP                           566.2767 19.2580\n#> intenciongrPSOE                         635.4325 18.5286\n#> intenciongrCiudadanos                    96.7651  8.9299\n#> intenciongrMés Compromís                 21.7971  3.8757\n#> intenciongrERC                           69.4503  6.1644\n#> intenciongrJxCat                         43.1817  5.2533\n#> intenciongrEAJ-PNV                       37.1996  4.1088\n#> intenciongrEH Bildu                      17.3237  4.0213\n#> intenciongrCCa-NC                        14.3188  4.9252\n#> intenciongrNA+                           16.8526  4.4475\n#> intenciongrPACMA                         31.5831  5.6846\n#> intenciongrVOX                          434.0434 17.2445\n#> intenciongrCUP                           18.8067  4.1303\n#> intenciongrLos Verdes                     0.0000  0.0000\n#> intenciongrUnidas Podemos               302.0278 12.7507\n#> intenciongrEQUO                           0.0000  0.0000\n#> intenciongrPAR                            0.0000  0.0000\n#> intenciongrBNG                           11.5789  2.4683\n#> intenciongrMÉS (PSM-Entesa)               0.0000  0.0000\n#> intenciongrFalange Española de las JONS   0.0000  0.0000\n#> intenciongrEscaños en Blanco              0.0000  0.0000\n#> intenciongrEspaña 2000                    0.0000  0.0000\n#> intenciongrPartido Libertario             0.0000  0.0000\n#> intenciongrCHA                            0.0000  0.0000\n#> intenciongrRecortes Cero                  0.0000  0.0000\n#> intenciongrDirecte 68                     0.0000  0.0000\n#> intenciongrPartido Feminista de España    0.0000  0.0000\n#> intenciongrGeroa Bai                      0.0000  0.0000\n#> intenciongrBloc                           0.0000  0.0000\n#> intenciongrConvergència                   0.0000  0.0000\n#> intenciongrCompromís-Podemos-EUPV         0.0000  0.0000\n#> intenciongrUPyD                           0.0000  0.0000\n#> intenciongrPCPE                           0.0000  0.0000\n#> intenciongrPI                             0.0000  0.0000\n#> intenciongrPAYJ                           0.0000  0.0000\n#> intenciongrIGRE                           0.0000  0.0000\n#> intenciongrPRC                            4.1735  2.2022\n#> intenciongrUPL                            0.0000  0.0000\n#> intenciongrRecortes Cero-Grupo Verde      0.0000  0.0000\n#> intenciongrCoalición Caballas             0.0000  0.0000\n#> intenciongrMDyC                           0.0000  0.0000\n#> intenciongrCoalición por Melilla          0.0000  0.0000\n#> intenciongrPPL                            0.0000  0.0000\n#> intenciongrMás País                      69.1656  7.8136\n#> intenciongrPR+                            0.0000  0.0000\n#> intenciongrActúa                          0.0000  0.0000\n#> intenciongrAnova-Irmandade Nacionalista   0.0000  0.0000\n#> intenciongrCompromiso por Galicia         0.0000  0.0000\n#> intenciongrBarcelona pel Canvi            0.0000  0.0000\n#> intenciongrZaragoza en Común              0.0000  0.0000\n#> intenciongrSantiago Aberta                0.0000  0.0000\n#> intenciongrCNxR                           0.0000  0.0000\n#> intenciongrDemocracia Nacional            0.0000  0.0000\n#> intenciongrPoble Lliure                   0.0000  0.0000\n#> intenciongrPartido Humanista              0.0000  0.0000\n#> intenciongrSom Valencians                 0.0000  0.0000\n#> intenciongrConverxencia Galega            0.0000  0.0000\n#> intenciongrTerra Galega                   0.0000  0.0000\n#> intenciongrTeruel Existe                  3.6379  2.6304\n#> intenciongrExtremadura Unida              0.0000  0.0000\n#> intenciongrPP+C s                         0.0000  0.0000\n#> intenciongrPDeCAT                         0.0000  0.0000\n#> intenciongrPNC                            0.0000  0.0000\n#> intenciongrVoto nulo                     44.0260  6.7859\n#> intenciongrAdelante Sevilla               0.0000  0.0000\n#> intenciongrOtro partido                  84.9910  9.7962\n#> intenciongrEn blanco                    186.4136 14.3569\n#> intenciongrNo votaría                   376.8699 19.3696\n#> intenciongrNo sabe todavía              524.4669 21.7009\n#> intenciongrN.C.                         169.0461 13.5892\n\ncis_estim <- estim_simple1 %>% \n  as.data.frame() %>%  # as.data.frame para no perder los nombre de filas\n  rownames_to_column(var = \"partido\") %>% \n  mutate(partido       = str_sub(partido, 12, -1),\n         tot_low       = total - 1.96 * SE , # intervalos simples\n         tot_high      = total + 1.96 * SE, \n         pct_voto      = total    / 3779.429, \n         pct_voto_low  = tot_low  / 3779.429, \n         pct_voto_high = tot_high / 3779.429\n           ) \n\n\ncañi_estim <- estim_simple2 %>% \n  as.data.frame() %>%  # as.data.frame para no perder los nombre de filas\n  rownames_to_column(var = \"partido\") %>% \n  mutate(partido       = str_sub(partido, 12, -1),\n         tot_low       = total - 1.96 * SE , \n         tot_high      = total + 1.96 * SE, \n         pct_voto      = total    / 3779.429, \n         pct_voto_low  = tot_low  / 3779.429, \n         pct_voto_high = tot_high / 3779.429\n  ) \n\n\np_cis <- cis_estim %>% \n  top_n(22, pct_voto) %>% \n  ggplot(aes(y = reorder(partido, pct_voto ), x = pct_voto))  +\n  geom_point(color = \"darkred\", size = rel(3)) +\n  geom_errorbarh(aes(xmin = pct_voto_low, xmax = pct_voto_high)) +\n  scale_x_continuous(labels = scales::percent, \n                     limits = c(0, 0.22)) +\n  labs(title = \"Estimación intención voto (CIS)\", \n       subtitle = \"Usando ponderación cis\",\n       x = \"Proporción voto\",\n       y = \"Partido\")\n      \np_cañi <- cañi_estim %>% \n  top_n(22, pct_voto) %>% \n  ggplot(aes(y = reorder(partido, pct_voto ), x = pct_voto))  +\n  geom_point(color = \"darkblue\", size = rel(3)) +\n  geom_errorbarh(aes(xmin = pct_voto_low, xmax = pct_voto_high)) +\n  scale_x_continuous(labels = scales::percent, \n                     limits = c(0, 0.22)) +\n  labs(title = \"Estimación intención voto (con raking) \", \n       subtitle = \"Ajustando ponderación por edad,\\nsexo y recuerdo voto\",\n       x = \"Proporción voto\",\n       y = \"Partido\")\n\np_cis + p_cañi"
  },
  {
    "objectID": "2022/01/01/cocinando/index.html#nota.",
    "href": "2022/01/01/cocinando/index.html#nota.",
    "title": "Cocinando",
    "section": "Nota.",
    "text": "Nota.\nEn vez de raking es usual utilizar modelos como MRP (multilevel regression and estratification), pero este último tiene el incoveniente (aunque muchas otras ventajas) de que necesita saber la distribución conjunta de las variables por las que se postestratifica. Aquí os dejo un artículo interesante"
  },
  {
    "objectID": "2022/01/08/cachitos-2021/index.html",
    "href": "2022/01/08/cachitos-2021/index.html",
    "title": "Cachitos 2021",
    "section": "",
    "text": "Retomando la entrada de cachitos de la nochevieja de 2020\nActualizo el script para bajar el video de la nochevieja de este año, extraer los fotogramas y tener los subtítulos.\nEste año parece (o yo no me he enterado) que ha habido menos polémica. Pero como siempre, nos hemos reído bastante.\nEjemplo:\n\n\n\n\n\nY el texto extraído con tesseract\nEl cámara se arrima, pero sin tocar.... NW,\nSN\n4 como el PSOE con la monarquía. | aaa\nAquí os dejo el script para bajar el vídeo y extraer los subtítulos.\n#!/bin/bash\n\nroot_directory=/home/jose/proyecto_cachitos\nmkdir -p $root_directory\ncd $root_directory\n\necho \"First arg: $1\"\nmkdir -p video\n\ncd video\n\nANNO=$1\necho $ANNO\nsuffix_video=\"_cachitos.mp4\"\nsuffix_jpg_dir=\"_jpg\"\nsuffix_txt_dir=\"_txt\"\n\nvideo_file=$ANNO$suffix_video\necho $video_file\n\nif [ \"$ANNO\" == \"2021\" ] ;\nthen\n    wget https://lote5-vod-hls-geoblockurl.akamaized.net/resources/TE_GLUCA/mp4/4/0/1641020001504.mp4 \n    mv 1641020001504.mp4 $video_file\nfi\n \nif [ \"$ANNO\" == \"2020\" ] ;\nthen\n    wget http://mediavod-lvlt.rtve.es/resources/TE_GLUCA/mp4/2/4/1609487028742.mp4\n    mv 1609487028742.mp4 $video_file\nfi\n\nif [ \"$ANNO\" == \"2019\" ] ;\nthen\n    wget https://rtvehlsvod2020a-fsly.vod-rtve.cross-media.es/resources/TE_GLUCA/mp4/0/9/1577860099590.mp4\n    mv 1577860099590.mp4 $video_file\nfi\n\n# Pasar a jpg uno de cada 220 fotogramas\n\nmplayer -vf framestep=200 -framedrop -nosound $video_file -speed 100 -vo jpeg:outdir=$ANNO$suffix_jpg_dir \n \ncd $ANNO$suffix_jpg_dir \n \n# Convertir a formato más pequño\nfind . -name '*.jpg' |  parallel -j 6 mogrify -resize 642x480 {}\n\n# Seleccionar cacho dond estan subtitulos\nfind . -name '*.jpg' |  parallel -j 6 convert {} -crop 460x50+90+295 +repage -compress none -depth 8 {}.subtitulo.tif\n\n# Poner en negativo para que el ocr funcione mejor\nfind . -name '*.tif' |  parallel -j 6 convert {} -negate -fx '.8*r+.8*g+0*b' -compress none -depth 8 {}\n\n# Pasar el ocr con idioma en español\nfind . -name '*.tif' |  parallel -j 6 tesseract -l spa {} {}\n\n# mover a directorio texto\nmkdir -p $root_directory/$ANNO$suffix_txt_dir\n\nmv *.txt $root_directory/$ANNO$suffix_txt_dir\n\ncd $root_directory"
  },
  {
    "objectID": "2022/01/16/cachitos-tercera-parte/index.html",
    "href": "2022/01/16/cachitos-tercera-parte/index.html",
    "title": "Cachitos. Tercera parte",
    "section": "",
    "text": "Cómo aún ando medio “covitoso”, reciclo el código y comentarios de la entrada de 2021 y con solo cambiar la ruta del fichero de subtítulos ya nos vale todo el código.\nEl csv con el texto de los subtítulos para 2021 lo tenéis en este enlace.\nVamos al lío\n\n\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno <- \"2021\"\n\nLeemos el csv. Uso DT y así podéis ver todos los datos o buscar cosas, por ejemplo Ayuso o pandemia , monarquía o podemos\n\nsubtitulos_proces <-  read_csv(str_glue(\"{root_directory}{anno}_txt_unido.csv\"))\n\nsubtitulos_proces %>% \n  select(texto, n_fichero, n_caracteres) %>% \n  DT::datatable()\n\n\n\n\n\n\nOye, pues sólo con esto ya nos valdría ¿no?\nQuitamos stopwords\n\nto_remove <- c(tm::stopwords(\"es\"),\n               \"110\", \"4\",\"1\",\"2\",\"7\",\"10\",\"0\",\"ñ\",\"of\",\n               \"5\",\"á\",\"i\",\"the\",\"3\", \"n\", \"p\",\n               \"ee\",\"uu\",\"mm\",\"ema\", \"zz\",\n               \"wr\",\"wop\",\"wy\",\"x\",\"xi\",\"xl\",\"xt\",\n               \"xte\",\"yí\", \"your\", \"si\")\n\nhead(to_remove, 40)\n#>  [1] \"de\"      \"la\"      \"que\"     \"el\"      \"en\"      \"y\"       \"a\"      \n#>  [8] \"los\"     \"del\"     \"se\"      \"las\"     \"por\"     \"un\"      \"para\"   \n#> [15] \"con\"     \"no\"      \"una\"     \"su\"      \"al\"      \"lo\"      \"como\"   \n#> [22] \"más\"     \"pero\"    \"sus\"     \"le\"      \"ya\"      \"o\"       \"este\"   \n#> [29] \"sí\"      \"porque\"  \"esta\"    \"entre\"   \"cuando\"  \"muy\"     \"sin\"    \n#> [36] \"sobre\"   \"también\" \"me\"      \"hasta\"   \"hay\"\n\nPero en nuestros datos, las palabras no están separadas, tendríamos que separarlas y luego quitar las que no queremos. Para eso voy a utilizar la librería tidytext\n\nlibrary(tidytext)\n\n# Con unnest token pasamos a un dataframe qeu tiene tantas filas como palabras\n\nprint(str_glue(\"Filas datos originales: {tally(subtitulos_proces)}\"))\n#> Filas datos originales: 687\n\nsubtitulos_proces_one_word <- subtitulos_proces %>% \n    unnest_tokens(input = texto,\n                  output = word) %>% \n    filter(! word %in% to_remove) %>% # quito palabras de la lista \n    filter(nchar(word)>1) # Nos quedamos con palabras que tengan más de un cáracter\n\n\nprint(str_glue(\"Filas datos tokenizado: {tally(subtitulos_proces_one_word)}\"))\n#> Filas datos tokenizado: 4735\n\nsubtitulos_proces_one_word %>% \n  select(name,n_fichero,word, n_caracteres)\n#> # A tibble: 4,735 × 4\n#>     name n_fichero                      word          n_caracteres\n#>    <dbl> <chr>                          <chr>                <dbl>\n#>  1    14 00000014.jpg.subtitulo.tif.txt servicio               118\n#>  2    14 00000014.jpg.subtitulo.tif.txt meteorológico          118\n#>  3    14 00000014.jpg.subtitulo.tif.txt cachitos               118\n#>  4    14 00000014.jpg.subtitulo.tif.txt informa                118\n#>  5    14 00000014.jpg.subtitulo.tif.txt prevén                 118\n#>  6    14 00000014.jpg.subtitulo.tif.txt vientos                118\n#>  7    14 00000014.jpg.subtitulo.tif.txt fiesta                 118\n#>  8    14 00000014.jpg.subtitulo.tif.txt fuertes                118\n#>  9    14 00000014.jpg.subtitulo.tif.txt próximas               118\n#> 10    14 00000014.jpg.subtitulo.tif.txt tres                   118\n#> # … with 4,725 more rows\n\nUna cosa simple que podemos hacer es contar palabras, y vemos que lo que más se repite es canción, obvio\n\npalabras_ordenadas <- subtitulos_proces_one_word %>% \n    group_by(word) %>% \n    summarise(veces = n()) %>% \n    arrange(desc(veces))\n\npalabras_ordenadas %>% \n    slice(1:20) %>% \n    ggplot(aes(x = reorder(word, veces), y = veces)) +\n    geom_col(show.legend = FALSE) +\n    ylab(\"veces\") +\n    xlab(\"\") +\n    coord_flip() +\n    theme_bw()\n\n\n\n\n\n\n\n\nO pintarlas en plan nube de palabras.\n\nlibrary(wordcloud)\npal <- brewer.pal(8,\"Dark2\")\nsubtitulos_proces_one_word %>% \n    group_by(word) %>% \n    count() %>% \n    with(wordcloud(word, n, random.order = FALSE, max.words = 80, colors=pal))    \n\n\n\n\n\n\n\n\nPues una vez que tenemos las palabras de cada subtítulo separadas podemos buscar palabras polémicas, aunque antes al usar la librería DT ya podíamos buscar, veamos como sería con el código.\nCreamos lista de palabras a buscar.\n\npalabras_1 <- c(\"monarca\",\"pp\",\"vox\",\"rey\",\"coron\",\"zarzuela\",\n                \"prisión\", \"democracia\", \"abascal\",\"casado\",\n                \"ultra\",\"ciudada\", \"oposición\",\"derech\",\n                \"podem\",\"sanchez\",\"iglesias\",\"errejon\",\"izquier\",\n                \"gobierno\",\"illa\",\"redondo\",\"ivan\",\"celaa\",\n                \"guardia\",\"príncipe\",\"principe\",\"ayuso\",\n                \"tezanos\",\"cis\",\"republic\", \"simon\", \"pandem\",\"lazo\",\"arrim\",\n                \"toled\",\"alber\",\"fach\", \"zarzu\", \"democr\",\"vicepre\", \"minist\",\n                \"irene\",\"montero\",\"almeida\", \"monarq\")\n\nConstruimos una regex para que encuentre las palabras que empiecen así.\n\n(exp_regx <- paste0(\"^\",paste(palabras_1, collapse = \"|^\")))\n#> [1] \"^monarca|^pp|^vox|^rey|^coron|^zarzuela|^prisión|^democracia|^abascal|^casado|^ultra|^ciudada|^oposición|^derech|^podem|^sanchez|^iglesias|^errejon|^izquier|^gobierno|^illa|^redondo|^ivan|^celaa|^guardia|^príncipe|^principe|^ayuso|^tezanos|^cis|^republic|^simon|^pandem|^lazo|^arrim|^toled|^alber|^fach|^zarzu|^democr|^vicepre|^minist|^irene|^montero|^almeida|^monarq\"\n\nY nos creamos una variable que valga TRUE cuando suceda esto\n\n\nsubtitulos_proces_one_word <- subtitulos_proces_one_word %>% \n    mutate(polemica= str_detect(word, exp_regx))\n\nsubtitulos_proces_one_word %>% \n  filter(polemica) %>% \n  select(name, word, n_fichero) \n#> # A tibble: 32 × 3\n#>     name word      n_fichero                     \n#>    <dbl> <chr>     <chr>                         \n#>  1   139 reyes     00000139.jpg.subtitulo.tif.txt\n#>  2   169 arrima    00000169.jpg.subtitulo.tif.txt\n#>  3   169 monarquía 00000169.jpg.subtitulo.tif.txt\n#>  4   330 pandemia  00000330.jpg.subtitulo.tif.txt\n#>  5   397 ministro  00000397.jpg.subtitulo.tif.txt\n#>  6   398 pandemia  00000398.jpg.subtitulo.tif.txt\n#>  7   404 guardia   00000404.jpg.subtitulo.tif.txt\n#>  8   581 iglesias  00000581.jpg.subtitulo.tif.txt\n#>  9   621 podemos   00000621.jpg.subtitulo.tif.txt\n#> 10   641 illa      00000641.jpg.subtitulo.tif.txt\n#> # … with 22 more rows\n\nPodríamos ver el texto de los subtítulos, para eso, nos quedamos con un identificador, como el nombre del fichero txt, que nos servirá luego para leer la imagen.\nPues en realidad tenemos sólo 27 subtítulos polémicos de los de alrededor de 680 que hay\n\nsubtitulos_polemicos <- subtitulos_proces_one_word %>% \n    filter(polemica) %>% \n    pull(n_fichero) %>% \n    unique()\nsubtitulos_polemicos\n#>  [1] \"00000139.jpg.subtitulo.tif.txt\" \"00000169.jpg.subtitulo.tif.txt\"\n#>  [3] \"00000330.jpg.subtitulo.tif.txt\" \"00000397.jpg.subtitulo.tif.txt\"\n#>  [5] \"00000398.jpg.subtitulo.tif.txt\" \"00000404.jpg.subtitulo.tif.txt\"\n#>  [7] \"00000581.jpg.subtitulo.tif.txt\" \"00000621.jpg.subtitulo.tif.txt\"\n#>  [9] \"00000641.jpg.subtitulo.tif.txt\" \"00000665.jpg.subtitulo.tif.txt\"\n#> [11] \"00000671.jpg.subtitulo.tif.txt\" \"00000680.jpg.subtitulo.tif.txt\"\n#> [13] \"00000763.jpg.subtitulo.tif.txt\" \"00000828.jpg.subtitulo.tif.txt\"\n#> [15] \"00000853.jpg.subtitulo.tif.txt\" \"00000865.jpg.subtitulo.tif.txt\"\n#> [17] \"00000866.jpg.subtitulo.tif.txt\" \"00000955.jpg.subtitulo.tif.txt\"\n#> [19] \"00000980.jpg.subtitulo.tif.txt\" \"00000981.jpg.subtitulo.tif.txt\"\n#> [21] \"00001135.jpg.subtitulo.tif.txt\" \"00001169.jpg.subtitulo.tif.txt\"\n#> [23] \"00001176.jpg.subtitulo.tif.txt\" \"00001183.jpg.subtitulo.tif.txt\"\n#> [25] \"00001189.jpg.subtitulo.tif.txt\" \"00001228.jpg.subtitulo.tif.txt\"\n#> [27] \"00001233.jpg.subtitulo.tif.txt\" \"00001254.jpg.subtitulo.tif.txt\"\n#> [29] \"00001262.jpg.subtitulo.tif.txt\"\n\n\n(texto_polemicos <- subtitulos_proces %>% \n    filter(n_fichero %in% subtitulos_polemicos) %>% \n    arrange(n_fichero) %>% \n    pull(texto))\n#>  [1] \"ella se conformaba con poquita cosa no como tú que sigues poniendo el scalextric en la carta a los reyes\"             \n#>  [2] \"el cámara se arrima pero sin tocar nw sn 4 como el psoe con la monarquía aaa\"                                         \n#>  [3] \"el mítico rockero henry stephen autor de esta canción también murió en 2021 a causa de la pandemia\"                   \n#>  [4] \"con el fary perdimos un buen taxista y un mejor ministro de economía\"                                                 \n#>  [5] \"ha hecho falta una crisis financiera de 10 años y una pandemia para que alguien hiciera caso a el fary\"               \n#>  [6] \"si en una verbena gallega la tortilla está muy hecha o no suena ana kiro viene la guardia civil y la cierra\"          \n#>  [7] \"4 la italiana le supo sacar más partido al bamboleo y que julio iglesias o los gypsy kings\"                           \n#>  [8] \"anita triunfó con un tema que al principio no le molaba nada como sánchez con podemos\"                                \n#>  [9] \"illa rusa tt y los coches del pasado\"                                                                                 \n#> [10] \"con tanto escenario para ellos solos parece la sede de ciudadanos\"                                                    \n#> [11] \"himno de los nuevos románticos al que también pertenecieron ultravox duran duran o hace poco pablo alborán y lópez\"   \n#> [12] \"marian gold era una mezcla entre isa serra e ivonne reyes\"                                                            \n#> [13] \"a una canción de pimpinela b sánchez y casado en una sesión de control\"                                               \n#> [14] \"se ganó la corona del rey de la bachata como líder del grupo aventura\"                                                \n#> [15] \"olvidaste la tabla periódica y a reyes godos pero esto sigue en algún lugar de tu cerebro junto al fijo de tus padres\"\n#> [16] \"sherpa a la izquierda solo de la imagen sigue siendo igual de barón pero bastante menos rojo\"                         \n#> [17] \"como buen sherpa es capaz de cargar con el pack completo plandemia inmigrantes invasores gobierno comunista\"          \n#> [18] \"cuentan que iván redondo susurraba este estribillo al oído de pedro sánchez un par de veces al día\"                   \n#> [19] \"o iglesias an e y canto a galicia dn d\"                                                                               \n#> [20] \"lo único virgen alrededor de julio iglesias eran 4 las islas en las que invertía su patrimonio\"                       \n#> [21] \"a estas alturas de la noche es cuando te arrimas y sentencias ni héroes ni mecano el último\"                          \n#> [22] \"e podría haber sido la música de la campaña electoral de ayuso 4 ye la campañn\"                                       \n#> [23] \"creéis que rodrigo de lorenzo sabe que no lleva maraca en la mano izquierda\"                                          \n#> [24] \"aparte de ser el mejor letrista en español el intelectual de la salsa y ministro de turismo hizo\"                     \n#> [25] \"este clásico tiene más versiones que ministros este gobierno\"                                                         \n#> [26] \"22 en realidad esas coletas siempre fueron polémicas me lo milagroso es llegar a vicepresidente con una\"              \n#> [27] \"azúcar no bueno mejor asi alberto garzón\"                                                                             \n#> [28] \"concretamente falín a vuestra izquierda era el padre de falete\"                                                       \n#> [29] \"amanece se recorta en el horizonte el perfil de santiago abascal p\"\n\nPodemos ver las imágenes\n\n(polemica_fotogramas <- unique(substr(subtitulos_polemicos, 1,12)))\n#>  [1] \"00000139.jpg\" \"00000169.jpg\" \"00000330.jpg\" \"00000397.jpg\" \"00000398.jpg\"\n#>  [6] \"00000404.jpg\" \"00000581.jpg\" \"00000621.jpg\" \"00000641.jpg\" \"00000665.jpg\"\n#> [11] \"00000671.jpg\" \"00000680.jpg\" \"00000763.jpg\" \"00000828.jpg\" \"00000853.jpg\"\n#> [16] \"00000865.jpg\" \"00000866.jpg\" \"00000955.jpg\" \"00000980.jpg\" \"00000981.jpg\"\n#> [21] \"00001135.jpg\" \"00001169.jpg\" \"00001176.jpg\" \"00001183.jpg\" \"00001189.jpg\"\n#> [26] \"00001228.jpg\" \"00001233.jpg\" \"00001254.jpg\" \"00001262.jpg\"\n\npolemica_fotogramas_full <- paste0(str_glue(\"{root_directory}video/{anno}_jpg/\"), polemica_fotogramas)\n\nsubtitulos_polemicos_full <- paste0(polemica_fotogramas_full,\".subtitulo.tif\")\n\nY ahora utilizando la librería magick en R y un poco de programación funcional (un simple map), tenemos la imagen leída\n\nlibrary(magick)\n\nfotogramas_polemicos_img <- map(polemica_fotogramas_full, image_read)\nsubtitulos_polemicos_img <- map(subtitulos_polemicos_full, image_read)\n\n\nsubtitulos_polemicos_img[[18]]\n\n\n\n\n\n\n\n\n\nfotogramas_polemicos_img[[18]]\n\n\n\n\n\n\n\n\nPodemos ponerlos todos juntos.\n\nlista_fotogram_polemicos <- map(fotogramas_polemicos_img, grid::rasterGrob)\ngridExtra::grid.arrange(grobs=lista_fotogram_polemicos)"
  },
  {
    "objectID": "2022/01/10/cachitos-segunda-parte/index.html",
    "href": "2022/01/10/cachitos-segunda-parte/index.html",
    "title": "Cachitos. Segunda parte",
    "section": "",
    "text": "Nada, esto es sólo para leernos con R los subtítulos del post anterior.\n\nlibrary(tidyverse)\n\nroot_directory = \"/media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/\"\nanno <- \"2021\"\n\n# Construims un data frame con los nombrs de los ficheros \n\nnombre_ficheros <- list.files(path = str_glue(\"{root_directory}{anno}_txt/\")) %>% \n    enframe() %>% \n    rename(n_fichero = value)\n\nnombre_ficheros\n#> # A tibble: 1,384 × 2\n#>     name n_fichero                     \n#>    <int> <chr>                         \n#>  1     1 00000001.jpg.subtitulo.tif.txt\n#>  2     2 00000002.jpg.subtitulo.tif.txt\n#>  3     3 00000003.jpg.subtitulo.tif.txt\n#>  4     4 00000004.jpg.subtitulo.tif.txt\n#>  5     5 00000005.jpg.subtitulo.tif.txt\n#>  6     6 00000006.jpg.subtitulo.tif.txt\n#>  7     7 00000007.jpg.subtitulo.tif.txt\n#>  8     8 00000008.jpg.subtitulo.tif.txt\n#>  9     9 00000009.jpg.subtitulo.tif.txt\n#> 10    10 00000010.jpg.subtitulo.tif.txt\n#> # … with 1,374 more rows\n\nAhora los podemos leer en orden\n\nsubtitulos <-  list.files(path = str_glue(\"{root_directory}{anno}_txt/\"), \n                        pattern = \"*.txt\", full.names = TRUE) %>% \n    map(~read_file(.)) %>% \n    enframe() %>%  \n  # hacemos el join con el dataframe anterior para tener el nombre del fichero original\n    left_join(nombre_ficheros)\n\nglimpse(subtitulos)\n#> Rows: 1,384\n#> Columns: 3\n#> $ name      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n#> $ value     <list> \"\\f\", \"\\f\", \"FUN MÚSICA Y CINTAS DE VÍDEO\\n\\f\", \" \\n\\f\", \"\\…\n#> $ n_fichero <chr> \"00000001.jpg.subtitulo.tif.txt\", \"00000002.jpg.subtitulo.ti…\nsubtitulos\n#> # A tibble: 1,384 × 3\n#>     name value     n_fichero                     \n#>    <int> <list>    <chr>                         \n#>  1     1 <chr [1]> 00000001.jpg.subtitulo.tif.txt\n#>  2     2 <chr [1]> 00000002.jpg.subtitulo.tif.txt\n#>  3     3 <chr [1]> 00000003.jpg.subtitulo.tif.txt\n#>  4     4 <chr [1]> 00000004.jpg.subtitulo.tif.txt\n#>  5     5 <chr [1]> 00000005.jpg.subtitulo.tif.txt\n#>  6     6 <chr [1]> 00000006.jpg.subtitulo.tif.txt\n#>  7     7 <chr [1]> 00000007.jpg.subtitulo.tif.txt\n#>  8     8 <chr [1]> 00000008.jpg.subtitulo.tif.txt\n#>  9     9 <chr [1]> 00000009.jpg.subtitulo.tif.txt\n#> 10    10 <chr [1]> 00000010.jpg.subtitulo.tif.txt\n#> # … with 1,374 more rows\n\nen n_fichero tenemos el nombre y en value el texto\n\n\nsubtitulos %>% \n  pull(value) %>%\n  ## usamos `[[` que es el operador para acceder a la lista el que normalemente se usa [[nombre_elemento]]\n  `[[`(16)\n#> [1] \"Ella resume a la perfección la filosofía de Cachitos:\\nmontar “La fiesta” “Buscando en el baúl de los recuerdos”.\\n\\n \\n\\f\"\n\n# equivalentemente\n\n# subtitulos %>% \n#     pull(value) %>% \n#     pluck(16)\n\nComo sabemos que hay muchos ficheros sin texto podemos contar letras.\n\nsubtitulos <- subtitulos %>% \n    mutate(n_caracteres = nchar(value)) \n\nsubtitulos %>% \n    group_by(n_caracteres) %>% \n    count()\n#> # A tibble: 128 × 2\n#> # Groups:   n_caracteres [128]\n#>    n_caracteres     n\n#>           <int> <int>\n#>  1            1   428\n#>  2            3    97\n#>  3            4    19\n#>  4            5    13\n#>  5            6    15\n#>  6            7     8\n#>  7            8     6\n#>  8            9     3\n#>  9           10     3\n#> 10           11     2\n#> # … with 118 more rows\n\nsubtitulos %>% \n    group_by(n_caracteres) %>% \n    count() %>% \n  ggplot(aes(x = n_caracteres, y = n)) +\n  geom_col()\n\n\n\n\n\n\n\n\nY vemos que hay muchos subtitulos con pocos caracteres. Si vemos por ejemplo los que tienen menos de 12 caracteres\n\nsubtitulos %>% \n    filter(n_caracteres <12) %>% \n    pull(value) %>% \n    head(10)\n#> [[1]]\n#> [1] \"\\f\"\n#> \n#> [[2]]\n#> [1] \"\\f\"\n#> \n#> [[3]]\n#> [1] \" \\n\\f\"\n#> \n#> [[4]]\n#> [1] \"\\f\"\n#> \n#> [[5]]\n#> [1] \" \\n\\f\"\n#> \n#> [[6]]\n#> [1] \" \\n\\f\"\n#> \n#> [[7]]\n#> [1] \"\\f\"\n#> \n#> [[8]]\n#> [1] \"\\f\"\n#> \n#> [[9]]\n#> [1] \"\\f\"\n#> \n#> [[10]]\n#> [1] \"\\f\"\n\nQue se corresponden con haber pillado parte no del subtítulo sino del nombre de la actuación\n\nsubtitulos %>% \n    filter(n_caracteres ==15)\n#> # A tibble: 2 × 4\n#>    name value     n_fichero                      n_caracteres\n#>   <int> <list>    <chr>                                 <int>\n#> 1   571 <chr [1]> 00000571.jpg.subtitulo.tif.txt           15\n#> 2  1361 <chr [1]> 00001361.jpg.subtitulo.tif.txt           15\n\nUsando la librería magick en R que permite usar imagemagick en R, ver post de Raúl Vaquerizo y su homenaje a Sean Connery, podemos ver el fotgrama correspondiente\n\nlibrary(magick)\n(directorio_imagenes <- str_glue(\"{root_directory}video/{anno}_jpg/\"))\n#> /media/hd1/canadasreche@gmail.com/public/proyecto_cachitos/video/2021_jpg/\n\nimage_read(str_glue(\"{directorio_imagenes}00000018.jpg\"))\n\n\n\n\n\n\n\n\nTambién podemos ver hasta cuando pasa eso, por ejemplo si vemos subtítulos con 18 caracteres\n\nsubtitulos %>% \n    filter(n_caracteres ==18) %>% \n    pull(value)\n#> [[1]]\n#> [1] \" \\n\\nJ0 EN EL AMOR\\n\\f\"\n#> \n#> [[2]]\n#> [1] \"¿EDITH BROOKS\\nch\\n\\f\"\n#> \n#> [[3]]\n#> [1] \" \\n\\nmme Tha Power\\n\\f\"\n#> \n#> [[4]]\n#> [1] \"  \\n\\n\\\"RONTERA\\n\\n  \\n\\f\"\n\n\nsubtitulos <- subtitulos %>% \n    filter(n_caracteres > 17) \n\nglimpse(subtitulos)\n#> Rows: 778\n#> Columns: 4\n#> $ name         <int> 3, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 27, 28, 29, 30…\n#> $ value        <list> \"FUN MÚSICA Y CINTAS DE VÍDEO\\n\\f\", \"El servicio meteoro…\n#> $ n_fichero    <chr> \"00000003.jpg.subtitulo.tif.txt\", \"00000014.jpg.subtitulo…\n#> $ n_caracteres <int> 30, 118, 82, 117, 117, 25, 100, 97, 86, 88, 84, 43, 52, 8…\n\nCon el fin de detectar cuáles están duplicados y aprovechando que están en orden de aparición, podemos hacer utilizar distancias de texto para calcular la distancia de cada subtítulo con el anterior, y si la distancia es pequeña es que es el mismo rótulo.\nPrimero hacemos una mini-limpieza.\n\nstring_mini_clean <-  function(string){\n    string <- gsub(\"?\\n|\\n\", \" \", string)\n    string <- gsub(\"\\r|?\\f|=\", \" \", string)\n    string <- gsub('“|”|—|>',\" \", string)\n    \n    string <- gsub(\"[[:punct:][:blank:]]+\", \" \", string)\n    string <- tolower(string)\n    string <- gsub(\"  \", \" \", string)\n    \n    return(string)\n}\n\n# Haciendo uso de programacion funciona con purrr es muy fácil pasar esta función a cada elemento. y decirle que # el reultado es string con map_chr\n\nsubtitulos_proces <- subtitulos %>% \n    mutate(texto = map_chr(value, string_mini_clean)) %>% \n    select(-value)\n\nsubtitulos_proces %>% \n  select(texto)\n#> # A tibble: 778 × 1\n#>    texto                                                                        \n#>    <chr>                                                                        \n#>  1 \"fun música y cintas de vídeo \"                                              \n#>  2 \"el servicio meteorológico de cachitos informa se prevén vientos de fiesta m…\n#>  3 \"no es para menos llevamos dos años conformándonos solo con aires de siesta \"\n#>  4 \"ella resume a la perfección la filosofía de cachitos montar la fiesta busca…\n#>  5 \" ella resume a la perfección la filosofía de cachitos montar la fiesta busc…\n#>  6 \" oncé a2y in love \"                                                         \n#>  7 \"esperamos que tengáis una tele bien grande no sabemos si cabrá tanto flow e…\n#>  8 \"liberté egalité fraternité vacunaté y beyoncé la lola flores negra ejercien…\n#>  9 \"mirad su pelo ya os dijimos que el aire de fiesta iba a soplar fuerte esta …\n#> 10 \"mirad su pelo ya os dijimos que el aire de fiesta iba a soplar fuerte esta …\n#> # … with 768 more rows\n\nY ya vemos a simple vista que hay algun duplicado. Calculemos ahora la distancia de strings, utilizando la función stringdist de la librería del mismo nombre.\n\n\nsubtitulos_proces %>% \n    mutate(texto_anterior = lag(texto)) %>% \n    # calculamos distancias con método lcs (que no me he leído que hace exactamente)\n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\")) %>% \n  # veamos algunos elementos\n    filter(distancia < 19) %>% \n    arrange(desc(distancia) ) %>% \n    select(texto, texto_anterior, distancia) \n#> # A tibble: 89 × 3\n#>    texto                                                         texto…¹ dista…²\n#>    <chr>                                                         <chr>     <dbl>\n#>  1 \" la rosalía emérita \"                                        \" alía…      18\n#>  2 \"chango llegó a españa como aspirante a estrella del rock y … \"chang…      15\n#>  3 \"leonard cohen y el pitufo gruñón en el cuerpo de un italian… \"leona…      13\n#>  4 \" el stress del año 2000 nos llegó con casi 20 os de retraso… \" el s…       7\n#>  5 \"aquí ya llevaba cuatro años de carrera luis miguel tiene má… \"aquí …       7\n#>  6 \"imborrable siempre la sonrisa de jerry aunque un poco incóm… \"la im…       6\n#>  7 \"7 literalmente significa puedes tocarme la campanita funcio… \"liter…       6\n#>  8 \" las palabras no vienen fácilmente paradójicamente no fue e… \" las …       5\n#>  9 \"en españa el g arm nació ya vintage porque el británico nos… \"en es…       5\n#> 10 \"nosotros también lo sentimos si alguien se ofende y en nues… \" 4 no…       5\n#> # … with 79 more rows, and abbreviated variable names ¹​texto_anterior,\n#> #   ²​distancia\n\nY parece que funciona. Así que decido quitar las filas dónde la distancia sea menos que 19 y así eliminar muchos de los duplicados.\n\nsubtitulos_proces <- subtitulos_proces %>% \n    mutate(texto_anterior = lag(texto)) %>% \n    mutate(distancia = stringdist::stringdist(texto, texto_anterior, method = \"lcs\")) %>% \n    filter(distancia > 19) %>% \n    select(-texto_anterior)\n\nsubtitulos_proces %>% \n  head()\n#> # A tibble: 6 × 5\n#>    name n_fichero                      n_caracteres texto                dista…¹\n#>   <int> <chr>                                 <int> <chr>                  <dbl>\n#> 1    14 00000014.jpg.subtitulo.tif.txt          118 \"el servicio meteor…     106\n#> 2    15 00000015.jpg.subtitulo.tif.txt           82 \"no es para menos l…     110\n#> 3    16 00000016.jpg.subtitulo.tif.txt          117 \"ella resume a la p…     102\n#> 4    18 00000018.jpg.subtitulo.tif.txt           25 \" oncé a2y in love \"     100\n#> 5    19 00000019.jpg.subtitulo.tif.txt          100 \"esperamos que teng…      92\n#> 6    20 00000020.jpg.subtitulo.tif.txt           97 \"liberté egalité fr…     105\n#> # … with abbreviated variable name ¹​distancia\n\n\nwrite_csv(subtitulos_proces,\n          file = str_glue(\"{root_directory}{anno}_txt_unido.csv\"))"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "",
    "text": "Voy a poner un ejemplo de como utilizar solvers para investigación operativa dentro de R.\nTenemos la siguiente información: * Listado de códigos postales de España con la longitud y latitud del centroide del polígono. * Listado de códigos postales de la ubicación de las sedes de una empresa. * En la empresa hay jefes e indios, no es necesario que haya un jefe por sede.\nSe quiere, para cada provincia de España\n\nAsignar cada código postal de esa provincia a un empleado de la empres (jefe o indio).\nUn mismo código postal no puede estar asignado a más de un empleado.\nEn la medida de lo posible asignar a los empleados los códigos postales más cercanos al lugar de su sede.\nA igualdad de distancia entre un código postal y una sede, se debería asignar ese código postal a un indio.\nNingún indio debe tener asignados menos códigos postales que ningún jefe.\nLos jefes como máximo han de tener 7 códigos postales asignados.\nLos indios como mínimo han de tener 3 códigos postales asignados.\nNo puede haber ningún empleado que esté “desasignado”.\n\nDados estos requisitos debería plantear como es la definición del problema, pero no tengo ganas de ponerme a escribir fórmulas en latex, así que en vez de eso voy a utilizar unos datos simulados y directamente al código.."
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#carga-de-datos-y-crear-datos-ficticios.",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#carga-de-datos-y-crear-datos-ficticios.",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "Carga de datos y crear datos ficticios.",
    "text": "Carga de datos y crear datos ficticios.\n\nCarga códigos postales\nCasualmente, tengo por mi pc un shapefile algo antiguo (de cuando está capa estaba en cartociudad) con la capa de códigos postales de España, la cual si se quiere actualizada vale un dinerillo. correos, 6000 Euros la versión sin actualizaciones.. Bueno, si hacienda y correos somos todos me gustaría al menos poder utilizar esto actualizado sin que me cueste 6k.\nVamos a cargar la capa, obtener los centroides, pasar la geometría a longitud y latitud\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n## ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n## ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n## ✔ readr   2.1.2     ✔ forcats 0.5.1\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(sf)\n## Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\ncod_postales_raw <- readRDS(here::here(\"data/cp_boundaries.rds\")) %>%\n  select(-cp_num, -cp_2_num)\n\nhead(cod_postales_raw)\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -1536953 ymin: 3373964 xmax: -41802.13 ymax: 5247186\n## Projected CRS: WGS 84 / Pseudo-Mercator\n##      cp cp_2   area_m2                       geometry\n## 1 35560   35 187875455 MULTIPOLYGON (((-1518970 33...\n## 2 27330   27   6659413 MULTIPOLYGON (((-821864.3 5...\n## 3 46680   46  69190773 MULTIPOLYGON (((-51610.46 4...\n## 4 49706   49  90229134 MULTIPOLYGON (((-641488.4 5...\n## 5 21120   21  20068648 MULTIPOLYGON (((-776955.2 4...\n## 6 16623   16 132859998 MULTIPOLYGON (((-256256.7 4...\nPintamos algunos códigos\nplot(st_geometry(cod_postales_raw[1:2000, ]))\n\nPara obtener los centroides, usamos la función st_centroid y pasamos la capa de polígonos a una de puntos\ncod_postales_raw <- st_centroid(cod_postales_raw)\n## Warning in st_centroid.sf(cod_postales_raw): st_centroid assumes attributes are\n## constant over geometries of x\nhead(cod_postales_raw)\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -1525406 ymin: 3382025 xmax: -47782.92 ymax: 5245455\n## Projected CRS: WGS 84 / Pseudo-Mercator\n##      cp cp_2   area_m2                  geometry\n## 1 35560   35 187875455  POINT (-1525406 3382025)\n## 2 27330   27   6659413 POINT (-823274.9 5245455)\n## 3 46680   46  69190773 POINT (-47782.92 4752325)\n## 4 49706   49  90229134 POINT (-637415.5 5057096)\n## 5 21120   21  20068648 POINT (-778872.1 4479315)\n## 6 16623   16 132859998 POINT (-262034.3 4818194)\nplot(st_geometry(cod_postales_raw[1:2000, ]), cex = 0.2)\n\nAhora extraemos de la geometría la longitud y latitud. Para eso hay que transformar la geometría.\ncod_postales_raw <- cod_postales_raw %>%\n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\ncod_postales <- cod_postales_raw %>%\n  mutate(\n    centroide_longitud = unlist(map(geometry, 1)),\n    centroide_latitud = unlist(map(geometry, 2))\n  ) %>%\n  st_drop_geometry() %>% # quitamos la geometría y nos quedamos solo con la longitud y latitud\n  rename(\n    cod_postal = cp,\n    cod_prov = cp_2\n  ) %>%\n  filter(!is.na(centroide_longitud)) # tenía un polígono con NAS\n\nhead(cod_postales)\n##   cod_postal cod_prov   area_m2 centroide_longitud centroide_latitud\n## 1      35560       35 187875455        -13.7029565          29.05011\n## 2      27330       27   6659413         -7.3956047          42.56144\n## 3      46680       46  69190773         -0.4292412          39.21368\n## 4      49706       49  90229134         -5.7260007          41.30272\n## 5      21120       21  20068648         -6.9967272          37.28791\n## 6      16623       16 132859998         -2.3538946          39.67063\nPor otro lado me interesa añadir el literal de provincia, tengo una tabla extraída del INE con la correspondencia entre cod_prov y el literal\nprovincia <- read_csv(here::here(\"data/codprov.csv\"))\n## Rows: 52 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (2): CODIGO, LITERAL\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(provincia)\n## # A tibble: 6 × 2\n##   CODIGO LITERAL         \n##   <chr>  <chr>           \n## 1 02     Albacete        \n## 2 03     Alicante/Alacant\n## 3 04     Almería         \n## 4 01     Araba/Álava     \n## 5 33     Asturias        \n## 6 05     Ávila\nNormalizo a mayúsculas y sin tildes y se lo pego a los códigos postales\nprovincia <- provincia %>%\n  mutate(provincia = toupper(stringi::stri_trans_general(LITERAL, \"Latin-ASCII\")))\n\ncod_postales <- cod_postales %>%\n  left_join(provincia %>%\n    select(\n      CODIGO,\n      provincia\n    ),\n  by = c(\"cod_prov\" = \"CODIGO\")\n  )\n\ndim(cod_postales)\n## [1] 10808     6\nhead(cod_postales %>%\n  select(provincia, cod_prov, everything()))\n##           provincia cod_prov cod_postal   area_m2 centroide_longitud\n## 1       PALMAS, LAS       35      35560 187875455        -13.7029565\n## 2              LUGO       27      27330   6659413         -7.3956047\n## 3 VALENCIA/VALENCIA       46      46680  69190773         -0.4292412\n## 4            ZAMORA       49      49706  90229134         -5.7260007\n## 5            HUELVA       21      21120  20068648         -6.9967272\n## 6            CUENCA       16      16623 132859998         -2.3538946\n##   centroide_latitud\n## 1          29.05011\n## 2          42.56144\n## 3          39.21368\n## 4          41.30272\n## 5          37.28791\n## 6          39.67063\n\n\nDatos ficticios de las sedes de las empresas\nLo que voy a hacer es seleccionar aleatoriamente un número de códigos postales en cada provincia, que serán las sedes de la empresa. En cada provincia pongo al menos a un empleado de tipo = “jefe”. Luego, reparto de forma aleatoria entre los códigos postales que han sido elegidos como sedes otros 120 jefes y 480 indios.\nset.seed(155)\n\n## En cada provincia nos quedamos con  un 6% de códigos postales\n\nsedes_alea <- cod_postales %>%\n  group_by(provincia) %>%\n  slice_sample(prop = 0.06)\n\n\n\n# en cada provincia al menos un jefe y resto de empleados de forma aleatoria, en las diferentes sedes elegidas\n\npersonal <- bind_rows(\n  sedes_alea %>%\n    select(provincia, cod_postal) %>%\n    group_by(provincia) %>%\n    slice_sample(n = 1) %>%\n    ungroup() %>%\n    select(cod_postal) %>%\n    mutate(tipo = \"jefe\"),\n  tibble(\n    tipo = c(rep(\"jefe\", 120), rep(\"indio\", 360)),\n    cod_postal = sample(sedes_alea$cod_postal, size = 480, replace = TRUE)\n  )\n)\nCreamos data set sedes\nsedes <- personal %>%\n  left_join(sedes_alea)\n## Joining, by = \"cod_postal\"\ndim(sedes)\n## [1] 530   7\nsample_n(sedes, 7)\n## # A tibble: 7 × 7\n##   cod_postal tipo  cod_prov    area_m2 centroide_longitud centroide_la…¹ provi…²\n##   <fct>      <chr> <chr>         <dbl>              <dbl>          <dbl> <chr>  \n## 1 37660      indio 37        36448279.              -5.99           40.5 SALAMA…\n## 2 41770      jefe  41       183345907.              -5.55           37.0 SEVILLA\n## 3 08011      jefe  08          968836.               2.16           41.4 BARCEL…\n## 4 34479      indio 34        49891663.              -4.42           42.4 PALENC…\n## 5 34859      indio 34       118812672.              -4.59           42.8 PALENC…\n## 6 09348      indio 09       249695400.              -3.61           42.0 BURGOS \n## 7 29750      indio 29        14389642.              -4.04           36.8 MALAGA \n## # … with abbreviated variable names ¹​centroide_latitud, ²​provincia"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-en-granada",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-en-granada",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "IO al servicio del mal en GRANADA",
    "text": "IO al servicio del mal en GRANADA\nComo ejemplo, vamos a ver como sería para Granada\ncod_postales_granada <- cod_postales %>%\n  filter(provincia == \"GRANADA\") %>%\n  mutate(id = row_number())\n\nsedes_granada <- sedes %>%\n  filter(provincia == \"GRANADA\") %>%\n  arrange(desc(tipo)) %>%\n  mutate(id_sede = row_number())\n\nsedes_granada\n## # A tibble: 11 × 8\n##    cod_postal tipo  cod_prov    area_m2 centroide_long…¹ centr…² provi…³ id_sede\n##    <fct>      <chr> <chr>         <dbl>            <dbl>   <dbl> <chr>     <int>\n##  1 18328      jefe  18        58574459.            -3.87    37.2 GRANADA       1\n##  2 18006      jefe  18         3245912.            -3.61    37.2 GRANADA       2\n##  3 18516      jefe  18       146541813.            -3.24    37.2 GRANADA       3\n##  4 18516      jefe  18       146541813.            -3.24    37.2 GRANADA       4\n##  5 18197      indio 18        10003524.            -3.61    37.2 GRANADA       5\n##  6 18516      indio 18       146541813.            -3.24    37.2 GRANADA       6\n##  7 18414      indio 18        40411565.            -3.34    36.9 GRANADA       7\n##  8 18197      indio 18        10003524.            -3.61    37.2 GRANADA       8\n##  9 18369      indio 18        17670871.            -4.01    37.2 GRANADA       9\n## 10 18611      indio 18        33542783.            -3.60    36.8 GRANADA      10\n## 11 18514      indio 18       110524485.            -3.08    37.2 GRANADA      11\n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nEs importante haber ordenado por tipo , porque vamos a utilizar el mismo índice j para empleados jefe y empleados indios.\nAhora definimos: * m como el número de empleados en las sedes de Granada * n como el número de códigos postales a asignar en Granada * n_sedes como el número de sedes * njefes como el número de jefes * n_indios como el número de indios\nm <- nrow(sedes_granada)\nn <- nrow(cod_postales_granada)\nn_sedes <- length(unique(sedes_granada$cod_postal))\n\nnjefes <- sedes_granada %>%\n  filter(tipo == \"jefe\") %>%\n  count() %>%\n  pull(n)\n\nn_indios <- m - njefes\nNecesitamos definir una función de distancia entre los códigos postales a asignar y las sedes. Para eso usamos la distancia Haversine que está implementada en la librería geosphere. Y aquí ya introducimos uno de los requerimientos. Básicamente aumentamos la distancia un 10% si el empleado es un jefe, de forma que sea peor asignarle ese código postal al jefe en términos de minimizar el total de distancias.\ntransportcost_granada <- function(i, j) {\n  cliente <- cod_postales_granada[i, ]\n  comercial <- sedes_granada[j, ]\n  distancia <-\n    geosphere::distHaversine(\n      c(cliente$centroide_longitud, cliente$centroide_latitud),\n      c(comercial$centroide_longitud, comercial$centroide_latitud)\n    )\n\n  if (comercial[, \"tipo\"] == \"jefe\") {\n    distancia <- distancia * 1.1\n  }\n\n  return(distancia / 1000) # devolvemos la disancia en km\n}\n\n# distancia entre sede 1 y empleado 3\ntransportcost_granada(1, 3)\n## [1] 51.54738\nPintamos los códigos postales y las sedes. Los granadinos reconoceremos la forma de la provincia.\np <-\n  ggplot(\n    cod_postales_granada,\n    aes(centroide_longitud, centroide_latitud)\n  ) +\n  geom_point(size = rel(2), shape = 4) +\n  geom_point(\n    data = sedes_granada,\n    size = rel(3),\n    color = \"darkorange\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank()\n  )\np + ggtitle(\"Sin asignar\")"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#optimización",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#optimización",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "Optimización",
    "text": "Optimización\nPara optimizar el problema vamos a usar la librería ompr que permite plantear el problema de optimización lineal entera de forma sencilla, y se conecta a la librería ROI que es la que al final llama al solver. Como solver vamos a utilizar glpk que es software libre y lo suficientemente bueno para este ejemplo.\nlibrary(ompr)\nlibrary(ompr.roi)\nlibrary(ROI.plugin.glpk)\nlibrary(patchwork) # pa unir los ggplots resultantes\nDefinimos el modelo\nmip_model_granada <- MIPModel() %>%\n  # variable indicadora que indica si una tienda i se asigna a comercial j\n  add_variable(x[i, j], i = 1:n, j = 1:m, type = \"binary\") %>%\n  # Minimizar el objetivo de distancia\n  set_objective(sum_over(transportcost_granada(i, j) * x[i, j], i = 1:n, j = 1:m), \"min\") %>%\n  # cada tienda (código postal) solo debe ir a un comerciial. el comercial puede atender varios\n  add_constraint(sum_over(x[i, j], j = 1:m) == 1, i = 1:n) %>%\n  # todo el mundo tiene que atender al minimo a una tienda\n  add_constraint(sum_over(x[i, j], i = 1:n) >= 1, j = 1:m) %>%\n  #   Los jefes curran menos, como máximo 7 tiendas\n  add_constraint(sum_over(x[i, j], i = 1:n) <= 7, j = 1:njefes) %>%\n  #\n  # # Los indios al menos atienden a 3 tiendas\n  add_constraint(sum_over(x[i, j], i = 1:n) >= 3, j = (njefes + 1):m) %>%\n  # para no sobrecargar mucho a los indios, les pongo un máximo que sea 1.5 veces el núemro de tiendas entre total currantes (jefes + indios)\n  add_constraint(sum_over(x[i, j], i = 1:n) <= round(1.5 * n / m), j = (njefes + 1):m) %>%\n  add_constraint(sum_over(x[i, j], i = 1:n) >= sum_over(x[i, k], i = 1:n), j = (njefes + 1):m, k = 1:njefes)\nAlgunas aclaraciones de la sintaxis anterior.\n\nNuestra variable auxilizar es \\(X_{i,j}\\) dónde la i son los códigos postales y la j cada empleado.\nSe trata de minimizar la suma total de distancias cuando se asigna un código postal a un empleado, para todos los códigos postales y todos los empleados.\nLa restricción add_constraint(sum_over(x[i, j], j = 1:m) == 1  , i = 1:n) si nos fijamos en el sum_over significa sumar en j (empleados) para cada código postal (i) y que esa suma valga 1. Es decir, para cada código postal (i) sólo se permite que sea asignado a un empleado\nadd_constraint(sum_over(x[i, j], i = 1:n) >= 1  , j = 1:m) Que para cada empleado (j) la suma de todos los códigos postales que se le asignen sea mayor o igual que 1. Vamos que no se quede ninguno ocioso.\nadd_constraint(sum_over( x[i,j], i = 1:n)  <= 7, j = 1:njefes) por eso ordeanmos por tipo para que el índice 1:njefes corresponda a los empleados jefes, esta restricción asegura que a un jefe no se le asignen más de 7 códigos postales.\nadd_constraint(sum_over( x[i,j], i = 1:n)  >=  3 , j = (njefes +1):m) Mínimo 3 códigos postales para los indios.\nadd_constraint(sum_over( x[i,j], i = 1:n)  <=  round(1.5 * n/m) , j = (njefes +1):m) Esta restricción intenta equilibrar el número de asignaciones para los indios, de forma que como mucho a un empleado tenga 1.5 veces la media de códigos postales por empleado.\nadd_constraint(sum_over( x[i,j], i = 1:n)  >=  sum_over( x[i,k], i = 1:n) , j = (njefes +1):m, k = 1:njefes) En esta restricción es dónde aseguramos que ningún empleado tenga menos asignaciones que ningún jefe, por eso se ha usado el índice k.\n\nPues el problema tiene 2200 variables (todas binarias) y 257 restricciones.\nmip_model_granada\n## Mixed integer linear optimization problem\n## Variables:\n##   Continuous: 0 \n##   Integer: 0 \n##   Binary: 2200 \n## Model sense: minimize \n## Constraints: 257\nResolvemos con glpk\nresult_granada <- solve_model(mip_model_granada, with_ROI(solver = \"glpk\", verbose = TRUE))\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 257 rows, 2200 columns, 19200 non-zeros\n##       0: obj =   0.000000000e+00 inf =   2.320e+02 (218)\n##     397: obj =   9.427540716e+03 inf =   5.627e-13 (0) 1\n## *   870: obj =   3.723682515e+03 inf =   0.000e+00 (0) 2\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 257 rows, 2200 columns, 19200 non-zeros\n## 2200 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +   870: mip =     not found yet >=              -inf        (1; 0)\n## +   870: >>>>>   3.723682515e+03 >=   3.723682515e+03   0.0% (1; 0)\n## +   870: mip =   3.723682515e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\nresult_granada\n## Status: success\n## Objective value: 3723.683\nY ahora procedemos a ver las asignaciones. Para eso utilizamos la función get_solution que nos va a devolver la solución obtenida para nuestra variable \\(X_{i,j}\\)\nmatching <- result_granada %>%\n  get_solution(x[i, j]) %>%\n  select(i, j, value) %>%\n  filter(value > 0) # nons quedamos con las asignaciones\n\nmatching\n##       i  j value\n## 1    16  1     1\n## 2    27  1     1\n## 3    55  1     1\n## 4    68  1     1\n## 5   119  1     1\n## 6   157  1     1\n## 7   173  1     1\n## 8    13  2     1\n## 9    37  2     1\n## 10   96  2     1\n## 11  113  2     1\n## 12  161  2     1\n## 13  169  2     1\n## 14  178  2     1\n## 15  141  3     1\n## 16   34  4     1\n## 17    1  5     1\n## 18    8  5     1\n## 19   23  5     1\n## 20   30  5     1\n## 21   32  5     1\n## 22   71  5     1\n## 23   98  5     1\n## 24  102  5     1\n## 25  108  5     1\n## 26  112  5     1\n## 27  117  5     1\n## 28  120  5     1\n## 29  122  5     1\n## 30  124  5     1\n## 31  130  5     1\n## 32  132  5     1\n## 33  134  5     1\n## 34  137  5     1\n## 35  138  5     1\n## 36  140  5     1\n## 37  149  5     1\n## 38  170  5     1\n## 39  182  5     1\n## 40  191  5     1\n## 41  192  5     1\n## 42  194  5     1\n## 43  198  5     1\n## 44   10  6     1\n## 45   15  6     1\n## 46   20  6     1\n## 47   65  6     1\n## 48   69  6     1\n## 49   82  6     1\n## 50   83  6     1\n## 51   86  6     1\n## 52   87  6     1\n## 53   92  6     1\n## 54   93  6     1\n## 55  116  6     1\n## 56  128  6     1\n## 57  133  6     1\n## 58  135  6     1\n## 59  144  6     1\n## 60  151  6     1\n## 61  153  6     1\n## 62  163  6     1\n## 63  168  6     1\n## 64  174  6     1\n## 65  177  6     1\n## 66  190  6     1\n## 67  199  6     1\n## 68    2  7     1\n## 69    5  7     1\n## 70    6  7     1\n## 71    7  7     1\n## 72   11  7     1\n## 73   12  7     1\n## 74   17  7     1\n## 75   24  7     1\n## 76   26  7     1\n## 77   28  7     1\n## 78   31  7     1\n## 79   44  7     1\n## 80   48  7     1\n## 81   53  7     1\n## 82   56  7     1\n## 83   72  7     1\n## 84   77  7     1\n## 85   91  7     1\n## 86  104  7     1\n## 87  105  7     1\n## 88  131  7     1\n## 89  147  7     1\n## 90  156  7     1\n## 91  166  7     1\n## 92  171  7     1\n## 93  187  7     1\n## 94  193  7     1\n## 95   14  8     1\n## 96   39  8     1\n## 97   40  8     1\n## 98   47  8     1\n## 99   54  8     1\n## 100  59  8     1\n## 101  60  8     1\n## 102  62  8     1\n## 103  70  8     1\n## 104  73  8     1\n## 105  75  8     1\n## 106  78  8     1\n## 107  79  8     1\n## 108  84  8     1\n## 109  85  8     1\n## 110  90  8     1\n## 111  97  8     1\n## 112  99  8     1\n## 113 101  8     1\n## 114 109  8     1\n## 115 110  8     1\n## 116 118  8     1\n## 117 126  8     1\n## 118 167  8     1\n## 119 185  8     1\n## 120 189  8     1\n## 121 195  8     1\n## 122   9  9     1\n## 123  25  9     1\n## 124  29  9     1\n## 125  33  9     1\n## 126  35  9     1\n## 127  46  9     1\n## 128  50  9     1\n## 129  51  9     1\n## 130  57  9     1\n## 131  63  9     1\n## 132  67  9     1\n## 133  74  9     1\n## 134  80  9     1\n## 135  88  9     1\n## 136 103  9     1\n## 137 107  9     1\n## 138 111  9     1\n## 139 114  9     1\n## 140 115  9     1\n## 141 125  9     1\n## 142 136  9     1\n## 143 162  9     1\n## 144 172  9     1\n## 145 175  9     1\n## 146 179  9     1\n## 147 180  9     1\n## 148 196  9     1\n## 149   3 10     1\n## 150   4 10     1\n## 151  22 10     1\n## 152  36 10     1\n## 153  38 10     1\n## 154  45 10     1\n## 155  49 10     1\n## 156  61 10     1\n## 157  64 10     1\n## 158  76 10     1\n## 159  89 10     1\n## 160 106 10     1\n## 161 127 10     1\n## 162 129 10     1\n## 163 139 10     1\n## 164 143 10     1\n## 165 148 10     1\n## 166 152 10     1\n## 167 154 10     1\n## 168 155 10     1\n## 169 159 10     1\n## 170 176 10     1\n## 171 181 10     1\n## 172 183 10     1\n## 173 186 10     1\n## 174  18 11     1\n## 175  19 11     1\n## 176  21 11     1\n## 177  41 11     1\n## 178  42 11     1\n## 179  43 11     1\n## 180  52 11     1\n## 181  58 11     1\n## 182  66 11     1\n## 183  81 11     1\n## 184  94 11     1\n## 185  95 11     1\n## 186 100 11     1\n## 187 121 11     1\n## 188 123 11     1\n## 189 142 11     1\n## 190 145 11     1\n## 191 146 11     1\n## 192 150 11     1\n## 193 158 11     1\n## 194 160 11     1\n## 195 164 11     1\n## 196 165 11     1\n## 197 184 11     1\n## 198 188 11     1\n## 199 197 11     1\n## 200 200 11     1\nAhora vemos cuántas asignaciones tiene cada empleado y pintamos los resultados\nasignaciones <- matching %>%\n  group_by(j) %>%\n  summarise(asignaciones = sum(value)) %>%\n  arrange(desc(asignaciones)) %>%\n  left_join(sedes_granada, by = c(\"j\" = \"id_sede\"))\n\nasignaciones\n## # A tibble: 11 × 9\n##        j asignaciones cod_postal tipo  cod_prov  area_m2 centr…¹ centr…² provi…³\n##    <int>        <dbl> <fct>      <chr> <chr>       <dbl>   <dbl>   <dbl> <chr>  \n##  1     5           27 18197      indio 18         1.00e7   -3.61    37.2 GRANADA\n##  2     7           27 18414      indio 18         4.04e7   -3.34    36.9 GRANADA\n##  3     8           27 18197      indio 18         1.00e7   -3.61    37.2 GRANADA\n##  4     9           27 18369      indio 18         1.77e7   -4.01    37.2 GRANADA\n##  5    11           27 18514      indio 18         1.11e8   -3.08    37.2 GRANADA\n##  6    10           25 18611      indio 18         3.35e7   -3.60    36.8 GRANADA\n##  7     6           24 18516      indio 18         1.47e8   -3.24    37.2 GRANADA\n##  8     1            7 18328      jefe  18         5.86e7   -3.87    37.2 GRANADA\n##  9     2            7 18006      jefe  18         3.25e6   -3.61    37.2 GRANADA\n## 10     3            1 18516      jefe  18         1.47e8   -3.24    37.2 GRANADA\n## 11     4            1 18516      jefe  18         1.47e8   -3.24    37.2 GRANADA\n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nplot_assignment <- matching %>%\n  inner_join(cod_postales_granada, by = c(\"i\" = \"id\")) %>%\n  inner_join(sedes_granada, by = c(\"j\" = \"id_sede\"), suffix = c(\"_clientes\", \"_comerciales\"))\n\n\n\n\n\np_jefes <- p +\n  geom_segment(\n    data = plot_assignment %>%\n      filter(tipo == \"jefe\"),\n    aes(\n      x = centroide_longitud_comerciales,\n      y = centroide_latitud_comerciales,\n      xend = centroide_longitud_clientes,\n      yend = centroide_latitud_clientes\n    )\n  ) +\n  ggtitle(paste0(\"Asignaciones para los jefes\"))\n\n\np_indios <- p +\n  geom_segment(\n    data = plot_assignment %>%\n      filter(tipo == \"indio\"),\n    aes(\n      x = centroide_longitud_comerciales,\n      y = centroide_latitud_comerciales,\n      xend = centroide_longitud_clientes,\n      yend = centroide_latitud_clientes\n    )\n  ) +\n  ggtitle(paste0(\"Asignaciones para los indios\"))\n\n\np_or <- p +\n  labs(\n    title = \"sin asignar\",\n    subtitle = \"Granada\"\n  )\np_final <- p_or / p_jefes / p_indios\n\np_final"
  },
  {
    "objectID": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-eligiendo-provincia",
    "href": "2022/08/01/indios-y-jefes-io-al-servicio-del-mal/index.html#io-al-servicio-del-mal-eligiendo-provincia",
    "title": "Indios y jefes, IO al servicio del mal.",
    "section": "IO al servicio del mal eligiendo provincia",
    "text": "IO al servicio del mal eligiendo provincia\nCreo función (francamente mejorable y modularizable) para poder elegir provincia o provincias\nget_asignaciones_x_provincia <- function(cod_postales = cod_postales, sedes = sedes,\n                                         provincia_sel = \"MADRID\", plot = TRUE, ...) {\n  cod_postales_filt <- cod_postales %>%\n    filter(provincia %in% provincia_sel) %>%\n    mutate(id = row_number())\n\n  sedes_filt <- sedes %>%\n    filter(provincia %in% provincia_sel) %>%\n    arrange(desc(tipo)) %>%\n    mutate(id_sede = row_number())\n\n  m <- nrow(sedes_filt)\n  n <- nrow(cod_postales_filt)\n  n_sedes <- length(unique(sedes_filt$cod_postal))\n\n  njefes <- sedes_filt %>%\n    filter(tipo == \"jefe\") %>%\n    count() %>%\n    pull(n)\n\n  n_indios <- m - njefes\n\n  transportcost <- function(i, j) {\n    cliente <- cod_postales_filt[i, ]\n    comercial <- sedes_filt[j, ]\n    distancia <- geosphere::distHaversine(\n      c(cliente$centroide_longitud, cliente$centroide_latitud),\n      c(comercial$centroide_longitud, comercial$centroide_latitud)\n    )\n\n    if (comercial[, \"tipo\"] == \"jefe\") distancia <- distancia * 1.1\n\n    return(distancia / 1000)\n  }\n\n\n  p <- ggplot(cod_postales_filt, aes(centroide_longitud, centroide_latitud)) +\n    geom_point(size = rel(2), shape = 4) +\n    geom_point(data = sedes_filt, size = rel(3), color = \"darkorange\") +\n    # scale_x_continuous(limits = c(0, grid_size+1)) +\n    # scale_y_continuous(limits = c(0, grid_size+1)) +\n    theme(\n      axis.title = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text = element_blank(), panel.grid = element_blank()\n    )\n\n  mip_model <- MIPModel() %>%\n    # variable indicadora que indica si una tienda i se asigna a comercial j\n    add_variable(x[i, j], i = 1:n, j = 1:m, type = \"binary\") %>%\n    # Minimizar el objetivo de distancia\n    set_objective(sum_over(transportcost(i, j) * x[i, j], i = 1:n, j = 1:m), \"min\") %>%\n    # cada tienda (código postal) solo debe ir a un comerciial. el comercial puede atender varios\n    add_constraint(sum_over(x[i, j], j = 1:m) == 1, i = 1:n) %>%\n    # todo el mundo tiene que atender al minimo a una tienda\n    add_constraint(sum_over(x[i, j], i = 1:n) >= 1, j = 1:m) %>%\n    # %>%\n\n    #   Los jefes curran menos, como máximo 7 tiendas\n    add_constraint(sum_over(x[i, j], i = 1:n) <= 7, j = 1:njefes) %>%\n    #\n    # # Los indios al menos atienden a 3 tiendas\n    add_constraint(sum_over(x[i, j], i = 1:n) >= 3, j = (njefes + 1):m) %>%\n    # para no sobrecargar mucho a los indios, les pongo un máximo que sea 1.5 veces el núemro de tiendas entre total currantes (jefes + indios)\n    add_constraint(sum_over(x[i, j], i = 1:n) <= round(1.5 * n / m), j = (njefes + 1):m) %>%\n    add_constraint(sum_over(x[i, j], i = 1:n) >= sum_over(x[i, k], i = 1:n), j = (njefes + 1):m, k = 1:njefes)\n\n\n  result2 <- solve_model(mip_model, with_ROI(solver = \"glpk\", verbose = TRUE))\n\n\n  matching <- result2 %>%\n    get_solution(x[i, j]) %>%\n    select(i, j, value) %>%\n    filter(value > 0)\n\n\n\n  asignaciones <- matching %>%\n    group_by(j) %>%\n    summarise(asignaciones = sum(value)) %>%\n    arrange(desc(asignaciones)) %>%\n    left_join(sedes_filt, by = c(\"j\" = \"id_sede\"))\n\n\n  plot_assignment <- matching %>%\n    inner_join(cod_postales_filt, by = c(\"i\" = \"id\")) %>%\n    inner_join(sedes_filt, by = c(\"j\" = \"id_sede\"), suffix = c(\"_clientes\", \"_comerciales\"))\n\n\n\n  p_jefes <- p +\n    geom_segment(\n      data = plot_assignment %>%\n        filter(tipo == \"jefe\"),\n      aes(\n        x = centroide_longitud_comerciales,\n        y = centroide_latitud_comerciales,\n        xend = centroide_longitud_clientes,\n        yend = centroide_latitud_clientes\n      )\n    ) +\n    ggtitle(paste0(\"Asignaciones para los jefes\"))\n\n\n  p_indios <- p +\n    geom_segment(\n      data = plot_assignment %>%\n        filter(tipo == \"indio\"),\n      aes(\n        x = centroide_longitud_comerciales,\n        y = centroide_latitud_comerciales,\n        xend = centroide_longitud_clientes,\n        yend = centroide_latitud_clientes\n      )\n    ) +\n    ggtitle(paste0(\"Asignaciones para los indios\"))\n\n  subtitulo <- reduce(provincia_sel, function(x, y) paste(x, y, sep = \"-\"))\n  p_or <- p +\n    labs(\n      title = \"sin asignar\",\n      subtitle = subtitulo\n    )\n  p_final <- p_or / p_jefes / p_indios\n\n  if (plot) print(p_final)\n\n  return(list(\n    comerciales = sedes_filt,\n    cod_postales = cod_postales_filt,\n    matching = matching, tot_asignaciones = asignaciones, plot_final = p_final\n  ))\n}\nY veamos algunos ejemplos.\n\nMADRID\nmadrid <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = \"MADRID\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 385 rows, 4425 columns, 45725 non-zeros\n##       0: obj =   0.000000000e+00 inf =   3.400e+02 (320)\n##     498: obj =   1.415569938e+04 inf =   5.690e-14 (0) 1\n## Perturbing LP to avoid stalling [939]...\n## Removing LP perturbation [1341]...\n## *  1341: obj =   5.881701905e+03 inf =   0.000e+00 (0) 4\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 385 rows, 4425 columns, 45725 non-zeros\n## 4425 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1341: mip =     not found yet >=              -inf        (1; 0)\n## +  1341: >>>>>   5.881701905e+03 >=   5.881701905e+03   0.0% (1; 0)\n## +  1341: mip =   5.881701905e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\nPodemos ver cuántos códigos postales le han tocado a cada empleado.\nSe ve que se cumplen las restricciones. Seguramente para ser más equitativo habría que tocar algo a mano, para que a los empleados indios de la misma sede se repartan mejor los códigos postales. pero como primera aproximación no está mal\nmadrid$tot_asignaciones %>% \n  arrange(cod_postal)\n## # A tibble: 15 × 9\n##        j asignaciones cod_postal tipo  cod_prov  area_m2 centr…¹ centr…² provi…³\n##    <int>        <dbl> <fct>      <chr> <chr>       <dbl>   <dbl>   <dbl> <chr>  \n##  1     2            7 28011      jefe  28         3.03e7   -3.75    40.4 MADRID \n##  2     1            7 28015      jefe  28         2.59e6   -3.71    40.4 MADRID \n##  3     5            7 28015      jefe  28         2.59e6   -3.71    40.4 MADRID \n##  4    12           30 28035      indio 28         2.20e7   -3.74    40.5 MADRID \n##  5    14           30 28213      indio 28         8.52e7   -4.19    40.4 MADRID \n##  6     9           30 28521      indio 28         3.53e7   -3.50    40.3 MADRID \n##  7     7           30 28668      indio 28         3.65e6   -3.84    40.4 MADRID \n##  8    13           30 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n##  9     6           25 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 10     8           17 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 11    11            8 28755      indio 28         1.23e8   -3.60    41.1 MADRID \n## 12     4            7 28817      jefe  28         6.04e7   -3.26    40.5 MADRID \n## 13    15           30 28901      indio 28         1.62e6   -3.73    40.3 MADRID \n## 14    10           30 28931      indio 28         8.78e5   -3.86    40.3 MADRID \n## 15     3            7 28931      jefe  28         8.78e5   -3.86    40.3 MADRID \n## # … with abbreviated variable names ¹​centroide_longitud, ²​centroide_latitud,\n## #   ³​provincia\nPodemos ver el detalle, por ejemplo qué códigos postales le toca al empleado j=4\nmadrid_asignaciones <-  madrid$tot_asignaciones  %>% \n  left_join(madrid$matching, by = \"j\") %>% \n  left_join(madrid$cod_postales, by = c(\"i\" = \"id\"), suffix = c(\"\",\"_tienda\")) \n\nmadrid_asignaciones %>% \n  filter(j==4) %>% \n  select(tipo ,j, i, cod_postal, cod_postal_tienda)\n## # A tibble: 7 × 5\n##   tipo      j     i cod_postal cod_postal_tienda\n##   <chr> <int> <int> <fct>      <fct>            \n## 1 jefe      4    61 28817      28810            \n## 2 jefe      4    71 28817      28812            \n## 3 jefe      4    89 28817      28818            \n## 4 jefe      4   121 28817      28515            \n## 5 jefe      4   155 28817      28804            \n## 6 jefe      4   172 28817      28817            \n## 7 jefe      4   219 28817      28811\n\n\nBarcelona\nbarcelona <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel =\"BARCELONA\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 471 rows, 5715 columns, 59055 non-zeros\n##       0: obj =   0.000000000e+00 inf =   4.260e+02 (406)\n##     600: obj =   1.389502410e+04 inf =   9.258e-13 (0) 1\n## Perturbing LP to avoid stalling [1077]...\n## Removing LP perturbation [1716]...\n## *  1716: obj =   7.841913058e+03 inf =   0.000e+00 (0) 5\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 471 rows, 5715 columns, 59055 non-zeros\n## 5715 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1716: mip =     not found yet >=              -inf        (1; 0)\n## +  1716: >>>>>   7.841913058e+03 >=   7.841913058e+03   0.0% (1; 0)\n## +  1716: mip =   7.841913058e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\n\n\nSevilla\nsevilla <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = \"SEVILLA\")\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 182 rows, 1064 columns, 7448 non-zeros\n##       0: obj =   0.000000000e+00 inf =   1.710e+02 (163)\n##     243: obj =   8.653234667e+03 inf =   5.145e-13 (0)\n## *   572: obj =   3.623165871e+03 inf =   0.000e+00 (0) 1\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 182 rows, 1064 columns, 7448 non-zeros\n## 1064 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +   572: mip =     not found yet >=              -inf        (1; 0)\n## +   572: >>>>>   3.623165871e+03 >=   3.623165871e+03   0.0% (1; 0)\n## +   572: mip =   3.623165871e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\n\n\nGranada y Málaga juntas\ngranada_malaga <- get_asignaciones_x_provincia(cod_postales, sedes, provincia_sel = c(\"GRANADA\",\"MALAGA\"))\n## <SOLVER MSG>  ----\n## GLPK Simplex Optimizer, v4.65\n## 488 rows, 7160 columns, 80550 non-zeros\n##       0: obj =   0.000000000e+00 inf =   4.230e+02 (393)\n##     515: obj =   2.754380624e+04 inf =   4.807e-13 (0) 1\n## Perturbing LP to avoid stalling [1388]...\n## Removing LP perturbation [1688]...\n## *  1688: obj =   7.728634950e+03 inf =   0.000e+00 (0) 5\n## OPTIMAL LP SOLUTION FOUND\n## GLPK Integer Optimizer, v4.65\n## 488 rows, 7160 columns, 80550 non-zeros\n## 7160 integer variables, all of which are binary\n## Integer optimization begins...\n## Long-step dual simplex will be used\n## +  1688: mip =     not found yet >=              -inf        (1; 0)\n## +  1688: >>>>>   7.728634950e+03 >=   7.728634950e+03   0.0% (1; 0)\n## +  1688: mip =   7.728634950e+03 >=     tree is empty   0.0% (0; 1)\n## INTEGER OPTIMAL SOLUTION FOUND\n## <!SOLVER MSG> ----\n\nY hasta aquí ha llegado el uso de la IO para el mal. Feliz verano !!"
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "",
    "text": "Tengo una relación extraña con Julia, por un lado me gusta bastante y por otro me parece que aún le falta algo para que lo adopte de forma más seria. Quizá tenga que ver con mi forma de aprender (que seguro que no es óptima), en vez de irme a los tutoriales típicos, me voy directamente a ver cómo se hace algo que me interesa. En este caso hacer modelos bayesianos con Julia usando Turing.\nTuring es una librería escrita en Julia para programación probabilística, podría considerarse como un competidor de Stan, aunque todavía es una librería joven. Turing añade sólo una pequeña capa de programación probabilística, y promete cosas como modelos de redes neuronales dónde los pesos sigan una distribución probabilística\nNo me voy a meter en esos lares, yo soy más prosaico y por el momento sólo quiero ejemplificar con Turing el modelo que cuento en pluralista.\nRecordemos que habías simulado unos datos tal que así.\nEn la simulación se ha forzado que el efecto del número de hijos de la madre (M) sobre el número de hijos de la hija (D) sea cero.\nEl DAG era algo así. En este dag para estimar el efecto de M sobre D, hace falta condicionar por U, pero al ser una variable de confusión no observada, no habría forma de estimarlo de la forma tradicional (a lo Pearl). La solución es estimar el DAG completo."
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#ajuste-en-turing",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#ajuste-en-turing",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Ajuste en Turing",
    "text": "Ajuste en Turing\nRecordemos que nuestra U es una variable que no tenemos, se podría asimilar a una variable con todos sus valores perdidos y cada uno de esos valores perdidos es un parámetro a estimar.\nLibrerías : Aparte de Turing, hace falta ReverseDiff (diferenciación automática) y alguna más.\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nLeo los datos simulados que había guardado en un csv previamente\n\npl = DataFrame(CSV.File(\"data/pluralista.csv\"))\ndescribe(pl)\njulia> describe(pl)\n4×7 DataFrame\n Row │ variable  mean     min       median    max      nmissing  eltype   \n     │ Symbol    Float64  Real      Float64   Real     Int64     DataType \n─────┼────────────────────────────────────────────────────────────────────\n   1 │ D         1.00621  -3.55365  0.986136  6.03293         0  Float64\n   2 │ M         1.00836  -3.91626  0.90395   6.69591         0  Float64\n   3 │ B1        0.473     0        0.0       1               0  Int64\n   4 │ B2        0.487     0        0.0       1               0  Int64\nNos construimos el modelo con Turing.\nAlgunas cosas a comentar.\n\nEl uso de filldist para crear el vector de U y que cada valor siga una Normal(0,1).\n.+ para sumar un escalar como a1 con un vector. El uso del “.operacion” es habitual en julia para hacer broadcast.\nMvNormal al final. Esto lo he leído por ahí para que haga mejor el sampleo.\nAl igual que en Stan se tiene que escribir en cierto orden (y si no no funciona bien) porque Turing no es declarativo.\n\n@model function pluralista(D, M, B1, B2)\n\n    N = Int(length(D))\n\n    # Variable no observada\n    U ~ filldist(Normal(0, 1), N)\n\n\n    # Prior coeficientes\n    a1 ~ Normal(0, 0.5)\n    a2 ~ Normal(0, 0.5)\n    m  ~ Normal(0, 0.5)\n    b  ~ Normal(0, 0.5)\n    p  ~ Beta(2,2)\n    \n    \n    k ~  Exponential(1)\n    σ₁ ~ Exponential(1)\n    σ₂ ~ Exponential(1)\n    \n    B1 ~ Bernoulli(p)\n    B2 ~ Bernoulli(p)\n    \n    #  transformed parameters\n    mu1 = a1 .+ b * B1 + k * U\n    mu2 = a2 .+ b * B2 + m * M + k * U\n    \n    # likelihood\n\n\n    M ~ MvNormal(mu1, σ₁ * I) \n    D ~ MvNormal(mu2, σ₂ * I)\n\nend\nComparando con el código del mismo modelo en Stan (al final del post) se observa que la sintaxis es parecida."
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#muestreo-de-la-posterior-en-turing",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#muestreo-de-la-posterior-en-turing",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Muestreo de la posterior en Turing",
    "text": "Muestreo de la posterior en Turing\nHay que usar reversediff porque si no no acaba nunca.\nRandom.seed!(155)\n\n\nTuring.setadbackend(:reversediff)\nTuring.setrdcache(true)\n\nflbi = sample(\n    pluralista(pl.D, pl.M, pl.B1, pl.B2), \n    NUTS(1000, 0.65),\n    MCMCThreads(),\n    2_000, 4)\njulia> flbi = sample(\n           pluralista(pl.D, pl.M, pl.B1, pl.B2), \n           NUTS(1000, 0.65),\n           MCMCThreads(),\n           2_000, 4)\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.025\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n\nChains MCMC chain (2000×1020×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 136.29 seconds\nCompute duration  = 510.14 seconds\nY ha tardado unos 2 minutos por cadena. Ciertamente no está mal, pero no se acerca a la velocidad de Stan, que lo hace en unos 18 segundos.\nY podemos extraer un resumen de los parámetros que nos interesan con\njulia> summarize(flbi[[:a1, :a2, :b, :m, :σ₁, :σ₂]])\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n          a1    0.0682    0.0538     0.0006    0.0009   3268.9064    1.0007        6.4079\n          a2    0.0326    0.0759     0.0008    0.0024   1015.7923    1.0059        1.9912\n           m    0.0063    0.0430     0.0005    0.0018    554.1348    1.0096        1.0862\n           b    1.9865    0.0593     0.0007    0.0012   2403.5462    1.0008        4.7116\n          σ₁    1.1427    0.1205     0.0013    0.0049    535.2307    1.0086        1.0492\n          σ₂    0.9621    0.0719     0.0008    0.0016   2496.8176    1.0009        4.8944\n          \nY efectivamente, lo ha hecho bien y ha recuperado los verdaderos valores de los parámetros y estimado que el efecto de M sobre D es 0.\nmyplot = plot(flbi[[:a1, :a2, :b, :m, :σ₁, :σ₂]])\n\nsavefig(myplot,\"plurarlista_turing.png\")\n\n\n\nimagen"
  },
  {
    "objectID": "2022/03/20/palabras-para-julia-parte-3-n/index.html#reflexiones.",
    "href": "2022/03/20/palabras-para-julia-parte-3-n/index.html#reflexiones.",
    "title": "Palabras para Julia ( Parte 3/n)",
    "section": "Reflexiones.",
    "text": "Reflexiones.\n\nMe ha parecido fácil escribir un modelo bayesiano como este en Turing\nNo he conseguido ver como hacer que me funcione un predict sobre nuevos datos que tengan B1 y B2, pero no M y D. Cuestión de empezar más poco a poco con los tutoriales que hay por ahí.\nPor el momento parece que Stan sigue siendo el estado del arte en estas cosas, aunque lo de integrar Turing con Flux por ejemplo, promete.\n\nMismo modelo en Stan.\n\ndata{\n    int N;\n    vector[N] D;\n    vector[N] M;\n    int B1[N];\n    int B2[N];\n}\n\n\nparameters{\n    vector[N] U;\n    real m;\n    real b;\n    real a2;\n    real a1;\n    real<lower=0> tau;\n    real<lower=0> sigma;\n    real<lower=0> k;\n    real<lower=0,upper=1> p;\n}\n\ntransformed parameters {\n  vector[N] nu;\n  vector[N] mu;\n\n\n  for ( i in 1:N ) {\n        nu[i] = a2 + b * B2[i] + m * M[i] + k * U[i];\n    }\n    \n  for ( i in 1:N ) {\n        mu[i] = a1 + b * B1[i] + k * U[i];\n    }\n\n\n}\n\nmodel{\n    \n    U ~ normal( 0 , 1 );\n    \n    a1 ~ normal( 0 , 0.5 );\n    a2 ~ normal( 0 , 0.5 );\n    m  ~ normal( 0 , 0.5 );\n    b  ~ normal( 0 , 0.5 );\n    p  ~ beta( 2 , 2 );\n    \n    k ~ exponential( 1 );\n    sigma ~ exponential( 1 );\n    tau   ~ exponential( 1 );\n    B2    ~ bernoulli( p );\n    B1    ~ bernoulli( p );\n\n    D ~ normal( nu , tau );\n    M ~ normal( mu , sigma );\n}\n\n// genero point_loglikelihood, util para evaluar modelo con psis loo\ngenerated quantities {\n vector[N] log_lik_D;\n vector[N] log_lik_M;\n\n  for (i in 1:N)\n    log_lik_D[i] = normal_lpdf(D[i] | nu[i], tau);\n\n  for (i in 1:N)\n    log_lik_M[i] = normal_lpdf(M[i] | mu[i], sigma);\n\n\n  }"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html",
    "href": "2022/09/18/veloooosidad/index.html",
    "title": "Veeelooosidad",
    "section": "",
    "text": "No, este post no va sobre la canción de Medina Azahara sino de comparar un par de librerías para lectura y procesamiento de datos. A saber, polars escrita en Rust y con api en python versus vroom en combinación con librerías como data.table o collapse en R. Estas últimas usan por debajo C++, así que tanto por el lado de python como por el de R el principal mérito se debe a usar Rust y C++."
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#datos-hardware-y-entornos",
    "href": "2022/09/18/veloooosidad/index.html#datos-hardware-y-entornos",
    "title": "Veeelooosidad",
    "section": "Datos, hardware y entornos",
    "text": "Datos, hardware y entornos\nPara hacer la comparación vamos a usar un dataset de 100 millones de filas y 9 columnas, el mismo que se usa en h2o.ai db-benchmark.\nLo voy a probar en mi pc, que es un slimbook de justo antes de la pandemia, con 1gb de ssd, 32Gb de RAM y procesador Intel i7-9750H (12) @ 4.500GHz con 6 núcleos (12 hilos) y corriendo Linux Mint 20.\n\nR\nPara R voy a chequear vroom y data.table para leer los datos y data.table, tidytable y collapse para el procesamiento\nR: Uso R version 4.2.1 (2022-06-23) – “Funny-Looking Kid” vroom: 1.5.7 data.table: 1.14.2 tidytable: 0.8.1.9 collapse: 1.8.8\n\n\nPython\nUso un entorno de conda con python 3.6.12 polars: ‘0.12.5’"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#scripts",
    "href": "2022/09/18/veloooosidad/index.html#scripts",
    "title": "Veeelooosidad",
    "section": "Scripts",
    "text": "Scripts\n\nR\nEn R voy a usar microbenchmark para realizar varias ejecuciones\nFichero: tests.R\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vroom)\nlibrary(tidytable)\n\nlibrary(microbenchmark)\n\n# Check lectu\n\nsetDTthreads(0L)\n\nlectura <- microbenchmark(\n    vroom  = vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE), \n    data.table = data.table::fread(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\"),\n    times = 3L\n)\n\nprint(lectura)\n\n# group by sum\n\nx <- vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE)\n\n# x= sample_frac(x, size = 0.1)\nx_dt <- qDT(x)\n\ngroup_by_performance <- microbenchmark(\n    data.table = x_dt[, lapply(.SD, mean, na.rm = TRUE), keyby = id1, .SDcols = 7:9],\n    # dplyr      = x %>%\n    #     group_by(id1, id2) %>%\n    #     summarise(v1 = sum(v1, na.rm = TRUE)) %>% \n    #     ungroup(),\n    tidytable = x_dt %>%\n        summarize.(v1 = sum(v1),\n                   v2 = sum(v2),\n                   v3 = sum(v3),\n                   .by = c(id1, id2)),\n    # base_R = tapply(x$v1, list(x$id1, x$id2), sum, na.rm = TRUE),\n\n    collapse= x_dt %>%\n        fgroup_by(id1, id2) %>%\n        fsummarise(v1 = fsum(v1),\n                   v2 = fsum(v2),\n                   v3 = fsum(v3)),\n\n    collapse_pure = {\n        g <- GRP(x, ~ id1 +id2)\n        fsum(x$v1, g)\n        fsum(x$v2, g)\n    },\n    times = 5L\n)\n\nprint(group_by_performance)\n\n\nPython\nFichero: tests.py\nimport polars as pl\nimport time\n\nstart = time.time()\ndf = pl.read_csv(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\")\nend = time.time()\n\nprint(end -start)\n\nstart = time.time()\n\n(\n    df\n.lazy()\n    .groupby(['id1','id2'])\n    .agg(\n        [\n            pl.col(\"v1\").sum().alias('v1_sum'),\n            pl.col(\"v2\").sum().alias('v2_sum'),\n            pl.col(\"v3\").sum().alias('v3_sum')\n        ]\n    )\n.collect()\n)\nend = time.time()\nprint(end - start)"
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#resultados",
    "href": "2022/09/18/veloooosidad/index.html#resultados",
    "title": "Veeelooosidad",
    "section": "Resultados",
    "text": "Resultados\nPara comparar, ejecuto los scripts desde consola y teniendo cerrado navegadores, ides y demás.\n\nR\nRscript tests.R\nLectura en R\nUnit: seconds\n       expr       min        lq      mean    median        uq       max neval\n      vroom  7.783958  7.953598  8.185716  8.123239  8.386596  8.649953     3\n data.table 41.914928 42.809751 45.213309 43.704575 46.862499 50.020424     3\n\nGroup by y sum en R.\nUnit: seconds\n          expr      min       lq     mean   median       uq       max neval cld\n    data.table 1.469617 1.476545 1.550360 1.486647 1.633409  1.685581     5   a\n     tidytable 1.182273 1.189111 1.291734 1.279313 1.314287  1.493686     5   a\n      collapse 1.799175 1.813744 6.255215 1.891603 2.076616 23.694936     5   a\n collapse_pure 1.553002 1.555598 1.570758 1.566454 1.571605  1.607132     5   a\n\nPor lo que más o menos, usar vroom para leer y tidytable, data.table o collapse para hacer el cálculo sale por unos 10 segundos o un poco menos.\n\n\nPython\npython tests.py \n7.755492448806763\n1.8228027820587158\n\nY vemos que con polars tenemos más o menos los mismos tiempos."
  },
  {
    "objectID": "2022/09/18/veloooosidad/index.html#conclusiones",
    "href": "2022/09/18/veloooosidad/index.html#conclusiones",
    "title": "Veeelooosidad",
    "section": "Conclusiones",
    "text": "Conclusiones\nTanto en R como en Python tenemos librerías muy rápidas que , si tenemos suficiente RAM podemos trabajar con conjunto de datos bastante tochos y hacer cosas en tiempos más que razonables.\nPolars es una librería muy nueva y muy bien hecha, ojalá hagan api para R. No obstante, data.table lleva tiempo en R y su desempeño es consistente en múltiples situaciones. Mi consejo es echarle un ojo al fastverse."
  },
  {
    "objectID": "2022/04/10/transparente/2022-04-10-transparente.html",
    "href": "2022/04/10/transparente/2022-04-10-transparente.html",
    "title": "Transparente",
    "section": "",
    "text": "El otro día le decía a mis compañeros que hay cosas que no entiendo de la jerga del mundillo en el que nos movemos, (para echar unas risas ver el video de Pantomima Full) .\nYa lo de “tengo una call”, o lo de “estamos alineados” me toca bastante los … pero bueno. Pero hay varias expresiones que me repatean profundamente, y voy a enumerarlas por orden decreciente de odio.\n\n“Reglas de negocio”. Esta frase te la sueltan cuándo no quieren decirte que lo que se hace son 3 “wheres” que alguien decidió en su día y que ni se evaluó su eficacia entonces, ni ahora. Es como un mantra bajo el cual cabe todo, cuándo no te quieren decir alguna cosa absurda que se hace, se dice “son reglas de negocio”, y ya es como que no puedes preguntar de qué se trata, es eso y fin.\n“Hacer foco”. ¿Cómo que hacer foco? ¿quíén ha empezado a maltratar el idioma de tal manera? Uno se enfoca, se pone el foco, etc, pero no se hace foco. Con lo bonito que sería decir “énfasis” o “hincapié”. Supongo que esto lo dijo alguien con poder en el pasado, y ya nadie se atrevió a corregir, y no sólo eso, sino que se adoptó y ahora es ley.\n“Transparente”. Igual que con reglas de negocio, alguien dice, “para vosotros esto será transparente” y te quedas igual, no sabes si significa que te va a afectar lo que sea que se haga, si no te afecta, si te afecta pero poco, o vete tu a saber el qué. Con lo que fácil que sería un “sujeto , verbo, predicado”.\n\nEn fin, buena semana santa, no hagan muchos “quick win” , beban algo “transparente” y si no saben como explicar algún desastre digan que “son reglas de negocio” y que “hay que hacer foco” en “el roadmap” para que estemos todos “alineados” y llegar a los “Okr’s” del próximo “Q”.\nAdvertencia: Huyan de todo áquel que use estas expresiones más de 2 veces cada media hora. Su productividad aumentará"
  },
  {
    "objectID": "2022/05/29/no-mentir-s/index.html",
    "href": "2022/05/29/no-mentir-s/index.html",
    "title": "No mentirás",
    "section": "",
    "text": "Hay veces que uno se deja llevar por la emoción cuando hace algo y a veces se exagera un poco con lo que hace tu criatura.\nTal es el caso de la librería Nanyml, la cual tiene buena pinta pero exagera en al menos dos partes. La primera y más evidente es cuándo dice que puede estimar el desempeño futuro de un modelo sin comparar con lo que realmente pase, así promete el Estimating Performance without Targets\nOs juro que me he leído la documentación varias veces e incluso he visto el código y en ningún lado he visto que haga eso que promete.\nEn realidad lo que hace no es más que basarse en dos asunciones que, si se leen en primer lugar, hace que la afirmación presuntuosa de estimar el desempeño de un modelo sin ver el target se caiga por su propio peso. A saber, las dos asunciones son.\n\nEl modelo retorna probabilidades bien calibradas siempre.\nLa relación de \\(P[y | X]\\) no cambia .\n\nEstas dos asunciones por si solas lo que nos dicen es que vas a medir el desempeño de un modelo (sin ver el verdadero valor del target) asumiendo de partida que el modelo es tan bueno como lo era cuando lo entrenaste.\nLa segunda parte es en lo que denomina CBPE algorithm que si se lee con atención no es otra cosa que simplemente utilizar el modelo para obtener predicciones sobre un nuevo conjunto de datos.\nAsí, para calcular el AUC estimado, lo que hace es asumir que el modelo es bueno, y obtener las diferentes matrices de confusión que se derivan de escoger los posibles puntos de corte y, aquí viene el tema, considerar que el valor predicho por el modelo, es el verdadero valor.\nCon estas asunciones , cualquier cambio en la métrica del AUC se debería sólo y exclusivamente a cambios en la estructura de la población y no a que el modelo haya dejado de ser bueno (lo cual es imposible puesto que es una de las asunciones)..\nEjemplo. Si tenemos 3 grupos distintos dónde tenemos un evento binario. Supongamos que el primero de ellos viene de una población con proporción igual a 0.25, el segundo grupo viene de una población con proporción de 0.8 y el tercero de una población con proporción de 0.032. Si tomamos 1000, 300 y 600 observaciones de cada población respectivamente podemos simular tener un score que cumpla la condición de estar bien calibrado\n\nps1 <- rbeta(1000, 1, 3)\nps2 <- rbeta(300, 4, 1)\nps3 <- rbeta(600, 2, 60)\n\nps <- c(ps1, ps2, ps3)\n\nmean(ps1) ;  mean(ps2); mean(ps3)\n#> [1] 0.2456151\n#> [1] 0.7942984\n#> [1] 0.03301053\n\nLa distribución de los “scores” sería\n\n\n\n\n\n\n\n\n\nPues el CBPE no sería otra cosa que calcular el auc del modelo ¡¡asumiendo que las probabilidades estimadas son correctas!! . Es como intentar demostrar algo teniendo como asunción que es cierto. Pero vayamos al cálculo.\nSiguiendo lo descrito por la documentación y comprobando con el código de la librería se tendría que\n\n\ntpr_fpr <- function(threshold, ps) {\n  yj <- ifelse(ps >= threshold, 1, 0) \n  p_false = abs(yj - ps)\n  p_true = 1- p_false\n  n <- length(yj)\n  tp <- sum(p_true[yj == 1])\n  fp <- sum(p_false[yj==1])\n  tn <- sum(p_true[yj==0] )\n  fn <- sum(p_false[yj==0] )\n  tpr <- tp / (tp + fn)\n  fpr <- fp /(fp + tn)\n  return(data.frame(tpr = tpr, fpr = fpr))\n}\n\n\npscortes = sort(unique(ps), decreasing = TRUE)\n\ndfs <-  lapply(pscortes, function(x) tpr_fpr(x, ps))\n\nvalores <- do.call(rbind, dfs)\n\n\nplot(valores$fpr, valores$tpr, type = \"l\")\n\n\n\n\n\n\n\n\nsimple_auc <- function(TPR, FPR){\n  # inputs already sorted, best scores first \n  dFPR <- c(diff(FPR), 0)\n  dTPR <- c(diff(TPR), 0)\n  sum(TPR * dFPR) + sum(dTPR * dFPR)/2\n}\n\nwith(valores, simple_auc(tpr, fpr))\n#> [1] 0.8908841\n\nCómo se ve, para calcular el auc sólo se tiene en cuenta las probabilidades estimadas, por lo que pierde todo el sentido para obtener un desempeño de cómo de bien lo hace el modelo.\nDe hecho, si hubiera simulado para cada observación una bernoulli tomando como probabilidad de éxito el score tendría lo siguiente, y tomo esa simulación como el valor real , obtengo el mismo auc que con CBPE.\n\nlabels <- rbinom(length(ps), 1, ps)\n(res <- pROC::auc(labels, ps))\n#> Area under the curve: 0.8917\n\nEs decir, en la misma definición de lo que es una matriz de confusión y las métricas asociadas va implícita la idea de comparar la realidad con la estimación, si sustituyes la realidad por la estimación , entonces pierde el sentido.\nPero veamos para qué si puede servir esta cosa. Pues nos puede servir para detectar cambios de distribuciones conjuntas entre dos conjuntos de datos. Me explico, supongamos que quiero predecir sobre un conjunto de datos que en vez de tener 1000 observaciones de la primera población hay 200, y que de la segunda hay 100 y 10000 de la tercera. Pues en este caso, el cambio en el auc se debe solo a eso, al cambio de la estructura de la población global.\n\n\nps1_new <- rbeta(200, 1, 3)\nps2_new <- rbeta(100, 4, 1)\nps3_new <- rbeta(10000, 2, 60)\n\nps_new <- c(ps1_new, ps2_new, ps3_new)\n\n\nlabels <- rbinom(length(ps_new), 1, ps_new)\n(res <- pROC::auc(labels, ps_new))\n#> Area under the curve: 0.7519\n\nLa bajada del “auc estimado” solo se debe a cambios en la estructura de la nueva población que tiene muchas más observaciones de la población 3.\nPor lo tanto, lo que nannyml hace y no está mal, ojo, es simplemente ver cuál serían métricas agregadas (como el auc) cuando cambia la estructura pero no la probabilidad condicionada de y con respecto a las variables independientes.\nLo que no me parece bien es poner en la documentación que calcula el desempeño de un modelo sin ver el target, puesto que confunde y ya ha dado lugar a algún post en “towards data science” (gente, formaros primero con libros antes de leer post de estos sitios) con más humo que madera.\nY como se suele decir “No mentirás”."
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "",
    "text": "En Palabras para Julia parte 3 hablaba de modelos bayesianos con Turing.jl, y me quedé con una espinita clavada, que era la de poder predecir de forma relativamente fácil con Turing, o incluso guardar de alguna forma la “posterior samples” y poder usar mi modelo en otra sesión de Julia.\nEmpiezo una serie de entradas cuyo objetivo es ver si puedo llegar a la lógica para poner “en producción” un modelo bayesiando con Turing, pero llegando incluso a crear un binario en linux que me permita predecir con un modelo y desplegarlo incluso en entornos dónde no está instalado Julia. La verdad, que no sé si lo conseguiré, pero al menos aprendo algo por el camino.\nSi, ya sé que existen los dockers y todo eso, pero no está de más saber que existen alternativas que quizá sean mejores. Ya en el pasado he tratado temas de cómo productivizar modelos de h2o sobre spark aquí o con Julia aquí. El objetivo final será llegar a tener un binario en linux que tome como argumento la ruta dónde se haya guardado las posterior samples de un modelo bayesiano y la ruta con especificación de dicho modelo en texto (para que Turing sepa como usar esas posterior samples) y que nos genere la posterior predictive para nuevos datos.\nAsí que vamos al lío. Empezamos por ver como entrenamos un modelo bayesiano con Turing y como se puede guardar y utilizar posteriormente."
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html#entrenamiento-con-julia",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html#entrenamiento-con-julia",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "Entrenamiento con Julia",
    "text": "Entrenamiento con Julia\nVamos a hacer un ejemplo sencillo, entrenando una regresión lineal múltiple de forma bayesiana. El dataset forma parte del material del libro Introduction to Statistical Learning. Advertising\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\n\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\nmm = DataFrame(CSV.File(\"data/Advertising.csv\"))\ndescribe(mm)\n\n\n200×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │       1    230.1     37.8       69.2     22.1\n   2 │       2     44.5     39.3       45.1     10.4\n   3 │       3     17.2     45.9       69.3      9.3\n  ⋮  │    ⋮        ⋮        ⋮         ⋮         ⋮\n 198 │     198    177.0      9.3        6.4     12.8\n 199 │     199    283.6     42.0       66.2     25.5\n 200 │     200    232.1      8.6        8.7     13.4\n                                     194 rows omitted\n\njulia> describe(mm)\n5×7 DataFrame\n Row │ variable   mean      min   median   max    nmissing  eltype   \n     │ Symbol     Float64   Real  Float64  Real   Int64     DataType \n─────┼───────────────────────────────────────────────────────────────\n   1 │ Column1    100.5      1     100.5   200           0  Int64\n   2 │ TV         147.043    0.7   149.75  296.4         0  Float64\n   3 │ radio       23.264    0.0    22.9    49.6         0  Float64\n   4 │ newspaper   30.554    0.3    25.75  114.0         0  Float64\n   5 │ sales       14.0225   1.6    12.9    27.0         0  Float64\nEspecificamos el modelo, y aquí tengo que comentar un par de cosas. Una que julia gracias a que implementa eficazmente el Multiple dispatch, podemos tener una misma función que devuelva cosas diferentes dependiendo de que le pasemos, así una función puede tener diferentes métodos. El otro aspecto es el uso del condition en Turing (alias |) se puede especificar el modelo sin pasar como argumento la variable dependiente y usarla solo para obtener la posterior, lo cual nos va a permitir hacer algo como predict( modelo(Xs), cadena_mcmc), y no tener que pasar la y como un valor perdido.\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n    \n    radio::AbstractVector{<:Real},\n    newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\nAhora tenemos el mismo modelo, que me a servir tanto para pasarle como argumentos escalares como vectores, nótese que la función Normal tomo como argumento la desviación típica, mientrar que MvNormal toma una matriz de varianzas/covarianzas. Se aconseja el uso de MvNormal en Turing pues mejora el tiempo de cálculo de la posteriori.\nObtenemos la posteriori de los parámetros, pasándole como datos el dataset de Advertising. Es importante que la columna de la variable dependiente se pase como NamedTuple, esto se puede hacer en julia usando (; vector_y) .\n\n# utilizamos 4 cadenas con n_samples = 2000  para cada una\n\n# usamos | para pasarle los datos de Y que no habiamos pasado en la especificacion del modelo\n\nchain = sample(mm_model_sin_sales(mm.TV, mm.radio, mm.newspaper) | (; mm.sales),\n    NUTS(0.65),MCMCThreads(),\n    2_000, 4)\n    \nY en unos 18 segundos tenemos nuestra MCMC Chain.\nChains MCMC chain (2000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 18.06 seconds\nCompute duration  = 71.54 seconds\nparameters        = a, tv_coef, radio_coef, newspaper_coef, σ₁\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n      parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n          Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n               a    2.0952    0.2712     0.0030    0.0038   5123.1176    0.9999       71.6139\n         tv_coef    0.0481    0.0013     0.0000    0.0000   7529.0954    0.9998      105.2461\n      radio_coef    0.1983    0.0087     0.0001    0.0001   5230.9995    1.0000       73.1220\n  newspaper_coef    0.0040    0.0059     0.0001    0.0001   6203.9490    1.0002       86.7224\n              σ₁    1.7205    0.0874     0.0010    0.0011   5441.9631    1.0000       76.0709\n\nQuantiles\n      parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n          Symbol   Float64   Float64   Float64   Float64   Float64 \n\n               a    1.5635    1.9104    2.0982    2.2788    2.6182\n         tv_coef    0.0455    0.0472    0.0481    0.0489    0.0508\n      radio_coef    0.1814    0.1923    0.1983    0.2042    0.2155\n  newspaper_coef   -0.0077    0.0001    0.0040    0.0078    0.0157\n              σ₁    1.5585    1.6607    1.7169    1.7781    1.8997\nVale, estupendo,en chain tenemos las 8000 samples para cada uno de los 5 parámetros , y también las de temas del ajuste interno por HMC, de ahí lo de (2000×17×4 Array{Float64, 3}).\nPero ¿cómo podemos predecir para nuevos datos?\nPues podemos pasarle simplemente 3 escalares correspondientes a las variables TV, radio y newspaper.\nEs necesario pasarle a la función predict la llamada al modelo con los nuevos datos mm_model_sin_sales(tv_valor, radio_valor,newspaper_valor) y las posterioris (la cadena MCMC) de los parámetros.\n\njulia> predict(mm_model_sin_sales(2, 5, 7), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales    3.2203    1.7435     0.0195    0.0176   8053.9924    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales   -0.1863    2.0441    3.2553    4.4030    6.6547\nTambién podemos pasarle más valores\n\njulia> mm_last = last(mm, 3)\n3×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │     198    177.0      9.3        6.4     12.8\n   2 │     199    283.6     42.0       66.2     25.5\n   3 │     200    232.1      8.6        8.7     13.4\n\n\njulia> predicciones = predict(mm_model_sin_sales(mm_last.TV, mm_last.radio, mm_last.newspaper), chain)\nChains MCMC chain (2000×3×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales[1], sales[2], sales[3]\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n    sales[1]   12.5192    1.7427     0.0195    0.0170   8270.6268    1.0000\n    sales[2]   24.3266    1.7560     0.0196    0.0222   7720.4172    1.0001\n    sales[3]   14.9901    1.7327     0.0194    0.0188   8039.4940    0.9999\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n    sales[1]    9.0888   11.3344   12.5241   13.6990   15.9571\n    sales[2]   20.8369   23.1519   24.3414   25.4967   27.7429\n    sales[3]   11.6549   13.8304   14.9617   16.1471   18.3733\nPodría quedarme con las predicciones para sales[1] y calcular el intervalo de credibilidad el 80%\njulia> quantile(reshape(Array(predicciones[\"sales[1]\"]), 8000), [0.1, 0.5, 0.9])\n3-element Vector{Float64}:\n 10.28185973755853\n 12.524091380928425\n 14.74877121738519"
  },
  {
    "objectID": "2022/07/01/palabras-para-julia-parte-4-n/index.html#guardar-cadena-y-predecir",
    "href": "2022/07/01/palabras-para-julia-parte-4-n/index.html#guardar-cadena-y-predecir",
    "title": "Palabras para Julia (Parte 4 /n). Predicción con Turing",
    "section": "Guardar cadena y predecir",
    "text": "Guardar cadena y predecir\nAhora viene la parte que nos interesa a los que nos dedicamos a esto y queremos usar un modelo entrenado hace 6 meses sobre datos de hoy. Guardar lo que hicimos y predecir sin necesidad de reentrenar.\nGuardamos la posteriori\n\n\nwrite( \"cadena.jls\", chain)\nY ahora, cerramos julia y abrimos de nuevo.\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\n# posteriori guardada\nchain = read(\"cadena.jls\", Chains)\n\n# Especificación del modelo (esto puede ir en otro fichero .jl)\n\n# Si tengo en un fichero jl el código de @model, lo puedo incluir ahí. \n\n\n# ruta = \"especificacion_modelo.jl\"\n# include(ruta)\n\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n     radio::AbstractVector{<:Real},\n      newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\n\n\n\nY aqui viene la parte importante. En la que utilizamos el modelo guardado, que no es más que las posterioris de los parámetros que hemos salvado en disco previamente.\n\n## predecimos la misma observación , fila 198 del dataset\n\npredict(mm_model_sin_sales(177, 9.3, 6.4 ), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales   12.4723    1.7285     0.0193    0.0186   8326.7650    0.9998\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales    9.0844   11.3106   12.4727   13.6334   15.7902\n       \nY voilá. Sabiendo que se puede guardar la posteriori y usarla luego , veo bastante factible poder llegar al objetivo de crear un “motor de predicción” de modelos bayesianos con Turing, que sea un ejecutable y que tome como argumentos la posteriori guardada de un modelo ajustado y en texto (con extensión jl ) y escriba el resultado en disco. Y lo dicho, que pueda desplegar este ejecutable en cualquier sistema linux, sin tener que instalar docker ni nada, solo hacer un unzip"
  },
  {
    "objectID": "2022/10/26/sigo-trasteando-con-julia/index.html",
    "href": "2022/10/26/sigo-trasteando-con-julia/index.html",
    "title": "Sigo trasteando con julia",
    "section": "",
    "text": "Siguiendo con lo que contaba aquí me he construido un binario para predecir usando un modelo de xgboost con Julia. La ventaja es que tengo un tar.gz que puedo descomprimir en cualquier linux (por ejemplo un entorno de producción sin acceso a internet y que no tenga ni vaya a tener julia instalado, ni docker ni nada de nada), descomprimir y poder hacer un miapp_para_predecir mi_modelo_entrenado.jls csv_to_predict.csv resultado.csv y que funcione y vaya como un tiro.\nPongo aquí los ficheros relevantes.\nPor ejemplo mi fichero para entrenar un modelo y salvarlo .\nFichero train_ boston.jl\n# Training model julia\nusing  CSV,CategoricalArrays, DataFrames, MLJ, MLJXGBoostInterface\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\ndf1[:, :target] .= ifelse.(df1[!, :medv_20].== \"NG20\", 1, 0)\nconst target = CategoricalArray(df1[:, :target])\n\nconst X = df1[:, Not([:medv_20, :target])]\n\nTree = @load XGBoostClassifier pkg=XGBoost\ntree_model = Tree(objective=\"binary:logistic\", max_depth = 6, num_round = 800)\nmach = machine(tree_model, X, target)\n\nThreads.nthreads()\nevaluate(tree_model, X, target, resampling=CV(shuffle=true),measure=log_loss, verbosity=0)\nevaluate(tree_model, X, target,\n                resampling=CV(shuffle=true), measure=bac, operation=predict_mode, verbosity=0)\n\n\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\nyhat = predict(mach, X[test,:])\n\nevaluate(tree_model, X[test,:], target[test], measure=auc, operation=predict_mode, verbosity=0)\n\nniveles = levels.(yhat)[1]\nniveles[1]\n\nlog_loss(yhat, target[test]) |> mean\n\nres = pdf(yhat, niveles)\nres_df = DataFrame(res,:auto)\n\nMLJ.save(\"models/boston_xg.jls\", mach)\nY luego los ficheros que uso para construirme la app binaria .. Recordemos del post que mencionaba que lo que necesito es el código del programa principal (el main) y un fichero de precompilación que sirve para que al crear la app se compilen las funciones que voy a usar.\nfichero precomp.jl,\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"models/mimodelo_xg_binario.jls\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, [\"target_0\", \"target_1\"])\n\nCSV.write(\"data/predicciones.csv\", res_df)\nfichero xgboost_predict_binomial.jl , aquí es dónde está el main\nmodule xgboost_predict_binomial\n\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, ntasks= Sys.CPU_THREADS)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, [\"target_0\", \"target_1\"])          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\ny si todo está correcto y siguiendo las instrucciones del post anterior, se compilaría haciendo por ejemplo esto\nusing PackageCompiler\ncreate_app(\"../xgboost_predict_binomial\", \"../xg_binomial_inference\",\n precompile_execution_file=\"../xgboost_predict_binomial/src/precomp_file.jl\", force=true, filter_stdlibs = true, cpu_target = \"x86_64\")\nY esto me crea una estructura de directorios dónde está mi app y todo lo necesario para ejecutar julia en cualqueir linux.\n\n╰─ $ ▶ tree -L 2 xg_binomial_inference\nxg_binomial_inference\n├── bin\n│   ├── julia\n│   └── xgboost_predict_binomial\n├── lib\n│   ├── julia\n│   ├── libjulia.so -> libjulia.so.1.8\n│   ├── libjulia.so.1 -> libjulia.so.1.8\n│   └── libjulia.so.1.8\n└── share\n    └── julia\ny poner por ejemplo en el .bashrc el siguiente alias.\nalias motor_xgboost=/home/jose/Julia_projects/xgboost_model/xg_binomial_inference/bin/xgboost_predict_binomial\ny ya está listo.\nAhora tengo un dataset a predecir de 5 millones de filas\n\n╰─ $ ▶ wc -l data/test.csv \n5060001 data/test.csv\n\n head -n4 data/test.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\ny bueno, tardo unos 11 segundos en obtener las predicciones y escribir el resultado\n╰─ $ ▶ time motor_xgboost models/boston_xg.jls data/test.csv pred.csv\n\nreal    0m11,091s\nuser    0m53,293s\nsys 0m2,321s\n\ny comprobamos que lo ha hecho bien\n\n╰─ $ ▶ wc -l  pred.csv \n5060001 pred.csv\n\n\n╰─ $ ▶ head -n 5 pred.csv \ntarget_0,target_1\n0.9999237,7.63197e-5\n0.99120975,0.008790266\n0.99989164,0.00010834133\n0.99970543,0.00029458306\nY nada, pues esto puede servir para subir modelos a producción en entornos poco amigables (sin python3, sin R, sin julia, sin spark, sin docker, sin internet). Es un poco old style que me diría mi arquenazi favorito Rubén, pero\nOs dejo el tar.gz para que probéis, también os dejo el Project.tomly el Manifest.toml y el fichero con el que he entrenado los datos. para que uséis el mismo entorno de julia que he usado yo.\nenlace_drive"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html",
    "title": "Api y docker con R. parte 1",
    "section": "",
    "text": "Todo el mundo anda haciendo apis para poner modelos en producción, y oye, está bien. Si además lo complementas con dockerizarlo para tener un entorno controlado y que te valga para ponerlo en cualquier sitio dónde esté docker instalado pues mejor.\nAquí voy a contar un ejemplo de como se puede hacer con R usando plumber y docker, en siguentes post contaré como hacerlo con vetiver que es una librería que está para R y Python que tiene algún extra, como versionado de modelos y demás.\nLo primero de todo es trabajar en un proyecto nuevo y usar renv. renv es para gestionar entornos de R, ojo que también funciona bien si tienes que mezclar R y python. Tiene cosas interesantes como descubrir las librerías que usas en tu proyecto y aún mejor, si estas librerías ya las tienes instaladas pues te crea enlaces simbólicos a dónde están y te permite ahorrar un montón de espacio, que al menos yo, no he conseguido ver cómo hacer eso con conda."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#objetivo",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#objetivo",
    "title": "Api y docker con R. parte 1",
    "section": "Objetivo",
    "text": "Objetivo\nMi objetivo es ver cómo pondría un modelo bayesiano ajustado con brms para que me devuelva predicciones puntuales y las posterioris en un entorno de producción."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#entrenando-modelo",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#entrenando-modelo",
    "title": "Api y docker con R. parte 1",
    "section": "Entrenando modelo",
    "text": "Entrenando modelo\nPara eso voy a usar datos de un antiguo post.\nUna vez que estemos en ese nuevo proyecto, ajustamos y guardamos un modelo .\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n## ✔ tibble  3.1.8      ✔ dplyr   1.0.10\n## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n## ✔ readr   2.1.3      ✔ forcats 0.5.2 \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\nlibrary(brms)\n## Loading required package: Rcpp\n## Loading 'brms' package (version 2.18.0). Useful instructions\n## can be found by typing help('brms'). A more detailed introduction\n## to the package is available through vignette('brms_overview').\n## \n## Attaching package: 'brms'\n## \n## The following object is masked from 'package:stats':\n## \n##     ar\nlibrary(cmdstanr)\n## Warning: package 'cmdstanr' was built under R version 4.3.0\n## This is cmdstanr version 0.5.2\n## - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n## - Use set_cmdstan_path() to set the path to CmdStan\n## - Use install_cmdstan() to install CmdStan\n## Using all cores. 12 in my machine, y que haga las cadenas en paralelo\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n## CmdStan path set to: /home/jose/cmdstan\ntrain <- read_csv(here::here(\"data/train_local.csv\"))\n## Rows: 662 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): segmento, tipo, edad_cat\n## dbl (2): valor_cliente, n\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# guiña a librería antigua\ncar::some(train)\n## # A tibble: 10 × 5\n##    segmento tipo  valor_cliente edad_cat     n\n##    <chr>    <chr>         <dbl> <chr>    <dbl>\n##  1 Rec      SM                2 21- 40       4\n##  2 Best     SM                1 41-50      475\n##  3 Best     C                 4 >60       2807\n##  4 No_way   C                 1 41-50      356\n##  5 No_way   B                 5 40-60      221\n##  6 Rec      SF                2 >60        152\n##  7 Rec      B                 4 40-60      194\n##  8 Best     C                 5 41-50     4934\n##  9 No_way   B                 3 41-50     1064\n## 10 No_way   SF                8 41-50       29\nAjustamos un modelo bayesiano con efectos aleatorios y usando la columna n como pesos de las filas. (leer el post dónde usé estos datos para saber más)\ntrain <- train %>% \n    mutate(target1 = as_factor(ifelse(segmento == \"Best\", \"Best\", \"Other\")))\n\n\nformula <- brmsformula(\n    target1| resp_weights(n)  ~ (1 | edad_cat) + (1 | valor_cliente) + (1 | tipo)\n    )\n\nmod <- brm(\n    formula,\n     family = \"bernoulli\", data = train, \n    iter = 4000, warmup = 1000, cores = 4, chains = 4,\n    seed = 10,\n    backend = \"cmdstanr\", \n     refresh = 0) # refresh 0 qu eno quiero que se me llene el post de los output de las cadenas mcm\n\nsaveRDS(mod, here::here(\"brms_model.rds\"))"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#comprobamos-que-nuestro-modelo-funciona",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#comprobamos-que-nuestro-modelo-funciona",
    "title": "Api y docker con R. parte 1",
    "section": "Comprobamos que nuestro modelo funciona",
    "text": "Comprobamos que nuestro modelo funciona\nlibrary(tidybayes)\n## \n## Attaching package: 'tidybayes'\n## The following objects are masked from 'package:brms':\n## \n##     dstudent_t, pstudent_t, qstudent_t, rstudent_t\nmod_reload <- readRDS(here::here(\"brms_model.rds\"))\n \n# \n\ntest <-  read_csv(here::here(\"data/test_local.csv\"))\n## Rows: 656 Columns: 5\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): segmento, tipo, edad_cat\n## dbl (2): valor_cliente, n\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# estimacion puntual\npredict(mod_reload, head(test))\n##        Estimate Est.Error Q2.5 Q97.5\n## [1,] 0.23216667 0.4222324    0     1\n## [2,] 0.13233333 0.3388669    0     1\n## [3,] 0.16075000 0.3673155    0     1\n## [4,] 0.13825000 0.3451766    0     1\n## [5,] 0.12716667 0.3331735    0     1\n## [6,] 0.07333333 0.2606937    0     1\n# full posterior\n# para 6 filas guarda los valores obtenidos en las 3000 iteraciones de cada cadena\n# 3000 * 4 * 6 = 72000 valores \n\nposterior_pred <- add_epred_draws(head(test), mod_reload) \n\nhead(posterior_pred )\n## # A tibble: 6 × 10\n## # Groups:   segmento, tipo, valor_cliente, edad_cat, n, .row [1]\n##   segmento tipo  valor_cliente edad_cat     n  .row .chain .itera…¹ .draw .epred\n##   <chr>    <chr>         <dbl> <chr>    <dbl> <int>  <int>    <int> <int>  <dbl>\n## 1 Rec      C                 0 21- 40     132     1     NA       NA     1  0.230\n## 2 Rec      C                 0 21- 40     132     1     NA       NA     2  0.234\n## 3 Rec      C                 0 21- 40     132     1     NA       NA     3  0.233\n## 4 Rec      C                 0 21- 40     132     1     NA       NA     4  0.230\n## 5 Rec      C                 0 21- 40     132     1     NA       NA     5  0.232\n## 6 Rec      C                 0 21- 40     132     1     NA       NA     6  0.226\n## # … with abbreviated variable name ¹​.iteration\ndim(posterior_pred)\n## [1] 72000    10\nPara la primer fila podemos tener la distribución a posteriori\nposterior_pred %>% \n  filter(.row == 1) %>% \n  ggplot(aes(x=.epred)) +\n  geom_density() \n\nPues listo, ya tenemos el modelo entrenado y guardado, ahora sólo queda escribir el código para la api y el Dockerfile.."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creando-el-plumber.r",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creando-el-plumber.r",
    "title": "Api y docker con R. parte 1",
    "section": "Creando el plumber.R",
    "text": "Creando el plumber.R\nUna cosa importante, si hemos usado renv es escribir el fichero con las dependencias que usamos. Eso se hace con renv::snapshot() y se crea un fichero dónde están descritas las dependencias versionadas de nuestro proyecto.\nPero quizá para el docker no necesitemos todas, en este caso, partiendo del fichero anterior nos creamos otro con sólo las dependencias necesarias. Yo lo he llamado vetiver_renv.lock porque empecé trasteando con vetiver y soy demasiado vago como para cambiar ahora el nombre. El contenido del fichero es\nvetiver_renv.lock\n{\n  \"R\": {\n    \"Version\": \"4.2.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"binarios\",\n        \"URL\": \"https://packagemanager.rstudio.com/all/latest\"\n      },\n      {\n        \"Name\": \"ropenspain\",\n        \"URL\": \"https://ropenspain.r-universe.dev\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"plumber\": {\n      \"Package\": \"plumber\",\n      \"Version\": \"1.2.1\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"8b65a7a00ef8edc5ddc6fabf0aff1194\",\n      \"Requirements\": [\n        \"R6\",\n        \"crayon\",\n        \"ellipsis\",\n        \"httpuv\",\n        \"jsonlite\",\n        \"lifecycle\",\n        \"magrittr\",\n        \"mime\",\n        \"promises\",\n        \"rlang\",\n        \"sodium\",\n        \"stringi\",\n        \"swagger\",\n        \"webutils\"\n      ]\n    },\n    \"brms\": {\n      \"Package\": \"brms\",\n      \"Version\": \"2.18.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"afcb0d871e1759b68b29eb6affd37a10\",\n      \"Requirements\": [\n        \"Matrix\",\n        \"Rcpp\",\n        \"abind\",\n        \"backports\",\n        \"bayesplot\",\n        \"bridgesampling\",\n        \"coda\",\n        \"future\",\n        \"ggplot2\",\n        \"glue\",\n        \"loo\",\n        \"matrixStats\",\n        \"mgcv\",\n        \"nleqslv\",\n        \"nlme\",\n        \"posterior\",\n        \"rstan\",\n        \"rstantools\",\n        \"shinystan\"\n      ]\n    },\n    \"tidybayes\": {\n      \"Package\": \"tidybayes\",\n      \"Version\": \"3.0.2\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"d501501261b724f35ec9f2b80f4421b5\",\n      \"Requirements\": [\n        \"arrayhelpers\",\n        \"coda\",\n        \"dplyr\",\n        \"ggdist\",\n        \"ggplot2\",\n        \"magrittr\",\n        \"posterior\",\n        \"rlang\",\n        \"tibble\",\n        \"tidyr\",\n        \"tidyselect\",\n        \"vctrs\",\n        \"withr\"\n      ]\n    }\n  }\n}\n\nCómo veis también he añadido la librería tidybayes, porque me va a resultar útil para sacar la posteriori de las predicciones de los nuevos datos.\nCreamos el fichero plumber.R que no es más que decir cómo se va a predecir y crear un par de endpoints que permiten tanto obtener estimaciones puntuales como la full posterior. Con la librería plumber se hace fácil sin más que usar decoradores.\nFichero plumber.R\nlibrary(brms)\nlibrary(plumber)\nlibrary(tidybayes)\n\nbrms_model <- readRDS(\"brms_model.rds\")\n\n\n#* @apiTitle brms predict Api\n#* @apiDescription Endpoints for working with brms model\n\n## ---- filter-logger\n#* Log some information about the incoming request\n#* @filter logger\nfunction(req){\n    cat(as.character(Sys.time()), \"-\",\n        req$REQUEST_METHOD, req$PATH_INFO, \"-\",\n        req$HTTP_USER_AGENT, \"@\", req$REMOTE_ADDR, \"\\n\")\n    forward()\n}\n\n## ---- post-data\n#* Submit data and get a prediction in return\n#* @post /predict\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    predict(brms_model, data) |>\n        as.data.frame()\n}\n\n\n#* @post /full_posterior\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    add_epred_draws(data, brms_model) \n        \n}\nNo tiene mucho misterio, los endpoint se crean usando\n#* @post  /nombre_endpoing\ny creando una función que va a tomar los datos que le pasemos en formato json a la api, los pasa a data.frame y usa el modelo previamente cargado para obtener las estimaciones puntuales en un caso y la full posterior (con add_epred_draws) en el otro."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creamos-el-docker",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#creamos-el-docker",
    "title": "Api y docker con R. parte 1",
    "section": "Creamos el docker",
    "text": "Creamos el docker\nIba a contar lo que es docker, pero mejor lo miráis en su web. Sólo quedarnos con la idea que es como tener una máquina virtual que puedo usar en otro sitio, pero es mucho más ligera y puede usar cosas del sistema anfitrión e interactuar con él.\nPara crear nuestra imagen docker tenemos que crear un fichero que se llame Dockerfile dónde vamos a ir diciéndole como cree nuestra máquina virtual.\nEs importante que estén los ficheros anteriores, el modelo salvado , el plumber.R y el fichero .lock en las rutas correctas dónde los busca el Dockerfile, en mi caso, lo he puesto todo en el mismo sitio.\nContendido del Dockerfile\n# Docker file para modelo brms\n\nFROM rocker/r-ver:4.2.1\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  default-jdk \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  make \\\n  zlib1g-dev \\\n  libxml2-dev \\\n  libglpk-dev \\\n  && apt-get clean\n\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\n\n## Copio el modelo y el fichero de la api\nCOPY brms_model.rds /opt/ml/brms_model.rds\nCOPY plumber.R /opt/ml/plumber.R\n\nEXPOSE 8081\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8081)\"]\n\n\nImportante que el puerto que se exponga con EXPOSE sea el mismo que usa el plumber, en este caso el 8081.\nAhora para construir la imagen docker y ejecutarla\ndocker build -t mi_modelo_brms .\n\nY despues de un rato podemos ejecutarlo mapeando el puerto\nnohup docker container run --rm -p 8081:8081 mi_modelo_brms &"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#funciona",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#funciona",
    "title": "Api y docker con R. parte 1",
    "section": "¿Funciona?",
    "text": "¿Funciona?\nPodemos usar curl, python, php o cualquier otra cosa para mandar peticiones a la api y que nos devuelva predicciones, con R sería algo así.\ntest %>% \n    head(2) \n## # A tibble: 2 × 5\n##   segmento tipo  valor_cliente edad_cat     n\n##   <chr>    <chr>         <dbl> <chr>    <dbl>\n## 1 Rec      C                 0 21- 40     132\n## 2 Best     B                 0 41-50       19\nbase_url <- \"http://0.0.0.0:8081\"\n\napi_res <- httr::POST(url = paste0(base_url, \"/predict\"),\n                      body = head(test),\n                      encode = \"json\")\npredicted_values <- httr::content(api_res, as = \"text\", encoding = \"UTF-8\")\n\njsonlite::fromJSON(predicted_values)\n##   Estimate Est.Error Q2.5 Q97.5\n## 1   0.2283    0.4198    0     1\n## 2   0.1356    0.3424    0     1\n## 3   0.1604    0.3670    0     1\n## 4   0.1320    0.3385    0     1\n## 5   0.1215    0.3267    0     1\n## 6   0.0737    0.2612    0     1\napi_res2 <- httr::POST(url = paste0(base_url, \"/full_posterior\"),\n                      body = head(test,1),\n                      encode = \"json\")\nposterior_values <- httr::content(api_res2, as = \"text\", encoding = \"UTF-8\")\n\n\njsonlite::fromJSON(posterior_values)  %>% \n  head(100)\n##     segmento tipo valor_cliente edad_cat   n .row .draw .epred\n## 1        Rec    C             0   21- 40 132    1     1 0.2297\n## 2        Rec    C             0   21- 40 132    1     2 0.2341\n## 3        Rec    C             0   21- 40 132    1     3 0.2330\n## 4        Rec    C             0   21- 40 132    1     4 0.2296\n## 5        Rec    C             0   21- 40 132    1     5 0.2321\n## 6        Rec    C             0   21- 40 132    1     6 0.2256\n## 7        Rec    C             0   21- 40 132    1     7 0.2211\n## 8        Rec    C             0   21- 40 132    1     8 0.2215\n## 9        Rec    C             0   21- 40 132    1     9 0.2259\n## 10       Rec    C             0   21- 40 132    1    10 0.2245\n## 11       Rec    C             0   21- 40 132    1    11 0.2330\n## 12       Rec    C             0   21- 40 132    1    12 0.2263\n## 13       Rec    C             0   21- 40 132    1    13 0.2262\n## 14       Rec    C             0   21- 40 132    1    14 0.2426\n## 15       Rec    C             0   21- 40 132    1    15 0.2307\n## 16       Rec    C             0   21- 40 132    1    16 0.2348\n## 17       Rec    C             0   21- 40 132    1    17 0.2293\n## 18       Rec    C             0   21- 40 132    1    18 0.2281\n## 19       Rec    C             0   21- 40 132    1    19 0.2304\n## 20       Rec    C             0   21- 40 132    1    20 0.2277\n## 21       Rec    C             0   21- 40 132    1    21 0.2283\n## 22       Rec    C             0   21- 40 132    1    22 0.2355\n## 23       Rec    C             0   21- 40 132    1    23 0.2297\n## 24       Rec    C             0   21- 40 132    1    24 0.2257\n## 25       Rec    C             0   21- 40 132    1    25 0.2191\n## 26       Rec    C             0   21- 40 132    1    26 0.2275\n## 27       Rec    C             0   21- 40 132    1    27 0.2328\n## 28       Rec    C             0   21- 40 132    1    28 0.2312\n## 29       Rec    C             0   21- 40 132    1    29 0.2190\n## 30       Rec    C             0   21- 40 132    1    30 0.2370\n## 31       Rec    C             0   21- 40 132    1    31 0.2303\n## 32       Rec    C             0   21- 40 132    1    32 0.2252\n## 33       Rec    C             0   21- 40 132    1    33 0.2190\n## 34       Rec    C             0   21- 40 132    1    34 0.2269\n## 35       Rec    C             0   21- 40 132    1    35 0.2311\n## 36       Rec    C             0   21- 40 132    1    36 0.2309\n## 37       Rec    C             0   21- 40 132    1    37 0.2313\n## 38       Rec    C             0   21- 40 132    1    38 0.2361\n## 39       Rec    C             0   21- 40 132    1    39 0.2335\n## 40       Rec    C             0   21- 40 132    1    40 0.2414\n## 41       Rec    C             0   21- 40 132    1    41 0.2333\n## 42       Rec    C             0   21- 40 132    1    42 0.2283\n## 43       Rec    C             0   21- 40 132    1    43 0.2354\n## 44       Rec    C             0   21- 40 132    1    44 0.2314\n## 45       Rec    C             0   21- 40 132    1    45 0.2357\n## 46       Rec    C             0   21- 40 132    1    46 0.2240\n## 47       Rec    C             0   21- 40 132    1    47 0.2241\n## 48       Rec    C             0   21- 40 132    1    48 0.2355\n## 49       Rec    C             0   21- 40 132    1    49 0.2260\n## 50       Rec    C             0   21- 40 132    1    50 0.2268\n## 51       Rec    C             0   21- 40 132    1    51 0.2278\n## 52       Rec    C             0   21- 40 132    1    52 0.2213\n## 53       Rec    C             0   21- 40 132    1    53 0.2246\n## 54       Rec    C             0   21- 40 132    1    54 0.2316\n## 55       Rec    C             0   21- 40 132    1    55 0.2313\n## 56       Rec    C             0   21- 40 132    1    56 0.2209\n## 57       Rec    C             0   21- 40 132    1    57 0.2269\n## 58       Rec    C             0   21- 40 132    1    58 0.2323\n## 59       Rec    C             0   21- 40 132    1    59 0.2280\n## 60       Rec    C             0   21- 40 132    1    60 0.2357\n## 61       Rec    C             0   21- 40 132    1    61 0.2275\n## 62       Rec    C             0   21- 40 132    1    62 0.2387\n## 63       Rec    C             0   21- 40 132    1    63 0.2387\n## 64       Rec    C             0   21- 40 132    1    64 0.2231\n## 65       Rec    C             0   21- 40 132    1    65 0.2370\n## 66       Rec    C             0   21- 40 132    1    66 0.2313\n## 67       Rec    C             0   21- 40 132    1    67 0.2243\n## 68       Rec    C             0   21- 40 132    1    68 0.2335\n## 69       Rec    C             0   21- 40 132    1    69 0.2275\n## 70       Rec    C             0   21- 40 132    1    70 0.2340\n## 71       Rec    C             0   21- 40 132    1    71 0.2250\n## 72       Rec    C             0   21- 40 132    1    72 0.2373\n## 73       Rec    C             0   21- 40 132    1    73 0.2259\n## 74       Rec    C             0   21- 40 132    1    74 0.2405\n## 75       Rec    C             0   21- 40 132    1    75 0.2227\n## 76       Rec    C             0   21- 40 132    1    76 0.2210\n## 77       Rec    C             0   21- 40 132    1    77 0.2337\n## 78       Rec    C             0   21- 40 132    1    78 0.2306\n## 79       Rec    C             0   21- 40 132    1    79 0.2242\n## 80       Rec    C             0   21- 40 132    1    80 0.2235\n## 81       Rec    C             0   21- 40 132    1    81 0.2247\n## 82       Rec    C             0   21- 40 132    1    82 0.2188\n## 83       Rec    C             0   21- 40 132    1    83 0.2129\n## 84       Rec    C             0   21- 40 132    1    84 0.2415\n## 85       Rec    C             0   21- 40 132    1    85 0.2293\n## 86       Rec    C             0   21- 40 132    1    86 0.2312\n## 87       Rec    C             0   21- 40 132    1    87 0.2189\n## 88       Rec    C             0   21- 40 132    1    88 0.2236\n## 89       Rec    C             0   21- 40 132    1    89 0.2262\n## 90       Rec    C             0   21- 40 132    1    90 0.2317\n## 91       Rec    C             0   21- 40 132    1    91 0.2316\n## 92       Rec    C             0   21- 40 132    1    92 0.2288\n## 93       Rec    C             0   21- 40 132    1    93 0.2299\n## 94       Rec    C             0   21- 40 132    1    94 0.2288\n## 95       Rec    C             0   21- 40 132    1    95 0.2311\n## 96       Rec    C             0   21- 40 132    1    96 0.2264\n## 97       Rec    C             0   21- 40 132    1    97 0.2269\n## 98       Rec    C             0   21- 40 132    1    98 0.2287\n## 99       Rec    C             0   21- 40 132    1    99 0.2283\n## 100      Rec    C             0   21- 40 132    1   100 0.2191\nSeguramente usar una api para obtener la posteriori que tiene tantos valores para cada dato no sea lo más eficiente, porque lo devuelve en formato json y luego hay que convertirlo a data.frame, pero funciona."
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#salvar-docker-en-un-tar.gz",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#salvar-docker-en-un-tar.gz",
    "title": "Api y docker con R. parte 1",
    "section": "Salvar docker en un tar.gz",
    "text": "Salvar docker en un tar.gz\nSi no tenemos un sitio estilo docker hub dónde registrar nuestros docker o por cualquier otra causa, podemos usar docker save para generar un fichero comprimido y docker load para importarlo.\nSería algo así como\n\ndocker save mi_modelo_brms | gzip > mi_modelo_brms_docker.tar.gz\nCopiar ese tar.gz a dónde toque\ndocker load < mi_modelo_brms_docker.tar.gz"
  },
  {
    "objectID": "2022/10/12/api-y-docker-con-r-parte-1/index.html#adelanto-con-vetiver",
    "href": "2022/10/12/api-y-docker-con-r-parte-1/index.html#adelanto-con-vetiver",
    "title": "Api y docker con R. parte 1",
    "section": "Adelanto con vetiver",
    "text": "Adelanto con vetiver\nCon la librería vetiver se simplifica todo este proceso, puesto que crea por ti el plumber.R y el dockerfile y tiene movidas para guardar la monitorización del modelo y demás. Está tanto para R como para python. En R soporta los modelos que estén en tidymodels y en python soporta scikit-learn, statmodels, xgboost y creo que también pytorch"
  },
  {
    "objectID": "2022/10/30/api-y-docker-con-r-parte-2.html",
    "href": "2022/10/30/api-y-docker-con-r-parte-2.html",
    "title": "Api y docker con R. parte 2",
    "section": "",
    "text": "En la entrada de api y docker con R parte I veíamos que es muy fácil construir una api y dockerizarla para tener un modelo bayesiano en producción. Pero hay un pequeño incoveniente, el docker que hemos creado se base en rocker/verse que se basan en ubuntu. Y ubuntu ocupa mucho. Pero gracias a gente como Gabor Csardi (autor entre otras librerías de igraph), tenemos r-hub/minimal, que permiten tener una imagen de docker con R basadas en alpine, de hecho una imagen de docker con R y dplyr son unos 50 mb.\nLo primero de todo es ver cuánto ocupa el docker creado en el primer post.\nPues son unos cuántos gigas, mayoritariamente al estar basado en ubuntu y al que los docker de rocker/verse instalan todo el software de R recomendado, los ficheros de ayuda, las capacidades gráficas, etc..\nPero con r-hub/minimal podemos dejar bastante limpio el tema. Leyendo el Readme del repo vemos que han configurado una utilidad a la que llaman installr que permite instalar librerías del sistema o de R, instalando los compiladores de C, fortran etc que haga falta y eliminarlos una vez están compiladas la librerías.\nSin más, cambiamos el Dockerfile del otro día por este otro .\nY haciendo docker build -t mi_modelo_brms_rminimal . pasado un rato puesto que ha de compilar las librerías tenemos nuestra api dockerizada con la misma funcionalidad que el otro día.\nY con un tamaño mucho más contenido\nque se va a unos 655 mb, de los cuales unos 300 MB se deben a stan y rstan. Pero vamos, no está mal, pasar de 3.4 Gb a 665MB."
  },
  {
    "objectID": "2022/10/30/api-y-docker-con-r-parte-2.html#actualización-usando-renv",
    "href": "2022/10/30/api-y-docker-con-r-parte-2.html#actualización-usando-renv",
    "title": "Api y docker con R. parte 2",
    "section": "Actualización, usando renv",
    "text": "Actualización, usando renv\nPor temas de buenas prácticas es recomendable usar renv para crear el archivo renv.lock dónde se guarda qué versión de las librerías estamos usando, y además porque usa por defecto un repo con las librerías compiladas.\nLo primero que hago es crearme un nuevo proyecto dónde pongo el modelo entrenado que queremos usar brms_model.rds que entrené en el primer post y el fichero plumber.R y ningún fichero más.\nFichero plumber.R\n\n\n#\n# This is a Plumber API. In RStudio 1.2 or newer you can run the API by\n# clicking the 'Run API' button above.\n#\n# In RStudio 1.1 or older, see the Plumber documentation for details\n# on running the API.\n#\n# Find out more about building APIs with Plumber here:\n#\n#    https://www.rplumber.io/\n#\n# save as bos_rf_score.R\n\nlibrary(brms)\nlibrary(plumber)\nlibrary(tidybayes)\n\nbrms_model <- readRDS(\"brms_model.rds\")\n\n\n#* @apiTitle brms predict Api\n#* @apiDescription Endpoints for working with brms model\n## ---- filter-logger\n#* Log some information about the incoming request\n#* @filter logger\nfunction(req){\n    cat(as.character(Sys.time()), \"-\",\n        req$REQUEST_METHOD, req$PATH_INFO, \"-\",\n        req$HTTP_USER_AGENT, \"@\", req$REMOTE_ADDR, \"\\n\")\n    forward()\n}\n\n## ---- post-data\n#* Submit data and get a prediction in return\n#* @post /predict\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    predict(brms_model, data) |>\n        as.data.frame()\n}\n\n\n#* @post /full_posterior\nfunction(req, res) {\n    data <- tryCatch(jsonlite::parse_json(req$postBody, simplifyVector = TRUE),\n                     error = function(e) NULL)\n    if (is.null(data)) {\n        res$status <- 400\n        return(list(error = \"No data submitted\"))\n    }\n    \n    add_epred_draws(data, brms_model) \n    \n}\n\nA continuación activo renv en el proyecto\n\n renv::activate()\n* Project '~/Rstudio_projects/r-api-minimal' loaded. [renv 0.16.0]\n\nUna vez que está activado y el fichero plumber.R está creado en el directorio uso hydrate para que encuentre qué librerías se usan en el proyecto\n\n\n> renv::hydrate()\n* Discovering package dependencies ... Done!\n* Copying packages into the cache ... Done!\n\ny ya podemos crear el fichero renv::snapshot(), donde pone todas las librerías que se van a instalar y si vienen de CRAN , de GitHub o de RSPM(rstudio package manager)\n\nrenv::snapshot()\nThe following package(s) will be updated in the lockfile:\n\n# CRAN ===============================\n- Matrix           [* -> 1.5-1]\n- R6               [* -> 2.5.1]\n- RColorBrewer     [* -> 1.1-3]\n- Rcpp             [* -> 1.0.9]\n- base64enc        [* -> 0.1-3]\n- bslib            [* -> 0.4.0]\n- cachem           [* -> 1.0.6]\n- codetools        [* -> 0.2-18]\n- colorspace       [* -> 2.0-3]\n- ellipsis         [* -> 0.3.2]\n- fansi            [* -> 1.0.3]\n- farver           [* -> 2.1.1]\n- fastmap          [* -> 1.1.0]\n- generics         [* -> 0.1.3]\n- ggplot2          [* -> 3.3.6]\n- htmltools        [* -> 0.5.3]\n- jquerylib        [* -> 0.1.4]\n- labeling         [* -> 0.4.2]\n- lattice          [* -> 0.20-45]\n- lifecycle        [* -> 1.0.3]\n- magrittr         [* -> 2.0.3]\n- memoise          [* -> 2.0.1]\n- mgcv             [* -> 1.8-40]\n- mime             [* -> 0.12]\n- munsell          [* -> 0.5.0]\n- pkgconfig        [* -> 2.0.3]\n- prettyunits      [* -> 1.1.1]\n- processx         [* -> 3.7.0]\n- ps               [* -> 1.7.1]\n- rappdirs         [* -> 0.3.3]\n- rprojroot        [* -> 2.0.3]\n- sass             [* -> 0.4.2]\n- stringi          [* -> 1.7.8]\n- tibble           [* -> 3.1.8]\n- utf8             [* -> 1.2.2]\n- withr            [* -> 2.5.0]\n\n# GitHub =============================\n- glue             [* -> jimhester/fstrings@HEAD]\n\n# RSPM ===============================\n- BH               [* -> 1.78.0-0]\n- Brobdingnag      [* -> 1.2-9]\n- DT               [* -> 0.26]\n- HDInterval       [* -> 0.2.2]\n- MASS             [* -> 7.3-58.1]\n- RcppEigen        [* -> 0.3.3.9.2]\n- RcppParallel     [* -> 5.1.5]\n- StanHeaders      [* -> 2.21.0-7]\n- abind            [* -> 1.4-5]\n- arrayhelpers     [* -> 1.1-0]\n- backports        [* -> 1.4.1]\n- bayesplot        [* -> 1.9.0]\n- bridgesampling   [* -> 1.1-2]\n- brms             [* -> 2.18.0]\n- callr            [* -> 3.7.2]\n- checkmate        [* -> 2.1.0]\n- cli              [* -> 3.4.1]\n- coda             [* -> 0.19-4]\n- colourpicker     [* -> 1.1.1]\n- commonmark       [* -> 1.8.1]\n- cpp11            [* -> 0.4.3]\n- crayon           [* -> 1.5.2]\n- crosstalk        [* -> 1.2.0]\n- curl             [* -> 4.3.3]\n- desc             [* -> 1.4.2]\n- digest           [* -> 0.6.30]\n- distributional   [* -> 0.3.1]\n- dplyr            [* -> 1.0.10]\n- dygraphs         [* -> 1.1.1.6]\n- fontawesome      [* -> 0.3.0]\n- fs               [* -> 1.5.2]\n- future           [* -> 1.28.0]\n- ggdist           [* -> 3.2.0]\n- ggridges         [* -> 0.5.4]\n- globals          [* -> 0.16.1]\n- gridExtra        [* -> 2.3]\n- gtable           [* -> 0.3.1]\n- gtools           [* -> 3.9.3]\n- htmlwidgets      [* -> 1.5.4]\n- httpuv           [* -> 1.6.6]\n- igraph           [* -> 1.3.5]\n- inline           [* -> 0.3.19]\n- isoband          [* -> 0.2.6]\n- jsonlite         [* -> 1.8.2]\n- later            [* -> 1.3.0]\n- lazyeval         [* -> 0.2.2]\n- listenv          [* -> 0.8.0]\n- loo              [* -> 2.5.1]\n- markdown         [* -> 1.2]\n- matrixStats      [* -> 0.62.0]\n- miniUI           [* -> 0.1.1.1]\n- mvtnorm          [* -> 1.1-3]\n- nleqslv          [* -> 3.3.3]\n- nlme             [* -> 3.1-160]\n- numDeriv         [* -> 2016.8-1.1]\n- parallelly       [* -> 1.32.1]\n- pillar           [* -> 1.8.1]\n- pkgbuild         [* -> 1.3.1]\n- plumber          [* -> 1.2.1]\n- plyr             [* -> 1.8.7]\n- posterior        [* -> 1.3.1]\n- promises         [* -> 1.2.0.1]\n- purrr            [* -> 0.3.5]\n- renv             [* -> 0.16.0]\n- reshape2         [* -> 1.4.4]\n- rlang            [* -> 1.0.6]\n- rstan            [* -> 2.21.7]\n- rstantools       [* -> 2.2.0]\n- scales           [* -> 1.2.1]\n- shiny            [* -> 1.7.2]\n- shinyjs          [* -> 2.1.0]\n- shinystan        [* -> 2.6.0]\n- shinythemes      [* -> 1.2.0]\n- sodium           [* -> 1.2.1]\n- sourcetools      [* -> 0.1.7]\n- stringr          [* -> 1.4.1]\n- svUnit           [* -> 1.0.6]\n- swagger          [* -> 3.33.1]\n- tensorA          [* -> 0.36.2]\n- threejs          [* -> 0.3.3]\n- tidybayes        [* -> 3.0.2]\n- tidyr            [* -> 1.2.1]\n- tidyselect       [* -> 1.2.0]\n- vctrs            [* -> 0.4.2]\n- viridisLite      [* -> 0.4.1]\n- webutils         [* -> 1.1]\n- xfun             [* -> 0.34]\n- xtable           [* -> 1.8-4]\n- xts              [* -> 0.12.2]\n- yaml             [* -> 2.3.6]\n- zoo              [* -> 1.8-11]\n\nThe version of R recorded in the lockfile will be updated:\n- R                [*] -> [4.2.1]\n\nDo you want to proceed? [y/N]: y\n* Lockfile written to '~/Rstudio_projects/r-api-minimal/renv.lock'.\n\nY ya sólo queda crear el Dockerfile usando como base r-hub/minimal\nDockerfile\n# Docker file para modelo brms\n\nFROM rhub/r-minimal:4.2.1\n\n# copio fichero de las librerías\nCOPY renv.lock renv.lock\n\n# uso -c para que se queden instaladas los compiladores de c y fortran\n\nRUN installr -c -a \"curl-dev linux-headers gfortran libcurl libxml2 libsodium-dev libsodium automake autoconf\"\n\n#instalo renv\nRUN installr -c renv\n\n# uso renv para instlar la versión de las librerías que hay en renv.lock\nRUN Rscript -e \"renv::restore()\"\n\n## Copio el modelo y el fichero de la api\nCOPY brms_model.rds /opt/ml/brms_model.rds\nCOPY plumber.R /opt/ml/plumber.R\n\n# exponemos el puerto\nEXPOSE 8081\nENTRYPOINT [\"R\", \"-e\", \"pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8081)\"]\ny como antes construimos el docker image\ndocker build -t mi_modelo_brms_rminimal_renv .\nEl docker usando renv es sustancialmente más pesado, ocupa 1.29 Gb\nSeguramente se puede optimizar más si no usara brms, puesto que importa shinystan, bayesplot y otras librerías que no son estrictamente necesarias para nuestro propósito. Habrá que esperar a que Virgilio haga la función predict de INLA para darle una vuelta a esto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Me llamo José Luis Cañadas Reche y llevo unos cuantos años dedicado a esto del análisis de datos con éxito desigual."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Aquí estoy de nuevo",
    "section": "",
    "text": "Estoy cambiando el blog de blogdown a quarto Welcome!"
  },
  {
    "objectID": "posts/leaflet_ejemplo/index.html",
    "href": "posts/leaflet_ejemplo/index.html",
    "title": "Leaflet example",
    "section": "",
    "text": "library(leaflet)\n\nm <- leaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=174.768, lat=-36.852, popup=\"The birthplace of R\")\nm  # Print the map"
  },
  {
    "objectID": "2021/12/12/Modelos-mixtos-spark/index.html",
    "href": "2021/12/12/Modelos-mixtos-spark/index.html",
    "title": "Modelos mixtos en spark. Intento 1",
    "section": "",
    "text": "A los que nos dedicamos a esto siempre echamos de menos un lme4 en python o en Spark. En Julia afortunadamente tenemos MixedModels.jl.\nTotal que buscando alguna posible solución para poder usar esto en spark me encuentro con dos posibles soluciones.\n\nphoton-ml\nMomentMixedModels\n\nAmbos repos llevan un tiempo sin actualizarse así que no sé yo.\nphoton-ml es de linkedin y tiene buena pinta, al menos el tutorial, que tienes que bajarte un docker y tal, funciona. Aunque la sintaxis es rara. Aún tengo que probarlo más y probar a crear el jar del proyecto ya que no está en maven central y tal (y no me funcionó)\n\nEjemplo de sintaxis de photon-ml\n\n// Define another feature shard for our random effect coordinate, and create a new mapping\n// with both our 'global' and 'perUser' shards.\nval perUserFeatureShardId = \"perUser\"\nval perUserFeatureShard = Set(\"genreFeatures\", \"movieLatentFactorFeatures\")\nval mixedFeatureShardBags = Map(\n    globalFeatureShardId -> globalFeatureShard,\n    perUserFeatureShardId -> perUserFeatureShard)\n\n// Since we have a new shard, re-read the training and validation data into a new DataFrame\n// (and a new index map for the new feature shard).\nval (mixedInputData, mixedFeatureShardInputIndexMaps) = dataReader.readMerged(\n    Seq(\"/data/movielens/trainData.avro\"),\n    mixedFeatureShardBags,\n    numPartitions)\nval mixedValidateData = dataReader.readMerged(\n    Seq(\"/data/movielens/validateData.avro\"),\n    mixedFeatureShardInputIndexMaps,\n    mixedFeatureShardBags,\n    numPartitions)\nDonde mixedInputData es un dataframe de spark con esta pinta.\nmixedInputData.show()\n\n+----+--------+------+-------+------+------+--------------------+--------------------+\n| uid|response|userId|movieId|weight|offset|              global|             perUser|\n+----+--------+------+-------+------+------+--------------------+--------------------+\n|null|     4.0|     1|   1215|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|     1|   1350|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     1|   2193|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|     1|   3476|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|     1|   4993|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     3|   1544|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|    440|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|    914|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|   1894|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|     7|   2112|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|   3524|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.0|     7|   3911|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    11|    256|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    11|   1200|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     4.5|    11|  48394|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     1.0|    11|  56003|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     0.5|    11|  64508|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     5.0|    14|    471|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.0|    14|   2018|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n|null|     3.5|    14|   6936|  null|  null|(51,[0,1,2,3,4,5,...|(51,[0,1,2,3,4,5,...|\n\nDonde las columna global y perUser son iguales, pero una se usa para estimar la parte de los efectos fijos y la otra para los aleatorios.\nY luego sigue con\n// A 'RandomEffectDataConfiguration' requires an identifier field to use for grouping data from the\n// same entity, in addition to the fields that a 'FixedEffectDataConfiguration' requires. It also has\n// some additional optional parameters not covered in this tutorial.\nval perUserRandomEffectId = \"userId\"\nval perUserDataConfig = RandomEffectDataConfiguration(\n    perUserRandomEffectId,\n    perUserFeatureShardId,\n    numPartitions,\n    projectorType = IndexMapProjection)\n\n// A 'RandomEffectOptimizationConfiguration' is defined much like a\n// 'FixedEffectOptimizationConfiguration'. The options below are varied from those above primarily\n// for variety and demonstration.\nval perUserOptimizerConfig = OptimizerConfig(\n    optimizerType = TRON,\n    tolerance = 1e-3,\n    maximumIterations = 4)\nval perUserRegularizationContext = L2RegularizationContext\nval perUserRegularizationWeight = 1\nval perUserOptimizationConfig = RandomEffectOptimizationConfiguration(\n    perUserOptimizerConfig,\n    perUserRegularizationContext,\n    perUserRegularizationWeight)\n\n// Assign a coordinate ID to the random effect configurations we defined above. This time, we have\n// multiple coordinates and need to determine the update sequence. In general, it's recommended to\n// order coordinates from least to most granular, i.e. those that correlate most with the response to\n// those that correlate least.\nval perUserCoordinateId = \"perUser\"\nval mixedCoordinateDataConfigs = Map(\n    globalCoordinateId -> globalDataConfig,\n    perUserCoordinateId -> perUserDataConfig)\nval mixedCoordinateOptConfigs = Map(\n    globalCoordinateId -> globalOptimizationConfig,\n    perUserCoordinateId -> perUserOptimizationConfig)\nval mixedUpdateSequence = Seq(globalCoordinateId, perUserCoordinateId)\n\n// Reset our estimator. The training task hasn't changed, but the data configurations and update\n// sequence have. Furthermore, since there are now multiple coordinates, we should try multiple\n// passes of coordinate descent.\nestimator.setCoordinateDataConfigurations(mixedCoordinateDataConfigs)\nestimator.setCoordinateUpdateSequence(mixedUpdateSequence)\nestimator.setCoordinateDescentIterations(2)\n\n// Train a new model.\nval (mixedModel, _, mixedModelConfig) = estimator.fit(\n    mixedInputData,\n    Some(mixedValidateData),\n    Seq(mixedCoordinateOptConfigs)).head\n\n// Save the trained model.\nModelProcessingUtils.saveGameModelToHDFS(\n    sc,\n    new Path(\"output/mixed\"),\n    mixedModel,\n    trainingTask,\n    mixedModelConfig,\n    None,\n    mixedFeatureShardInputIndexMaps)\n    \nY guarda los coeficientes en avro\n\"avro cat -n 1 ./output/mixed/random-effect/perUser/coefficients/part-00000.avro\" #| \"jq .\" !\n{\n  \"variances\": null,\n  \"means\": [\n    {\n      \"term\": \"Drama\",\n      \"name\": \"Genre\",\n      \"value\": -0.35129547272878503\n    },\n    {\n      \"term\": \"Musical\",\n      \"name\": \"Genre\",\n      \"value\": -0.2967514108349342\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"7\",\n      \"value\": -0.13789947075029355\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"14\",\n      \"value\": -0.13577029316450503\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"8\",\n      \"value\": -0.12850130065314527\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"26\",\n      \"value\": -0.11646520581859549\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"15\",\n      \"value\": -0.09620039918539182\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"6\",\n      \"value\": 0.08934738779979344\n    },\n    {\n      \"term\": \"Comedy\",\n      \"name\": \"Genre\",\n      \"value\": 0.08833383209245319\n    },\n    {\n      \"term\": \"\",\n      \"name\": \"2\",\n      \"value\": -0.08756438537931642\n    },\n\nmore coefficients\n    \"modelClass\": \"com.linkedin.photon.ml.supervised.regression.LinearRegressionModel\",\n  \"lossFunction\": \"\",\n  \"modelId\": \"7\"\n}\nLo dicho, no tiene mala pinta y ajusta rápido, me falta probar a crear el jar del proyecto\nPor otro lado MomentMixedModels también parecía prometedora pero al intentar crear el jar con sbt (tampoco está en maven central) peta con (*:update) sbt.ResolveException: unresolved dependency: com.stitchfix.algorithms.spark#sfs3_2.11;0.7.0-spark2.2.0: not found y viendo el build.sbt hace referencia a http://artifactory.vertigo.stitchfix.com/artifactory/releases que parece que ya no existe, así que mi gozo en un pozo. La sintaxis parecía sencilla.\nval linearModelFitter = {\n    new MixedEffectsRegression()\n      .setResponseCol(\"Reaction\")\n      .setFixedEffectCols(Seq(\"Days\"))\n      .setRandomEffectCols(Seq(\"Days\"))\n      .setFamilyParam(\"gaussian\")\n      .setGroupCol(\"Subject\")\n  }\n\n  val linearModel = linearModelFitter.fit(sleepstudyData)\n  println(linearModel.β)\nPues nada, a ver si algún ingenazi con alma de analista se digna a hacer una implementación de lme4 en Spark , porque, reconozcámoslo Spark-ml es una ñapa. Lo único que medio funciona bien es usar los algoritmos de h2o sobre spark con sparkling-water y me falta probar un poco más su implementación de modelos jerárquicos\nHasta otra."
  },
  {
    "objectID": "2021/12/01/lectura-para-el-finde/index.html",
    "href": "2021/12/01/lectura-para-el-finde/index.html",
    "title": "Lecturas para el finde",
    "section": "",
    "text": "El Vol 100 del Journal Of Statistical Software promete, y mucho. Artículo del gran Virgilio y muchos más sobre software para estadística bayesiana. Virgilio, sólo falta que le eches un vistazo a las cositas que hay en Julia. Pues nada, ya tengo entretenimiento. Aquí os dejo el enlace"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "",
    "text": "¿Qué os parecería tener un modelo guardado y un binario en linux que tomando como parámetros el modelo y el dataset a predecir guardara las predicciones en un csv?\nY todo eso que funcione en cualquier Linux, de forma que puedas copiar esa aplicación de un Ubuntu a un EC2 con amazon linux (un centos) y que funcione igual sin tener que tener Julia instalado en el EC2.\nY no estoy hablando de tener un docker o tener un entorno de conda dónde lo despliegas y tu script dónde se predice necesita ser interpretado, sino de una aplicación compilada dónde tiene el runtime de Julia y todo lo necesario para correr. De hecho la estructura de esa aplicación sería algo así.\n $ ▶ tree -L 1 ../bin_blog\n../bin_blog\n├── artifacts\n├── bin\n└── lib\nBueno, pues vamos a ver cómo se consigue eso utilizando el paquete PackageCompiler En primer lugar algunas consideraciones sobre latencia y precompilados que podéis ver en el video de Kristoffer Carlson.\n\nJulia tiene los paquetes precompilados, por lo que cuando arrancas el pc y haces using paquete tarda un rato.\nUna vez compilado, la primera vez que lo invocas tarda un tiempo.\nAunque ya hayas hecho using paquete la primera vez que usas una función también tiene que compilarla y tarda otro rato.\n\npor ejemplo, vemos que la primera vez que hago using Plots tarda unos 3 segundos, o que la primera vez que uso la función plot también tarda, pero la siguiente vez es muy rápido.\njulia> @time using Plots\n  3.267061 seconds (7.93 M allocations: 551.989 MiB, 3.89% gc time, 0.17% compilation time)\n\njulia> @time using Plots\n  0.666359 seconds (665.66 k allocations: 37.314 MiB, 6.76% gc time, 99.99% compilation time)\n\n\njulia> @time p = plot(rand(2,2))\n  2.436100 seconds (3.11 M allocations: 186.676 MiB, 7.61% gc time, 57.23% compilation time)\n[ Info: Precompiling GR_jll [d2c73de3-f751-5644-a686-071e5b155ba9]\n\njulia> @time p = plot(rand(2,2))\n  0.000883 seconds (3.93 k allocations: 228.672 KiB)\nPero, ¿no era una de las características de Julia su velocidad, cómo podemos apañar esto?. Pues hay varias formas.\n\nCrear un archivo startup.jl en \\.julia\\config\\ dónde escribamos lo que queremos que se cargue al iniciar julia.\nCrear una sysimage de julia con PackageCompiler de forma que podamos hacer algo como julia --sysimage mi_sysimage.so y se inicie Julia con los paquetes que queramos ya compilados y cargados.\n\nEl paquete PackageCompiler permite también crear una aplicación de forma que crea una sysimage de julia junto con un script con una función main que es la que se ejecuta al llamar al binario que crea. La ventaja de esta aproximación es que podemos crear un binario que funcione en cualquier linux aunque no tengamos Julia instalado."
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#objetivo",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#objetivo",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Objetivo",
    "text": "Objetivo\nEl objetivo es construir un “Motor de Modelos” (recuerdos a mi amigo Roberto Sancho) que funciones en cualquier linux, y que dado un modelo previamente entrenado y la ruta de un fichero con los datos, haga la predicción del modelo y escriba un csv con el resultado.\nAl final se podría usar de la siguiente forma\nmotor_modelos modelo_entrenado.jlso datos_to_predict.csv resultado.csv\nEn las pruebas que he hecho, para predecir un fichero de 5 millones de filas y escribir el csv con el resultado de la predicción de un modelo randomForest ha tardado unos 15 segundos en todo el proceso."
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#creando-el-entorno-necesario",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#creando-el-entorno-necesario",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Creando el entorno necesario",
    "text": "Creando el entorno necesario\nLo primero que tenemos que hacer es seguir el proceso como para crear un paquete en julia. Iniciamos julia en el REPL y entrando en el modo paquete con ] utilizamos generate\n(@v1.6) pkg> generate decision_tree_app\n  Generating  project decision_tree_app:\n    decision_tree_app/Project.toml\n    decision_tree_app/src/decision_tree_app.jl\nEsto crea el directorio decision_tree_app así como un Project.toml dónde se va a ir guardando la referencia y las versiones de las librerías que usemos, y también crea el fichero src/decision_tree_app.jl con la estructura mínima.\n╭─ jose @ jose-PROX15 ~\n│\n╰─ $ ▶ cat decision_tree_app/src/decision_tree_app.jl \nmodule decision_tree_app\n\ngreet() = print(\"Hello World!\")\n\nend # module\nPues sobre esta base es la que vamos a trabajar. Ahora tenemos que activar el entorno con\n(@v1.6) pkg> activate .\n  Activating environment at `~/decision_tree_app/Project.toml`\n \nDe esta forma cada vez que añadamos un paquete con add nombrepquete se queda guardado la referencia en el el Project.toml y se creará un Manifest.toml, estos dos archivos son los que nos servirán para reproducir el mismo entorno en otro sitio, equivalente a un requirements en python."
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#crear-la-aplicación",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#crear-la-aplicación",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Crear la aplicación",
    "text": "Crear la aplicación\nEn el paquete PackageCompiler existe la función create_app que tomando como argumentos, el directorio de la aplicación, directorio dónde compilar y uno o varios ficheros de ejemplo del flujo que se va a realizar, creará la apliación compilada.\n\nFichero de precompilación\nEs importante tener un fichero de precompilación que sea ejemplo simple de lo que tiene que hacer la aplicación. A saber, leer un modelo, leer unos datos, predecir y escribir el resultado.\nPara eso, entrenamos un modelo sobre iris, y guardamos el modelo\nFichero train.jl\n\n# Training model julia\nusing CSV,CategoricalArrays, DataFrames, MLJ\n\n\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\n\nconst target =  CategoricalArray(df1[:, :Species])\nconst X = df1[:, Not(:Species)]\n\n\nTree = @load RandomForestClassifier pkg=DecisionTree\ntree = Tree(n_trees = 20)\ntree.max_depth = 3\n\nmach = machine(tree, X, target)\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\n\nMLJ.save(\"mimodelo.jlso\", mach, compression=:gzip)\n\nY una vez que tenemos el modelo guardado creamos el fichero de precompilación que pasaremos como argumento a create_app\nFichero precomp_file.jl en directorio src\nEn este fichero al llamar a las funciones CSV.read, predict, o CSV.write se consigue que al crear la aplicación compilada esas funciones se compilen y la latencia sea mínima.\nusing MLJ, CSV, DataFrames, MLJDecisionTreeInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"/home/jose/Julia_projects/decision_tree_app/data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"/home/jose/Julia_projects/decision_tree_app/mimodelo.jlso\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, niveles)\n\nCSV.write(\"/home/jose/Julia_projects/decision_tree_app/data/predicciones.csv\", res_df)\nCuando compilamos una aplicación con PackageCompiler lo que se ejecuta es la función julia_main que se encuentre en el módulo que creamos con el mismo nombre que el nombre de la aplicación.\nFichero decision_tree_app.jl en src\nmodule decision_tree_app\n\nusing MLJ, CSV, DataFrames, MLJDecisionTreeInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, tasks=10)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, niveles)          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\nAsí nos queda la estructura\n tree -L 1 src/\nsrc/\n├── decision_tree_app.jl\n├── precomp_file.jl\n└── train.jl\nAhora ya podemos compilar la aplicación\n\nusing PackageCompiler\n\ncreate_app(\"../decision_tree_app\", \"../bin_blog\", precompile_execution_file=\"../decision_tree_app/src/precomp_file.jl\", force=true, filter_stdlibs = true)\n\nY después de unos 30 minutos ya tenemos en el directorio bin_blog todo lo necesario, el runtime de julia embebido, las librerías compiladas , etcétera, de forma que copiando esa estructura en otro ordenador (con linux) ya funcionaría nuestra app sin tener Julia instalado.\n\ntree -L 1 bin_blog/\nbin_blog/\n├── artifacts\n├── bin\n└── lib\nPor ejemplo en lib tenemos\ntotal 272\ndrwxrwxr-x 3 jose jose   4096 ago 14 10:37 ./\ndrwxrwxr-x 5 jose jose   4096 ago 14 10:37 ../\ndrwxrwxr-x 2 jose jose   4096 ago 14 10:37 julia/\nlrwxrwxrwx 1 jose jose     15 ago 14 10:37 libjulia.so -> libjulia.so.1.6*\nlrwxrwxrwx 1 jose jose     15 ago 14 10:37 libjulia.so.1 -> libjulia.so.1.6*\n-rwxr-xr-x 1 jose jose 266232 ago 14 10:37 libjulia.so.1.6*\ny en lib/julia tiene este aspecto\ndrwxrwxr-x 2 jose jose     4096 ago 14 10:37 ./\ndrwxrwxr-x 3 jose jose     4096 ago 14 10:37 ../\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libamd.so -> libamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libamd.so.2 -> libamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    39059 ago 14 10:37 libamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libatomic.so.1 -> libatomic.so.1.2.0*\n-rwxr-xr-x 1 jose jose   147600 ago 14 10:37 libatomic.so.1.2.0*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libbtf.so -> libbtf.so.1.2.6*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libbtf.so.1 -> libbtf.so.1.2.6*\n-rwxr-xr-x 1 jose jose    13108 ago 14 10:37 libbtf.so.1.2.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcamd.so -> libcamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcamd.so.2 -> libcamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    43470 ago 14 10:37 libcamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    28704 ago 14 10:37 libccalltest.so*\n-rwxr-xr-x 1 jose jose    39128 ago 14 10:37 libccalltest.so.debug*\nlrwxrwxrwx 1 jose jose       19 ago 14 10:37 libccolamd.so -> libccolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       19 ago 14 10:37 libccolamd.so.2 -> libccolamd.so.2.9.6*\n-rwxr-xr-x 1 jose jose    47652 ago 14 10:37 libccolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libcholmod.so -> libcholmod.so.3.0.13*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libcholmod.so.3 -> libcholmod.so.3.0.13*\n-rwxr-xr-x 1 jose jose  1005880 ago 14 10:37 libcholmod.so.3.0.13*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libcolamd.so -> libcolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libcolamd.so.2 -> libcolamd.so.2.9.6*\n-rwxr-xr-x 1 jose jose    31250 ago 14 10:37 libcolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcurl.so -> libcurl.so.4.7.0*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcurl.so.4 -> libcurl.so.4.7.0*\n-rwxr-xr-x 1 jose jose   654080 ago 14 10:37 libcurl.so.4.7.0*\n-rwxr-xr-x 1 jose jose    22120 ago 14 10:37 libdSFMT.so*\n-rwxr-xr-x 1 jose jose   758680 ago 14 10:37 libgcc_s.so.1*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libgfortran.so.4 -> libgfortran.so.4.0.0*\n\ny en bin\ntotal 245180\ndrwxrwxr-x 2 jose jose      4096 ago 14 10:50 ./\ndrwxrwxr-x 5 jose jose      4096 ago 14 10:37 ../\n-rwxrwxr-x 1 jose jose     17928 ago 14 10:50 decision_tree_app*\n-rwxrwxr-x 1 jose jose 251030272 ago 14 10:50 decision_tree_app.so*"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#utilizando-la-aplicación",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#utilizando-la-aplicación",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Utilizando la aplicación",
    "text": "Utilizando la aplicación\nLo primero que comprobamos es si la aplicación funciona con el modelo que tenemos sobre iris.\nIntentamos predecir un fichero tal que así\nhead test_to_predict.csv \nSepal.Length,Sepal.Width,Petal.Length,Petal.Width\n5.1,3.5,1.4,0.2\n4.9,3,1.4,0.2\n4.7,3.2,1.3,0.2\n4.6,3.1,1.5,0.2\n5,3.6,1.4,0.2\n5.4,3.9,1.7,0.4\n4.6,3.4,1.4,0.3\n5,3.4,1.5,0.2\n4.4,2.9,1.4,0.2\nAhora ejecutamos la app pasándole el modelo guardado en train.jl y el csv\n╰─ $ ▶ time ../bin_blog/bin/decision_tree_app mimodelo.jlso test_to_predict.csv predicho.csv\n\nreal    0m2,046s\nuser    0m2,344s\nsys 0m0,581s\n╰─ $ ▶ head -20 predicho.csv \nsetosa,versicolor,virginica\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n0.9,0.1,0.0\n0.95,0.05,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n0.95,0.05,0.0"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#uso-como-motor-para-predecir.",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#uso-como-motor-para-predecir.",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Uso como motor para predecir.",
    "text": "Uso como motor para predecir.\nUna vez tenemos la app queremos utilizarla con otros modelos y otros datos sin necesidad de tener que compilar de nuevo.\nLo primero es usar las mismas versiones de las librerías que hemos usado en la app. Para eso copiamos los archivos Project.toml y Manifest.toml en otro directorio y activamos el entorno con activate . e instalamos los paquetes con instanstiate\n\n─ $ ▶ cd entorno_modelos/\n╭─ jose @ jose-PROX15 ~/Julia_projects/entorno_modelos\n│\n╰─ $ ▶ julia \n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.6.2 (2021-07-14)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\n(@v1.6) pkg> activate .\n  Activating environment at `~/Julia_projects/entorno_modelos/Project.toml`\n\n(decision_tree_app) pkg> instantiate\n\nY ya podemos entrenar un nuevo modelo. En este caso voy a entrenar un modelo sobre los datos del precio de las viviendas en Boston, pero dónde he creado variables categórica que diferencie entre si el precio es mayor que 20 o menor, variable medv_20 con niveles (G20, NG20)\n─ $ ▶ head data/boston.csv \n\"crim\",\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\",\"ptratio\",\"lstat\",\"medv_20\"\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98,\"G20\"\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14,\"G20\"\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03,\"G20\"\n0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,2.94,\"G20\"\n0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,5.33,\"G20\"\n0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,5.21,\"G20\"\n0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,12.43,\"G20\"\n0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,19.15,\"G20\"\n0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,29.93,\"NG20\"\nY nuestro fichero de entrenamiento es el siguiente. Dejo solo lo de entrenar y guardar el modelo, otro día vemos la validación cruzada etc..\n# Training model julia\nusing CSV,CategoricalArrays, DataFrames, MLJ\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\nconst target =  CategoricalArray(df1[:, :medv_20])\nconst X = df1[:, Not(:medv_20)]\n\nTree = @load RandomForestClassifier pkg=DecisionTree\ntree = Tree(n_trees = 20)\ntree.max_depth = 5\n\n\nmach = machine(tree, X, target)\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\n\nMLJ.save(\"boston_rf.jlso\", mach, compression=:gzip)\nY ya podemos usar nuestra aplicación compilada para predecir con el modelo que acabamos de entrenar. Para simular más un proceso real, usamos el conjunto de datos de boston pero con 5 millones de filas.\nFichero sin la variable a predecir\n╰─ $ ▶ head -10  boston_to_predict.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\n0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,2.94\n0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,5.33\n0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,5.21\n0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,12.43\n0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,19.15\n0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,29.93\n\n\n╰─ $ ▶ wc -l  boston_to_predict.csv \n5060001 boston_to_predict.csv\nY ahora utilizamos nuestreo “Motor de Modelos” . Y en mi pc, tarda en predecir y escribir el resultado en torno a los 15 segundos.\n╰─ $ ▶ time ../bin_blog/bin/decision_tree_app boston_rf.jlso boston_to_predict.csv res.csv\n\nreal    0m14,080s\nuser    0m28,949s\nsys 0m3,442s\n╰─ $ ▶ wc -l res.csv \n5060001 res.csv\n╭─ jose @ jose-PROX15 ~/Julia_projects/entorno_modelos\n│\n╰─ $ ▶ head -10 res.csv \nG20,NG20\n0.95,0.05\n0.95,0.05\n1.0,0.0\n1.0,0.0\n1.0,0.0\n1.0,0.0\n0.85,0.15\n0.4,0.6\n0.25,0.75"
  },
  {
    "objectID": "2021/08/16/palabras-para-julia-parte-2-n/index.html#conclusión",
    "href": "2021/08/16/palabras-para-julia-parte-2-n/index.html#conclusión",
    "title": "Palabras para Julia ( Parte 2/n)",
    "section": "Conclusión",
    "text": "Conclusión\nEsto es sólo un ejemplo de como crear una aplicación compilada con Julia, en este caso para temas de “machín lenin”, pero podría ser para otra cosa.\nComo ventaja, que podemos crear un tar.gz con toda la estructura creada en directorio bin_blog y ponerlo en otro linux y qué funcione sin necesidad de que sea la misma distribución de linux ni de que esté Julia instalado. Esto podría ser útil en entornos productivos en los que haya restricciones a la hora de instalar software.\nAún tengo que explorar como leer y escribir ficheros de s3 con AWSS3.jl y más cosas relacionadas.\nHasta otra."
  },
  {
    "objectID": "2021/08/07/palabras-para-julia-parte-1-n/index.html",
    "href": "2021/08/07/palabras-para-julia-parte-1-n/index.html",
    "title": "Palabras para Julia ( Parte 1/n)",
    "section": "",
    "text": "A pesar del título, no voy a hablar sobre la excelente canción de los Suaves, sino del lenguaje de programación Julia. Ya en otra entrada del blog de hace un par de años comparé glmer con INLA y la librería MixedModels. Por aquel entonces la versión de Julia era la 1.0.3, ya va por la 1.6.2. Debido a reciente entrada de Carlos dónde apostaba por Julia para el larguísimo plazo, he decidido echarle un vistazo un poco más en profundidad.\nLo cierto es que me está gustando bastante el lenguaje y voy a escribir un par de entradas dónde contar alguna cosilla. Ya Carlos mencionaba que Julia corre sobre LLVM, pero también cabe mencionar que Julia tiene características más que interesantes, como multiple dispatch o tipos abstractos que permiten al desarrollador escribir código sin preocuparse demasiado por el tipado y que sea el compilador el que cree los métodos específicos. Si, has oído bien, Julia compila las funciones, por lo que tiene la doble ventaja de ser un lenguaje rápido a la vez que sencillo, bueno, su lema dice “Tan fácil como Python, tan rápido como C”.\nEn esta primera entrada voy a poner un ejemplo sencillo de cómo sería hacer un modelo de “Machín Lenin” utilizando la librería MLJ, y en el post siguiente os contaré como tener un binario para predecir usando ese modelo de forma que funcione en cualquier Linux sin importar si está basado en Debian, Centos o lo que sea, y sin necesidad de tener instalado Julia, ni docker, ni nada."
  },
  {
    "objectID": "2021/08/07/palabras-para-julia-parte-1-n/index.html#modelo-con-mlj",
    "href": "2021/08/07/palabras-para-julia-parte-1-n/index.html#modelo-con-mlj",
    "title": "Palabras para Julia ( Parte 1/n)",
    "section": "Modelo con MLJ",
    "text": "Modelo con MLJ\nMLJ es una librería que pretende servir de interfaz común a otras muchas librerías. Veamos un ejemplo de como ajustar un RandomForest implementado en la librería DecisionTree.\nLo primero, para instalar paquetes podéis mirar esto, básicamente haces\nusing Pkg\nPkg.import(\"nombre_paquete\")\nO en el REPL de Julia entras en el modo Package pulsando ] y pones add nombre_paquete . Esto bajará la librería correspondiente precompilado y la añade a ~/.julia/packages/\nVamos al ejemplo. Aunque voy a usar chunks de julia (gracias a la librería JuliaCall) en el rmarkdown dónde escribo los posts, en realidad como editor par Julia me gusta VSCode.\nLos datos de ejemplo son de la librería bootde R . puedes ver la ayuda haciendo en R\n\nlibrary(boot)\nhelp(channing)\n\nChanning House Data\nDescription\nThe channing data frame has 462 rows and 5 columns.\n\nChanning House is a retirement centre in Palo Alto, California. These data were collected between the opening of the house in 1964 until July 1, 1975. In that time 97 men and 365 women passed through the centre. For each of these, their age on entry and also on leaving or death was recorded. A large number of the observations were censored mainly due to the resident being alive on July 1, 1975 when the data was collected. Over the time of the study 130 women and 46 men died at Channing House. Differences between the survival of the sexes, taking age into account, was one of the primary concerns of this study.\n\nUsage\nchanning\nFormat\nThis data frame contains the following columns:\n\nsex\nA factor for the sex of each resident (\"Male\" or \"Female\").\n\nentry\nThe residents age (in months) on entry to the centre\n\nexit\nThe age (in months) of the resident on death, leaving the centre or July 1, 1975 whichever event occurred first.\n\ntime\nThe length of time (in months) that the resident spent at Channing House. (time=exit-entry)\n\ncens\nThe indicator of right censoring. 1 indicates that the resident died at Channing House, 0 indicates that they left the house prior to July 1, 1975 or that they were still alive and living in the centre at that date.\n\nEn Julia podemos instalar la librería RDatasets y usar esos datos\n\nusing RDatasets, MLJ\nchanning = dataset(\"boot\", \"channing\")\n\n462×5 DataFrame\n Row │ Sex     Entry  Exit   Time   Cens\n     │ Cat…    Int32  Int32  Int32  Int32\n─────┼────────────────────────────────────\n   1 │ Male      782    909    127      1\n   2 │ Male     1020   1128    108      1\n   3 │ Male      856    969    113      1\n   4 │ Male      915    957     42      1\n   5 │ Male      863    983    120      1\n   6 │ Male      906   1012    106      1\n   7 │ Male      955   1055    100      1\n   8 │ Male      943   1025     82      1\n  ⋮  │   ⋮       ⋮      ⋮      ⋮      ⋮\n 456 │ Female    986   1030     44      1\n 457 │ Female   1039   1132     93      1\n 458 │ Female    968    990     22      1\n 459 │ Female    955    990     35      1\n 460 │ Female    837    911     74      1\n 461 │ Female    861    915     54      1\n 462 │ Female    967    983     16      1\n                          447 rows omitted\n\n\nMLJ necesita que las columnas tenga los tipos correctos en scitypes. Podemos verlos con\n\nschema(channing)\n\n┌─────────┬─────────────────────────────────┬───────────────┐\n│ _.names │ _.types                         │ _.scitypes    │\n├─────────┼─────────────────────────────────┼───────────────┤\n│ Sex     │ CategoricalValue{String, UInt8} │ Multiclass{2} │\n│ Entry   │ Int32                           │ Count         │\n│ Exit    │ Int32                           │ Count         │\n│ Time    │ Int32                           │ Count         │\n│ Cens    │ Int32                           │ Count         │\n└─────────┴─────────────────────────────────┴───────────────┘\n_.nrows = 462\n\n\nQueremos modelar la variable exit. MLJ quiere la y por un lado y las X’s por otro, para eso vamos a usar la función unpack que además de permitir eso permite cambiar el tipo de las variables, y convertir la variable Cens a categórica por ejemplo\n\ny, X =  unpack(channing,\n                      ==(:Exit),            # con el doble igual seleccionamos la y\n                      !=(:Time);            # Quitamos variable Time\n                      :Exit=>Continuous,    # Convertimos al tipo correcto en scitypes\n                      :Entry=>Continuous,\n                      :Cens=>Multiclass)\n\n([909.0, 1128.0, 969.0, 957.0, 983.0, 1012.0, 1055.0, 1025.0, 1043.0, 945.0  …  905.0, 1040.0, 926.0, 1030.0, 1132.0, 990.0, 990.0, 911.0, 915.0, 983.0], 462×3 DataFrame\n Row │ Sex     Entry    Cens\n     │ Cat…    Float64  Cat…\n─────┼───────────────────────\n   1 │ Male      782.0  1\n   2 │ Male     1020.0  1\n   3 │ Male      856.0  1\n   4 │ Male      915.0  1\n   5 │ Male      863.0  1\n   6 │ Male      906.0  1\n   7 │ Male      955.0  1\n   8 │ Male      943.0  1\n  ⋮  │   ⋮        ⋮      ⋮\n 456 │ Female    986.0  1\n 457 │ Female   1039.0  1\n 458 │ Female    968.0  1\n 459 │ Female    955.0  1\n 460 │ Female    837.0  1\n 461 │ Female    861.0  1\n 462 │ Female    967.0  1\n             447 rows omitted)\n\n\nAhora ya podemos ver como se hace el modelo.\n\nTree = @load RandomForestRegressor pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\nMLJDecisionTreeInterface.RandomForestRegressor\n\ntree = Tree(n_trees = 20) # tambien se puede instanciar sin paraámetros \n\nRandomForestRegressor(\n    max_depth = -1,\n    min_samples_leaf = 1,\n    min_samples_split = 2,\n    min_purity_increase = 0.0,\n    n_subfeatures = -1,\n    n_trees = 20,\n    sampling_fraction = 0.7,\n    pdf_smoothing = 0.0,\n    rng = Random._GLOBAL_RNG())\u001b[34m @364\u001b[39m\n\n# y usar tree.n_trees = 20\n\nComo tenemos la variable Censque es categórica necesitamos codificarla, aquí entra como hacer un pipeline en MLJ, que es una de las cosas más potentes que tiene junto con los ComposingModels models que permite mezclar varios modelos.\n\n# Definimos un ContinuosEncoder, ver la ayuda con ?ContinousEncoder en el repl de julia\n\n  \n\n  # Unsupervised model for arranging all features (columns) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n  # \n  #   •  If ftr is already Continuous retain it.\n  # \n  #   •  If ftr is Multiclass, one-hot encode it.\n  # \n  #   •  If ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case\n  #      one-hot encode it.\n  # \n  #   •  If ftr is Count, replace it with coerce(ftr, Continuous).\n  # \n  #   •  If ftr is of some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n  # \n  # If drop_last=true is specified, then one-hot encoding always drops the last class indicator colum\n\nhot = ContinuousEncoder(one_hot_ordered_factors=true, drop_last=true)\n\nContinuousEncoder(\n    drop_last = true,\n    one_hot_ordered_factors = true)\u001b[34m @409\u001b[39m\n\n\nUtilizamos la macro @pipeline para encadenar el onehot y el modelo\n\npipe = @pipeline hot tree\n\nPipeline259(\n    continuous_encoder = ContinuousEncoder(\n            drop_last = true,\n            one_hot_ordered_factors = true),\n    random_forest_regressor = RandomForestRegressor(\n            max_depth = -1,\n            min_samples_leaf = 1,\n            min_samples_split = 2,\n            min_purity_increase = 0.0,\n            n_subfeatures = -1,\n            n_trees = 20,\n            sampling_fraction = 0.7,\n            pdf_smoothing = 0.0,\n            rng = Random._GLOBAL_RNG()))\u001b[34m @525\u001b[39m\n\n\nY ya podemos ajustar el modelo por ejemplo utilizando evaluate y validación cruzada\n\nevaluate(pipe, X, y, resampling=CV(nfolds=5), measure = [rmse, mae])\n\nPerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nextract:\n┌─────────────────────────────────────────┬─────────────┬───────────┬───────────\n│ measure                                 │ measurement │ operation │ per_fold ⋯\n├─────────────────────────────────────────┼─────────────┼───────────┼───────────\n│ RootMeanSquaredError()\\e[34m @216\\e[39m │ 58.7        │ predict   │ [52.3, 6 ⋯\n│ MeanAbsoluteError()\\e[34m @160\\e[39m    │ 49.4        │ predict   │ [44.6, 5 ⋯\n└─────────────────────────────────────────┴─────────────┴───────────┴───────────\n                                                                1 column omitted\n\n\nHay más medidas que se pueden listar con measures()\nTambién podemos partir en train, test y similar\n\ntrain, test = partition(eachindex(y), 0.7, shuffle=true, rng=155);\n\nAhora instanciamos el modelo con machine especificando la X, y la y\n\nmodelo = machine(pipe, X,y)\n\n\u001b[34mMachine{Pipeline259,…} @037\u001b[39m trained 0 times; caches data\n  args: \n    1:  \u001b[34mSource @999\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`\n    2:  \u001b[34mSource @545\u001b[39m ⏎ `AbstractVector{Continuous}`\n\n\nY podemos usar fit! para ajustar “in place” (en julia si la función acaba en ! es una función que actua modificando el objeto que se le pasa) sin tener que crear otra variable\n\n\nfit!(modelo, rows = train)\n\n\u001b[34mMachine{Pipeline259,…} @037\u001b[39m trained 1 time; caches data\n  args: \n    1:  \u001b[34mSource @999\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`\n    2:  \u001b[34mSource @545\u001b[39m ⏎ `AbstractVector{Continuous}`\n\n\nY ya podríamos predecir sobre test, dónde se le aplicaría el onehot encoder que hemos definido en el pipeline\n\n# En julia podemos usar sintaxis latex por ejemplo \\beta\\hat  y tabulador despues de beta y hat en vscode \n# escribe β̂ (uso juliaMono https://juliamono.netlify.app/) como tipo de letra \nŷ = predict(modelo,X[test, :])\n\n139-element Vector{Float64}:\n 1087.8833333333332\n 1081.45\n 1028.3333333333333\n 1030.8\n  949.5666666666668\n  972.25\n 1142.05\n  940.6\n  966.35\n  860.6\n    ⋮\n  882.9599999999998\n  865.4\n 1022.32\n  931.5\n  922.95\n  930.3666666666668\n 1064.5\n  953.625\n  941.7666666666668\n\n\nEn este caso hemos hecho un modelo para predecir una variable continua, cuando sea categórica existen funciones que devuelven la clase predicha o la probabilidad de cada clase.\nTambién podemos ver como varía el error según el número de árboles\n\nmodelo\n\n\u001b[34mMachine{Pipeline259,…} @037\u001b[39m trained 1 time; caches data\n  args: \n    1:  \u001b[34mSource @999\u001b[39m ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Multiclass{2}}}}`\n    2:  \u001b[34mSource @545\u001b[39m ⏎ `AbstractVector{Continuous}`\n\n\n\nr_tree = range(pipe, :(random_forest_regressor.n_trees), lower=2, upper=20)\n\ntypename(MLJBase.NumericRange)(Int64, :(random_forest_regressor.n_trees), ... )\n\n\n\n\ncurve = MLJ.learning_curve(modelo;\n                           range=r_tree,\n                           resampling=CV(nfolds=5),\n                           measure=rmse)\n\n(parameter_name = \"random_forest_regressor.n_trees\",\n parameter_scale = :linear,\n parameter_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n measurements = [63.35482459725572, 61.03552680046299, 60.24292896603508, 60.08916085779008, 59.69393001433038, 59.447125040723066, 58.86877968632394, 58.16237867467842, 58.416479729576544, 58.48856899579231, 59.080742791298974, 58.524648546408145, 58.55468564478951, 58.71039303157832, 58.862784037321894, 58.78919657683484, 58.738659245669986, 58.562389017352345, 58.71936497846932],)\n\n\n\nusing Plots\ngr() # especificamos un \n\nPlots.GRBackend()\n\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"CV estimate of RMSE error\")\n\n\n\n\n\n\n\n\nY esto es todo, en el próximo post contaré como crear el binario que nos va a permitir tener un motor de predicción para los modelos de un árbol de decisión y que funcione en cualquier linux. Casi listo para producción (o al menos una parte importante) sin tener que tener julia en dónde se vaya a utilizar."
  },
  {
    "objectID": "2021/08/28/dos-ejes-de-ordenadas-parte-2-n/index.html",
    "href": "2021/08/28/dos-ejes-de-ordenadas-parte-2-n/index.html",
    "title": "¿Dos ejes de ordenadas? (Parte 2/n)",
    "section": "",
    "text": "Siguiendo con el tema de los dos ejes de ordenadas, a mi no me gustan especialmente este tipo de gráficos, pero puedo entender que se use y, cómo dice mi amigo Raúl Vaquerizo, lo importante es que se entienda.\nVeamos un ejemplo que nos comentó Jesús Lagos dónde se suele aplicar este tipo de gráficos, se trata de los climogramas, dónde se presentan en el eje X los meses del año y en los dos ejes de ordenadas la precipitación y la temperatura.\nVeamos unos datos de Madrid en 2018, extraídos de aquí. Y veamos como queda en R.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nLeemos los datos , que son estos.\nMes,T,PP\nEnero,5.9,30.48\nFebrero,5.4,44.19\nMarzo,8.9,143.49\nAbril,13,57.67\nMayo,16.6,57.93\nJunio,22.2,32.75\nJulio,26.1,2.03\nAgosto,27.5,1.02\nSeptiembre,23.9,4.07\nOctubre,15.1,72.64\nNoviembre,9.9,54.07\nDiciembre,6,9.14\n\nmad <- read.csv(\"climograma_mad\")\nmad$Mes <- as_factor(mad$Mes)\n\nY pintamos un gráfico de dos ejes de ordenadas. Es importante elegir la transformación que le hacemos a la segunda variable “y” para que se pueda representar bien, al fin y al cabo se pinta en un sistema de coordenadas y tenemos que poner ambas variables en escala parecida.\n\n\nmulti <- 2\n\nggplot(mad, aes(x = Mes)) +\n  geom_col(aes(y = PP), fill = \"yellow\", alpha = 0.7) +\n  geom_line(aes(y = T * multi), group = 1, color = \"red\") +\n  geom_text(\n    aes(y = T * multi, label = paste(round(T, 1))),\n    vjust = 1.4,\n    color = \"darkred\"\n  ) +\n  scale_y_continuous(sec.axis = sec_axis(~ . / multi,\n    name = \"Temperatura\"\n  )) +\n  theme(\n    axis.title.y.right = element_text(\n      color = \"darkred\",\n      hjust = 0.01\n    ),\n    axis.text.y.right = element_text(\n      face = \"bold\",color = \"darkred\" )\n  )\n\n\n\n\n\n\n\n\nY la verdad es que eligiendo un factor de 2 podemos distorsionar el gráfico. Por convención lo que se suele hacer es considerar un factor que haga que al pintar en el sistema de referencia los máximos de ambas variables coincidan. Ya que este tipo de gráficos “mapea” dos variables al mismo “aesthetics”, se necesita hacer esa transformación para equiparar las variables.\n\nmulti <- max(mad$PP) / max(mad$T)\n\nggplot(mad, aes(x = Mes)) +\n  geom_col(aes(y = PP), fill = \"yellow\", alpha = 0.7) +\n  geom_line(aes(y = T * multi), group = 1, color = \"red\") +\n  geom_text(\n    aes(y = T * multi, label = paste(round(T, 1))),\n    vjust = 1.4,\n    color = \"darkred\"\n  ) +\n  scale_y_continuous(\n    name = \"Precipitaciones\",\n    # segundo eje\n    sec.axis = sec_axis(~ . / multi,\n      name = \"Temperatura\"\n    )\n  ) +\n  theme(\n    axis.title.y.right = element_text(\n      color = \"darkred\",\n      hjust = 0.01\n    ),\n    axis.text.y.right = element_text(\n      face = \"bold\", color = \"darkred\"\n    )\n  )\n\n\n\n\n\n\n\n\nY bueno, no queda mal del todo, aunque Tufte me mataría por esto.\nYo en realidad habría hecho algo como esto.\n\np1 <- mad %>% \n  ggplot(aes(x = Mes, y = T)) +\n  geom_point() + \n  geom_path(group=1) +\n  labs(title = \"Temperatura\")\n\np2 <- mad %>% \n  ggplot(aes(x = Mes, y = PP)) +\n  geom_col() +\n  labs(title = \"Precipitaciones\")\n\np1 / p2\n\n\n\n\n\n\n\n\nY si acaso, para ver la relación entre Precipitaciones y Temperatura, pues algo así.\n\nggplot(mad,aes(x = T, y = PP)) + \n  geom_point() +\n  ggrepel::geom_label_repel(aes(label = Mes), size = 3) +\n  geom_smooth() +\n  labs(title = \"Precipitaciones ~ Temperatura\")\n\n\n\n\n\n\n\n\nY bueno, poco más que decir, siguen sin gustarme los gráficos de dos ejes de ordenadas, puede que por cuestiones filosóficas de no asignar dos variables distintas a mismo “aesthetic”.\nHasta otro día."
  },
  {
    "objectID": "2021/08/18/eje-y-secundario-1-n/index.html",
    "href": "2021/08/18/eje-y-secundario-1-n/index.html",
    "title": "¿Dos ejes de ordenadas? (Parte 1/n)",
    "section": "",
    "text": "Anoche me iba a ir a la cama tras escuchar un podcast, pero al final estuve entretenido debatiendo con Raúl Vaquerizo, Alberto González Almuiña y Jesús Lagos , sobre los gráficos con dos ejes de ordenadas. Aquí os dejo el tweet que puso Raúl.\nPues yendo al post que puso Raúl construía el siguiente gráfico.\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(insuranceData)\nlibrary(ggplot2)\n\n\n# url='http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/acst_docs/glms_for_insurance_data/data/claimslong.csv'\n# df <- read.csv(url)\n\ndata(\"ClaimsLong\")\ndf <- ClaimsLong\n\nresumen <- df %>% group_by(period) %>%\n  summarise(pct_exposicion = n(),\n            frecuencia = sum(claim)/n())\n\n\ng2 <- ggplot(resumen,aes(x = period)) + \n  geom_col(aes(y = pct_exposicion), fill=\"yellow\",alpha=0.7) + \n  geom_line(aes(y=frecuencia * 500000 ), group = 1,color=\"red\") + \n  geom_text(aes(y = frecuencia * 500000, label = paste(round(frecuencia*100),'%')),\n            vjust = 1.4, color = \"red\", size = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~.* 500000, name = \"Frecuencia [%]\")) \n\ng2\n\n\n\n\n\n\n\n\nY bueno, no está mal, pero no me acaba de gustar, sobre todo porque al fin y al cabo todos sus períodos tienen el mismo valor\n\nresumen\n#> # A tibble: 3 × 3\n#>   period pct_exposicion frecuencia\n#>    <int>          <int>      <dbl>\n#> 1      1          40000      0.131\n#> 2      2          40000      0.141\n#> 3      3          40000      0.156\n\nAhora que lo pienso, casi que con la tabla valdría. Pero que pasaría si tenemos diferentes valores\n\nset.seed(155)\nresumen %<>% \n  mutate(pct_exposicion = pct_exposicion * 10*runif(3))\nresumen\n#> # A tibble: 3 × 3\n#>   period pct_exposicion frecuencia\n#>    <int>          <dbl>      <dbl>\n#> 1      1        315266.      0.131\n#> 2      2        173523.      0.141\n#> 3      3        304162.      0.156\n\n\nggplot(resumen,aes(x = period)) + \n  geom_col(aes(y = pct_exposicion), fill=\"yellow\",alpha=0.7) + \n  geom_line(aes(y=frecuencia * 500000 , group=), group = 1,color=\"red\") + \n  geom_text(aes(y = frecuencia * 500000, label = paste(round(frecuencia*100),'%')),\n            vjust = 1.4, color = \"red\", size = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~.* 500000, name = \"Frecuencia [%]\")) \n\n\n\n\n\n\n\n\nMe gusta aún menos este gráfico, yo propongo el siguiente en su lugar.\n\nresumen %>% \n  ggplot(aes(x = period, y = frecuencia)) +\n  geom_point(aes(size=pct_exposicion)) + \n  geom_line() +\n  scale_size_continuous(range=c(4, 10)) +\n  scale_y_continuous(labels = scales::percent, limits = c(0,0.2))\n\n\n\n\n\n\n\n\nDónde representamos la variable frecuencia en el eje de ordenadas y con el tamaño de los puntos representamos la otra variable."
  },
  {
    "objectID": "2021/11/01/a-dónde-va-vicente/index.html",
    "href": "2021/11/01/a-dónde-va-vicente/index.html",
    "title": "¿A dónde va Vicente?",
    "section": "",
    "text": "Cuando estamos haciendo un modelo y tratamos con variables categóricas como predictoras, hay que ser muy cuidadoso. Por ejemplo hay que tener en cuenta qué pasa cuándo tenemos un nuevo nivel en el conjunto de datos a predecir que no estaba en el de entrenamiento. Por ejemplo, si estoy utilizando un algoritmo moderno tipo xgboost, y tengo como variable predictora la provincia. ¿Qué pasa si en el conjunto de entrenamiento no tengo datos de “Granada”, pero en el de predicción si?\nEn el xgboost por defecto las categóricas se codifican con One-Hot encoder, por lo que al no tener datos de Granada en entrenamiento a la hora de predecir la fila de Granada siempre va a tirar hacia el 0, por ejemplo, un corte en uno de los árboles podría ser Almeria = 0 para la izquierda y Almeria = 1 para la derecha. Esto es lo que suelen hacer la mayoría de las implementaciones. Pero cabe preguntarse si es la mejor solución. Otra alternativa podría ser, dado que tengo que predecir para un nivel no visto en entrenamiento, podría asignarle el valor del target que había en el nodo superior. Esta decisión plantea el problema de como sigues el proceso de partición de datos del árbol. Otra posible decisión podría ser recorrer todos los caminos posibles y promediar. En el caso anterior sería ver qué predicción acaba teniendo cuando Almería = 0 y cuando Almería = 1 y promediar. Sería una solución más justa, aunque plantea el problema de tener que recorrer más ramas.\nOtra solución es en cada corte que implique a la variable categórica en cuestión, tirar hacia dónde van la mayoría de los caso “¿a dónde va Vicente? A dónde va la gente”. Esta es la solución que tiene la gente de h2o en su implementación de los Random Forest o de los Gradient Boosting. Dicen textualmente."
  },
  {
    "objectID": "2021/11/01/a-dónde-va-vicente/index.html#ejemplo",
    "href": "2021/11/01/a-dónde-va-vicente/index.html#ejemplo",
    "title": "¿A dónde va Vicente?",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nIniciamos h2o y cargamos datos\n\n## Not run: \nlibrary(h2o)\nh2o.init( max_mem_size = \"25G\")\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         4 minutes 48 seconds \n#>     H2O cluster timezone:       Europe/Madrid \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.38.0.1 \n#>     H2O cluster version age:    1 month and 28 days  \n#>     H2O cluster name:           H2O_started_from_R_jose_ltm884 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   21.82 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.2 Patched (2022-11-10 r83330)\n\nImportamos los datos del titanic. Ponemos como variables predictoras de la supervivencia, solo la clase y el sexo.\n\n\nf <- \"https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv\"\ntitanic <- h2o.importFile(f)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======================================================================| 100%\n\ntitanic['survived'] <- as.factor(titanic['survived'])\npredictors <- c(\"pclass\",\"sex\")\nresponse <- \"survived\"\n\n# convertimos la clase a factor \ntitanic$pclass <- as.factor(titanic$pclass)\nh2o.getTypes(titanic$pclass)\n#> [[1]]\n#> [1] \"enum\"\n\nPartimos en train y test\n\nsplits <- h2o.splitFrame(data =  titanic, ratios = .8, seed = 1234)\ntrain <- splits[[1]]\nvalid <- splits[[2]]\n\nh2o.table(train$pclass)\n#>   pclass Count\n#> 1      1   260\n#> 2      2   223\n#> 3      3   571\n#> \n#> [3 rows x 2 columns]\nh2o.table(train$sex)\n#>      sex Count\n#> 1 female   387\n#> 2   male   667\n#> \n#> [2 rows x 2 columns]\n\nh2o.table(valid$pclass)\n#>   pclass Count\n#> 1      1    63\n#> 2      2    54\n#> 3      3   138\n#> \n#> [3 rows x 2 columns]\n\nY ahora cambio en test para que aparezcan valores en pclass y en sex que no están en train.\n\n\nvalid$pclass = h2o.ifelse(valid$pclass == \"3\", \"unknown\", valid$pclass)\nvalid$sex    = h2o.ifelse(valid$sex == \"male\", \"unknown\", valid$sex )\n\n\nh2o.table(valid$pclass)\n#>    pclass Count\n#> 1       1    63\n#> 2       2    54\n#> 3 unknown   138\n#> \n#> [3 rows x 2 columns]\nh2o.table(valid$sex)\n#>       sex Count\n#> 1  female    79\n#> 2 unknown   176\n#> \n#> [2 rows x 2 columns]\n\nPara ver bien qué sucede con los casos en que tenemos nivel nuevo en clase y sexo nos quedamos con el siguiente conjunto de datos a predecir\n\ntest <-  valid[valid$pclass== \"unknown\" & valid$sex == \"unknown\",]\ntest\n#>    pclass survived                          name     sex age sibsp parch ticket\n#> 1 unknown        0 Abbott  Master. Eugene Joseph unknown  13     0     2    NaN\n#> 2 unknown        1 Abelseth  Mr. Olaus Jorgensen unknown  25     0     0 348122\n#> 3 unknown        0                Ali  Mr. Ahmed unknown  24     0     0    NaN\n#> 4 unknown        0   Andersen  Mr. Albert Karvin unknown  32     0     0    NaN\n#> 5 unknown        0   Andersson  Mr. Anders Johan unknown  39     1     5 347082\n#> 6 unknown        0    Andreasson  Mr. Paul Edvin unknown  20     0     0 347466\n#>      fare cabin embarked boat body           home.dest\n#> 1 20.2500  <NA>        S  NaN  NaN East Providence  RI\n#> 2  7.6500 F G63        S  NaN  NaN  Perkins County  SD\n#> 3  7.0500  <NA>        S  NaN  NaN                <NA>\n#> 4 22.5250  <NA>        S  NaN  260      Bergen  Norway\n#> 5 31.2750  <NA>        S  NaN  NaN Sweden Winnipeg  MN\n#> 6  7.8542  <NA>        S  NaN  NaN  Sweden Chicago  IL\n#> \n#> [99 rows x 14 columns]\n\n\n\nModelo xgboost\nHacemos un sólo árbol\n\nmodeloxg<-  h2o.xgboost(\n  seed = 155,\n  x = predictors, \n  y = response,\n  max_depth = 3,\n  training_frame = train,\n  ntrees =1\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nY al predecir, nos da un warning que nos dice ¡¡ojo, tengo nuevos niveles que no estaban en train!! . Aún así , no casca y devuelve una predicción\n\nh2o.predict(modeloxg, test)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#>   predict        p0        p1\n#> 1       0 0.6016703 0.3983298\n#> 2       0 0.6016703 0.3983298\n#> 3       0 0.6016703 0.3983298\n#> 4       0 0.6016703 0.3983298\n#> 5       0 0.6016703 0.3983298\n#> 6       0 0.6016703 0.3983298\n#> \n#> [99 rows x 3 columns]\nh2o.predict_leaf_node_assignment(modeloxg, test)\n#>   T1.C1\n#> 1    LL\n#> 2    LL\n#> 3    LL\n#> 4    LL\n#> 5    LL\n#> 6    LL\n#> \n#> [99 rows x 1 column]\n\nPodemos extraer información del árbol con\n\narbol_ind_xg <- h2o.getModelTree(model = modeloxg, tree_number = 1)\n\nNodesInfo <-  function(arbol_ind){\n  for (i in 1:length(arbol_ind)) {\n    info <-\n      sprintf(\n        \"Node ID %s has left child node with index %s and right child node with index %s The split feature is %s. The NA direction is %s\",\n        arbol_ind@node_ids[i],\n        arbol_ind@left_children[i],\n        arbol_ind@right_children[i],\n        arbol_ind@features[i], \n        arbol_ind@nas[i]\n      )\n    print(info)\n  }}\n\nNodesInfo(arbol_ind_xg)\n#> [1] \"Node ID 0 has left child node with index 1 and right child node with index 2 The split feature is sex.female. The NA direction is LEFT\"\n#> [1] \"Node ID 1 has left child node with index 3 and right child node with index 4 The split feature is pclass.1. The NA direction is LEFT\"\n#> [1] \"Node ID 2 has left child node with index 5 and right child node with index 6 The split feature is pclass.3. The NA direction is LEFT\"\n#> [1] \"Node ID 3 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 4 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 5 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 6 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n\nY vemos que nos da información de hacia dónde van los NA (los niveles nuevos no vistos en train) y siempre van hacia la izquierda.\nPodemos pintar el árbol. Script Y vemos que los NA, siempre van hacia los 0’s.\n\n## importo funciones , encontradas por internet, como no, para pintar el árbol\nsource(\"plot_h2o_tree.R\")\n\ntitanicDataTree_XG = createDataTree(arbol_ind_xg)\n\n\nplotDataTree(titanicDataTree_XG, rankdir = \"TB\")\n\n\n\n\n\n\n\n\nModelo h2o.gbm\nVeamos qué hace la implementación de h2o.\n\n\ngbm_h2o <-  h2o.gbm(\n  seed = 155,\n  x = predictors, \n  y = response,\n  max_depth = 3,\n  distribution = \"bernoulli\",\n  training_frame = train,\n  ntrees = 1\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\nh2o.predict(gbm_h2o, test)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#>   predict        p0        p1\n#> 1       0 0.6375663 0.3624337\n#> 2       0 0.6375663 0.3624337\n#> 3       0 0.6375663 0.3624337\n#> 4       0 0.6375663 0.3624337\n#> 5       0 0.6375663 0.3624337\n#> 6       0 0.6375663 0.3624337\n#> \n#> [99 rows x 3 columns]\nh2o.predict_leaf_node_assignment(gbm_h2o, test)\n#>   T1.C1\n#> 1   LLR\n#> 2   LLR\n#> 3   LLR\n#> 4   LLR\n#> 5   LLR\n#> 6   LLR\n#> \n#> [99 rows x 1 column]\n\nY pintando lo mismo\n\narbol_ind <- h2o.getModelTree(model = gbm_h2o, tree_number = 1)\nNodesInfo(arbol_ind )\n#> [1] \"Node ID 0 has left child node with index 1 and right child node with index 2 The split feature is sex. The NA direction is LEFT\"\n#> [1] \"Node ID 1 has left child node with index 3 and right child node with index 4 The split feature is pclass. The NA direction is LEFT\"\n#> [1] \"Node ID 2 has left child node with index 5 and right child node with index 6 The split feature is pclass. The NA direction is RIGHT\"\n#> [1] \"Node ID 3 has left child node with index 7 and right child node with index 8 The split feature is pclass. The NA direction is RIGHT\"\n#> [1] \"Node ID 11 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 12 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 6 has left child node with index 9 and right child node with index 10 The split feature is pclass. The NA direction is RIGHT\"\n#> [1] \"Node ID 13 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 14 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 15 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n#> [1] \"Node ID 16 has left child node with index -1 and right child node with index -1 The split feature is NA. The NA direction is NA\"\n\nAl pintar vemos un par de cosas curiosas, en primer lugar, h2o.gbm no ha codificado con one-hot las variables categóricas, esto permite por ejemplo que se pueden obtener reglas de corte como Izquierda:(Madrid, Barcelona, Valencia), Derecha: (resto de provincias), mientras que One Hot ese tipo de partición requiere más profundidad en el árbol. Y en segundo lugar vemos que por ejemplo los NA’s (y los nuevos niveles en test) de la variable pclass en un nodo van junto con p_class (2,3), en otro junto con p_class=1 y en otro junto p_class=3. El criterio elegido es en cada corte, los Nas y por ende los nuevos niveles no vistos en train van hacia dónde va la gente.\n\ntitanicDataTree = createDataTree(arbol_ind)\nplotDataTree(titanicDataTree, rankdir = \"TB\")\n\n Y nada más. hasta otra.\nNota: Los valores de prediction que saca plotDataTree no son las predicciones del modelo, sino las raw que saca ese árbol en particular. Como en los modelos de gradient boosting se va construyendo cada árbol sobre los errores del anterior, ni siquiera es la probabilidad en escala logit. He buscado en la docu de h2o pero no está claro qué es este valor. Eso sí, las ramas en el árbol están bien."
  },
  {
    "objectID": "2021/09/27/a-b-que/index.html",
    "href": "2021/09/27/a-b-que/index.html",
    "title": "¿A/B qué?",
    "section": "",
    "text": "Recuerdo siendo yo más bisoño cuando escuché a los marketinianos hablar del A/B testing para acá , A/B testing para allá. En mi ingenuidad pensaba que era alguna clase de rito que sólo ellos conocían, y encima lo veía como requisito en las ofertas de empleo que miraba.\nMi decepción fue mayúscula cuando me enteré que esto del A/B testing no es más que un nombre marketiniano para hacer un contraste de proporciones o contrastes de medias, vamos, un prop.test o un t.test, ya que ni siquera trataban el caso de tener varios grupos o la existencia de covariables. Ains, esas dos asignaturas en la carrera de diseño de experimentos y de ver fórmulas y sumas de cuadrados a diestro y siniestro, esos anovas, y ancovas.\nTotal que hoy vengo a contar alguna forma diferente a la de la fórmula para hacer este tipo de contrastes.\nSupongamos que tenemos los siguientes datos, inventados\n\ndf <-  data.frame( \n  exitos         = c(2, 200,  10, 20,  4,  200, 300,  20,  90,  90),\n  fracasos       = c(8, 1000, 35, 80, 20,  400, 900, 400, 230, 150) ,\n  gcontrol       = factor(c(1,0,1,1,1,0,0,0,0,0)))\n\ndf$n = df$exitos + df$fracasos\ndf\n#>    exitos fracasos gcontrol    n\n#> 1       2        8        1   10\n#> 2     200     1000        0 1200\n#> 3      10       35        1   45\n#> 4      20       80        1  100\n#> 5       4       20        1   24\n#> 6     200      400        0  600\n#> 7     300      900        0 1200\n#> 8      20      400        0  420\n#> 9      90      230        0  320\n#> 10     90      150        0  240\n\nTenemos 10 experimentos binomiales y hemos obtenido esos resultados, (podría ser por ejemplo la proporción de clientes que han contratado un producto A en 10 meses)\nPodriamos ver la proporción en cada fila\n\nlibrary(tidyverse)\n\ndf$prop <- df$exitos/df$n\ndf\n#>    exitos fracasos gcontrol    n       prop\n#> 1       2        8        1   10 0.20000000\n#> 2     200     1000        0 1200 0.16666667\n#> 3      10       35        1   45 0.22222222\n#> 4      20       80        1  100 0.20000000\n#> 5       4       20        1   24 0.16666667\n#> 6     200      400        0  600 0.33333333\n#> 7     300      900        0 1200 0.25000000\n#> 8      20      400        0  420 0.04761905\n#> 9      90      230        0  320 0.28125000\n#> 10     90      150        0  240 0.37500000\n\nggplot(df, aes(prop, fill = gcontrol)) +\n  geom_density(alpha = 0.3) +\n  labs(fill = \"Gcontrol\")\n\n\n\n\n\n\n\n\nPues como ahora me estoy volviendo bayesiano, ¿por qué no ajustar un modelo bayesiano a estos datos y obtener la posteriori de cada uno de las proporciones y de su diferencia. Vamos a ajustar lo que a veces se denomina una regresión binomial, dónde tenemos éxitos y ensayos. Normalmente la gente está acostumbrada a ajustar regresiones logísticas dónde la variable dependiente es 1 o 0, en este caso, la información está agregada, pero es equivalente.\nAl tener el número de “ensayos” de cada experimento, se va a tener en cuenta, de forma que no va a ser lo mismo un experimento con 20 ensayos que uno con 200, aun cuando tengan la misma proporción.\nUsando la librería brms y stan sería así de sencillo\n\nlibrary(brms)\nlibrary(cmdstanr)\nset_cmdstan_path(\"~/cmdstan/\")\n\nprior <-  get_prior(exitos | trials(n) ~  0 + gcontrol , \n                    data = df, \n                    family = binomial)\n\nmod_brm <-\n    brm(data = df, family = binomial,\n        exitos | trials(n) ~ 0 + gcontrol,\n        prior = prior,\n        iter = 2500, warmup = 500, cores = 6, chains = 6, \n        seed = 10, \n        backend = \"cmdstanr\")\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 5 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 5 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 6 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 6 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 1 finished in 0.1 seconds.\n#> Chain 2 finished in 0.1 seconds.\n#> Chain 3 finished in 0.1 seconds.\n#> Chain 4 finished in 0.1 seconds.\n#> Chain 5 finished in 0.1 seconds.\n#> Chain 6 finished in 0.1 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 0.1 seconds.\n#> Total execution time: 0.8 seconds.\n\n\nfixef(mod_brm) %>% \n    round(digits = 2)\n#>           Estimate Est.Error  Q2.5 Q97.5\n#> gcontrol0    -1.23      0.04 -1.31 -1.16\n#> gcontrol1    -1.39      0.19 -1.77 -1.03\n\nY ya tengo la estimación de cada proporción sin más que hacer el invlogit\n\nfixef(mod_brm) %>% \n    round(digits = 2) %>% \n  inv_logit_scaled()\n#>            Estimate Est.Error      Q2.5     Q97.5\n#> gcontrol0 0.2261814 0.5099987 0.2124868 0.2386673\n#> gcontrol1 0.1994078 0.5473576 0.1455423 0.2630841\n\nUna cosa buena de la estimación bayesiana es que tengo la posteriori completa de ambas proporciones\n\npost <- as_tibble(mod_brm)\npost\n#> # A tibble: 12,000 × 4\n#>    b_gcontrol0 b_gcontrol1 lprior  lp__\n#>          <dbl>       <dbl>  <dbl> <dbl>\n#>  1       -1.17       -1.39      0 -128.\n#>  2       -1.18       -1.38      0 -128.\n#>  3       -1.20       -1.10      0 -128.\n#>  4       -1.24       -1.45      0 -127.\n#>  5       -1.23       -1.79      0 -129.\n#>  6       -1.21       -1.60      0 -128.\n#>  7       -1.21       -1.62      0 -128.\n#>  8       -1.19       -1.68      0 -129.\n#>  9       -1.19       -1.59      0 -128.\n#> 10       -1.30       -1.54      0 -129.\n#> # … with 11,990 more rows\n\nY tenemos 2000 muestras por 6 cadenas, 12000 muestras aleatorias de cada proporción.\nAhora puedo hacer cosas como ver la distribución a posteriori de la diferencia\n\npost$diferencia = inv_logit_scaled(post$b_gcontrol1) - inv_logit_scaled(post$b_gcontrol0)\n\nggplot(post, aes(diferencia)) +\n  geom_density()\n\n\n\n\n\n\n\n\nIntervalo de credibilidad al 80%\n\nquantile(post$diferencia, probs = c(0.1,0.9))\n#>         10%         90% \n#> -0.06377131  0.01499445\n\nY si sospechamos que hay más estructura en nuestros datos podemos modelarla igulmente, por ejemplo las proporciones podrían tener relación con el mes del año o con cualquier otra cosa.\nEn fin, un método alternativo para hacer A/B testing o como se llame ahora."
  },
  {
    "objectID": "2021/09/10/los-viejos-rockeros-model-matrix/index.html",
    "href": "2021/09/10/los-viejos-rockeros-model-matrix/index.html",
    "title": "Los viejos [R]ockeros. model.matrix",
    "section": "",
    "text": "Nota: He cambiado la parte final para que hiciera lo mismo que el código de python, gracias a mi tocayo José Luis Hidalgo\nEl otro día por linkedin, mi jefe compartió el siguiente artículo recomendable por otro lado. El repo con el código y datos está aquí.\nEn el artículo hacen referencia a que una forma de ver el CATE (Conditional Average Treatmen Effect) cuando hay variables categóricas puede ser construirse los términos de interacción de alto orden entre las variables categóricas y calcular la diferencia entre la media de la variable de interés antes del tratamiento y después del tratamiento, en cada una de las variables de interacción consideradas.\nPara eso el autor del artículo hace lo siguiente\nY crea una función para crear las interacciones de orden n.\nY crea las variables, al medir cuánta tarda vemos que es en torno al minuto.\nY aquí es dónde vienen los viejos [R]ockeros. Cada vez que oigo hablar de interacciones pienso en R y en nuestras queridas fórmulas. En R podemos hacer lo mismo tirando de nuestro viejo amigo model.matrix\nConvertimos las variables que nos interesan a tipo factor\nY al utilizar model matrix R hace por defecto codificación parcial de las variables (One Hot quitando la que sobra para los modernos), así que para tener lo mismo hay que tocar un argumento de model matrix. el truco es definir para cada variable el contrasts = FALSE. Por ejemplo\nPor defecto el contrasts para una variable categórica elimina la categoría redundante.\nPero podemos decir que no, y así nos construirá tantas variables dicotómicas como categorías tenga nuestra variable.\nYa podemos crear nuestra función binarize\nPara crear interacciones de orden n en R basta con definir la fórmula ~ 0 + ( var1 + var2 + var3)^n\nY podemos medir cuanto tarda nuestra función sobre el mismo conjunto de datos. Y vemos, que en crear las variables tarda unos pocos segundos.\nY ya estaría ."
  },
  {
    "objectID": "2021/09/10/los-viejos-rockeros-model-matrix/index.html#cate",
    "href": "2021/09/10/los-viejos-rockeros-model-matrix/index.html#cate",
    "title": "Los viejos [R]ockeros. model.matrix",
    "section": "CATE",
    "text": "CATE\nLa parte interesante del artículo es la de calcular el CATE como la diferencia de medias de la variable order_value en cada uno de los segmentos antes de una determinada fecha y después.\nEn el artículo lo hacen así\n\n\nstart_time = time.time()\n\ndf_before = df[df[time_axis] <= '2019-09-11']\ndf_after  = df[df[time_axis] > '2019-09-11']\nfeatures = copy(df.drop([time_axis,kpi_axis], axis=1).columns)\n\nK = 10 \nsubgroups=[]\nscore=[]\nfor k in range(0,K):\n    CATE = []\n    y_before = df_before[kpi_axis]\n    y_after= df_after[kpi_axis]\n    \n    #compute CATEs for all subgroups\n    for d in features:\n        g = df_before[d] == True\n        m_before = np.mean(y_before[g])\n        g = df_after[d] == True\n        m_after = np.mean(y_after[g])\n        CATE.append(m_after-m_before)\n    \n    #find subgroup with biggest CATE\n    index = np.argsort(-abs(np.array(CATE)))\n    subgroups.append(features[index[0]])\n    score.append(abs( CATE [index[0]]))\n    \n    #remove found subgroups from dataset\n    df_before = df_before[df_before[features[index[0]]] == False]\n    df_after = df_after[df_after[features[index[0]]] == False] \n    features = features.drop(features[index[0]])\n    \n\ndf_nuevo = pd.DataFrame(np.array([score,subgroups]).T, columns=['CATE','features'])\n\nelapsed_time = time.time() - start_time\n\nprint(elapsed_time)\n\n39.18984651565552\n\n\n\ndf_nuevo\n\n                 CATE                                           features\n0   289.4008630608073  customer_age == 46+ AND first_order_made == ye...\n1   8.979524530417706  customer_age == 30-35 AND customer_country == ...\n2   8.690151515151513  customer_age == 36-45 AND customer_country == ...\n3   8.567118700265269  customer_age == 36-45 AND customer_country == ...\n4   7.811875000000015  customer_age == 30-35 AND customer_country == ...\n5   7.510393162393143  customer_age == 36-45 AND customer_country == ...\n6    8.40514915254235  customer_age == 36-45 AND customer_country == ...\n7   7.597928321678324  customer_age == 36-45 AND customer_country == ...\n8  7.4170337760987906  customer_age == 46+ AND customer_country == ca...\n9  7.2043861024033475  customer_age == 21-24 AND customer_country == ...\n\n\nY tarda su ratillo, pero no está mal\nEn R lo podemos hacer utilizando nuestro viejo amigo el R base para poner las condiciones\n\nCalcularCate_old <-  function(f, df){\n  \n  filtro_antes   = df[[f]] == 1 & df$corte_fecha == \"antes\"\n  filtro_despues = df[[f]] == 1 & df$corte_fecha != \"antes\"\n  \n  media_antes   = mean(df$order_value[filtro_antes])\n  media_despues = mean(df$order_value[filtro_despues])\n  \n  cate = media_despues - media_antes\n  \n  return(cate)\n  \n  \n}\n\n# usando fmean de collapse\n\nCalcularCate <-  function(f, df){\n  \n  filtro_antes   = df[[f]] == 1 & df$corte_fecha == \"antes\"\n  filtro_despues = df[[f]] == 1 & df$corte_fecha != \"antes\"\n  \n  media_antes   = fmean(df$order_value[filtro_antes])\n  media_despues = fmean(df$order_value[filtro_despues])\n  \n  cate = media_despues - media_antes\n  \n  return(cate)\n  \n  \n}\n\n\ntictoc::tic()\nK = 10\ncate = c()\ntmp <-  df_final\n\nfor ( k in 1:K) {\n  \n  features <- colnames(tmp)[3:ncol(tmp)]\n  res <-  unlist(lapply(features, function(x) CalcularCate(x, df = tmp)))\n  names(res) <- features\n  ordenado <-  sort(abs(res), decreasing = TRUE)[1]\n  f <-  names(ordenado)\n  cate <- c(cate, ordenado)\n  tmp <-  tmp[tmp[[f]]== 0, c(\"corte_fecha\", \"order_value\", setdiff(features, f))]\n}\n\n \n \ntictoc::toc()\n\n40.084 sec elapsed\n\ncate\n\n              systemandroid-tv:first_order_madeyes:customer_age46+ \n                                                        289.400863 \n                systemios-mob:customer_age30-35:customer_countryuk \n                                                          8.979525 \n     household_incomelow:customer_age36-45:customer_countrygermany \n                                                          8.690152 \n            systemwin-pc:customer_age36-45:customer_countrygermany \n                                                          8.567119 \n     household_incomehigh:customer_age30-35:customer_countrycanada \n                                                          7.811875 \n   product_categorybooks:customer_age36-45:customer_countrygermany \n                                                          7.510393 \n        systemandroid-tv:customer_age36-45:customer_countrygermany \n                                                          8.405149 \n            genderfemale:customer_age36-45:customer_countrygermany \n                                                          7.597928 \nproduct_categoryelectronics:customer_age46+:customer_countrycanada \n                                                          7.417034 \n                 systemios-pc:customer_age21-24:customer_countryuk \n                                                          7.204386 \n\n\nY bueno, parece que en este caso, los viejos [R]ockeros no lo hacen mal del todo, sobre todo la parte de model.matrix es muy rápida, y usando collapse para calcular la media es aún más rápido\nEn resumen, model.matrix de rbase es muy rápido, y usar fmean de collapse en vez del mean de R-base mejor, con lo que con esta implementación en R es mucho más rápida que la vista en python (que seguramente se puede mejorar hasta igualar)"
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html",
    "title": "Análisis de correspondencias “old_style”",
    "section": "",
    "text": "Quién me conoce sabe que siento debilidad por el análisis de datos categóricos, en particular por técnicas como el análisis de correspondencias simple o múltiple o por las cosas más modernas que hay. No en vano se me dió especialmente bien en la universidad, en parte debido a que por fin me centré después de unos años locos, y en parte debido a algún buen profesor. El caso es que en el curro utilizamos este tipo de técnicas para encontrar relaciones entre variables categóricas que quizá hayan pasado desapercibidas en un primer análisis.\nAntes de nada voy a dar un par de referencias en castellano, bastante útiles.\nDe hecho el ejemplo que voy a contar y la notación que voy a usar viene en el libro de Daniel. Es un ejemplo de Fisher (si, ese al que todo el mundo odia hoy en día) de 1940, sobre la relación entre el color de los ojos (en filas) y el color del pelo (en columnas). Se trata de una simple tabla de contingencia.\nCuyos totales por filas y columnas son"
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-filas.",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-filas.",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Proyección de las filas.",
    "text": "Proyección de las filas.\nPodríamos plantearnos la relación entre las filas (color de ojos) , ¿cómo de similares son los que tienen los ojos claros con los que tienen los ojos azules, respecto a su color del pelo?\nParece claro que deberíamos centrarnos en los porcentajes por filas (perfiles fila)\n\nprop.table(df_tabla, 1)\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646\n#> azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273\n#> castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144\n#> oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374\n\nA partir de ahora vamos a usar la tabla de frecuencias relativas, cuyos elementos llamaremos \\(f_{ij}\\)\n\n(tabla_frecuencias <-  prop.table(df_tabla))\n#>               rubio   pelirrojo    castaño     oscuro        negro\n#> claros   0.12922615 0.021788129 0.10969196 0.03531180 0.0007513148\n#> azules   0.06123216 0.007137491 0.04526672 0.02066116 0.0005634861\n#> castaños 0.06442524 0.015777611 0.17073629 0.07738542 0.0048835462\n#> oscuros  0.01840721 0.009015778 0.07569497 0.11607814 0.0159654395\n\nQue tiene los mismos porcentajes por filas.\n\n(perfiles_fila <- prop.table(tabla_frecuencias, 1))\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646\n#> azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273\n#> castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144\n#> oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374\n\nSe podría considerar utilizar la distancia euclídea para ver como de parecidas son las filas ojos claros y ojos azules, pero eso presenta un problema, que es la distribución del color del pelo en la tabla, dónde por ejemplo el porcentaje de rubios es mayor que el de pelirrojos. Así que usar esa distancia no sería justo. Podríamos definir una distancia ponderada que fuera \\(d(i, i') = \\sum_j ((f_{ij} - f_{i'j})^2/f_{.j})\\) dónde \\(f_{.j}\\) es la distribución de las columnas en la población, vamos, qué % de rubios, pelirrojos, etc hay en mis datos.\nEn forma matricial, en notación de Daniel sería \nEsta distancia es equivalente a la euclídea en la matriz Y definida como dividir cada elemento de los perfiles fila por la raíz cuadrada del peso de la columna.\n\n# peso de loas columnas\n(f.j <- colSums(tabla_frecuencias))\n#>      rubio  pelirrojo    castaño     oscuro      negro \n#> 0.27329076 0.05371901 0.40138993 0.24943651 0.02216379\n\n# peso de las filas\n(fi. <- rowSums(tabla_frecuencias))\n#>    claros    azules  castaños   oscuros \n#> 0.2967693 0.1348610 0.3332081 0.2351615\n\nPara hacerlo lo hacemos usando matrices\n\n# matriz diagonal con la raíz de los porcentjes totales de las columnas( col masses)\n(DC_12 <- diag(1/sqrt(f.j)))\n#>          [,1]     [,2]     [,3]     [,4]     [,5]\n#> [1,] 1.912879 0.000000 0.000000 0.000000 0.000000\n#> [2,] 0.000000 4.314555 0.000000 0.000000 0.000000\n#> [3,] 0.000000 0.000000 1.578399 0.000000 0.000000\n#> [4,] 0.000000 0.000000 0.000000 2.002258 0.000000\n#> [5,] 0.000000 0.000000 0.000000 0.000000 6.717041\n\n# Matriz diagonal con 1/fi. \n(DF_1 <- diag(1/fi.))\n#>         [,1]     [,2]     [,3]     [,4]\n#> [1,] 3.36962 0.000000 0.000000 0.000000\n#> [2,] 0.00000 7.415042 0.000000 0.000000\n#> [3,] 0.00000 0.000000 3.001127 0.000000\n#> [4,] 0.00000 0.000000 0.000000 4.252396\n\nMatriz Y\n\n# se puede hacer usando los perfiles fila o directamente utilizando DF_1 y DC_12\n# Y <- perfiles_fila %*% DC_12\n\nY <- DF_1 %*% as.matrix(tabla_frecuencias) %*% DC_12\nrownames(Y) <- rownames(tabla_frecuencias)\ncolnames(Y) <- colnames(tabla_frecuencias)\nY\n#>              rubio pelirrojo   castaño    oscuro      negro\n#> claros   0.8329499 0.3167648 0.5834082 0.2382433 0.01700517\n#> azules   0.8685217 0.2283469 0.5297968 0.3067526 0.02806563\n#> castaños 0.3698521 0.2042969 0.8087737 0.4650114 0.09844593\n#> oscuros  0.1497302 0.1654142 0.5080629 0.9883349 0.45602916\n\nEn esta tabla Y, la distancia euclídea entre filas coincide con la distancia ponderada que habíamos definido dónde la distancia entre dos filas venía ponderada por el peso de cada columna.\nPodríamos ahora plantearnos utilizar una descomposición en valores y vectores propios sobre esta tabla, pero tendríamos el problema de que el peso de cada fila sería el mismo, por eso se hace necesario tener en cuenta el peso de cada fila.\nPodemos construir ahora una matriz Z dónde se pondere por el peso de las filas y el de las columnas.\n\n# cremaos matriz diagonal con 1/sqrt(fi.)\n(DF_12 <- diag(1 / sqrt(fi.)))\n#>          [,1]     [,2]     [,3]     [,4]\n#> [1,] 1.835653 0.000000 0.000000 0.000000\n#> [2,] 0.000000 2.723057 0.000000 0.000000\n#> [3,] 0.000000 0.000000 1.732376 0.000000\n#> [4,] 0.000000 0.000000 0.000000 2.062134\n\nCreamos matriz Z como\n\nZ <- DF_12 %*% as.matrix(tabla_frecuencias) %*% DC_12\n\nrownames(Z) <- rownames(tabla_frecuencias)\ncolnames(Z) <- colnames(tabla_frecuencias)\nZ\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.45376229 0.17256250 0.3178206 0.1297867 0.009263827\n#> azules   0.31895094 0.08385681 0.1945596 0.1126501 0.010306662\n#> castaños 0.21349407 0.11792869 0.4668580 0.2684240 0.056827106\n#> oscuros  0.07260933 0.08021509 0.2463773 0.4792778 0.221144304\n\nSobre esta matriz Z que no es más que la tabla de frecuencias relativas estandarizada por el peso de las filas y el de las columnas podemos diagonalizar la matriz Z’Z. Esta matriz tiene un valor propio igual a 1, pero los importantes son los siguientes.\n\nres_diag <- eigen(t(Z) %*% Z)\nres_diag$values\n#> [1]  1.000000e+00  1.873957e-01  2.847581e-02  9.026139e-04 -5.424744e-18\nres_diag$vectors\n#>            [,1]        [,2]        [,3]        [,4]        [,5]\n#> [1,] -0.5227722 -0.64488569  0.50331668 -0.21984084  0.09578107\n#> [2,] -0.2317736 -0.11671207  0.06710395  0.91248080 -0.30908759\n#> [3,] -0.6335534 -0.02776729 -0.74923836 -0.04663792  0.18521831\n#> [4,] -0.4994362  0.65005451  0.30376531 -0.21371886 -0.43593979\n#> [5,] -0.1488751  0.38361288  0.29755318  0.26682945  0.81911020\n\nTeniendo la matriz Y, que son los perfiles (porcentajes) fila ponderados por el peso de las columnas, podemos proyectar esas filas sobre las dimensiones obtenidas por los vectores propios obtenidos asociados a los autovalores menores que 1. Esa será la mejor representación de las filas en un subespacio de las columnas.\n\n\n# proyeccion 1. con valor menor que 1 \nvector_propio1 <- res_diag$vectors[,2]\nvector_propio2 <- res_diag$vectors[,3]\n\n(coord1 <- Y %*% vector_propio1 )\n#>                 [,1]\n#> claros   -0.42893286\n#> azules   -0.39128686\n#> castaños  0.05523421\n#> oscuros   0.68743802\n(coord2 <- Y %*% vector_propio2 )\n#>                 [,1]\n#> claros    0.08081194\n#> azules    0.15705214\n#> castaños -0.23555524\n#> oscuros   0.14171621\n\nY ya lo puedo pintar\n\nlibrary(tidyverse)\n\n(to_plot <-  data.frame(Dim1 = coord1[,1], Dim2 = coord2[,1], color_ojos= rownames(Y)))\n#>                 Dim1        Dim2 color_ojos\n#> claros   -0.42893286  0.08081194     claros\n#> azules   -0.39128686  0.15705214     azules\n#> castaños  0.05523421 -0.23555524   castaños\n#> oscuros   0.68743802  0.14171621    oscuros\n\n\nto_plot %>% \n  ggplot(aes(x=Dim1, y=Dim2)) +\n  geom_label(aes(label= color_ojos)) +\n  scale_x_continuous(limits = c(-0.8,0.8))"
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-columnas",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#proyección-de-las-columnas",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Proyección de las columnas",
    "text": "Proyección de las columnas\nLa proyección de las columnas en un subespacio de las filas se hace de manera análoga , solo que en vez de diagonalizar Z’Z se hace con Z Z’, que tiene los mismos valores propios que los obtenidos.\nDe aquí viene la relación baricéntrica entre las filas y las columnas."
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#relación-con-la-distancia-chi-cuadrado",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#relación-con-la-distancia-chi-cuadrado",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Relación con la distancia Chi-cuadrado",
    "text": "Relación con la distancia Chi-cuadrado\nSi se desarrolla la expresión de la distancia Chi-cuadrado , tal como se hace en el libro de Daniel Peña se llega a que se corresponde con la distancia euclídea en la matriz Y que son los perfiles filas ponderados por el peso de cada columna.\nEsta implicación es importante, puesto que al descomponer el estadístico Chi-cuadrado que nos mide la asociación entre variables categóricas (filas y columnas), estamos descubriendo qué filas están asociados con determinadas columnas."
  },
  {
    "objectID": "2021/10/21/análisis-de-correspondencias-old-style/index.html#uso-con-factominer",
    "href": "2021/10/21/análisis-de-correspondencias-old-style/index.html#uso-con-factominer",
    "title": "Análisis de correspondencias “old_style”",
    "section": "Uso con FactoMineR",
    "text": "Uso con FactoMineR\nSólo trataba de dar una pequeña explicación de la relación entre el análisis de correspondencias y la diagonalización de matrices. Hay mucha más explicación en los libros que he enlazado al principio. En el día a día, podemos usar librerías específicas para calcular este análisis, como FactoMineR en R o prince en python.\nVeamos como se usa con FactoMiner\n\nlibrary(FactoMineR)\nlibrary(factoextra) # pa los dibujitos\n\n\nres_ca <- CA(df_tabla, graph = FALSE)\n\nY podemos pintar las filas en el espacio de las columnas\n\nfviz_ca_row(res_ca) +\n   scale_x_continuous(limits = c(-0.8,0.8))\n\n\n\n\n\n\n\n\nY vemos que las coordenadas son las mismas que hemos obtenido nosotros antes\n\nres_ca$row$coord\n#>                Dim 1       Dim 2        Dim 3\n#> claros   -0.42893286  0.08081194  0.032336831\n#> azules   -0.39128686  0.15705214 -0.065353061\n#> castaños  0.05523421 -0.23555524 -0.005724586\n#> oscuros   0.68743802  0.14171621  0.004781725\n\n\nto_plot\n#>                 Dim1        Dim2 color_ojos\n#> claros   -0.42893286  0.08081194     claros\n#> azules   -0.39128686  0.15705214     azules\n#> castaños  0.05523421 -0.23555524   castaños\n#> oscuros   0.68743802  0.14171621    oscuros\n\nLa representación conjunta.\n\nfviz_ca(res_ca) \n\n\n\n\n\n\n\n\nLa librería FactoMineR junto con factoextra devuelven también múltiples ayudas a la interpretación como la contribución de cada fila o columna a la estructura factorial etc. Por otro lado, la librería FactoInvestigate que toma como input un análisis factorial (pca, ca o mca), devuelve un informe en inglés (en Rmd) describiendo lo que significa cada dimensión obtenida."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Series\n\nCachitos nochevieja\nPost relacionados con extracción de imágenes y análisis de subtítulos de “cachitos nochevieja”\n\n\nJulia\nPost relacionados Julia\n\n\n\nTodos los posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\nJulia\n\n\nproduccion\n\n\nlinux\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\ndocker\n\n\nR\n\n\napi\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n36 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\nestadística\n\n\nInvestigación operativa\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n47 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\nJulia\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\nestadística\n\n\npython\n\n\nR\n\n\nInvestigación operativa\n\n\nJulia\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPluralista\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\ntextmining\n\n\nocr\n\n\n2022\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nCocinando\n\n\n\n\n\n\n\nmuestreo\n\n\n2022\n\n\nencuestas electorales\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\n\n\n\nestadística\n\n\nspark\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecturas para el finde\n\n\n\n\n\n\n\nestadística\n\n\nanálisis bayesiano\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n¿A dónde va Vicente?\n\n\n\n\n\n\n\nárboles\n\n\nciencia de datos\n\n\nh2o\n\n\nestadísticca\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\ncorrespondencias\n\n\nfactorización\n\n\nfactorial\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿A/B qué?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\n\n\n\nR\n\n\npython\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\nsoftware\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "julia.html",
    "href": "julia.html",
    "title": "Julia",
    "section": "",
    "text": "Sigo trasteando con julia\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "en_rmd.html",
    "href": "en_rmd.html",
    "title": "Correspondence Analysis",
    "section": "",
    "text": "This dataset contains 4 rows and 5 columns."
  },
  {
    "objectID": "en_rmd.html#annexes",
    "href": "en_rmd.html#annexes",
    "title": "Correspondence Analysis",
    "section": "Annexes",
    "text": "Annexes\n\nres.hcpc$desc.var\n\n$`1`\n            Intern %    glob % Intern freq Glob freq        p.value     v.test\nrubio     44.1253264 27.329076        1014       1455 2.052502e-127  24.012628\npelirrojo  6.7014795  5.371901         154        286  2.432399e-04   3.669274\ncastaño   35.9007833 40.138993         825       2137  4.252364e-08  -5.480037\nnegro      0.3046127  2.216379           7        118  8.802089e-20  -9.102814\noscuro    12.9677981 24.943651         298       1328  8.110151e-73 -18.048473\n\n$`2`\n         Intern %    glob % Intern freq Glob freq       p.value    v.test\ncastaño 51.240135 40.138993         909       2137 3.926728e-31 11.604127\noscuro  23.224352 24.943651         412       1328 4.317198e-02 -2.022042\nnegro    1.465614  2.216379          26        118 9.197007e-03 -2.604643\nrubio   19.334837 27.329076         343       1455 5.460448e-21 -9.399920\n\n$`3`\n           Intern %    glob % Intern freq Glob freq        p.value     v.test\noscuro    49.361022 24.943651         618       1328 1.792994e-105  21.811796\nnegro      6.789137  2.216379          85        118  3.994847e-29  11.201810\npelirrojo  3.833866  5.371901          48        286  5.584247e-03  -2.771245\ncastaño   32.188498 40.138993         403       2137  4.381154e-11  -6.590579\nrubio      7.827476 27.329076          98       1455  5.597942e-83 -19.297861\n\nattr(,\"class\")\n[1] \"descfreq\" \"list\"    \n\n\nFigure 5 - List of variables characterizing the clusters of the classification."
  },
  {
    "objectID": "cachitos.html",
    "href": "cachitos.html",
    "title": "Cachitos nochevieja",
    "section": "",
    "text": "Cachitos. Tercera parte\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 29, 2022\n\n\nLeaflet example\n\n\n\n\nOct 27, 2022\n\n\nAquí estoy de nuevo\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Hola a todos\nSoy José Luis Cañadas Reche, científico de datos (o estadístico que sabe algo de programación )\nMe puedes encontrar en Twitter o GitHub y en Fosstodon.\n\n\n\nBlogs antiguo y nuevo\n\nBlog nuevo\n\n\nBlog antiguo\n\n\n\nInvestigación operativa\n\nEjemplo simple\nEjemplo simple de como usar Julia, R y python para investigación operativa\n\n\nLocalización: Indios y jefes\nAsignar localizaciones según demanda y diferentes restricciones. Un problema usual"
  }
]
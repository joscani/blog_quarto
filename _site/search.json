[
  {
    "objectID": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "href": "2020/04/28/epa-muestreo-y-partial-pooling/index.html",
    "title": "EPA, muestreo y partial pooling",
    "section": "",
    "text": "Sale la EPA a a finales de Abril, con datos de Enero a Marzo. Es proverbial el retraso en la publicación de resultados por parte de las administraciones públicas. En intercambio de tweets con Carlos Gil, comentaba la posibilidad de ir actualizando datos poco a poco, en plan como las elecciones: - Al 20% del escrutinio de la EPA, el número de parados y ocupados en España es de X y cosas así.\nGracias a que la EPA tiene un buen diseño muestral no sería tan difícil hacerlo, e incluso realizar buensa estimaciones con poco escrutado, al fin y al cabo la epa es un panel,(los sujetos permanecen en la EPA varias oleadas) y es de suponer que hay cierta relación entre la variable latente “estar en paro” y que se trate del mismo individuo, y relación de esa variable con los de determinado grupo de edad al que pertenece, y que la estimación en otros grupos de edad ayude a estimar la tasa de paro en otro grupo, etc…. En fin, que me lío.\nPues de toda esa estructura e información compartida es de lo que van, grosso modo, los modelos mixtos y el partial pooling. En este mismo blog los he comentado alguna vez y he puesto algún ejemplo. Con la EPA hice hace unos años un ejercicio para ver precisamente cómo, con poca muestra, se pueden tener buenas estimaciones. Aquí os lo dejo, al final hay algunas referencias, justo las que usé (no me gusta poner referencias de cosas que no he leído solo por rellenar).\nPues nada, buen confinamiento, yo voy a ver si instalo la nueva versión de R en mi linux, dudo entre arriesgarme y hacerlo a pelo o usar un docker."
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "",
    "text": "Y seguimos dando vueltas a los datos de post anteriores. Siempre hay quien dice que el bayesiano no sirve para big data y qué se acaba el universo antes de que termine de ajustar tu modelo (esto último creo que se lo he dicho yo alguna vez a Carlos).\nPero ya hemos visto en los dos post anteriores que podemos condensar los datos en menos filas sin perder información, así que , ¿por qué no utilizar un modelo bayesiano?\nDel post anterior\nY tenemos nuestros conjuntos de train y de test en local"
  },
  {
    "objectID": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "href": "2021/06/04/big-data-para-pobres-iii-bayesiano/index.html#modelo-bayesiano.",
    "title": "Big data para pobres III. ¿Bayesiano?",
    "section": "Modelo bayesiano.",
    "text": "Modelo bayesiano.\nPues ahora vamos a probar a hacer un modelo bayesiano jerárquico, podríamos hacer el equivalente a glmer usando la librería rstanarm y ajustar varias regresiones logísticas independientes, pero en vez de eso vamos a ver como ajustar directamente la distribución multinomial usando brms.\nLos modelos serían algo así como\n\\[\n\\begin{equation} ans \\sim Multinomial(\\boldsymbol{\\theta}) \\end{equation}\n\\]\nDónde\n\\[\n\\begin{equation}\n\\boldsymbol{\\theta} = \\{\\theta_{Rec}, \\theta_{Best}, \\theta_{Neut}, \\theta_{\\text{No_way}}\\}\n\\end{equation}\n\\]\nLo bueno de stan y de brms es que se puede modelar directamente la Multinomial, es decir, el número de “éxitos” en cada categoría dado un número de intentos. En brms podemos usar trials para especificarlo. Sería el equivalente al weights en glmer. De esta forma podemos trabajar con los datos agregados en vez de tenerlos individuales. Si tengo, 1000 clientes con edad < 21 y valor_cliente = 8, en vez de poner 1000 filas, pongo una columna de frecuencias, que es lo que hemos hecho.\n\nLibrerías\nYo uso cmdstan como backend para brms en vez de rstan, está más actualizado y tarda menos en muestrear.\n\n# Core libraries\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\nlibrary(cmdstanr)\n\n# For beauty plots\nlibrary(ggridges)\n\n## Using all cores. 12 in my machine\noptions(mc.cores = parallel::detectCores())\nset_cmdstan_path(\"~/cmdstan/\")\n\n\n\nAdecuando los datos\nPara poder ajustar el modelo de regresión multinomial se necesita tener los datos de una determinada forma, básicamente tener una columna de tipo matriz. Para eso vamos a pivotar los datos y usar cbind\nPivotamos\n\ntrain_wider <-   train_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)), \n    total = Rec + Neut + Best + No_way\n  ) \n\ntest_wider <- test_local %>% \n  pivot_wider(\n    id_cols = c(tipo, valor_cliente, edad_cat),\n    names_from = segmento, \n    values_from = n) %>% \n  mutate(\n    across(where(is.numeric), ~replace_na(.x, 0)),\n    total = Rec + Neut + Best + No_way\n  )\n\n\nDT::datatable(train_wider)\n\n\n\n\n\n\nY ahora unimos las columnas que indican el conteo en cada perfil de Rec, Best, Neut y NoWay en un columna que es una matriz\n\n# lo hacemos solo para el train, para el test no hace falta\n\ntrain_wider$cell_counts <- with(train_wider, cbind(Rec, Best, Neut, No_way))\nclass(train_wider$cell_counts)\n#> [1] \"matrix\" \"array\"\n\n\nDT::datatable( train_wider %>% \n                 select(tipo, valor_cliente,\n                        cell_counts, everything()\n))\n\n\n\n\n\n\nPues ya podemos ajustar el modelo. Brms tiene una función get_prior para poner las priors por defecto.\nVoy a usar un modelo con efectos aleatorios que tarda unos pocos minutos, pero si usamos cell_counts | trials(total) ~ edad_cat + valor_cliente el modelo se ajusta en menos de 60 segundos. Bueno, vamos a verlo\n\n\nAjuste de los modelos\nModelo efectos fijos\n\nformula_efectos_fijos <- brmsformula(\n  cell_counts | trials(total) ~ edad_cat + valor_cliente\n)\n\n# get priors\npriors <- get_prior(formula_efectos_fijos, train_wider, family = multinomial())\n\ntictoc::tic(\"Modelo efectos fijos\")\nmodel_multinomial1 <- brm(formula_efectos_fijos, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\",\n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 2 finished in 47.9 seconds.\n#> Chain 3 finished in 48.8 seconds.\n#> Chain 1 finished in 49.5 seconds.\n#> Chain 4 finished in 53.0 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 49.8 seconds.\n#> Total execution time: 53.1 seconds.\ntictoc::toc()\n#> Modelo efectos fijos: 70.554 sec elapsed\n\nModelo con efectos aleatorios\nY tarda unos 9 minutos o así\n\nformula <- brmsformula(\n  cell_counts | trials(total) ~ (1|edad_cat) + (1|valor_cliente\n))\n\n# get priors\npriors <- get_prior(formula, train_wider, family = multinomial())\n\nPodemos ver las priors que ha considerado por defecto. Y vemos las priors que ha tomado para modelar la distribución de las \\(\\sigma\\) asociadas a edad_cat y valor_cliente\n\npriors\n#>                 prior     class      coef         group resp    dpar nlpar lb\n#>                (flat) Intercept                                              \n#>  student_t(3, 0, 2.5) Intercept                               muBest         \n#>  student_t(3, 0, 2.5)        sd                               muBest        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muBest        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muBest        0\n#>  student_t(3, 0, 2.5) Intercept                               muNeut         \n#>  student_t(3, 0, 2.5)        sd                               muNeut        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat       muNeut        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente       muNeut        0\n#>  student_t(3, 0, 2.5) Intercept                              muNoway         \n#>  student_t(3, 0, 2.5)        sd                              muNoway        0\n#>  student_t(3, 0, 2.5)        sd                edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept      edad_cat      muNoway        0\n#>  student_t(3, 0, 2.5)        sd           valor_cliente      muNoway        0\n#>  student_t(3, 0, 2.5)        sd Intercept valor_cliente      muNoway        0\n#>  ub       source\n#>          default\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>          default\n#>          default\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n#>     (vectorized)\n\n\ntictoc::tic(\"modelo mixto\")\nmodel_multinomial2 <- brm(formula, train_wider, multinomial(), priors,\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 10,\n  backend = \"cmdstanr\", \n  refresh = 0\n)\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 715.6 seconds.\n#> Chain 4 finished in 728.4 seconds.\n#> Chain 2 finished in 728.8 seconds.\n#> Chain 3 finished in 732.7 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 726.4 seconds.\n#> Total execution time: 732.8 seconds.\ntictoc::toc()\n#> modelo mixto: 755.055 sec elapsed\n\nPodemos ver el modelo con\n\nsummary(model_multinomial2)\n#>  Family: multinomial \n#>   Links: muBest = logit; muNeut = logit; muNoway = logit \n#> Formula: cell_counts | trials(total) ~ (1 | edad_cat) + (1 | valor_cliente) \n#>    Data: train_wider (Number of observations: 184) \n#>   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 12000\n#> \n#> Group-Level Effects: \n#> ~edad_cat (Number of levels: 5) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.95      0.45     0.43     2.14 1.00     2460\n#> sd(muNeut_Intercept)      0.55      0.31     0.23     1.40 1.00     2566\n#> sd(muNoway_Intercept)     0.55      0.29     0.24     1.38 1.00     2237\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      4370\n#> sd(muNeut_Intercept)      4061\n#> sd(muNoway_Intercept)     3810\n#> \n#> ~valor_cliente (Number of levels: 10) \n#>                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n#> sd(muBest_Intercept)      0.98      0.30     0.58     1.74 1.00     1792\n#> sd(muNeut_Intercept)      0.53      0.16     0.31     0.94 1.00     1841\n#> sd(muNoway_Intercept)     1.71      0.44     1.07     2.78 1.00     1681\n#>                       Tail_ESS\n#> sd(muBest_Intercept)      3351\n#> sd(muNeut_Intercept)      3528\n#> sd(muNoway_Intercept)     2940\n#> \n#> Population-Level Effects: \n#>                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> muBest_Intercept     -0.05      0.55    -1.15     1.01 1.00     1209     2238\n#> muNeut_Intercept      1.02      0.33     0.35     1.67 1.01     1036     2136\n#> muNoway_Intercept     0.68      0.60    -0.55     1.85 1.00      907     1612\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n\nPintarlo\n\nplot(model_multinomial2, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE incluso ver el modelo en stan\n\nmodel_multinomial2$model\n#> // generated with brms 2.18.0\n#> functions {\n#>   /* multinomial-logit log-PMF\n#>    * Args:\n#>    *   y: array of integer response values\n#>    *   mu: vector of category logit probabilities\n#>    * Returns:\n#>    *   a scalar to be added to the log posterior\n#>    */\n#>   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#>     return multinomial_lpmf(y | softmax(mu));\n#>   }\n#> }\n#> data {\n#>   int<lower=1> N; // total number of observations\n#>   int<lower=2> ncat; // number of categories\n#>   array[N, ncat] int Y; // response array\n#>   array[N] int trials; // number of trials\n#>   // data for group-level effects of ID 1\n#>   int<lower=1> N_1; // number of grouping levels\n#>   int<lower=1> M_1; // number of coefficients per level\n#>   array[N] int<lower=1> J_1; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_1_muBest_1;\n#>   // data for group-level effects of ID 2\n#>   int<lower=1> N_2; // number of grouping levels\n#>   int<lower=1> M_2; // number of coefficients per level\n#>   array[N] int<lower=1> J_2; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_2_muBest_1;\n#>   // data for group-level effects of ID 3\n#>   int<lower=1> N_3; // number of grouping levels\n#>   int<lower=1> M_3; // number of coefficients per level\n#>   array[N] int<lower=1> J_3; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_3_muNeut_1;\n#>   // data for group-level effects of ID 4\n#>   int<lower=1> N_4; // number of grouping levels\n#>   int<lower=1> M_4; // number of coefficients per level\n#>   array[N] int<lower=1> J_4; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_4_muNeut_1;\n#>   // data for group-level effects of ID 5\n#>   int<lower=1> N_5; // number of grouping levels\n#>   int<lower=1> M_5; // number of coefficients per level\n#>   array[N] int<lower=1> J_5; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_5_muNoway_1;\n#>   // data for group-level effects of ID 6\n#>   int<lower=1> N_6; // number of grouping levels\n#>   int<lower=1> M_6; // number of coefficients per level\n#>   array[N] int<lower=1> J_6; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_6_muNoway_1;\n#>   int prior_only; // should the likelihood be ignored?\n#> }\n#> transformed data {\n#>   \n#> }\n#> parameters {\n#>   real Intercept_muBest; // temporary intercept for centered predictors\n#>   real Intercept_muNeut; // temporary intercept for centered predictors\n#>   real Intercept_muNoway; // temporary intercept for centered predictors\n#>   vector<lower=0>[M_1] sd_1; // group-level standard deviations\n#>   array[M_1] vector[N_1] z_1; // standardized group-level effects\n#>   vector<lower=0>[M_2] sd_2; // group-level standard deviations\n#>   array[M_2] vector[N_2] z_2; // standardized group-level effects\n#>   vector<lower=0>[M_3] sd_3; // group-level standard deviations\n#>   array[M_3] vector[N_3] z_3; // standardized group-level effects\n#>   vector<lower=0>[M_4] sd_4; // group-level standard deviations\n#>   array[M_4] vector[N_4] z_4; // standardized group-level effects\n#>   vector<lower=0>[M_5] sd_5; // group-level standard deviations\n#>   array[M_5] vector[N_5] z_5; // standardized group-level effects\n#>   vector<lower=0>[M_6] sd_6; // group-level standard deviations\n#>   array[M_6] vector[N_6] z_6; // standardized group-level effects\n#> }\n#> transformed parameters {\n#>   vector[N_1] r_1_muBest_1; // actual group-level effects\n#>   vector[N_2] r_2_muBest_1; // actual group-level effects\n#>   vector[N_3] r_3_muNeut_1; // actual group-level effects\n#>   vector[N_4] r_4_muNeut_1; // actual group-level effects\n#>   vector[N_5] r_5_muNoway_1; // actual group-level effects\n#>   vector[N_6] r_6_muNoway_1; // actual group-level effects\n#>   real lprior = 0; // prior contributions to the log posterior\n#>   r_1_muBest_1 = sd_1[1] * z_1[1];\n#>   r_2_muBest_1 = sd_2[1] * z_2[1];\n#>   r_3_muNeut_1 = sd_3[1] * z_3[1];\n#>   r_4_muNeut_1 = sd_4[1] * z_4[1];\n#>   r_5_muNoway_1 = sd_5[1] * z_5[1];\n#>   r_6_muNoway_1 = sd_6[1] * z_6[1];\n#>   lprior += student_t_lpdf(Intercept_muBest | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNeut | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muNoway | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_5 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_6 | 3, 0, 2.5)\n#>             - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] muBest = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNeut = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muNoway = rep_vector(0.0, N);\n#>     // linear predictor matrix\n#>     array[N] vector[ncat] mu;\n#>     muBest += Intercept_muBest;\n#>     muNeut += Intercept_muNeut;\n#>     muNoway += Intercept_muNoway;\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n]\n#>                    + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNeut[n] += r_3_muNeut_1[J_3[n]] * Z_3_muNeut_1[n]\n#>                    + r_4_muNeut_1[J_4[n]] * Z_4_muNeut_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muNoway[n] += r_5_muNoway_1[J_5[n]] * Z_5_muNoway_1[n]\n#>                     + r_6_muNoway_1[J_6[n]] * Z_6_muNoway_1[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       mu[n] = transpose([0, muBest[n], muNeut[n], muNoway[n]]);\n#>     }\n#>     for (n in 1 : N) {\n#>       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#>     }\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#>   target += std_normal_lpdf(z_1[1]);\n#>   target += std_normal_lpdf(z_2[1]);\n#>   target += std_normal_lpdf(z_3[1]);\n#>   target += std_normal_lpdf(z_4[1]);\n#>   target += std_normal_lpdf(z_5[1]);\n#>   target += std_normal_lpdf(z_6[1]);\n#> }\n#> generated quantities {\n#>   // actual population-level intercept\n#>   real b_muBest_Intercept = Intercept_muBest;\n#>   // actual population-level intercept\n#>   real b_muNeut_Intercept = Intercept_muNeut;\n#>   // actual population-level intercept\n#>   real b_muNoway_Intercept = Intercept_muNoway;\n#> }\n\nViendo el código en stan que genera brms utiliza parametrización con multinomial_lpmf que toma el log de la probabilidad de la multinomial y usa softmax sobre el predictor lineal. multivariate_discrete_stan\nEn la parte de functions tiene\nreal multinomial_logit2_lpmf(int[] y, vector mu) {\n      return multinomial_lpmf(y | softmax(mu));\n  }\nY en la de model\n   for (n in 1:N) {\n      target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n     }\nY en la parte del predictor lineal mu[n] es dónde ha ido añadiendo los group levels effects.\nPor ejemplo la parte de la edad_cat para la categoría Best está en la parte de transformed parameters dónde z_1[1] se modela como normal y sd_1 como una t de student\nr_1_muBest_1 = (sd_1[1] * (z_1[1]));\nY en la parte de model va añadiendo términos al muBest que es al final el que entra en la parte de la verosimilitud.\nmuBest[n] += r_1_muBest_1[J_1[n]] * Z_1_muBest_1[n] + r_2_muBest_1[J_2[n]] * Z_2_muBest_1[n];\nAquí añade el efecto de la edad r_1_muBest_1[J_1[n]] lo multiplica por Z_1_mubest_1[n] que es el indicador en los datos de la matriz Z para los efectos aleatorios (todo igual a 1) y luego añade el efecto de la variable valor_cliente.\nLa verdad es que eel bloque model que genera brms es un poco complicado. Imagino que genera código optimizado. Para los que quieran verlo todo con stan directamente este libro tiene un ejemplo básico\nEn brms tenemos la función make_standata que nos genera los datos tal y como se los pasa a Stan.\n\ndatos_stan <- make_standata(formula, data = train_wider, \n              family = multinomial(),\n              prior =  priors)\n\n\nnames(datos_stan)\n#>  [1] \"N\"             \"Y\"             \"trials\"        \"ncat\"         \n#>  [5] \"K_muBest\"      \"X_muBest\"      \"Z_1_muBest_1\"  \"Z_2_muBest_1\" \n#>  [9] \"K_muNeut\"      \"X_muNeut\"      \"Z_3_muNeut_1\"  \"Z_4_muNeut_1\" \n#> [13] \"K_muNoway\"     \"X_muNoway\"     \"Z_5_muNoway_1\" \"Z_6_muNoway_1\"\n#> [17] \"J_1\"           \"J_2\"           \"J_3\"           \"J_4\"          \n#> [21] \"J_5\"           \"J_6\"           \"N_1\"           \"M_1\"          \n#> [25] \"NC_1\"          \"N_2\"           \"M_2\"           \"NC_2\"         \n#> [29] \"N_3\"           \"M_3\"           \"NC_3\"          \"N_4\"          \n#> [33] \"M_4\"           \"NC_4\"          \"N_5\"           \"M_5\"          \n#> [37] \"NC_5\"          \"N_6\"           \"M_6\"           \"NC_6\"         \n#> [41] \"prior_only\"\n\n\n# datos\ndatos_stan$N\n#> [1] 184\n\n# numero de niveles edad\ndatos_stan$N_1\n#> [1] 5\n\n# numero niveles valor_cliente\ndatos_stan$N_2\n#> [1] 10\n\nEn los J_1, J_2, está codificado a que nivel de edad y valor_cliente perteneces esa fila. J_3 y J_4 es igual a J_1 y J_2. Lo repite para cada categoría de respuesta.\n\ndatos_stan$J_1\n#>   [1] 1 2 2 3 4 2 4 5 1 5 3 3 4 2 2 2 3 4 1 3 4 4 5 1 2 2 3 4 5 1 1 2 3 3 4 1 3\n#>  [38] 4 4 1 1 1 2 5 1 2 4 4 5 1 1 2 4 4 5 5 2 3 4 5 2 2 4 4 4 5 5 3 4 1 4 1 3 4\n#>  [75] 1 1 2 2 3 4 5 5 5 4 5 1 3 3 4 5 1 1 3 4 5 1 1 3 5 1 2 3 3 1 4 5 3 1 1 3 1\n#> [112] 1 2 3 3 1 2 3 1 2 2 5 3 3 2 3 5 1 4 5 3 5 5 2 4 5 1 2 2 5 1 3 3 2 1 2 4 2\n#> [149] 3 1 3 4 4 1 3 4 5 5 3 4 5 2 4 5 3 4 2 5 1 4 1 2 2 1 2 3 5 2 2 4 3 2 5 3\n\n\ndatos_stan$J_2\n#>   [1]  1  1  1  1  1  2  2  3  3  3  3  3  3  4  5  5  5  5  6  6  6  6  7  7  7\n#>  [26]  7  7  7  8  8  8  8  8  8  8  9  9  9  9 10 10  1  1  2  2  2  2  2  4  3\n#>  [51]  3  3  3  3  4  4  4  4  4  5  5  5  5  5  5  6  6  6  6  7  7  8  8  8  9\n#>  [76]  9  9  9  9  9  1  1  1  1  2  2  2  2  3  4  4  4  4  4  5  5  5  5  6  6\n#> [101]  6  6  6  7  7  8 10  1  1  1  2  2  2  2  3  3  3  3  5  6  6  7  7  7  8\n#> [126]  8  9  9  9  1  1  2  2  2  2  3  4  4  4  5  5  5  5  6  8  8  8  9  9 10\n#> [151]  1  1  1  4  4  4  6  7  7  7  8  8  8  9  2  4  3  5  6  6  7  7  1  6  9\n#> [176]  9  3  3  7 10  4 10  8 10\n\nPero yo estoy interesado en ver 2 cosas, como de bien predice sobre test y cuál es la probabilidad de cada clase condicionada a cada perfil\nPredicción\nPodemos obtener o bien todas las estimaciones o resumirlas\n\npredicciones_test <-  posterior_predict(model_multinomial2, newdata = test_wider)\n\nAquí lo que tenemos es un array de dimensiones 12000, 180, 4 . Que se corresponde a tener las 12000 estimaciones ( 4 cadenas x 3000 muestras efectivas) , para las 180 filas del conjunto de test\n\ndim(predicciones_test)\n#> [1] 12000   181     4\n\nPor ejemplo para la fila 35 de test que sería\n\ntest_wider[1,]\n#> # A tibble: 1 × 8\n#>   tipo  valor_cliente edad_cat   Rec  Best  Neut No_way total\n#>   <fct>         <dbl> <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl>\n#> 1 C                 0 21- 40     141   110   336     80   667\n\nY las predicciones (de la 1 a la 20) de las 1200\n\npredicciones_test[1:20, 1, ]\n#>       Rec Best Neut No_way\n#>  [1,]  75  153  255    184\n#>  [2,]  74  151  259    183\n#>  [3,]  78  126  239    224\n#>  [4,]  88  157  224    198\n#>  [5,]  79  148  260    180\n#>  [6,]  82  134  257    194\n#>  [7,]  61  145  250    211\n#>  [8,]  60  152  257    198\n#>  [9,]  80  132  242    213\n#> [10,]  62  136  263    206\n#> [11,]  72  151  272    172\n#> [12,]  75  133  259    200\n#> [13,]  78  150  240    199\n#> [14,]  78  129  244    216\n#> [15,]  78  137  249    203\n#> [16,]  73  136  265    193\n#> [17,]  68  142  251    206\n#> [18,]  68  157  234    208\n#> [19,]  67  143  248    209\n#> [20,]  62  180  227    198\n\nComo ahora todo es tidy voy a usar tidybayespara tener esa predicción.\n\npredicciones_tidy <- test_wider %>% \n  add_epred_draws(model_multinomial2) \n\nY se nos ha quedado un dataset muy muy grande\n\ndim(predicciones_tidy)\n#> [1] 8688000      14\n\n\nDT::datatable(predicciones_tidy %>% \n                ungroup() %>% \n                sample_n(30) %>% \n                select(edad_cat, valor_cliente,.category, .epred))\n\n\n\n\n\n\nPero si quisiéramos pintar las probabilidades estimadas tendríamos que dividir el valor predicho de cada categoría por el total de clientes en cada fila del conjunto de datos de test. Hay una forma más sencilla construyendo un conjunto de datos que tenga todas las combinaciones de edad_cat y valor_cliente y añadiendo columna totalcon valor 1.\n\n\nfake_data <- test_wider %>% \n  tidyr::expand(edad_cat, valor_cliente) %>% \n  mutate(total = 1)\n\n\ndf_pintar <-  fake_data %>% \n  add_epred_draws(model_multinomial2) %>% \n  mutate(valor_cliente = as_factor(valor_cliente))\n\nDe esta forma, al tener total = 1, el modelo devuelve la probabilidad de cada clase, si total = 13, hubiera devuelto “el reparto” de esos 13 individuos en los 4 grupos\n\nDT::datatable(df_pintar %>% \n  sample_n(30) %>% \n  select(edad_cat, valor_cliente, .category, .epred))\n\n\n\n\n\n\nAñadir las 12000 predicciones por fila ya “sólo” nos deja unos 2 millones de filas\n\ndim(df_pintar)\n#> [1] 2400000       9\n\nPintemos\nPor ejemplo si queremos ver las estimaciones que le da según la edad podemos ver la distribución posteriori de la probabilidad de cada segmento condicionada a cada grupo de edad. Salen distribuciones con varias modas debido a la variable valor_cliente que no estamos representando\n\ndf_pintar %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n             ) +\n  geom_density_ridges(scale = 0.8, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() \n\n\n\n\n\n\n\n\nSi vemos la posteriori para los clientes de mayor valor. Se ve claramente que a menor edad mayor probabilidad de pertenecer al segmento “Best” , mientras que a mayor edad es mucho más probabilidad del segmento “No_way”.\n\ndf_pintar %>%  \n  filter(valor_cliente == 0) %>% \n  ggplot(aes(x=.epred, y = edad_cat, fill = .category) \n) +\n  geom_density_ridges(scale = 1.5, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Cliente valor: 0\")\n\n\n\n\n\n\n\n\nTeniendo toda la distribución podemos ver los resultados desde otro punto de vista. Por ejemplo, ver las probabilidades para los menores de 21.\n\ndf_pintar %>%  \n  filter(edad_cat %in% c(\"<21\")) %>% \n  ggplot(aes(x=.epred, y = valor_cliente, fill = .category) \n  ) +\n  geom_density_ridges(scale = 3, rel_min_height = 0.01, alpha=.4) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_ridges() + \n  labs(title = \"Clientes menores de 21\\n Probabilidades estimadas\")\n\n\n\n\n\n\n\n\nEn fin, que se puede hacer estadística bayesiana aún con grandes volúmenes de datos, si te conviertes en lo que mi amigo Antonio llama un “artesano del dato”.\nFeliz semana"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Series\n\nCachitos nochevieja\nPost relacionados con extracción de imágenes y análisis de subtítulos de “cachitos nochevieja”\n\n\nJulia\nPost relacionados Julia\n\n\n\nTodos los posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\n\n\n\nEstadística\n\n\nmachine learning\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nConformal prediction. Estilo compadre\n\n\n\n\n\n\n\nEstadística\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArrow y S3\n\n\n\n\n\n\n\nbig data\n\n\nR\n\n\nC++\n\n\nS3\n\n\nAWS\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplicatividad no usual\n\n\n\n\n\n\n\nestadística\n\n\nranger\n\n\nExplicatividad\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\n\n\n\nestadística\n\n\nbrms\n\n\nanálisis bayesiano\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2023\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsejos para dejar spss\n\n\n\n\n\n\n\nestadística\n\n\nsociología\n\n\n2022\n\n\nspss\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 2\n\n\n\n\n\n\n\napi\n\n\ndocker\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaflet example\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nAquí estoy de nuevo\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSigo trasteando con julia\n\n\n\n\n\n\n\nJulia\n\n\nproduccion\n\n\nlinux\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApi y docker con R. parte 1\n\n\n\n\n\n\n\ndocker\n\n\nR\n\n\napi\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n36 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeeelooosidad\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nC++\n\n\nRust\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\n\n\n\nestadística\n\n\nInvestigación operativa\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n47 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\n\n\n\nJulia\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nIO Parte 1\n\n\n\n\n\n\n\nestadística\n\n\npython\n\n\nR\n\n\nInvestigación operativa\n\n\nJulia\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo mentirás\n\n\n\n\n\n\n\nestadística\n\n\nmachine learning\n\n\npython\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransparente\n\n\n\n\n\n\n\nagile\n\n\nempresas\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\n\n\n\nJulia\n\n\nR\n\n\nStan\n\n\nanálisis bayesiano\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMediator. Full luxury bayes\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nestadística\n\n\ncausal inference\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollider Bias?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\ncausal inference\n\n\nestadística\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPluralista\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\ntextmining\n\n\nocr\n\n\n2022\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos 2021\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2022\n\n\ntextmining\n\n\nocr\n\n\nlinux\n\n\ncachitos\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nCocinando\n\n\n\n\n\n\n\nmuestreo\n\n\n2022\n\n\nencuestas electorales\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\n\n\n\nestadística\n\n\nspark\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecturas para el finde\n\n\n\n\n\n\n\nestadística\n\n\nanálisis bayesiano\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n¿A dónde va Vicente?\n\n\n\n\n\n\n\nárboles\n\n\nciencia de datos\n\n\nh2o\n\n\nestadísticca\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\nR\n\n\ncorrespondencias\n\n\nfactorización\n\n\nfactorial\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿A/B qué?\n\n\n\n\n\n\n\nanálisis bayesiano\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\n\n\n\nR\n\n\npython\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\n\n\n\ngráficos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2021\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\n\n\n\nJulia\n\n\nciencia de datos\n\n\nsoftware\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImputando datos. La estructura importa\n\n\n\n\n\n\n\nestadística\n\n\nimputación\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\n\n\n\nestadística\n\n\nbig data\n\n\nanálisis bayesiano\n\n\nmodelos mixtos\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\nbig data\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\n\n\n\n2021\n\n\nR\n\n\nanálisis bayesiano\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\n\n\n\nciencia de datos\n\n\nR\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\n\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2021\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nUna colina\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Tercera parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCachitos. Segunda parte\n\n\n\n\n\n\n\nestadística\n\n\npolémica\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCachitos. Primera parte\n\n\n\n\n\n\n\nestadística\n\n\nlinux\n\n\npolémica\n\n\nocr\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTendencias\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\ncausal inference\n\n\n2021\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte II\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Y si … ? Parte I\n\n\n\n\n\n\n\nestadística\n\n\ncausal inference\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplillo con NMF\n\n\n\n\n\n\n\nestadística\n\n\ncorrespondencias\n\n\nfactorización\n\n\nnmf\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\n\n\n\nestadística\n\n\nempresas\n\n\nbig data\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 4.0.2 en amazon linux\n\n\n\n\n\n\n\nR\n\n\nlinux\n\n\naws\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\n\n\n\nciencia de datos\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicción, Estimación y Atribución\n\n\n\n\n\n\n\nestadística\n\n\nciencia de datos\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\n\n\n\nestadística\n\n\nfactorial\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactoriales….\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPA, muestreo y partial pooling\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmodelos mixtos\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncuesta\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nmuestreo\n\n\nencuestas\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimación muy burda del número de contagios.\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl virus\n\n\n\n\n\n\n\nestadística\n\n\n2020\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecciones aprendidas instalando paquetes de R\n\n\n\n\n\n\n\nestadística\n\n\nR\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosas de pandas\n\n\n\n\n\n\n\nR python\n\n\npython\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinde de cacharreo\n\n\n\n\n\n\n\nsoftware\n\n\nh2o\n\n\nestadística\n\n\n2020\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa fatal arrogancia\n\n\n\n\n\n\n\nciencia de datos\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2019\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 23, 2023\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\nMar 26, 2023\n\n\nConformal prediction. Estilo compadre\n\n\n\n\nFeb 19, 2023\n\n\nArrow y S3\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\nDec 4, 2022\n\n\nConsejos para dejar spss\n\n\n\n\nOct 30, 2022\n\n\nApi y docker con R. parte 2\n\n\n\n\nOct 29, 2022\n\n\nLeaflet example\n\n\n\n\nOct 27, 2022\n\n\nAquí estoy de nuevo\n\n\n\n\nOct 26, 2022\n\n\nSigo trasteando con julia\n\n\n\n\nOct 12, 2022\n\n\nApi y docker con R. parte 1\n\n\n\n\nSep 18, 2022\n\n\nVeeelooosidad\n\n\n\n\nAug 1, 2022\n\n\nIndios y jefes, IO al servicio del mal.\n\n\n\n\nJul 1, 2022\n\n\nPalabras para Julia (Parte 4 /n). Predicción con Turing\n\n\n\n\nJun 21, 2022\n\n\nIO Parte 1\n\n\n\n\nMay 29, 2022\n\n\nNo mentirás\n\n\n\n\nApr 10, 2022\n\n\nTransparente\n\n\n\n\nMar 20, 2022\n\n\nPalabras para Julia ( Parte 3/n)\n\n\n\n\nFeb 12, 2022\n\n\nMediator. Full luxury bayes\n\n\n\n\nFeb 9, 2022\n\n\nCollider Bias?\n\n\n\n\nFeb 6, 2022\n\n\nPluralista\n\n\n\n\nJan 16, 2022\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 10, 2022\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 8, 2022\n\n\nCachitos 2021\n\n\n\n\nJan 1, 2022\n\n\nCocinando\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\nDec 30, 2020\n\n\n¿Y si … ? Parte II\n\n\n\n\nNov 15, 2020\n\n\n¿Y si … ? Parte I\n\n\n\n\nOct 21, 2020\n\n\nEjemplillo con NMF\n\n\n\n\nOct 18, 2020\n\n\nPCA I. El álgebra es tu amiga\n\n\n\n\nOct 15, 2020\n\n\nLos viejos rockeros nunca mueren\n\n\n\n\nAug 20, 2020\n\n\nR 4.0.2 en amazon linux\n\n\n\n\nJun 11, 2020\n\n\n¿PCA con ordinales y nominales? Tercera entrega. ¡ Que vienen los holandeses !\n\n\n\n\nJun 7, 2020\n\n\nPredicción, Estimación y Atribución\n\n\n\n\nJun 4, 2020\n\n\n¿PCA con ordinales? ¿Y con nominales? Segunda entrega\n\n\n\n\nJun 2, 2020\n\n\n¿PCA con ordinales? Primera entrega\n\n\n\n\nMay 24, 2020\n\n\nFactoriales….\n\n\n\n\nApr 28, 2020\n\n\nEPA, muestreo y partial pooling\n\n\n\n\nApr 8, 2020\n\n\nEncuesta\n\n\n\n\nMar 29, 2020\n\n\nEstimación muy burda del número de contagios.\n\n\n\n\nMar 10, 2020\n\n\nEl virus\n\n\n\n\nMar 1, 2020\n\n\nLecciones aprendidas instalando paquetes de R\n\n\n\n\nFeb 17, 2020\n\n\nCosas de pandas\n\n\n\n\nFeb 8, 2020\n\n\nFinde de cacharreo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2021.html",
    "href": "2021.html",
    "title": "2021",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 12, 2021\n\n\nModelos mixtos en spark. Intento 1\n\n\n\n\nDec 1, 2021\n\n\nLecturas para el finde\n\n\n\n\nNov 1, 2021\n\n\n¿A dónde va Vicente?\n\n\n\n\nOct 21, 2021\n\n\nAnálisis de correspondencias “old_style”\n\n\n\n\nSep 27, 2021\n\n\n¿A/B qué?\n\n\n\n\nSep 10, 2021\n\n\nLos viejos [R]ockeros. model.matrix\n\n\n\n\nAug 28, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 2/n)\n\n\n\n\nAug 18, 2021\n\n\n¿Dos ejes de ordenadas? (Parte 1/n)\n\n\n\n\nAug 16, 2021\n\n\nPalabras para Julia ( Parte 2/n)\n\n\n\n\nAug 7, 2021\n\n\nPalabras para Julia ( Parte 1/n)\n\n\n\n\nJun 13, 2021\n\n\nImputando datos. La estructura importa\n\n\n\n\nJun 4, 2021\n\n\nBig data para pobres III. ¿Bayesiano?\n\n\n\n\nMay 21, 2021\n\n\nBig data para pobres II. ¿AUC?\n\n\n\n\nMay 14, 2021\n\n\nCosas viejunas. O big data para pobres\n\n\n\n\nMar 27, 2021\n\n\nEstimación Bayesiana, estilo compadre\n\n\n\n\nMar 13, 2021\n\n\nPurrr, furrr, maps y future_maps\n\n\n\n\nMar 8, 2021\n\n\nAUC = Wilcoxon , de nuevo\n\n\n\n\nFeb 14, 2021\n\n\nUna colina\n\n\n\n\nJan 26, 2021\n\n\nCachitos. Tercera parte\n\n\n\n\nJan 13, 2021\n\n\nCachitos. Segunda parte\n\n\n\n\nJan 11, 2021\n\n\nCachitos. Primera parte\n\n\n\n\nJan 7, 2021\n\n\nTendencias\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2023.html",
    "href": "2023.html",
    "title": "2023",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nApr 23, 2023\n\n\nRegresión cuantil a lo machín lenin con catboost\n\n\n\n\nMar 26, 2023\n\n\nConformal prediction. Estilo compadre\n\n\n\n\nFeb 19, 2023\n\n\nArrow y S3\n\n\n\n\nJan 29, 2023\n\n\nExplicatividad no usual\n\n\n\n\nJan 21, 2023\n\n\nUna regresión de poisson, plagiando a Carlos\n\n\n\n\nJan 4, 2023\n\n\nCachitos 2022. Tercera parte\n\n\n\n\nJan 3, 2023\n\n\nCachitos 2022. Segunda parte\n\n\n\n\nJan 2, 2023\n\n\nCachitos 2022. Primera parte\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2020/04/08/encuesta/index.html",
    "href": "2020/04/08/encuesta/index.html",
    "title": "Encuesta",
    "section": "",
    "text": "Estudio serológico covid19\nEl muestreo, ese gran olvidado. Se trata de una herramienta muy útil cuando se quiere saber una o varias características de una población pero, por lo que sea, no es factible abordar a toda la población o porque se necesita tener una idea aproximada de dicha característica de forma rápida.\nBueno, pues si queremos saber el porcentaje y el número de personas que han tenido el coronavirus por provincia la herramienta adecuada es el muestreo. Se trata de elegir una muestra representativa a nivel provincial y hacerles test serológicos a todos los incluidos en esa muestra y así poder extrapolar los resultados al conjunto de la provincia. Por fin, el Instituto de Salud Carlos III junto con el INE van a realizar dicho estudio.\nEn todo muestreo hay una fase crucial, que es la del diseño muestral, tengo que decir que después de leer el documento técnico que me parece un muy buen diseño. Se trata de un diseño bietápico estratificado con un tamaño mínimo de 600 personas por provincia y dónde por ejemplo se van a hacer 5000 encuestas en Madrid.\nEl tamaño muestral total elegido, 30 mil hogares (unos 62 mil personas), y la implicación del INE garantizan la rigurosidad y la representatividad de la encuesta. Yo por mi parte, solo comentar que una vez se tengan los microdatos, existen formas de mejorar algo las estimaciones en áreas pequeñas, entendiendo áreas pequeñas a combinaciones de variables con poca representación en la muestra, por ejemplo, si quisieramos saber la proporción de mujeres contagiadas en Cádiz cuya edad esté entre 20 y 25 años. En ese caso, es probable que una estimación directa\n\\[ \\hat{Prop} = \\dfrac{\\text{Positivos en ese grupo}}{\\text{Total personas encuestadas en ese grupo}} \\] sea poco precisa debido a que haya caído poca muestra en ese grupo.\nPara estos casos puede ser útil la utilización de estimaciones con partial pooling, entrada blog. A colación de esto, hice un estudio hace unos años sobre como incluso con poca muestra las estimaciones de este tipo suelen arrojar mejores estimaciones, aquí"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html",
    "href": "2023/02/19/Arrow y S3/index.html",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#ejemplo-1",
    "href": "2023/02/19/Arrow y S3/index.html#ejemplo-1",
    "title": "Arrow y S3",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\nMostrar / ocultar código\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket <- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#>  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#>  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#>  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#> [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#> [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n\n\n\n\nMostrar / ocultar código\ntic()\nbucket <- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi <- open_dataset(bucket) \nremote_taxi\n#> FileSystemDataset with 158 Parquet files\n#> vendor_name: string\n#> pickup_datetime: timestamp[ms]\n#> dropoff_datetime: timestamp[ms]\n#> passenger_count: int64\n#> trip_distance: double\n#> pickup_longitude: double\n#> pickup_latitude: double\n#> rate_code: string\n#> store_and_fwd: string\n#> dropoff_longitude: double\n#> dropoff_latitude: double\n#> payment_type: string\n#> fare_amount: double\n#> extra: double\n#> mta_tax: double\n#> tip_amount: double\n#> tolls_amount: double\n#> total_amount: double\n#> improvement_surcharge: double\n#> congestion_surcharge: double\n#> pickup_location_id: int64\n#> dropoff_location_id: int64\n#> year: int32\n#> month: int32\ntoc()\n#> 6.287 sec elapsed\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero. (Viendo el htop de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\nMostrar / ocultar código\ntic()\nresult <- remote_taxi |> \n    filter(year == 2019, month == 1) |>\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n    ) |>\n    mutate(pct_shared = shared_trips / all_trips * 100) |>\n    collect()\n\nresult |> \n    print(n = 200)\n#> # A tibble: 1 × 3\n#>   all_trips shared_trips pct_shared\n#>       <int>        <int>      <dbl>\n#> 1   7667255      2094091       27.3\ntoc()\n#> 12.59 sec elapsed\n\n\nNo está mal , ¿verdad?"
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#ejemplo-2",
    "href": "2023/02/19/Arrow y S3/index.html#ejemplo-2",
    "title": "Arrow y S3",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\n\nMostrar / ocultar código\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun <- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds <- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n\n\nCuántos datos\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <int>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 24.383 sec elapsed\n\n\nY usando duckdb como engine de consulta .\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    to_duckdb() %>%\n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <dbl>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 33.624 sec elapsed\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente."
  },
  {
    "objectID": "2023/02/19/Arrow y S3/index.html#nota",
    "href": "2023/02/19/Arrow y S3/index.html#nota",
    "title": "Arrow y S3",
    "section": "Nota",
    "text": "Nota\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta documentación\nFeliz domingo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Muestrear no es pecado",
    "section": "",
    "text": "Hola a todos\nSoy José Luis Cañadas Reche, científico de datos (o estadístico que sabe algo de programación )\nMe puedes encontrar en Twitter o GitHub y en Fosstodon.\n\n\n\nBlogs: Últimos post y enlaces a blog antiguo y nuevo\n\nRegresión cuantil con catboost\n\n\nConformal prediction. Estilo compadre\n\n\nArrow y S3\n\n\nExplicatividad no usual\n\n\nBlog nuevo\n\n\nBlog antiguo\n\n\n\nInvestigación operativa\n\nEjemplo simple\nEjemplo simple de como usar Julia, R y python para investigación operativa\n\n\nLocalización: Indios y jefes\nAsignar localizaciones según demanda y diferentes restricciones. Un problema usual"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html",
    "href": "2023/02/19/Arrow_y_s3/index.html",
    "title": "Arrow y S3",
    "section": "",
    "text": "Apache Arrow está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería polars está escrita en Rust.\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R. Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de Danielle Navarro que os dejo aquí . Y el segundo ejemplo es como ver lo mismo pero con unos datos fake que he dejado en un bucket de S3 (del que no puedo poner la dirección)"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#ejemplo-1",
    "href": "2023/02/19/Arrow_y_s3/index.html#ejemplo-1",
    "title": "Arrow y S3",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\nMostrar / ocultar código\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket <- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#>  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#>  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#>  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#> [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#> [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n\n\n\n\nMostrar / ocultar código\ntic()\nbucket <- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi <- open_dataset(bucket) \nremote_taxi\n#> FileSystemDataset with 158 Parquet files\n#> vendor_name: string\n#> pickup_datetime: timestamp[ms]\n#> dropoff_datetime: timestamp[ms]\n#> passenger_count: int64\n#> trip_distance: double\n#> pickup_longitude: double\n#> pickup_latitude: double\n#> rate_code: string\n#> store_and_fwd: string\n#> dropoff_longitude: double\n#> dropoff_latitude: double\n#> payment_type: string\n#> fare_amount: double\n#> extra: double\n#> mta_tax: double\n#> tip_amount: double\n#> tolls_amount: double\n#> total_amount: double\n#> improvement_surcharge: double\n#> congestion_surcharge: double\n#> pickup_location_id: int64\n#> dropoff_location_id: int64\n#> year: int32\n#> month: int32\ntoc()\n#> 8.916 sec elapsed\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero. (Viendo el htop de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\nMostrar / ocultar código\ntic()\nresult <- remote_taxi |> \n    filter(year == 2019, month == 1) |>\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n    ) |>\n    mutate(pct_shared = shared_trips / all_trips * 100) |>\n    collect()\n\nresult |> \n    print(n = 200)\n#> # A tibble: 1 × 3\n#>   all_trips shared_trips pct_shared\n#>       <int>        <int>      <dbl>\n#> 1   7667255      2094091       27.3\ntoc()\n#> 12.982 sec elapsed\n\n\nNo está mal , ¿verdad?"
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#ejemplo-2",
    "href": "2023/02/19/Arrow_y_s3/index.html#ejemplo-2",
    "title": "Arrow y S3",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\n\nMostrar / ocultar código\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun <- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds <- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n\n\nCuántos datos\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <int>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 33.974 sec elapsed\n\n\nY usando duckdb como engine de consulta .\n\n\nMostrar / ocultar código\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    to_duckdb() %>%\n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <dbl>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 38.924 sec elapsed\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente."
  },
  {
    "objectID": "2023/02/19/Arrow_y_s3/index.html#nota",
    "href": "2023/02/19/Arrow_y_s3/index.html#nota",
    "title": "Arrow y S3",
    "section": "Nota",
    "text": "Nota\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta documentación\nFeliz domingo"
  },
  {
    "objectID": "2020/03/29/estimación-muy-burda-del-número-de-contagios/index.html",
    "href": "2020/03/29/estimación-muy-burda-del-número-de-contagios/index.html",
    "title": "Estimación muy burda del número de contagios.",
    "section": "",
    "text": "Leo por ahí estimaciones de que hay en España más de 1 millón de contagiados y la verdad es que no tengo ni idea. Pero no se me ocurre ir poniendo ese dato por ahí como verdad absoluta, como hacen algunos .\n\n\nHagamos un ejercicio simple y muy burdo, lo reconozco. Supongamos que el número de fallecidos por coronavirus está bien recogido, lo miro en mi dashboard que para eso lo hice y me dice que hoy 29 de Marzo hay un total acumulado de 6528 fallecidos.\n\n\nLos de las estimaciones de más de un millón de contagiados me dicen que usan una tasa de letalidad global del 2% ergo, le saldrían que esos 6528 se corresponderían con una población de contagiados de 326400, bastante lejos del más de un millón..\n\n\nQue si, que ya sé que desde que te contagias hasta que te recuperas pueden pasar de 6 a 22 días, pero aún así me parece que no se puede dar esa cifra tan alegremente.\n\n\nOtras estimaciones algo mas serias, también burdas, pero al menos se reconoce y se intenta medir la incertidumbre dan una cifra (si se ejecuta el modelo) entre 150 mil y 300 mil contagiados, aquí y el github con el código\n\n\nEn fin, yo voy a hacer el ejercicio de aplicar unas tasas de letalidad por edad, por ver qué sale. Ya aviso, que esto tiene la validez que tiene , aunque no creo que mucha menos que la de la empresa del millón.\n\n\nLeemos fallecidos oficiales por edad y sexo del repo de datadista (muchas gracias )\n\nlibrary(tidyverse)\n## ── Attaching packages ───────────────────────────────────────────────────────────── tidyverse 1.3.0.9000 ──\n## ✓ ggplot2 3.3.0     ✓ purrr   0.3.3\n## ✓ tibble  2.1.3     ✓ dplyr   0.8.5\n## ✓ tidyr   1.0.2     ✓ stringr 1.4.0\n## ✓ readr   1.3.1     ✓ forcats 0.5.0\n## ── Conflicts ───────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\ndat1 <- read_csv(\"https://raw.githubusercontent.com/datadista/datasets/master/COVID%2019/nacional_covid19_rango_edad.csv\")\n## Parsed with column specification:\n## cols(\n##   fecha = col_date(format = \"\"),\n##   rango_edad = col_character(),\n##   sexo = col_character(),\n##   casos_confirmados = col_double(),\n##   hospitalizados = col_double(),\n##   ingresos_uci = col_double(),\n##   fallecidos = col_double()\n## )\n\nLas tasas de letalidad que vamos a utilizar vienen de un estudio en China, estudio\n\n\n\n\n\n\nAge\n\n\n(deaths/cases)\n\n\nCFR (95% CI)\n\n\n\n\n\n\n≤ 9 years\n\n\n(0/416)\n\n\n0%\n\n\n\n\n10 to 19 years\n\n\n(1/549)\n\n\n0.18% (0.03 to 1.02%)\n\n\n\n\n20 to 49 years\n\n\n(63/19790)\n\n\n0.32% (0.25% to 0.41%)\n\n\n\n\n50 to 59 years\n\n\n(130/10,008)\n\n\n1.3% (1.1% to 1.5%)\n\n\n\n\n60 to 69. years\n\n\n(309/8583)\n\n\n3.6% (3.2% to 4.0%)\n\n\n\n\n70 to 79 years\n\n\n(312/3918)\n\n\n8.0% (7.2% to 8.9%)\n\n\n\n\n≥80 years\n\n\n(208/1408)\n\n\n14.8% (13.0% to 16.7%)\n\n\n\n\n\ntmp <- dat1 %>%\n    filter(rango_edad != \"Total\" & sexo == \"ambos\")  %>%\n    mutate(rango_edad_2 =\n               fct_collapse(rango_edad,\n                   \"20-49\" = c(\"20-29\",\"30-39\",\"40-49\"),\n                   \">= 80\"  = c(\"80 y +\",\"80-89\",\"90 y +\")\n               )\n           ) %>% \n    group_by(rango_edad_2) %>% \n    summarise(fallecidos = sum(fallecidos))\n\ntmp$tasa_letalidad <-  c(0, 0.18, 0.32, 1.3, 3.6,8, 14.8)/100\n\nY nos saldría nuestra estimación estilo compadre de esta forma\n\n(tmp <-  tmp %>% \n    mutate(contagiados_estim = fallecidos / tasa_letalidad) )\n## # A tibble: 7 x 4\n##   rango_edad_2 fallecidos tasa_letalidad contagiados_estim\n##   <fct>             <dbl>          <dbl>             <dbl>\n## 1 0-9                   0         0                   NaN \n## 2 10-19                 7         0.0018             3889.\n## 3 20-49               194         0.0032            60625 \n## 4 50-59               245         0.013             18846.\n## 5 60-69               797         0.036             22139.\n## 6 70-79              2253         0.08              28162.\n## 7 >= 80              5966         0.148             40311.\n\nY un total de contagiados de\n\nsum(tmp$contagiados_estim, na.rm=T)\n## [1] 173972.2\n\nEn fin, y oigan, si con el número de fallecidos el número de contagiados es el que dice esa empresa, no sería tan mala noticia, significaría que la letalidad es más baja de lo que dicen ciertos estudios."
  },
  {
    "objectID": "2020/03/10/el-virus/index.html",
    "href": "2020/03/10/el-virus/index.html",
    "title": "El virus",
    "section": "",
    "text": "En estos tiempos tan asépticos ya no estamos acostumbrados (en algunos países), a tratar con agentes patógenos altamente contagiosos como el que llena los titulares de periódicos y televisiones estos días.\nSin más, vamos a comparar los datos de España e Italia, plagiando con total descaro a mi amigo Carlos Gil que puso este post de ayer y en este de hoy.\nYo me acabo de enterar de que estoy en cuarentena preventiva por un posible contagio de la mujer de un compañero, así que en casita a teletrabajar unos días.\nEl código\nA pintar.\nSi quitamos 9 días a la fecha de España vemos que estamso alineados a como estaba Itala en ese día.\nY bueno, sigo plagiando a Carlos."
  },
  {
    "objectID": "2020/03/10/el-virus/index.html#mapa-en-leaflet",
    "href": "2020/03/10/el-virus/index.html#mapa-en-leaflet",
    "title": "El virus",
    "section": "\nMapa en leaflet\n",
    "text": "Mapa en leaflet\n\n\nY ahora un mapita con los datos del último día, del 9 de Marzo en el momento de escribir estas líneas\n\ncvirus_map_data <- cvirus_longer %>%\n  filter(fecha == max(fecha))\n\n\npal <- colorNumeric(\n  palette = \"Reds\",\n  domain = c(-1, log(max(cvirus_map_data$casos + 1)))\n)\n\n\nleaflet(cvirus_map_data) %>%\n  # addProviderTiles('CartoDB.Positron') %>%\n  addProviderTiles(\"Stamen.Toner\") %>%\n  addCircleMarkers(\n    lng = ~Long,\n    lat = ~Lat,\n    label = ~ paste0( país, \": \", casos ),\n    radius = ~ 3 * log( casos + 1 ) ,\n    color = ~ pal(log( casos + 1 ) )\n  ) \n\n\n\n\n\nY me he quedado con ganas de hacer un mapa con mapview del estilo de estos, mapview, pero la verdad es que entre unas cosas y otras hoy ando bastante cansado. Mañana lo hago."
  },
  {
    "objectID": "2020/03/01/lecciones-aprendidas-instalando-paquetes-de-r/index.html",
    "href": "2020/03/01/lecciones-aprendidas-instalando-paquetes-de-r/index.html",
    "title": "Lecciones aprendidas instalando paquetes de R",
    "section": "",
    "text": "Ay, la nube.. que bien suena ¿verdad? Si, hasta que te toca pelearte con amazonlinux y versiones viejunas de R. Total, que me ha tocado lidiar un poco con la versión de R 3.4.1 de hace ya 3 años y tener que compilar en mi máquina un montón de librerías para amazon linux (que viene siendo un centos 7 modificado por aws)\nAsí que lo primero es montarse un Dockerfile dónde id diciendo qué librerías de sistemas hay que añadir, y alguna ñapa por problemas con el compilador de C.\nFROM amazonlinux:2018.03-with-sources\nMAINTAINER canadasreche@gmail.com \n\n# Update yum\nRUN yum -y update \n\n# set locales\nRUN echo \"LANG=en_US.utf8\" >> /etc/locale.conf\n#RUN localedef -c -f UTF-8 -i en_US en_US.UTF-8\nRUN export LC_ALL=en_US.UTF-8\n\n\n# Install system libraries\n# El make -j 8 es para que al compilar en c use 9 jobs\nRUN export MAKE='make -j 8'\nRUN yum install -y xorg-x11-xauth.x86_64 xorg-x11-server-utils.x86_64 xterm libXt libX11-devel \\\nlibXt-devel libcurl-devel git compat-gmp4 compat-libffi5 libxml2-devel libjpeg-devel openssl-devel \\\nboost boost-devel autoconf flex bison libssh2-devel java-1.8.0-openjdk java-1.8.0-openjdk-devel \\\nfontconfig-devel cairo-devel\n\n# Development tools \nRUN yum groupinstall 'Development Tools' -y\n\n# Install and update R\nRUN yum install -y R-core R-base R-core-devel R-devel\nRUN yum update -y R-core R-base R-core-devel R-devel\n\n\n# ENV JAVA_HOME /usr/java/latest\n\n# Fix problem with c compiler\nRUN mkdir ~/.R\nRUN echo \"CC=gcc64\" >> ~/.R/Makevars\n\nCMD [\"bash\"] \nY ahora una vez que nos ponemos en el directorio dónde tenemos el dockerfile, lo construimos con\ndocker build -t amazon-linux-r .\nSi todo ha ido bien, ya tenemos nuestra imagen de docker de amazon linux con R 3.4.1 instalado.\nCreamos y entramos en un container de esa imagen dónde adjuntamos un volumen (carpeta que se va a compartir entre mi máquina y el docker)\n docker run --rm -it -v ~/Descargas/libcentosR-3.4.1:/libR amazon-linux-r /bin/bash\nY listo ya estamos preparados para instalar paquetes\nEntramos en R y lo primero que hacemos es cambiar el .libPaths , para que todo lo que instalemos se quede en la carpeta que compartimos\n\n\nMostrar / ocultar código\n.libPaths(\"/libR\")\n\n\nComo me acabo de comprar un portátil con 6 cores, establezco la variable de entorno MAKE para que el código de C se compile usando 6 jobs. Esto hará que la instalación de la mayoría de librerías vaya mucho más rápida.\n\n\nMostrar / ocultar código\nSys.setenv(MAKE = \"make -j 6\")\n\n\nComo la versión de R que hay en amazon linux es viejuna (junio de 2017) y como hubo un cambio drástico en la versión 3.5 necesitamos hacer una vuelta al pasado para tener los repos de CRAN que habia en ese momento. Para eso, en primer lugar instalamos la librería checkpoint que nos va a facilitar el trabajo. Con esta librería podemos apuntar a los repos de CRAN que había en una fecha determinada. En realidad apuntamos a un repo de microsoft que hace mirror diarios del CRAN.\n\n\nMostrar / ocultar código\ninstall.packages(\"checkpoint\")\nlibrary(checkpoint)\n\n# apuntamos justo al repo que había antes de la verión  de R 3.5\nsetSnapshot(\"2018-03-31\")\n\n\nY ahora ya podemos instalar las librerías, por ejemplo estas.\n\n\nMostrar / ocultar código\n\nlist.of.packages <- c(\n  \"BayesFactor\", \"C50\", \"car\", \"caret\", \"catboost\",\n  \"coin\", \"cowplot\", \"DALEX\", \"DALEXtra\", \"DataExplorer\", \"dqrng\",\n  \"drifter\", \"EIX\", \"emmeans\", \"factoextra\", \"FactoMineR\", \"FFTrees\",\n  \"flextable\", \"forecast\", \"gdtools\", \"ggforce\", \"ggiraph\", \"ggiraphExtra\",\n  \"ggpubr\", \"glmnet\", \"highcharter\", \"iBreakDown\", \"igraph\", \"imbalance\",\n  \"iml\", \"ingredients\", \"inum\", \"KernelKnn\", \"libcoin\", \"lime\",\n  \"lme4\", \"minqa\", \"ModelMetrics\", \"multcomp\", \"mvtnorm\", \"networkD3\",\n  \"party\", \"partykit\", \"pbkrtest\", \"plotrix\", \"prediction\", \"randomForestExplainer\",\n  \"ranger\", \"RcppArmadillo\", \"RcppEigen\", \"RMySQL\", \"RSpectra\",\n  \"sitmo\", \"sjPlot\", \"sjstats\", \"smotefamily\",\n  \"survey\", \"systemfonts\", \"threejs\", \"uwot\", \"xgb2sql\",\n  \"xgboost\", \"yarrr\", \"ztable\", \"tcltk\"\n)\n\nnew.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, \"Package\"])]\nif (length(new.packages)) install.packages(new.packages)\n\n\nY una vez que estén instadas podemos hacer una vuelta al futuro y actualizar las que podamos.\n\n\nMostrar / ocultar código\n\nsetSnapshot(\"2018-03-31\")\nupdate.packages(ask=FALSE)\n\n\nY esto es todo, parece sencillo, pero me ha quitado bastante tiempo todas estas pequeñas vicisitudes.."
  },
  {
    "objectID": "2020/02/17/cosas-de-pandas/index.html",
    "href": "2020/02/17/cosas-de-pandas/index.html",
    "title": "Cosas de pandas",
    "section": "",
    "text": "Estoy usando Rmarkdown así que primero defino que versión o entorno de python quiero\n\nSys.setenv(RETICULATE_PYTHON = \"/usr/bin/python3\")\n\n\nimport pandas as pd\ndf = pd.DataFrame({'A' : [1, 2, 3, 4], 'B' : [4, 3, 2, 1]})\ndf\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n\n\nEl index es autonumérico\n\ndf.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nPues ahora viene lo raro, al menos para mí. Si hacemos iloc O:3 me devuelve las 3 primeras filas (como debe ser)\n\ndf.iloc[0:3,]\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n\n\nPero si hacemos loc poniendo 0:3 me devuelve 4 filas !!!\n\ndf.loc[0:3, ]\n\n   A  B\n0  1  4\n1  2  3\n2  3  2\n3  4  1\n\n\n¿Algún pythoniso que me pueda aclarar el porqué de este comportamiento?"
  },
  {
    "objectID": "2020/02/08/finde-de-cacharreo/index.html",
    "href": "2020/02/08/finde-de-cacharreo/index.html",
    "title": "Finde de cacharreo",
    "section": "",
    "text": "Bueno, pues he cambiado de portátil. Me he dado un capricho y me he pillado un slimbook prox 15 con 6 cores (12 hilos) , 32 Gb de RAM y una tarjeta gráfica nvidia de las normalitas.\nPues después de algunos (muchos) problemas con los drivers de la tarjeta nvidia en linux, más específicamente en linux mint xfce, he conseguido que todo vaya bien, y hasta he podido probar computación con gpu en R.\nHe probado h2o4gpu y también keras tirando de tensorflow, tirando de la docu de rstudio y de este pequeño tutorial para ver que al menos funcionaba.\nPues visto que ya va todo suave me dispongo a ver El gran carnaval y disfruta del sábado con el gran Kirk Douglas.\nSalud y no os flipéis mucho con el Deep Lenin, que tampoco es para tanto."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html",
    "href": "2023/01/29/explicatividad_no_usual/index.html",
    "title": "Explicatividad no usual",
    "section": "",
    "text": "Buscando en el portátil sobre otras cosas me he encontrado un pequeño ejercicio para implementar la idea que se comenta aquí\nLa idea es muy sencilla, tal y como comenta Carlos. Si tengo un modelo que sea tipo randomForest\n\nDe cada observación a predecir se anota en qué nodo terminal ha caído en cada árbol\nPara cada nodo terminal en cada árbol se recorre el camino hacia “arriba” para saber qué variables están implicadas en ese nodo\nSe cuenta para cada nodo terminal el número de veces que aparece una variable en cada árbol (o se da más importancia a las qeu estén más altos en el árbol)\nSe agrega de alguna manera para cada observación qué variables y cómo de importantes están en los caminos de los nodos terminales en los que han caído.\nEsa info sería la importancia de las variables a nivel individual\nSe podría clusterizar observaciones con similares variables importantes\n\nAntes de nada, sí, ya sé que existen cosas como los shap values y que a partir de ellos se puede hacer algo parecido. Pero no está de más de vez en cuando buscarse uno las habichueleas de forma más artesanal.."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-1",
    "href": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-1",
    "title": "Explicatividad no usual",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nPues ale, vamos a hacerlo con iris, tan denostado hoy en día. Pobre Fisher o Anderson si levantaran la cabeza.\n\n\nMostrar / ocultar código\nlibrary(tidyverse)\nlibrary(ranger)\n\n# ya veremos para que usamos furrr y FactoMineR\nlibrary(furrr) \nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\n\n\nMostrar / ocultar código\n\n# 5 arbolitos tiene mi.. \nset.seed(47)\nrg_iris <-  ranger(Species ~ . , data = iris, num.trees = 5)\n\n\nInfo del árbol 3\n\n\nMostrar / ocultar código\n(arbol3 <- treeInfo(rg_iris, tree = 3))\n#>    nodeID leftChild rightChild splitvarID splitvarName splitval terminal\n#> 1       0         1          2          0 Sepal.Length     5.55    FALSE\n#> 2       1         3          4          2 Petal.Length     2.45    FALSE\n#> 3       2         5          6          3  Petal.Width     1.70    FALSE\n#> 4       3        NA         NA         NA         <NA>       NA     TRUE\n#> 5       4         7          8          2 Petal.Length     4.25    FALSE\n#> 6       5         9         10          1  Sepal.Width     3.60    FALSE\n#> 7       6        11         12          0 Sepal.Length     6.00    FALSE\n#> 8       7        NA         NA         NA         <NA>       NA     TRUE\n#> 9       8        NA         NA         NA         <NA>       NA     TRUE\n#> 10      9        13         14          2 Petal.Length     4.95    FALSE\n#> 11     10        NA         NA         NA         <NA>       NA     TRUE\n#> 12     11        15         16          2 Petal.Length     4.85    FALSE\n#> 13     12        NA         NA         NA         <NA>       NA     TRUE\n#> 14     13        NA         NA         NA         <NA>       NA     TRUE\n#> 15     14        17         18          3  Petal.Width     1.55    FALSE\n#> 16     15        NA         NA         NA         <NA>       NA     TRUE\n#> 17     16        NA         NA         NA         <NA>       NA     TRUE\n#> 18     17        NA         NA         NA         <NA>       NA     TRUE\n#> 19     18        19         20          2 Petal.Length     5.45    FALSE\n#> 20     19        NA         NA         NA         <NA>       NA     TRUE\n#> 21     20        NA         NA         NA         <NA>       NA     TRUE\n#>    prediction\n#> 1        <NA>\n#> 2        <NA>\n#> 3        <NA>\n#> 4      setosa\n#> 5        <NA>\n#> 6        <NA>\n#> 7        <NA>\n#> 8  versicolor\n#> 9   virginica\n#> 10       <NA>\n#> 11     setosa\n#> 12       <NA>\n#> 13  virginica\n#> 14 versicolor\n#> 15       <NA>\n#> 16 versicolor\n#> 17  virginica\n#> 18  virginica\n#> 19       <NA>\n#> 20 versicolor\n#> 21  virginica\n\n\nAnalizando un poco, vemos que el nodo raíz (0) se parte por la variable Sepal.Length. Luego el nodo 1 se bifurca a la izquierda hacia el 3 y a la derecha hacia el 4, siendo Petal.Length la variable que decide esa partición.\nLa idea sería recorrer el árbol partiendo de un nodo terminal y ver qué camino ha seguido. Para eso hacemos el siguiente código\nHacemos un ejemplo, partiendo del nodo terminal 8\nCamino del Nodo 8\n\n\nMostrar / ocultar código\nnodo_terminal <-  8\n\nnodos <- nodo_terminal # vamos a ir sobreescribiendo la variable nodos hasta llegar al nodo raíz 0 \nvariables <- vector() # guardamos el nombre de las variables de split\n\n  while (!0 %in% nodos) {\n    tmp <- arbol3 %>%\n      filter(leftChild %in% nodos |\n        rightChild %in% nodos)\n    \n    print(str_glue(\"Nodo hijo: {nodos}\"))\n    \n    nodos <- unique(tmp$nodeID)\n    print(str_glue(\"Nodo padre: {nodos}\"))\n    \n    \n    print(str_glue(\"variable de split en nodo padre: {tmp$splitvarName}\"))\n    \n    variables <- c(variables, unique(tmp$splitvarName)) # la última variable de este vector es la que está más arriba en el árbol\n\n  }\n#> Nodo hijo: 8\n#> Nodo padre: 4\n#> variable de split en nodo padre: Petal.Length\n#> Nodo hijo: 4\n#> Nodo padre: 1\n#> variable de split en nodo padre: Petal.Length\n#> Nodo hijo: 1\n#> Nodo padre: 0\n#> variable de split en nodo padre: Sepal.Length\n\n\nY vemos que para llegar al nodo terminal 8 ha utilizado dos veces la variable Petal.Length y una la variable Sepal.Length\nNos creamos una funcioncita para esto, donde al final construyo un data.frame donde guardo eel nodo terminal que estamos investigando, las variables que se han usado para llegar a ese nodo y una variable peso que se calcula asignando un peso igual 1 a la variable que está más alta en el árbol y menos a las demás. Si hay 4 variables se crea un vector c(4,3,2,1) en orden de más alta en el árbol a más baja y se divide por el número de variables. así se tendrían estos pesos 1, 0.75, 0.5, 0.25\n\n\nMostrar / ocultar código\n\nextraerVariables_nodos <- function(nodo_terminal, info_arbol) {\n  nodos <- nodo_terminal\n  variables <- vector()\n\n  while (!0 %in% nodos) {\n    tmp <- info_arbol %>%\n      filter(leftChild %in% nodos |\n        rightChild %in% nodos)\n\n    variables <- c(variables, unique(tmp$splitvarName))\n\n    nodos <- unique(tmp$nodeID)\n  }\n\n  return(\n      data.frame(\n          nodo_terminal = nodo_terminal,\n          variables = variables,\n          peso = seq_along(variables) / sum(length(variables))\n      )\n  )\n}\n\n\nComprobamos\n\n\nMostrar / ocultar código\n\nextraerVariables_nodos(nodo_terminal = 8, info_arbol = arbol3)\n#>   nodo_terminal    variables      peso\n#> 1             8 Petal.Length 0.3333333\n#> 2             8 Petal.Length 0.6666667\n#> 3             8 Sepal.Length 1.0000000\n\n\nOk. Lo suyo sería extraer la misma info pero para cada nodo terminal del árbol que estamos considerando. Pues nos creamos la funcioncita, que dado un modelo y un número de árbol, saque la info anterior para todos los nodos terminales\n\n\nMostrar / ocultar código\n\nextraerVariablePorArbol <- function(modelo, arbol, verbose = FALSE) {\n  \n  info_arbol <- treeInfo(modelo, arbol)\n  nodos_terminales <- treeInfo(modelo, arbol) %>%\n    filter(terminal == TRUE) %>%\n    pull(nodeID) %>%\n    unique()\n if(verbose) print(nodos_terminales)\n  \n  variables_por_arbol <- map_df(\n    nodos_terminales,\n    function(nodos) {\n      extraerVariables_nodos(nodos, info_arbol)\n    }\n  )\n\n  variables_por_arbol$arbol <- arbol\n  variables_por_arbol\n}\n\n\nComprobemos\n\n\nMostrar / ocultar código\n# arbol 3 \n(importancia_individual_arbol3 <- extraerVariablePorArbol(rg_iris, 3))\n#>    nodo_terminal    variables      peso arbol\n#> 1              3 Petal.Length 0.5000000     3\n#> 2              3 Sepal.Length 1.0000000     3\n#> 3              7 Petal.Length 0.3333333     3\n#> 4              7 Petal.Length 0.6666667     3\n#> 5              7 Sepal.Length 1.0000000     3\n#> 6              8 Petal.Length 0.3333333     3\n#> 7              8 Petal.Length 0.6666667     3\n#> 8              8 Sepal.Length 1.0000000     3\n#> 9             10  Sepal.Width 0.3333333     3\n#> 10            10  Petal.Width 0.6666667     3\n#> 11            10 Sepal.Length 1.0000000     3\n#> 12            12 Sepal.Length 0.3333333     3\n#> 13            12  Petal.Width 0.6666667     3\n#> 14            12 Sepal.Length 1.0000000     3\n#> 15            13 Petal.Length 0.2500000     3\n#> 16            13  Sepal.Width 0.5000000     3\n#> 17            13  Petal.Width 0.7500000     3\n#> 18            13 Sepal.Length 1.0000000     3\n#> 19            15 Petal.Length 0.2500000     3\n#> 20            15 Sepal.Length 0.5000000     3\n#> 21            15  Petal.Width 0.7500000     3\n#> 22            15 Sepal.Length 1.0000000     3\n#> 23            16 Petal.Length 0.2500000     3\n#> 24            16 Sepal.Length 0.5000000     3\n#> 25            16  Petal.Width 0.7500000     3\n#> 26            16 Sepal.Length 1.0000000     3\n#> 27            17  Petal.Width 0.2000000     3\n#> 28            17 Petal.Length 0.4000000     3\n#> 29            17  Sepal.Width 0.6000000     3\n#> 30            17  Petal.Width 0.8000000     3\n#> 31            17 Sepal.Length 1.0000000     3\n#> 32            19 Petal.Length 0.1666667     3\n#> 33            19  Petal.Width 0.3333333     3\n#> 34            19 Petal.Length 0.5000000     3\n#> 35            19  Sepal.Width 0.6666667     3\n#> 36            19  Petal.Width 0.8333333     3\n#> 37            19 Sepal.Length 1.0000000     3\n#> 38            20 Petal.Length 0.1666667     3\n#> 39            20  Petal.Width 0.3333333     3\n#> 40            20 Petal.Length 0.5000000     3\n#> 41            20  Sepal.Width 0.6666667     3\n#> 42            20  Petal.Width 0.8333333     3\n#> 43            20 Sepal.Length 1.0000000     3\n\n\nSolo queda extraer lo mismo pero para cada arbolito\n\n\nMostrar / ocultar código\nextraerVariablesPorModelo <- function(modelo, parallel = TRUE) {\n  \n  \n  arboles <- modelo$num.trees\n\n  if (parallel) {\n    # Si hay muchos árboles usamos procesamiento en paralelo\n    future::plan(multisession)\n    \n    furrr::future_map_dfr(\n      seq_len(arboles),\n      function(arbol) {\n        extraerVariablePorArbol(modelo, arbol = arbol)\n      }\n    )\n  } else{\n      map_df(\n          seq_len(arboles),\n          function(arbol) {\n              extraerVariablePorArbol(modelo, arbol = arbol)\n          }\n      )  \n  }\n}\n\n\n\n\nMostrar / ocultar código\n(importancia_individual_todos_arboles <-  extraerVariablesPorModelo(rg_iris, parallel = FALSE))\n#>     nodo_terminal    variables      peso arbol\n#> 1               1  Petal.Width 1.0000000     1\n#> 2               5 Petal.Length 0.3333333     1\n#> 3               5  Petal.Width 0.6666667     1\n#> 4               5  Petal.Width 1.0000000     1\n#> 5               8  Petal.Width 0.3333333     1\n#> 6               8  Petal.Width 0.6666667     1\n#> 7               8  Petal.Width 1.0000000     1\n#> 8               9  Petal.Width 0.2500000     1\n#> 9               9 Petal.Length 0.5000000     1\n#> 10              9  Petal.Width 0.7500000     1\n#> 11              9  Petal.Width 1.0000000     1\n#> 12             10  Petal.Width 0.2500000     1\n#> 13             10 Petal.Length 0.5000000     1\n#> 14             10  Petal.Width 0.7500000     1\n#> 15             10  Petal.Width 1.0000000     1\n#> 16             11 Petal.Length 0.2500000     1\n#> 17             11  Petal.Width 0.5000000     1\n#> 18             11  Petal.Width 0.7500000     1\n#> 19             11  Petal.Width 1.0000000     1\n#> 20             12 Petal.Length 0.2500000     1\n#> 21             12  Petal.Width 0.5000000     1\n#> 22             12  Petal.Width 0.7500000     1\n#> 23             12  Petal.Width 1.0000000     1\n#> 24              3  Petal.Width 0.5000000     2\n#> 25              3 Sepal.Length 1.0000000     2\n#> 26              4  Petal.Width 0.5000000     2\n#> 27              4 Sepal.Length 1.0000000     2\n#> 28              8  Sepal.Width 0.3333333     2\n#> 29              8  Petal.Width 0.6666667     2\n#> 30              8 Sepal.Length 1.0000000     2\n#> 31             10 Sepal.Length 0.3333333     2\n#> 32             10  Petal.Width 0.6666667     2\n#> 33             10 Sepal.Length 1.0000000     2\n#> 34             11 Petal.Length 0.2500000     2\n#> 35             11  Sepal.Width 0.5000000     2\n#> 36             11  Petal.Width 0.7500000     2\n#> 37             11 Sepal.Length 1.0000000     2\n#> 38             13  Sepal.Width 0.2500000     2\n#> 39             13 Sepal.Length 0.5000000     2\n#> 40             13  Petal.Width 0.7500000     2\n#> 41             13 Sepal.Length 1.0000000     2\n#> 42             14  Sepal.Width 0.2500000     2\n#> 43             14 Sepal.Length 0.5000000     2\n#> 44             14  Petal.Width 0.7500000     2\n#> 45             14 Sepal.Length 1.0000000     2\n#> 46             15  Sepal.Width 0.2000000     2\n#> 47             15 Petal.Length 0.4000000     2\n#> 48             15  Sepal.Width 0.6000000     2\n#> 49             15  Petal.Width 0.8000000     2\n#> 50             15 Sepal.Length 1.0000000     2\n#> 51             17  Sepal.Width 0.1666667     2\n#> 52             17  Sepal.Width 0.3333333     2\n#> 53             17 Petal.Length 0.5000000     2\n#> 54             17  Sepal.Width 0.6666667     2\n#> 55             17  Petal.Width 0.8333333     2\n#> 56             17 Sepal.Length 1.0000000     2\n#> 57             18  Sepal.Width 0.1666667     2\n#> 58             18  Sepal.Width 0.3333333     2\n#> 59             18 Petal.Length 0.5000000     2\n#> 60             18  Sepal.Width 0.6666667     2\n#> 61             18  Petal.Width 0.8333333     2\n#> 62             18 Sepal.Length 1.0000000     2\n#> 63              3 Petal.Length 0.5000000     3\n#> 64              3 Sepal.Length 1.0000000     3\n#> 65              7 Petal.Length 0.3333333     3\n#> 66              7 Petal.Length 0.6666667     3\n#> 67              7 Sepal.Length 1.0000000     3\n#> 68              8 Petal.Length 0.3333333     3\n#> 69              8 Petal.Length 0.6666667     3\n#> 70              8 Sepal.Length 1.0000000     3\n#> 71             10  Sepal.Width 0.3333333     3\n#> 72             10  Petal.Width 0.6666667     3\n#> 73             10 Sepal.Length 1.0000000     3\n#> 74             12 Sepal.Length 0.3333333     3\n#> 75             12  Petal.Width 0.6666667     3\n#> 76             12 Sepal.Length 1.0000000     3\n#> 77             13 Petal.Length 0.2500000     3\n#> 78             13  Sepal.Width 0.5000000     3\n#> 79             13  Petal.Width 0.7500000     3\n#> 80             13 Sepal.Length 1.0000000     3\n#> 81             15 Petal.Length 0.2500000     3\n#> 82             15 Sepal.Length 0.5000000     3\n#> 83             15  Petal.Width 0.7500000     3\n#> 84             15 Sepal.Length 1.0000000     3\n#> 85             16 Petal.Length 0.2500000     3\n#> 86             16 Sepal.Length 0.5000000     3\n#> 87             16  Petal.Width 0.7500000     3\n#> 88             16 Sepal.Length 1.0000000     3\n#> 89             17  Petal.Width 0.2000000     3\n#> 90             17 Petal.Length 0.4000000     3\n#> 91             17  Sepal.Width 0.6000000     3\n#> 92             17  Petal.Width 0.8000000     3\n#> 93             17 Sepal.Length 1.0000000     3\n#> 94             19 Petal.Length 0.1666667     3\n#> 95             19  Petal.Width 0.3333333     3\n#> 96             19 Petal.Length 0.5000000     3\n#> 97             19  Sepal.Width 0.6666667     3\n#> 98             19  Petal.Width 0.8333333     3\n#> 99             19 Sepal.Length 1.0000000     3\n#> 100            20 Petal.Length 0.1666667     3\n#> 101            20  Petal.Width 0.3333333     3\n#> 102            20 Petal.Length 0.5000000     3\n#> 103            20  Sepal.Width 0.6666667     3\n#> 104            20  Petal.Width 0.8333333     3\n#> 105            20 Sepal.Length 1.0000000     3\n#> 106             1  Petal.Width 1.0000000     4\n#> 107             5  Petal.Width 0.3333333     4\n#> 108             5 Petal.Length 0.6666667     4\n#> 109             5  Petal.Width 1.0000000     4\n#> 110             6  Petal.Width 0.3333333     4\n#> 111             6 Petal.Length 0.6666667     4\n#> 112             6  Petal.Width 1.0000000     4\n#> 113             9 Sepal.Length 0.2500000     4\n#> 114             9 Petal.Length 0.5000000     4\n#> 115             9 Petal.Length 0.7500000     4\n#> 116             9  Petal.Width 1.0000000     4\n#> 117            10 Sepal.Length 0.2500000     4\n#> 118            10 Petal.Length 0.5000000     4\n#> 119            10 Petal.Length 0.7500000     4\n#> 120            10  Petal.Width 1.0000000     4\n#> 121            12  Petal.Width 0.2500000     4\n#> 122            12 Petal.Length 0.5000000     4\n#> 123            12 Petal.Length 0.7500000     4\n#> 124            12  Petal.Width 1.0000000     4\n#> 125            13 Sepal.Length 0.2000000     4\n#> 126            13  Petal.Width 0.4000000     4\n#> 127            13 Petal.Length 0.6000000     4\n#> 128            13 Petal.Length 0.8000000     4\n#> 129            13  Petal.Width 1.0000000     4\n#> 130            14 Sepal.Length 0.2000000     4\n#> 131            14  Petal.Width 0.4000000     4\n#> 132            14 Petal.Length 0.6000000     4\n#> 133            14 Petal.Length 0.8000000     4\n#> 134            14  Petal.Width 1.0000000     4\n#> 135             1 Petal.Length 1.0000000     5\n#> 136             6  Sepal.Width 0.3333333     5\n#> 137             6  Petal.Width 0.6666667     5\n#> 138             6 Petal.Length 1.0000000     5\n#> 139             8 Petal.Length 0.3333333     5\n#> 140             8  Petal.Width 0.6666667     5\n#> 141             8 Petal.Length 1.0000000     5\n#> 142            10 Petal.Length 0.2500000     5\n#> 143            10  Sepal.Width 0.5000000     5\n#> 144            10  Petal.Width 0.7500000     5\n#> 145            10 Petal.Length 1.0000000     5\n#> 146            11 Sepal.Length 0.2500000     5\n#> 147            11 Petal.Length 0.5000000     5\n#> 148            11  Petal.Width 0.7500000     5\n#> 149            11 Petal.Length 1.0000000     5\n#> 150            12 Sepal.Length 0.2500000     5\n#> 151            12 Petal.Length 0.5000000     5\n#> 152            12  Petal.Width 0.7500000     5\n#> 153            12 Petal.Length 1.0000000     5\n#> 154            14 Sepal.Length 0.2000000     5\n#> 155            14 Petal.Length 0.4000000     5\n#> 156            14  Sepal.Width 0.6000000     5\n#> 157            14  Petal.Width 0.8000000     5\n#> 158            14 Petal.Length 1.0000000     5\n#> 159            15 Petal.Length 0.1666667     5\n#> 160            15 Sepal.Length 0.3333333     5\n#> 161            15 Petal.Length 0.5000000     5\n#> 162            15  Sepal.Width 0.6666667     5\n#> 163            15  Petal.Width 0.8333333     5\n#> 164            15 Petal.Length 1.0000000     5\n#> 165            16 Petal.Length 0.1666667     5\n#> 166            16 Sepal.Length 0.3333333     5\n#> 167            16 Petal.Length 0.5000000     5\n#> 168            16  Sepal.Width 0.6666667     5\n#> 169            16  Petal.Width 0.8333333     5\n#> 170            16 Petal.Length 1.0000000     5\n\n\nAhora ya tenemos qué variables llevan a cada nodo terminal en cada árbol e incluso un peso que vale 1 si la variable es la primera en el “camino” hacia el nodo\nPero lo que nosotros queremos es para cada observación que predecimos, ver su nodo terminal en cada árbol y pegarle las variables importantes en cada nodo.\nSería algo así.\n\n\nMostrar / ocultar código\n# lo hacems de momento con todo iris, en la realidad serían los datos de test o el conjunto de datos a predecir. \n\n\nnodos_terminales <- predict(rg_iris, iris, type = \"terminalNodes\")$predictions\n\n# cada fila corresponde a una observación y cada columna al nodo terminal en cada árbol\nhead(nodos_terminales, 10 )\n#>       [,1] [,2] [,3] [,4] [,5]\n#>  [1,]    1    3    3    1    1\n#>  [2,]    1    3    3    1    1\n#>  [3,]    1    3    3    1    1\n#>  [4,]    1    3    3    1    1\n#>  [5,]    1    3    3    1    1\n#>  [6,]    1    3    3    1    1\n#>  [7,]    1    3    3    1    1\n#>  [8,]    1    3    3    1    1\n#>  [9,]    1    3    3    1    1\n#> [10,]    1    3    3    1    1\n\n\nLo ponemos de otra forma.\n\n\nMostrar / ocultar código\n# añadimos el id de la fila\nnodos_terminales_df <- nodos_terminales %>% \n    as.data.frame() %>% \n    rownames_to_column(var = \"id\")\n  \n  \ncolnames(nodos_terminales_df)[-1] <- 1:(ncol(nodos_terminales_df)-1)\n\nhead(nodos_terminales_df)\n#>   id 1 2 3 4 5\n#> 1  1 1 3 3 1 1\n#> 2  2 1 3 3 1 1\n#> 3  3 1 3 3 1 1\n#> 4  4 1 3 3 1 1\n#> 5  5 1 3 3 1 1\n#> 6  6 1 3 3 1 1\n\n\nPivotamos para facilitar luego las agregaciones por observaciones\n\n\nMostrar / ocultar código\n nodos_terminales_df <- nodos_terminales_df %>% \n    tidyr::pivot_longer( colnames(nodos_terminales_df)[-1], names_to = \"arbol\", values_to = \"nodo_terminal\")\n\nhead(nodos_terminales_df)\n#> # A tibble: 6 × 3\n#>   id    arbol nodo_terminal\n#>   <chr> <chr>         <dbl>\n#> 1 1     1                 1\n#> 2 1     2                 3\n#> 3 1     3                 3\n#> 4 1     4                 1\n#> 5 1     5                 1\n#> 6 2     1                 1\n\n\na la importancia en todos los árboles lo llamo info_modelo\n\n\nMostrar / ocultar código\n  \ninfo_modelo <-  importancia_individual_todos_arboles\ninfo_modelo$arbol <- as.character(info_modelo$arbol)\n\nhead(info_modelo)\n#>   nodo_terminal    variables      peso arbol\n#> 1             1  Petal.Width 1.0000000     1\n#> 2             5 Petal.Length 0.3333333     1\n#> 3             5  Petal.Width 0.6666667     1\n#> 4             5  Petal.Width 1.0000000     1\n#> 5             8  Petal.Width 0.3333333     1\n#> 6             8  Petal.Width 0.6666667     1\n\n\nHacemos el join con la info de cada nodo terminal para cada observación con las variables que llevan a cada nodo terminal (en cada árbol)\n\n\nMostrar / ocultar código\nfinal <- nodos_terminales_df %>% \n    left_join(info_modelo, by = c(\"nodo_terminal\", \"arbol\"))\n\n# para el individuo 30\n\nfinal %>% \n    filter(id == 30)\n#> # A tibble: 7 × 5\n#>   id    arbol nodo_terminal variables     peso\n#>   <chr> <chr>         <dbl> <chr>        <dbl>\n#> 1 30    1                 1 Petal.Width    1  \n#> 2 30    2                 3 Petal.Width    0.5\n#> 3 30    2                 3 Sepal.Length   1  \n#> 4 30    3                 3 Petal.Length   0.5\n#> 5 30    3                 3 Sepal.Length   1  \n#> 6 30    4                 1 Petal.Width    1  \n#> 7 30    5                 1 Petal.Length   1\n\n\nAgregamos la info para cada individuo, de forma que contemos cuántas veces aparece cada variable, sumamos los pesos y ordenamos\n\n\nMostrar / ocultar código\nres <- final %>%\n    group_by(id, variables) %>%\n    summarise(\n      total = n(),\n      ponderado = sum(peso)) %>%\n    group_by(id) %>%\n    mutate(\n      importancia_caso = total / sum(total),\n      importancia_ponderada = ponderado / sum(ponderado)\n    ) %>% \n    top_n(10, importancia_ponderada) %>% \n    ungroup() %>% \n    arrange(as.numeric(id), desc(importancia_ponderada))\n\n\n\n\nMostrar / ocultar código\nres %>% \n    filter(id == 30)\n#> # A tibble: 3 × 6\n#>   id    variables    total ponderado importancia_caso importancia_ponderada\n#>   <chr> <chr>        <int>     <dbl>            <dbl>                 <dbl>\n#> 1 30    Petal.Width      3       2.5            0.429                 0.417\n#> 2 30    Sepal.Length     2       2              0.286                 0.333\n#> 3 30    Petal.Length     2       1.5            0.286                 0.25\n\n\nY esa sería la importancia de las variables específica para la observación 30"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#todo-junto.",
    "href": "2023/01/29/explicatividad_no_usual/index.html#todo-junto.",
    "title": "Explicatividad no usual",
    "section": "Todo junto.",
    "text": "Todo junto.\nNos podemos crear una funcioncita que lo haga todo.\n\n\nMostrar / ocultar código\ngetIndividualImportance <-  function(modelo, data, top = modelo$num.independent.variables, ...){\n    \n params_ellipsis <- list(...)\n  \n  # get terminalNodes\n  nodos_terminales <- predict(modelo, data, type = \"terminalNodes\")$predictions\n  \n  nodos_terminales_df <- nodos_terminales %>% \n    as.data.frame() \n  nodos_terminales_df$id <- rownames(data)\n  nodos_terminales_df <-  nodos_terminales_df %>%\n      dplyr::select(id, everything())\n      \n  \n  \n  colnames(nodos_terminales_df)[-1] <- 1:(ncol(nodos_terminales_df)-1)\n  \n  nodos_terminales_df <- nodos_terminales_df %>% \n    tidyr::pivot_longer( colnames(nodos_terminales_df)[-1], names_to = \"arbol\", values_to = \"nodo_terminal\")\n  \n  # get variables_path for each tree and terminal node\n  info_modelo <-  extraerVariablesPorModelo(modelo, parallel = params_ellipsis$parallel)\n  info_modelo$arbol <- as.character(info_modelo$arbol)\n  \n  # join both\n  \n  final <- nodos_terminales_df %>% \n    left_join(info_modelo, by = c(\"nodo_terminal\", \"arbol\"))\n  \n  res <- final %>%\n    group_by(id, variables) %>%\n    summarise(\n      total = n(),\n      ponderado = sum(peso)) %>%\n    group_by(id) %>%\n    # para poder comparar luego observaciones, para cadda individuo, divido las veces qeu \n     # aparece una variable por el total de veces que han aparecido todas sus variables\n    mutate(\n      importancia_caso = total / sum(total),\n      importancia_ponderada = ponderado / sum(ponderado)\n    ) %>% \n    top_n(top, importancia_ponderada) %>% \n    ungroup() %>% \n    arrange(as.numeric(id), desc(importancia_ponderada))\n  \n}\n\n\nY comprobamos\n\n\nMostrar / ocultar código\nexplicatividad_iris <-  getIndividualImportance(rg_iris, iris, parallel = TRUE)\n\n\n\n\nMostrar / ocultar código\nDT::datatable(explicatividad_iris)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#agrupando-observaciones-con-similar-importancia-de-variables",
    "href": "2023/01/29/explicatividad_no_usual/index.html#agrupando-observaciones-con-similar-importancia-de-variables",
    "title": "Explicatividad no usual",
    "section": "Agrupando observaciones con similar importancia de variables",
    "text": "Agrupando observaciones con similar importancia de variables\nPodríamos hacer ahora un PCA pero yo voy a utilizar un CA usando la importancia_ponderada\n\n\nMostrar / ocultar código\n\n  tabla_para_diagonalizar <- xtabs(ponderado ~ id+ variables, data= explicatividad_iris)\n  tabla_para_diagonalizar\n#>      variables\n#> id    Petal.Length Petal.Width Sepal.Length Sepal.Width\n#>   1      1.5000000   2.5000000    2.0000000   0.0000000\n#>   10     1.5000000   2.5000000    2.0000000   0.0000000\n#>   100    2.5000000   5.1666667    2.0000000   1.3333333\n#>   101    2.5833333   5.2500000    2.6666667   0.0000000\n#>   102    2.8333333   5.4166667    3.0000000   0.2500000\n#>   103    2.5833333   5.2500000    2.6666667   0.0000000\n#>   104    2.8333333   5.5000000    2.6666667   0.0000000\n#>   105    2.5833333   5.2500000    2.6666667   0.0000000\n#>   106    2.5833333   5.2500000    2.6666667   0.0000000\n#>   107    3.6666667   4.3333333    2.3333333   0.6666667\n#>   108    2.8333333   5.5000000    2.6666667   0.0000000\n#>   109    2.8333333   5.5000000    2.6666667   0.0000000\n#>   11     1.5000000   2.5000000    2.0000000   0.0000000\n#>   110    2.5833333   5.2500000    2.6666667   0.0000000\n#>   111    2.5833333   5.2500000    2.6666667   0.0000000\n#>   112    2.5833333   5.2500000    2.6666667   0.0000000\n#>   113    2.5833333   5.2500000    2.6666667   0.0000000\n#>   114    2.8333333   5.1666667    3.2500000   0.2500000\n#>   115    2.8333333   5.4166667    3.0000000   0.2500000\n#>   116    2.5833333   5.2500000    2.6666667   0.0000000\n#>   117    2.8333333   5.5000000    2.6666667   0.0000000\n#>   118    2.5833333   5.2500000    2.6666667   0.0000000\n#>   119    2.5833333   5.2500000    2.6666667   0.0000000\n#>   12     1.5000000   2.5000000    2.0000000   0.0000000\n#>   120    3.8000000   5.5500000    2.2500000   1.9000000\n#>   121    2.5833333   5.2500000    2.6666667   0.0000000\n#>   122    2.2500000   5.5000000    3.0000000   0.2500000\n#>   123    2.5833333   5.2500000    2.6666667   0.0000000\n#>   124    2.2500000   5.5833333    2.6666667   0.0000000\n#>   125    2.5833333   5.2500000    2.6666667   0.0000000\n#>   126    2.8333333   5.5000000    2.6666667   0.0000000\n#>   127    2.4166667   5.6666667    2.9166667   0.0000000\n#>   128    2.2500000   5.5833333    2.6666667   0.0000000\n#>   129    2.5833333   5.2500000    2.6666667   0.0000000\n#>   13     1.5000000   2.5000000    2.0000000   0.0000000\n#>   130    4.0666667   6.0666667    2.2000000   2.1666667\n#>   131    2.5833333   5.2500000    2.6666667   0.0000000\n#>   132    2.5833333   5.2500000    2.6666667   0.0000000\n#>   133    2.5833333   5.2500000    2.6666667   0.0000000\n#>   134    3.8000000   5.9000000    2.2000000   2.1000000\n#>   135    3.9500000   5.9500000    2.2000000   1.9000000\n#>   136    2.5833333   5.2500000    2.6666667   0.0000000\n#>   137    2.5833333   5.2500000    2.6666667   0.0000000\n#>   138    2.8333333   5.5000000    2.6666667   0.0000000\n#>   139    2.6666667   5.7500000    3.0833333   0.0000000\n#>   14     1.5000000   2.5000000    2.0000000   0.0000000\n#>   140    2.5833333   5.2500000    2.6666667   0.0000000\n#>   141    2.5833333   5.2500000    2.6666667   0.0000000\n#>   142    2.5833333   5.2500000    2.6666667   0.0000000\n#>   143    2.8333333   5.4166667    3.0000000   0.2500000\n#>   144    2.5833333   5.2500000    2.6666667   0.0000000\n#>   145    2.5833333   5.2500000    2.6666667   0.0000000\n#>   146    2.5833333   5.2500000    2.6666667   0.0000000\n#>   147    2.5833333   5.0000000    2.9166667   0.0000000\n#>   148    2.5833333   5.2500000    2.6666667   0.0000000\n#>   149    2.5833333   5.2500000    2.6666667   0.0000000\n#>   15     1.0000000   3.3333333    2.0000000   0.6666667\n#>   150    3.0833333   5.6666667    3.0000000   0.2500000\n#>   16     1.0000000   3.3333333    2.0000000   0.6666667\n#>   17     1.5000000   2.5000000    2.0000000   0.0000000\n#>   18     1.5000000   2.5000000    2.0000000   0.0000000\n#>   19     1.0000000   3.3333333    2.0000000   0.6666667\n#>   2      1.5000000   2.5000000    2.0000000   0.0000000\n#>   20     1.5000000   2.5000000    2.0000000   0.0000000\n#>   21     1.5000000   2.5000000    2.0000000   0.0000000\n#>   22     1.5000000   2.5000000    2.0000000   0.0000000\n#>   23     1.5000000   2.5000000    2.0000000   0.0000000\n#>   24     1.5000000   2.5000000    2.0000000   0.0000000\n#>   25     1.5000000   2.5000000    2.0000000   0.0000000\n#>   26     1.5000000   2.5000000    2.0000000   0.0000000\n#>   27     1.5000000   2.5000000    2.0000000   0.0000000\n#>   28     1.5000000   2.5000000    2.0000000   0.0000000\n#>   29     1.5000000   2.5000000    2.0000000   0.0000000\n#>   3      1.5000000   2.5000000    2.0000000   0.0000000\n#>   30     1.5000000   2.5000000    2.0000000   0.0000000\n#>   31     1.5000000   2.5000000    2.0000000   0.0000000\n#>   32     1.5000000   2.5000000    2.0000000   0.0000000\n#>   33     1.5000000   2.5000000    2.0000000   0.0000000\n#>   34     1.5000000   2.6666667    2.0000000   0.3333333\n#>   35     1.5000000   2.5000000    2.0000000   0.0000000\n#>   36     1.5000000   2.5000000    2.0000000   0.0000000\n#>   37     1.5000000   2.6666667    2.0000000   0.3333333\n#>   38     1.5000000   2.5000000    2.0000000   0.0000000\n#>   39     1.5000000   2.5000000    2.0000000   0.0000000\n#>   4      1.5000000   2.5000000    2.0000000   0.0000000\n#>   40     1.5000000   2.5000000    2.0000000   0.0000000\n#>   41     1.5000000   2.5000000    2.0000000   0.0000000\n#>   42     1.5000000   2.5000000    2.0000000   0.0000000\n#>   43     1.5000000   2.5000000    2.0000000   0.0000000\n#>   44     1.5000000   2.5000000    2.0000000   0.0000000\n#>   45     1.5000000   2.5000000    2.0000000   0.0000000\n#>   46     1.5000000   2.5000000    2.0000000   0.0000000\n#>   47     1.5000000   2.5000000    2.0000000   0.0000000\n#>   48     1.5000000   2.5000000    2.0000000   0.0000000\n#>   49     1.5000000   2.5000000    2.0000000   0.0000000\n#>   5      1.5000000   2.5000000    2.0000000   0.0000000\n#>   50     1.5000000   2.5000000    2.0000000   0.0000000\n#>   51     2.5000000   5.1666667    2.0000000   1.3333333\n#>   52     2.5000000   5.1666667    2.0000000   1.3333333\n#>   53     2.5000000   5.1666667    2.0000000   1.3333333\n#>   54     3.6500000   4.5500000    2.2000000   1.1000000\n#>   55     2.5000000   5.1666667    2.0000000   1.3333333\n#>   56     2.5000000   5.1666667    2.0000000   1.3333333\n#>   57     2.5000000   5.1666667    2.0000000   1.3333333\n#>   58     3.6666667   4.3333333    2.3333333   0.6666667\n#>   59     2.5000000   5.1666667    2.0000000   1.3333333\n#>   6      1.5000000   2.5000000    2.0000000   0.0000000\n#>   60     3.0000000   4.1666667    2.0000000   0.3333333\n#>   61     3.4000000   4.3000000    2.2000000   0.6000000\n#>   62     2.5000000   5.1666667    2.0000000   1.3333333\n#>   63     2.9000000   5.3000000    2.2000000   1.6000000\n#>   64     2.5000000   5.1666667    2.0000000   1.3333333\n#>   65     2.5000000   5.1666667    2.0000000   1.3333333\n#>   66     2.5000000   5.1666667    2.0000000   1.3333333\n#>   67     2.5000000   5.1666667    2.0000000   1.3333333\n#>   68     2.5000000   5.1666667    2.0000000   1.3333333\n#>   69     2.9000000   5.3000000    2.2000000   1.6000000\n#>   7      1.5000000   2.5000000    2.0000000   0.0000000\n#>   70     2.9000000   5.3000000    2.2000000   1.6000000\n#>   71     2.6666667   5.8333333    3.2500000   0.2500000\n#>   72     2.5000000   5.1666667    2.0000000   1.3333333\n#>   73     2.9000000   5.3000000    2.2000000   1.6000000\n#>   74     2.5000000   5.1666667    2.0000000   1.3333333\n#>   75     2.5000000   5.1666667    2.0000000   1.3333333\n#>   76     2.5000000   5.1666667    2.0000000   1.3333333\n#>   77     2.5000000   5.1666667    2.0000000   1.3333333\n#>   78     3.9166667   5.6666667    2.2500000   2.1666667\n#>   79     2.5000000   5.1666667    2.0000000   1.3333333\n#>   8      1.5000000   2.5000000    2.0000000   0.0000000\n#>   80     2.9000000   5.3000000    2.2000000   1.6000000\n#>   81     3.6500000   4.5500000    2.2000000   1.1000000\n#>   82     3.6500000   4.5500000    2.2000000   1.1000000\n#>   83     2.5000000   5.1666667    2.0000000   1.3333333\n#>   84     4.0666667   6.0666667    2.2000000   2.1666667\n#>   85     3.0000000   4.1666667    2.0000000   0.3333333\n#>   86     2.5000000   5.1666667    2.0000000   1.3333333\n#>   87     2.5000000   5.1666667    2.0000000   1.3333333\n#>   88     2.9000000   5.3000000    2.2000000   1.6000000\n#>   89     2.5000000   5.1666667    2.0000000   1.3333333\n#>   9      1.5000000   2.5000000    2.0000000   0.0000000\n#>   90     3.6500000   4.5500000    2.2000000   1.1000000\n#>   91     3.6500000   4.5500000    2.2000000   1.1000000\n#>   92     2.5000000   5.1666667    2.0000000   1.3333333\n#>   93     2.9000000   5.3000000    2.2000000   1.6000000\n#>   94     3.4000000   4.3000000    2.2000000   0.6000000\n#>   95     2.5000000   5.1666667    2.0000000   1.3333333\n#>   96     2.5000000   5.1666667    2.0000000   1.3333333\n#>   97     2.5000000   5.1666667    2.0000000   1.3333333\n#>   98     2.5000000   5.1666667    2.0000000   1.3333333\n#>   99     3.4000000   4.3000000    2.2000000   0.6000000\n\n\nY al hacer un CA podemos ver qué individuos están asociados con las variables pero por la importancia ponderada.\n\n\nMostrar / ocultar código\n\nres_ca <- FactoMineR::CA(tabla_para_diagonalizar, graph = FALSE)\n\nfviz_ca(res_ca)"
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-2",
    "href": "2023/01/29/explicatividad_no_usual/index.html#ejemplo-2",
    "title": "Explicatividad no usual",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\nUtilicemos esto para los datos de Boston Housing\n\n\nMostrar / ocultar código\nboston_df <-  MASS::Boston\n\n\n\nHousing Values in Suburbs of Boston Description The Boston data frame has 506 rows and 14 columns.\n\n\nUsage Boston Format This data frame contains the following columns:\n\n\ncrim per capita crime rate by town.\n\n\nzn proportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus proportion of non-retail business acres per town.\n\n\nchas Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox nitrogen oxides concentration (parts per 10 million).\n\n\nrm average number of rooms per dwelling.\n\n\nage proportion of owner-occupied units built prior to 1940.\n\n\ndis weighted mean of distances to five Boston employment centres.\n\n\nrad index of accessibility to radial highways.\n\n\ntax full-value property-tax rate per $10,000.\n\n\nptratio pupil-teacher ratio by town.\n\n\nblack 1000(Bk−0.63)^2 where BkBk is the proportion of blacks by town.\n\n\nlstat lower status of the population (percent).\n\n\nmedv median value of owner-occupied homes in $1000s.\n\n\n\nMostrar / ocultar código\nset.seed(47)\n\nidx <-  sample(1:nrow(boston_df),300)\ntrain_df <- boston_df[idx,]\n\ntest_df <- boston_df[-idx, ]\n\n\n\nModelo con ranger\n\n\nMostrar / ocultar código\nrg_boston <-  ranger(medv ~ ., data = train_df, num.trees = 50)\n\n\nVariables importantes a nivel individual\nPor simplificar, voy a seleccionar solo las 5 variables más importantes para cada observación\n\n\nMostrar / ocultar código\n\nimportancia_individual <- getIndividualImportance(rg_boston, test_df,top = 5, parallel = TRUE)\n\n\n\n\nMostrar / ocultar código\ndim(importancia_individual)\n#> [1] 1030    6\n\n\n\n\nMostrar / ocultar código\nDT::datatable(importancia_individual)\n\n\n\n\n\n\n\n\n\nAgrupando\n\n\nMostrar / ocultar código\ntabla_diag_boston <- xtabs(ponderado ~ id+ variables, data= importancia_individual)\nhead(tabla_diag_boston)\n#>      variables\n#> id         age     crim      dis    indus    lstat      nox  ptratio       rm\n#>   100 18.34266  0.00000  0.00000 21.95556 37.81429 17.20397  0.00000 38.40952\n#>   108  0.00000  0.00000 28.51039 27.88223 50.73328 34.23095  0.00000 41.07002\n#>   110 27.78868  0.00000 29.29312  0.00000 51.24393 33.02176  0.00000 42.08247\n#>   111  0.00000  0.00000 27.27855 33.53332 49.47596 26.90144  0.00000 43.21389\n#>   112  0.00000  0.00000 24.53920 25.53335 54.25059 30.61195  0.00000 39.80125\n#>   119  0.00000  0.00000 26.33785 28.28968 53.85364 35.92384  0.00000 39.11696\n#>      variables\n#> id         tax\n#>   100  0.00000\n#>   108  0.00000\n#>   110  0.00000\n#>   111  0.00000\n#>   112  0.00000\n#>   119  0.00000\n\n\n\n\nMostrar / ocultar código\nres_ca <- FactoMineR::CA(tabla_diag_boston, graph = FALSE)\n\nfviz_ca(res_ca)\n\n\n\n\n\n\n\n\n\nPodemos hacer un HCPC usando las dimensiones obtenidas. Lo que hace es un cluster jerárquico usando las dimensiones obtenidas en la estructura factorial.\n\n\nMostrar / ocultar código\nres_hcpc <- HCPC(res_ca, graph = FALSE)\n\nfviz_cluster(res_hcpc,\n             repel = TRUE,            # Avoid label overlapping\n             show.clust.cent = TRUE, # Show cluster centers\n             palette = \"jco\",         # Color palette see ?ggpubr::ggpar\n             ggtheme = theme_minimal(),\n             main = \"Factor map\"\n             )\n\n\n\n\n\n\n\n\n\n\n\nMostrar / ocultar código\nplot(res_hcpc, choice = \"3D.map\")\n\n\n\n\n\n\n\n\n\nUna utilidad interesante es la descripción de las variables de los clusters. Dónde nos dice cuales son la variables más importantes para cada uno.\nCuando en un cluster su Intern % para una variable se desvíe mucho de glob % quiere decir que en esa variable la distribución es distinta de en la población general y por tanto es una variables que caracteriza al cluster.\nEn este caso estaremos encontrando grupos de individuos con mismas variables importantes en el randomForest.\nClaramente se ven grupos dónde es muy importante la variable de criminalidad o la edad\n\n\nMostrar / ocultar código\nres_hcpc$desc.var\n#> $`1`\n#>          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#> crim    15.592962  2.127929    642.8840   657.2566  0.000000e+00        Inf\n#> nox     20.157813 12.416539    831.0888  3835.1142  2.807733e-52  15.215104\n#> dis     11.939900  9.329212    492.2715  2881.5271  2.572108e-09   5.956809\n#> rm      20.191263 25.487484    832.4679  7872.3559  1.117021e-17  -8.561196\n#> age      4.411035  7.568723    181.8631  2337.7625  1.230524e-18  -8.811891\n#> tax      0.000000  1.001897      0.0000   309.4575  9.327733e-20  -9.096514\n#> ptratio  0.000000  3.118072      0.0000   963.0835  2.250504e-61 -16.529487\n#> indus    0.000000 10.860935      0.0000  3354.6328 2.069270e-222 -31.835808\n#> \n#> $`2`\n#>         Intern %    glob % Intern freq Glob freq        p.value     v.test\n#> age     14.32559  7.568723    803.0250  2337.7625  7.150050e-85  19.521920\n#> nox     19.56112 12.416539   1096.5037  3835.1142  9.886451e-65  16.989118\n#> dis     15.61069  9.329212    875.0616  2881.5271  3.449026e-63  16.779470\n#> rm      21.85203 25.487484   1224.9213  7872.3559  2.382720e-12  -7.010027\n#> tax      0.00000  1.001897      0.0000   309.4575  1.880511e-27 -10.855366\n#> crim     0.00000  2.127929      0.0000   657.2566  2.961603e-58 -16.090764\n#> ptratio  0.00000  3.118072      0.0000   963.0835  1.146579e-85 -19.615212\n#> indus    0.00000 10.860935      0.0000  3354.6328 2.480702e-311 -37.718398\n#> \n#> $`3`\n#>         Intern %    glob % Intern freq Glob freq        p.value     v.test\n#> dis     14.41601  9.329212    885.4575  2881.5271  7.965757e-48  14.528753\n#> indus   15.72645 10.860935    965.9468  3354.6328  4.377749e-39  13.078362\n#> nox     17.05504 12.416539   1047.5513  3835.1142  1.891207e-32  11.860817\n#> rm      24.28797 25.487484   1491.8110  7872.3559  1.531797e-02  -2.424773\n#> tax      0.00000  1.001897      0.0000   309.4575  2.402280e-30 -11.448146\n#> crim     0.00000  2.127929      0.0000   657.2566  1.885153e-64 -16.951216\n#> ptratio  0.00000  3.118072      0.0000   963.0835  8.318594e-95 -20.657729\n#> age      0.00000  7.568723      0.0000  2337.7625 1.165375e-235 -32.779199\n#> \n#> $`4`\n#>          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#> age     13.713854  7.568723    994.4487  2337.7625 7.010364e-100  21.214544\n#> indus   15.980898 10.860935   1158.8415  3354.6328  3.363370e-53  15.353354\n#> rm      27.297454 25.487484   1979.4522  7872.3559  6.332137e-05   4.000079\n#> nox     11.031380 12.416539    799.9313  3835.1142  3.337066e-05  -4.149154\n#> tax      0.000000  1.001897      0.0000   309.4575  1.541120e-36 -12.624810\n#> crim     0.000000  2.127929      0.0000   657.2566  1.026729e-77 -18.661061\n#> dis      3.479402  9.329212    252.3059  2881.5271 1.613296e-102 -21.498348\n#> ptratio  0.000000  3.118072      0.0000   963.0835 2.246449e-114 -22.730329\n#> \n#> $`5`\n#>           Intern %    glob % Intern freq Glob freq        p.value     v.test\n#> ptratio 13.4631336  3.118072   731.46909   963.0835  0.000000e+00        Inf\n#> indus   16.0021844 10.860935   869.41894  3354.6328  2.134777e-37  12.779485\n#> rm      30.0450942 25.487484  1632.38800  7872.3559  6.922077e-17   8.348354\n#> age      5.9723379  7.568723   324.48468  2337.7625  4.791243e-07  -5.034489\n#> dis      5.8922388  9.329212   320.13279  2881.5271  6.783780e-24 -10.079838\n#> tax      0.0000000  1.001897     0.00000   309.4575  1.567007e-26 -10.659939\n#> crim     0.2645368  2.127929    14.37262   657.2566  4.931237e-37 -12.714201\n#> nox      1.1050567 12.416539    60.03913  3835.1142 3.406088e-250 -33.783844\n#> \n#> $`6`\n#>          Intern %    glob % Intern freq Glob freq        p.value     v.test\n#> tax     13.270152  1.001897   309.45748   309.4575  0.000000e+00        Inf\n#> ptratio  9.932087  3.118072   231.61443   963.0835  3.328525e-58  16.083531\n#> indus   15.455762 10.860935   360.42551  3354.6328  2.442145e-12   7.006580\n#> rm      30.502626 25.487484   711.31557  7872.3559  1.684450e-08   5.641633\n#> crim     0.000000  2.127929     0.00000   657.2566  4.470886e-23  -9.892856\n#> dis      2.414161  9.329212    56.29780  2881.5271  1.144880e-43 -13.857568\n#> age      1.455459  7.568723    33.94103  2337.7625  5.050710e-44 -13.916197\n#> nox      0.000000 12.416539     0.00000  3835.1142 2.015379e-140 -25.227201\n#> \n#> attr(,\"class\")\n#> [1] \"descfreq\" \"list\"\n\n\nY por supuesto tenemos los datos con el cluster asignado y los valores de cada variable (no son los valores originales de las variables , sino la importancia ponderada que tenían con el procedimiento descrito para cada observación )\n\n\nMostrar / ocultar código\nres_hcpc$data.clust %>% \n         dplyr::select(clust, everything()) %>% \n    slice_sample(prop = 0.3) %>% \n    DT::datatable()\n\n\n\n\n\n\n\nSi unimos con el dataset original\n\n\nMostrar / ocultar código\ntest_df_with_cluster <-  res_hcpc$data.clust %>% \n    rownames_to_column(var = \"id\") %>% \n    dplyr::select(id, clust)\n\nunido <- test_df %>% \n    rownames_to_column(var = \"id\") %>% \n    inner_join(test_df_with_cluster, by = \"id\")\n\n\nY efectivamente vemos que el cluster 1 tiene mucho más ratio de criminalidad, y además es la variable más importante para ese grupo en relación con la variable dependiente medv. No causa sorpresa ver que es justo en ese cluster dónde el precio de la propiedad es más bajo\n\n\nMostrar / ocultar código\nunido %>% \n    group_by(clust) %>% \n    summarise(across(c(lstat,crim, age, black, medv), list(mean = mean, median = median), .names = \"{.col}_{.fn}\" )) %>% \n    DT::datatable()\n\n\n\n\n\n\n\nCarlos en el post que inspira este, comenta que este tipo de procedimientos sería útil para aquellas de las observaciones con un mayor score predicho. En este ejemplo se podría aplicar para clusterizar las observaciones con un mayor valor predicho del valor de la propiedad."
  },
  {
    "objectID": "2023/01/29/explicatividad_no_usual/index.html#nota",
    "href": "2023/01/29/explicatividad_no_usual/index.html#nota",
    "title": "Explicatividad no usual",
    "section": "Nota",
    "text": "Nota\n\nHice el código deprisa y corriendo, es claramente mejorable y podría ir mucho más rápido. El objetivo era mostrar como se puede obtener variables importantes a nivel de observación en este tipo de modelos, simplemente recorriendo por qué camino ha ido cada observación en cada árbol\n\n\nEstaría chulo representar espacialmente la distribución de los clusters obtenidos"
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html",
    "title": "Conformal prediction. Estilo compadre",
    "section": "",
    "text": "El jueves pasado asistí al más que recomendable meetup de PyData Madrid, que cuenta entre sus organizadores con el gran Juan Luis Cano Rodríguez, antiguo compañero mío de curro y tocayo de iniciales.\nEl caso es que en una de las charlas, Ignacio Peletier, mencionó de pasada lo del “Conformal Prediction”. Y siendo que Ignacio es un gran científico de datos, y que hacía unos meses que había tenido varias charlas con Carlos sobre el particular, pues he decidido ver un poco más en detalle de qué iba el asunto ."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#recursos",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#recursos",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Recursos",
    "text": "Recursos\nUn excelente sitio para empezar a bichear con este tema es el Readme de este repo, dónde han ido recopilando enlaces a libros, posts, papers y código relativo a lo de conformal prediction.\nEn particular, uno de los recursos que me ha gustado es este minicurso de Christoph Molnar.\nOtro recurso útil es este post de Carlos, dónde se esboza un poco en qué consiste esto de la predicción conforme y en por qué no es algo tan novedoso como se cree."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#experimentando",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#experimentando",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Experimentando",
    "text": "Experimentando\nLa predicción conforme se puede aplicar tanto a modelos de regresión de clasificación. Su objetivo es simplemente medir la incertidumbre de una predicción dada.\nEn el caso de regresión no tiene mucho misterio:\n\nSe entrena un modelo usando un conjunto de datos de entrenamiento.\nSe mide el error en un conjunto de datos de validación, calibración, utilizando la norma L1, es decir \\(\\mid y - \\hat{y}\\mid\\)\nSe elige una medida de dispersión del error, por ejemplo el cuantil \\((1- \\alpha) = 0.95\\) de los errores anteriores.\nPara una nueva predicción se da su intervalo como \\((\\hat{y} - q_{1-\\alpha}, \\hat{y} + q_{1-\\alpha})\\)\n\nEn el caso de clasificación la cosa es más divertida. Puesto que lo que se quiere obtener es un conjunto de etiquetas probables. Tipo {A} {A, B} {B, C}\nEn este caso según he leído aquí el algoritmo sería\n\nSe entrena un modelo usando un conjunto de datos de entrenamiento.\nSe mide el error en un conjunto de datos de validación, calibración, viendo para cada observación el valor que el modelo le ha dado para la predicción de la clase verdadera. Es un conjunto de validación , sabemos cuál es la verdad. Y se calcula el error como \\(1- p_{i}\\) siendo \\(p_i\\), la probabilidad predicha para la clase verdadera\nSe calcula el cuantil de orden \\(1-\\alpha\\) de esos errores y se guarda. Se entiende que el modelo está bien calibrado y que el conjunto de validación y que los scores que da el modelo se pueden asumir como probabilidades\nPara una nueva predicción se tendrá una \\(p_i\\) para cada clase. Se calcula \\(1-p_i\\) para cada clase y se considera que esa clase forma parte del prediction set si ese valor es menor o igual que el valor del cuantil anterior.\n\nPues vamos a ver como se haría con R en estilo compadre, y puede que con alguna pequeña modificación por mi parte.\n\nEjemplo\n\n\nMostrar / ocultar código\nlibrary(tidyverse)\nlibrary(MASS)\n\n\nVamos a usar el conjunto de datos housing \n\n\nMostrar / ocultar código\n\nskimr::skim(housing)\n\n\n\nData summary\n\n\nName\nhousing\n\n\nNumber of rows\n72\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSat\n0\n1\nTRUE\n3\nLow: 24, Med: 24, Hig: 24\n\n\nInfl\n0\n1\nFALSE\n3\nLow: 24, Med: 24, Hig: 24\n\n\nType\n0\n1\nFALSE\n4\nTow: 18, Apa: 18, Atr: 18, Ter: 18\n\n\nCont\n0\n1\nFALSE\n2\nLow: 36, Hig: 36\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nFreq\n0\n1\n23.35\n17.67\n3\n10\n19.5\n31.75\n86\n▇▅▂▁▁\n\n\n\n\n\nY vamos a justar un modelito tonto usando regresión logística ordinal, sobre los 40 primeros datos\n\n\nMostrar / ocultar código\n\nhouse.plr <- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing[1:40,])\n\nhead(predict(house.plr, type = \"probs\"))\n#>         Low    Medium      High\n#> 1 0.3639420 0.2575531 0.3785049\n#> 2 0.3639420 0.2575531 0.3785049\n#> 3 0.3639420 0.2575531 0.3785049\n#> 4 0.3263764 0.2552793 0.4183443\n#> 5 0.3263764 0.2552793 0.4183443\n#> 6 0.3263764 0.2552793 0.4183443\n\n\nGuardamos las predicciones para el conjunto de validación , que va a ser las filas de la 41 a la 55, junto con el valor de Sat verdadero\n\n\nMostrar / ocultar código\n\npredictions <- as.data.frame(predict(house.plr, type = \"probs\", newdata = housing[41:55,]))\n\ntt <- cbind(predictions, True_class=housing$Sat[41:55])\n\ntt\n#>          Low    Medium      High True_class\n#> 41 0.3055662 0.2524868 0.4419471     Medium\n#> 42 0.3055662 0.2524868 0.4419471       High\n#> 43 0.1595073 0.1930758 0.6474169        Low\n#> 44 0.1595073 0.1930758 0.6474169     Medium\n#> 45 0.1595073 0.1930758 0.6474169       High\n#> 46 0.4671694 0.2484190 0.2844116        Low\n#> 47 0.4671694 0.2484190 0.2844116     Medium\n#> 48 0.4671694 0.2484190 0.2844116       High\n#> 49 0.4260865 0.2544760 0.3194375        Low\n#> 50 0.4260865 0.2544760 0.3194375     Medium\n#> 51 0.4260865 0.2544760 0.3194375       High\n#> 52 0.2425400 0.2363201 0.5211399        Low\n#> 53 0.2425400 0.2363201 0.5211399     Medium\n#> 54 0.2425400 0.2363201 0.5211399       High\n#> 55 0.4014928 0.2566312 0.3418760        Low\n\n\nAhora, para la primera fila sería hacer (1-0.2524), puesto que la clase real es “Medium” y para la segunda sería (1-0.44), puesto que la clase real es “High”. No estoy muy inspirado hoy y no he conseguido una forma elegante de hacerlo en R, y ChatGpt no me ha servido de mucha ayuda, seguramente porque aún no soy muy ducho preguntándole.\nAsí que he tirado iterando para cada fila con un map y quedándode con el valor predicho de la columna cuyo nombre coincida con el valor en True_class\n\n\nMostrar / ocultar código\n\ntt$prob_true_class <- map_dbl(1:nrow(tt), .f = function(i) \n    tt[i, colnames(tt) == tt$True_class[i]])\n\ntt$resid <- 1-tt$prob_true_class\n\nhead(tt)\n#>          Low    Medium      High True_class prob_true_class     resid\n#> 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#> 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#> 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#> 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#> 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#> 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n\n\nDefinimos un \\(\\alpha = 0.3\\) y calculamos el cuantil 70 .\n\n\nMostrar / ocultar código\n(qhat = quantile(tt$resid, 0.7))\n#>       70% \n#> 0.7507675\n\n\nY ya estamos listos para hacer la predicción conforme para nuevos datos.\n\n\nMostrar / ocultar código\n# predecimos de la fila 51 a la 70 \npredicciones <- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\n\nhead(predicciones)\n#>          Low    Medium      High\n#> 51 0.4260865 0.2544760 0.3194375\n#> 52 0.2425400 0.2363201 0.5211399\n#> 53 0.2425400 0.2363201 0.5211399\n#> 54 0.2425400 0.2363201 0.5211399\n#> 55 0.4014928 0.2566312 0.3418760\n#> 56 0.4014928 0.2566312 0.3418760\n\n\nNos creamos un data.frame que indique si el valor de 1 - predicciones es menor o igual que el cuantil elegido\n\n\nMostrar / ocultar código\nset <- as.data.frame(1 - predicciones <= qhat)\n\nhead(set)\n#>      Low Medium High\n#> 51  TRUE   TRUE TRUE\n#> 52 FALSE  FALSE TRUE\n#> 53 FALSE  FALSE TRUE\n#> 54 FALSE  FALSE TRUE\n#> 55  TRUE   TRUE TRUE\n#> 56  TRUE   TRUE TRUE\n\n\nAl igual que antes, utilizo un map para obtener el conjunto de etiquetas, para la primera fila serían todas, para la segunda sería {“Medium”, “High”}\n\n\nMostrar / ocultar código\n\nset$conformal <-  map_chr(1:nrow(set), .f= function(i) {\n     set_list = colnames(set)[unlist(set[i,])]\n     paste0(set_list, collapse = \",\")\n     })\n\nhead(set)\n#>      Low Medium High       conformal\n#> 51  TRUE   TRUE TRUE Low,Medium,High\n#> 52 FALSE  FALSE TRUE            High\n#> 53 FALSE  FALSE TRUE            High\n#> 54 FALSE  FALSE TRUE            High\n#> 55  TRUE   TRUE TRUE Low,Medium,High\n#> 56  TRUE   TRUE TRUE Low,Medium,High\n\n\nSe lo pego al dataset original de test (filas 51 a 70), junto con las predicciones y la clase verdadera.\n\n\nMostrar / ocultar código\n\nset_fin <-  cbind( True_class = housing$Sat[51:70], as.data.frame(predicciones),\n                  set_conformal =set$conformal)\n\nhead(set_fin)\n#>    True_class       Low    Medium      High   set_conformal\n#> 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High\n#> 52        Low 0.2425400 0.2363201 0.5211399            High\n#> 53     Medium 0.2425400 0.2363201 0.5211399            High\n#> 54       High 0.2425400 0.2363201 0.5211399            High\n#> 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High\n#> 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High\n\n\nY ya estaría.\nUna cosa que se suele calcular es la cobertura de cada clase, es decir, la proporción de veces que cada clase está dentro del conjunto.\n\n\nMostrar / ocultar código\nset_fin <- set_fin |> \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                               .y = set_conformal , \n                               ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nhead(set_fin)\n#>    True_class       Low    Medium      High   set_conformal class_in_set\n#> 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High         TRUE\n#> 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#> 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n#> 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n\n\n\n\nMostrar / ocultar código\nset_fin |> \n    group_by(True_class) |> \n    summarise(cov = mean(class_in_set))\n#> # A tibble: 3 × 2\n#>   True_class   cov\n#>   <ord>      <dbl>\n#> 1 Low        0.571\n#> 2 Medium     0.5  \n#> 3 High       1\n\n\n\n\nModificación 1.\nNo me convence lo de tener un sólo cuantil, común a todas las clases, ¿no sería mejor tener una medida de cómo se distribuyen los errores para cada una de las clases?\nUsamos el conjunto de validación dónde tenemos el \\(1-p_i\\) que nos dice en cuánto se ha equivocado el modelo en predecir la clase real\n\n\nMostrar / ocultar código\nhead(tt)\n#>          Low    Medium      High True_class prob_true_class     resid\n#> 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#> 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#> 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#> 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#> 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#> 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n\n\nCalculamos el quantil 70 para cada clase, y así vemos que varía por clase\n\n\nMostrar / ocultar código\n(qhat_by_class <- tt |> \n    group_by(True_class) |> \n    summarise(qhat = quantile(resid, 0.7)) |> \n        pivot_wider(names_from = True_class, values_from = qhat))\n#> # A tibble: 1 × 3\n#>     Low Medium  High\n#>   <dbl>  <dbl> <dbl>\n#> 1 0.726  0.761 0.656\n\n\n\n\nMostrar / ocultar código\npredicciones <- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\ncomplementarios <- 1-predicciones\nhead(complementarios)\n#>          Low    Medium      High\n#> 51 0.5739135 0.7455240 0.6805625\n#> 52 0.7574600 0.7636799 0.4788601\n#> 53 0.7574600 0.7636799 0.4788601\n#> 54 0.7574600 0.7636799 0.4788601\n#> 55 0.5985072 0.7433688 0.6581240\n#> 56 0.5985072 0.7433688 0.6581240\n\n\nY vemos si cada \\(1-p_i\\) es menor o igual que el cuantil correspondiente de cada clase\n\n\nMostrar / ocultar código\nset_adjust <- data.frame(Low = complementarios[,1] <= qhat_by_class$Low,\n                        Medium = complementarios[,2] <= qhat_by_class$Medium,\n                         High = complementarios[,3] <= qhat_by_class$High )\n\n\nhead(set_adjust)\n#>      Low Medium  High\n#> 51  TRUE   TRUE FALSE\n#> 52 FALSE  FALSE  TRUE\n#> 53 FALSE  FALSE  TRUE\n#> 54 FALSE  FALSE  TRUE\n#> 55  TRUE   TRUE FALSE\n#> 56  TRUE   TRUE FALSE\n\n\n\n\nMostrar / ocultar código\nset_adjust$conformal <-  map_chr(1:nrow(set_adjust), .f= function(i) {\n    set_list = colnames(set_adjust)[unlist(set_adjust[i,])]\n    paste0(set_list, collapse = \",\")\n})\n\nhead(set_adjust)\n#>      Low Medium  High  conformal\n#> 51  TRUE   TRUE FALSE Low,Medium\n#> 52 FALSE  FALSE  TRUE       High\n#> 53 FALSE  FALSE  TRUE       High\n#> 54 FALSE  FALSE  TRUE       High\n#> 55  TRUE   TRUE FALSE Low,Medium\n#> 56  TRUE   TRUE FALSE Low,Medium\n\n\nComo antes, nos quedamos con la clase de verdad, la predicción en probabilidad de cada clase y la predicción conforme\n\n\nMostrar / ocultar código\nset_adjust_fin <-  cbind( True_class = housing$Sat[51:70], as.data.frame(predict(house.plr, newdata = housing[51:70,],type=\"probs\")),\n                   set_conformal =set_adjust$conformal)\n\nhead(set_adjust_fin)\n#>    True_class       Low    Medium      High set_conformal\n#> 51       High 0.4260865 0.2544760 0.3194375    Low,Medium\n#> 52        Low 0.2425400 0.2363201 0.5211399          High\n#> 53     Medium 0.2425400 0.2363201 0.5211399          High\n#> 54       High 0.2425400 0.2363201 0.5211399          High\n#> 55        Low 0.4014928 0.2566312 0.3418760    Low,Medium\n#> 56     Medium 0.4014928 0.2566312 0.3418760    Low,Medium\n\n\n\n\nMostrar / ocultar código\nset_adjust_fin <- set_adjust_fin |> \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                                .y = set_conformal , \n                                ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nset_adjust_fin\n#>    True_class       Low    Medium      High   set_conformal class_in_set\n#> 51       High 0.4260865 0.2544760 0.3194375      Low,Medium        FALSE\n#> 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#> 55        Low 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#> 56     Medium 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#> 57       High 0.4014928 0.2566312 0.3418760      Low,Medium        FALSE\n#> 58        Low 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#> 59     Medium 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#> 60       High 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#> 61        Low 0.1967801 0.2160332 0.5871868            High        FALSE\n#> 62     Medium 0.1967801 0.2160332 0.5871868            High        FALSE\n#> 63       High 0.1967801 0.2160332 0.5871868            High         TRUE\n#> 64        Low 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#> 65     Medium 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#> 66       High 0.4732214 0.2472855 0.2794931      Low,Medium        FALSE\n#> 67        Low 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#> 68     Medium 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#> 69       High 0.4320378 0.2537829 0.3141793      Low,Medium        FALSE\n#> 70        Low 0.2470311 0.2378946 0.5150743            High        FALSE\n\n\nY aquí ya vemos que la cobertura es distinta y que la clase “High” ya no está en el 100% de los prediction sets\n\n\nMostrar / ocultar código\nset_adjust_fin |> \n    group_by(True_class) |> \n    summarise(cov = mean(class_in_set))\n#> # A tibble: 3 × 2\n#>   True_class   cov\n#>   <ord>      <dbl>\n#> 1 Low        0.571\n#> 2 Medium     0.667\n#> 3 High       0.429\n\n\nDe hecho si tabulamos ambas predicciones conformes , vemos que de las 10 predicciones que el primer método ponía como {Low, Medium, High} , el segundo pone 7 como {Low, Medium } y 3 como {Low, Medium, High}\n\n\nMostrar / ocultar código\n\ntable(set_fin$set_conformal, set_adjust_fin$set_conformal)\n#>                  \n#>                   High Low,Medium Low,Medium,High\n#>   High               7          0               0\n#>   Low,High           0          3               0\n#>   Low,Medium,High    0          7               3\n\n\n\n\nModificación 2.\nVale, todo esto está muy bien, pero ¿y si simplemente para cada observación ordeno de forma decreciente su probabilidad predicha y me quedo con las clases que lleguen al 60% de probabilidad, por ejemplo?\n\n\nMostrar / ocultar código\n\n(predicciones_df <-  as.data.frame(predicciones ))\n#>          Low    Medium      High\n#> 51 0.4260865 0.2544760 0.3194375\n#> 52 0.2425400 0.2363201 0.5211399\n#> 53 0.2425400 0.2363201 0.5211399\n#> 54 0.2425400 0.2363201 0.5211399\n#> 55 0.4014928 0.2566312 0.3418760\n#> 56 0.4014928 0.2566312 0.3418760\n#> 57 0.4014928 0.2566312 0.3418760\n#> 58 0.3622588 0.2575226 0.3802186\n#> 59 0.3622588 0.2575226 0.3802186\n#> 60 0.3622588 0.2575226 0.3802186\n#> 61 0.1967801 0.2160332 0.5871868\n#> 62 0.1967801 0.2160332 0.5871868\n#> 63 0.1967801 0.2160332 0.5871868\n#> 64 0.4732214 0.2472855 0.2794931\n#> 65 0.4732214 0.2472855 0.2794931\n#> 66 0.4732214 0.2472855 0.2794931\n#> 67 0.4320378 0.2537829 0.3141793\n#> 68 0.4320378 0.2537829 0.3141793\n#> 69 0.4320378 0.2537829 0.3141793\n#> 70 0.2470311 0.2378946 0.5150743\n\n\n\n\nMostrar / ocultar código\nmodificacion_2 <- predicciones_df |> \n     rownames_to_column(var = \"individuo\") |> \n    pivot_longer(cols = Low:High) |> \n    group_by(individuo) |> \n    arrange( desc(value)) |> \n    mutate(suma_acumulada = cumsum(value)) |> \n    arrange(individuo)\n\nhead(modificacion_2, 10)\n#> # A tibble: 10 × 4\n#> # Groups:   individuo [4]\n#>    individuo name   value suma_acumulada\n#>    <chr>     <chr>  <dbl>          <dbl>\n#>  1 51        Low    0.426          0.426\n#>  2 51        High   0.319          0.746\n#>  3 51        Medium 0.254          1    \n#>  4 52        High   0.521          0.521\n#>  5 52        Low    0.243          0.764\n#>  6 52        Medium 0.236          1    \n#>  7 53        High   0.521          0.521\n#>  8 53        Low    0.243          0.764\n#>  9 53        Medium 0.236          1    \n#> 10 54        High   0.521          0.521\n\n\nUhmm, pero no me acaba de convencer ordenar de forma descendente por la probabilidad predicha de cada clase. Por ejemplo para el individuo 51, si tomo Low +High llegaría a 0.74, pero si tomo Low + Medium llego al 67% . Si quisiera el menor conjunto de etiquetas que lleguen como mínimo al 60% la opción buena sería Low + Medium para ese individuo.\nNo me veo con ganas de implementar todas las posibles sumas de probabilidades estimadas y elegir el conjunto que cumpla la restricción de llegar al menos al 60% y si hay varios para mismo individuos que se quede con el conjunto más pequeño."
  },
  {
    "objectID": "2023/03/26/Conformal_estilo_compadre/index.html#conclusión.",
    "href": "2023/03/26/Conformal_estilo_compadre/index.html#conclusión.",
    "title": "Conformal prediction. Estilo compadre",
    "section": "Conclusión.",
    "text": "Conclusión.\n\nLo de la predicción conforme para el caso de regresión me parece bastante sencillo, no es más que sumar y restar una medida de dispersión de los residuos a la predicción para nuevos datos.\nPara clasificación es un poco más interesante, sobre todo para casos en los que el usuario quiere una etiqueta o etiquetas y no se conforma con las probabilidades predichas de cada clase.\nSubyace la hipótesis de que los scores del modelo están bien calibrados y reflejan la verdadera probabilidad.\n\nPues nada más, tengan un feliz día."
  },
  {
    "objectID": "2019/11/30/la-fatal-arrogancia/index.html",
    "href": "2019/11/30/la-fatal-arrogancia/index.html",
    "title": "La fatal arrogancia",
    "section": "",
    "text": "Important\n\n\n\nNo, no voy a hablar de liberalismo ni de Hayek. Solo quería hacer una pequeña reflexión sobre las nuevas generaciones de científicos de datos o como se les quiera llamar.\n\n\nVengo observando hace cosa de 3 años, que las nuevas generaciones creen que es fácil utilizar modelos estadísticos (o de Machín Lenin como dice algún amigo mío) para predecir cosas como la bolsa, o acertar ,cual demiurgo, si se va a sufrir un cáncer y cosas por el estilo.\nLa fatal arrogancia a la que me refiero es aquella que hace que hasta los que trabajamos en estos temas, nos dejemos llevar por el “hype” (no sé bien como traducir esto) que se vive actualmente y nos creamos la cantidad de mentiras que se dicen. Es fácil entusiasmarse por cosas como el deep learning, reinforcement learning o el último algoritmo de boosting, pero ninguna de estas cosas puede hacer magia y separar totalmente la señal del ruido.\nTambién noto lo de la fatal arrogancia entre aquellos que dicen que para qué necesitan saber estadística, ¿a cuántos conocéis que den intervalos de confianza o de credibilidad en vez de solo estimaciones puntuales? Seguramente a muy pocos.\nEn fin, de lo que estoy seguro es que si hay alguno que ha conseguido a predecir con éxito notable los movimientos de la bolsa, lo que menos hace es alardear de su éxito dando charlas en meetups.\nNos vemos en los bares."
  },
  {
    "objectID": "2023/04/23/quantile-catboost/index.html",
    "href": "2023/04/23/quantile-catboost/index.html",
    "title": "Regresión cuantil a lo machín lenin con catboost",
    "section": "",
    "text": "Hay veces, más de las que se cree, en que nos interesa estimar un cuantil en vez de la media. Si tenemos una variable dependinte \\(y\\) y una o varias independientes \\(X\\), lo que se suele hacer es una regresión cuantil.\nSi visitamos uno de los papers originales de dicha técnica Computing Regression Quantiles vemos que trata de minimizar la siguiente expresión.\n\\[\n\\arg\\min_b R_{\\theta}(b)  = \\sum_{i = 1}^{n}\\rho_\\theta \\left( y_i - x_ib\\right)\n\\] Con \\(\\theta \\in (0,1)\\) y\n\\[\n\\begin{equation}\n    \\rho_\\theta(u) =\n        \\begin{cases}\n        \\theta u  &  u \\geq 0\\\\\n        (\\theta -1) & u  < 0 \\\\\n        \\end{cases}\n\\end{equation}\n\\]\nLo cual es simplemente “penalizar” por \\(\\theta\\) cuando el residuo sea mayor o igual que 0, es decir, cuando nos equivocamos por arriba y por \\((\\theta -1)\\) si nos equivocamos por abajo.\nEjemplo, si \\(y_i = 40\\) y \\(f(x) = 50\\) y queremos estimar el cuantil 0.95. Entonces como el residuo es menor que 0, se pondera por 0.05\n\\[\\rho_(40 - 50) = (0.95 -1) (40 - 50) = 0.5 \\] Si en cambio \\(f(x) = 30\\), es decir, nos equivocamos por abajo, pero a la misma distancia del valor real entonces\n\\[\\rho(40-30) = 0.95 (40-30) = 9.5 \\]\nY por tanto la función a minimizar \\(\\arg\\min_b R_{\\theta}(b)\\) cuando \\(\\theta > 0.5\\) va a tener un valor mucho mayor cuando nos “equivocamos” por abajo que por arriba. Y debido a cómo está definido \\(\\rho_\\theta(u)\\) se consigue la regresión cuantil con cuantil igual a \\(\\theta\\). En el paper (de 1987) viene mejor explicado y el algoritmo para resolverlo en el caso de que \\(f(x)\\) sea lineal.\nFuera coñas, el caso es que la gente de yandex en su librería catboost han utilizado esto para hacer la regresión cuantil, simplemente utilizando la expresión anterior como función de pérdida. Aquí se puede ver las diferentes funciones de pérdida que usan según el caso.\nPara la regresión cuantil usan\n\\[L(t, a, \\alpha) = \\dfrac{\\sum_{i}^{N} \\omega_i(\\alpha - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}\\] Dónde\nComo vemos, es lo mismo que se cuenta en el paper de 1987. Pero al meterlo como función de pérdida en el algoritmo sirve para el algoritmo de boosting que se utiliza en la librería.\nLa gente de catboost, atinadamente ha dicho, y ¿por qué no construimos un función de pérdida que minimice globalmente varios cuantiles? Lo cual es algo así como “encuéntrame la distribución de los parámetros que mejor se ajusta a estos datos en vez de un sólo parámetro”.\nPero esto son arbolitos y boosting, no hay lo que se dice un parámetro de la función propiamente dicho, por lo que al final lo que se “aprende” debe ser la configuración de árboles que minimiza globalmente los cuantiles indicados.\nBueno, la función de pérdida “multi-quantile” es una modificación simple de la anterior.\n\\[L(t, a, \\alpha_q) = \\dfrac{\\sum_{i}^{N} \\omega_i \\sum_{q=1}^{Q}(\\alpha_q - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}\\]"
  },
  {
    "objectID": "2023/04/23/quantile-catboost/index.html#ejemplo",
    "href": "2023/04/23/quantile-catboost/index.html#ejemplo",
    "title": "Regresión cuantil a lo machín lenin con catboost",
    "section": "Ejemplo",
    "text": "Ejemplo\nEl ejemplo no es mío, lo he visto por algún sitio que no me acuerdo.\n\n\n\n\n\n\nTip\n\n\n\ncatboost se puede utilizar en R y python.\n\n\n\n\nMostrar / ocultar código\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\nsns.set()\n\nn = 800\n\n# X aleatorias\nx_train = np.random.rand(n)\nx_test = np.random.rand(n)\n\n# un poquito de ruido gaussiano\n\nnoise_train = np.random.normal(0, 0.3, n)\nnoise_test = np.random.normal(0, 0.3, n)\n\n# Simulamos y_train e y _x como y = 2 + 3 * x + ruido\na, b = 2, 3\n\n# al lio\ny_train = a * x_train + b + noise_train\ny_test = a * x_test + b + noise_test\n\n\nPintamos\n\n\nMostrar / ocultar código\nsns.scatterplot(x = x_train, y = y_train).set(title = \"Ejemplillo\")\n\n\n\n\n\n\n\n\n\nVaos a predecir 10 cuantiles\n\n\nMostrar / ocultar código\nquantiles = [q/10 for q in range(1, 10)]\n\n# se ponen en string separados por commas\nquantile_str = str(quantiles).replace('[','').replace(']','')\n\nprint(quantile_str)\n#> 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n\n\nModelito\n\n\nMostrar / ocultar código\nmodel = CatBoostRegressor(iterations=100,\n                          loss_function=f'MultiQuantile:alpha={quantile_str}')\n\nmodel.fit(x_train.reshape(-1,1), y_train)\n#> 0:   learn: 0.1977907    total: 49.8ms   remaining: 4.93s\n#> 1:   learn: 0.1934878    total: 54.5ms   remaining: 2.67s\n#> 2:   learn: 0.1891389    total: 61.2ms   remaining: 1.98s\n#> 3:   learn: 0.1849242    total: 64.9ms   remaining: 1.56s\n#> 4:   learn: 0.1808818    total: 68.4ms   remaining: 1.3s\n#> 5:   learn: 0.1769520    total: 71.7ms   remaining: 1.12s\n#> 6:   learn: 0.1732251    total: 74.7ms   remaining: 993ms\n#> 7:   learn: 0.1696774    total: 77.9ms   remaining: 896ms\n#> 8:   learn: 0.1661482    total: 80.9ms   remaining: 818ms\n#> 9:   learn: 0.1628541    total: 84.5ms   remaining: 760ms\n#> 10:  learn: 0.1596378    total: 87.6ms   remaining: 709ms\n#> 11:  learn: 0.1565479    total: 90.7ms   remaining: 665ms\n#> 12:  learn: 0.1536016    total: 93.8ms   remaining: 628ms\n#> 13:  learn: 0.1507692    total: 96.9ms   remaining: 595ms\n#> 14:  learn: 0.1480845    total: 100ms    remaining: 567ms\n#> 15:  learn: 0.1454840    total: 103ms    remaining: 541ms\n#> 16:  learn: 0.1429863    total: 106ms    remaining: 518ms\n#> 17:  learn: 0.1407059    total: 109ms    remaining: 497ms\n#> 18:  learn: 0.1383697    total: 112ms    remaining: 477ms\n#> 19:  learn: 0.1361779    total: 115ms    remaining: 460ms\n#> 20:  learn: 0.1340707    total: 118ms    remaining: 444ms\n#> 21:  learn: 0.1320342    total: 122ms    remaining: 432ms\n#> 22:  learn: 0.1300775    total: 125ms    remaining: 418ms\n#> 23:  learn: 0.1282447    total: 129ms    remaining: 408ms\n#> 24:  learn: 0.1264588    total: 133ms    remaining: 399ms\n#> 25:  learn: 0.1247594    total: 137ms    remaining: 390ms\n#> 26:  learn: 0.1232137    total: 141ms    remaining: 381ms\n#> 27:  learn: 0.1216563    total: 145ms    remaining: 373ms\n#> 28:  learn: 0.1202034    total: 149ms    remaining: 365ms\n#> 29:  learn: 0.1187735    total: 153ms    remaining: 357ms\n#> 30:  learn: 0.1174263    total: 157ms    remaining: 350ms\n#> 31:  learn: 0.1161250    total: 162ms    remaining: 344ms\n#> 32:  learn: 0.1148583    total: 172ms    remaining: 350ms\n#> 33:  learn: 0.1136688    total: 176ms    remaining: 342ms\n#> 34:  learn: 0.1125292    total: 180ms    remaining: 335ms\n#> 35:  learn: 0.1114357    total: 184ms    remaining: 327ms\n#> 36:  learn: 0.1104088    total: 187ms    remaining: 318ms\n#> 37:  learn: 0.1094203    total: 190ms    remaining: 309ms\n#> 38:  learn: 0.1085302    total: 193ms    remaining: 301ms\n#> 39:  learn: 0.1076702    total: 196ms    remaining: 294ms\n#> 40:  learn: 0.1068348    total: 200ms    remaining: 288ms\n#> 41:  learn: 0.1060263    total: 204ms    remaining: 281ms\n#> 42:  learn: 0.1052530    total: 207ms    remaining: 274ms\n#> 43:  learn: 0.1045115    total: 212ms    remaining: 269ms\n#> 44:  learn: 0.1037861    total: 216ms    remaining: 264ms\n#> 45:  learn: 0.1031053    total: 220ms    remaining: 259ms\n#> 46:  learn: 0.1024645    total: 225ms    remaining: 254ms\n#> 47:  learn: 0.1018457    total: 229ms    remaining: 248ms\n#> 48:  learn: 0.1012497    total: 232ms    remaining: 242ms\n#> 49:  learn: 0.1006996    total: 235ms    remaining: 235ms\n#> 50:  learn: 0.1001835    total: 239ms    remaining: 229ms\n#> 51:  learn: 0.0996695    total: 242ms    remaining: 223ms\n#> 52:  learn: 0.0991990    total: 246ms    remaining: 218ms\n#> 53:  learn: 0.0987716    total: 249ms    remaining: 212ms\n#> 54:  learn: 0.0983443    total: 252ms    remaining: 206ms\n#> 55:  learn: 0.0979411    total: 255ms    remaining: 200ms\n#> 56:  learn: 0.0975621    total: 258ms    remaining: 195ms\n#> 57:  learn: 0.0972083    total: 261ms    remaining: 189ms\n#> 58:  learn: 0.0968639    total: 265ms    remaining: 184ms\n#> 59:  learn: 0.0965243    total: 269ms    remaining: 179ms\n#> 60:  learn: 0.0961923    total: 272ms    remaining: 174ms\n#> 61:  learn: 0.0958829    total: 275ms    remaining: 168ms\n#> 62:  learn: 0.0956101    total: 278ms    remaining: 163ms\n#> 63:  learn: 0.0953473    total: 281ms    remaining: 158ms\n#> 64:  learn: 0.0950908    total: 284ms    remaining: 153ms\n#> 65:  learn: 0.0948327    total: 287ms    remaining: 148ms\n#> 66:  learn: 0.0946033    total: 290ms    remaining: 143ms\n#> 67:  learn: 0.0943820    total: 293ms    remaining: 138ms\n#> 68:  learn: 0.0941723    total: 297ms    remaining: 133ms\n#> 69:  learn: 0.0939663    total: 300ms    remaining: 129ms\n#> 70:  learn: 0.0937677    total: 304ms    remaining: 124ms\n#> 71:  learn: 0.0935819    total: 307ms    remaining: 119ms\n#> 72:  learn: 0.0933969    total: 310ms    remaining: 115ms\n#> 73:  learn: 0.0932181    total: 313ms    remaining: 110ms\n#> 74:  learn: 0.0930514    total: 316ms    remaining: 105ms\n#> 75:  learn: 0.0928960    total: 320ms    remaining: 101ms\n#> 76:  learn: 0.0927433    total: 323ms    remaining: 96.4ms\n#> 77:  learn: 0.0925871    total: 326ms    remaining: 91.9ms\n#> 78:  learn: 0.0924635    total: 329ms    remaining: 87.6ms\n#> 79:  learn: 0.0923467    total: 333ms    remaining: 83.1ms\n#> 80:  learn: 0.0922292    total: 336ms    remaining: 78.7ms\n#> 81:  learn: 0.0921089    total: 339ms    remaining: 74.3ms\n#> 82:  learn: 0.0919860    total: 342ms    remaining: 70.1ms\n#> 83:  learn: 0.0918862    total: 346ms    remaining: 65.9ms\n#> 84:  learn: 0.0917925    total: 349ms    remaining: 61.6ms\n#> 85:  learn: 0.0916927    total: 353ms    remaining: 57.4ms\n#> 86:  learn: 0.0916089    total: 356ms    remaining: 53.1ms\n#> 87:  learn: 0.0915482    total: 356ms    remaining: 48.6ms\n#> 88:  learn: 0.0914624    total: 362ms    remaining: 44.7ms\n#> 89:  learn: 0.0913841    total: 365ms    remaining: 40.5ms\n#> 90:  learn: 0.0912981    total: 368ms    remaining: 36.4ms\n#> 91:  learn: 0.0912323    total: 371ms    remaining: 32.3ms\n#> 92:  learn: 0.0911620    total: 374ms    remaining: 28.2ms\n#> 93:  learn: 0.0910968    total: 378ms    remaining: 24.1ms\n#> 94:  learn: 0.0910390    total: 381ms    remaining: 20ms\n#> 95:  learn: 0.0909742    total: 384ms    remaining: 16ms\n#> 96:  learn: 0.0909243    total: 387ms    remaining: 12ms\n#> 97:  learn: 0.0908512    total: 390ms    remaining: 7.95ms\n#> 98:  learn: 0.0907883    total: 393ms    remaining: 3.97ms\n#> 99:  learn: 0.0907341    total: 396ms    remaining: 0us\n#> <catboost.core.CatBoostRegressor object at 0x7f75bdded3c0>\n\n\nPredecimos\n\n\nMostrar / ocultar código\n\n# Make predictions on the test set\npreds = model.predict(x_test.reshape(-1, 1))\npreds = pd.DataFrame(preds, columns=[f'pred_{q}' for q in quantiles])\n\npreds.head(6)\n#>    pred_0.1  pred_0.2  pred_0.3  ...  pred_0.7  pred_0.8  pred_0.9\n#> 0  3.401601  3.542894  3.619259  ...  3.938859  4.041568  4.165809\n#> 1  4.150397  4.269908  4.355643  ...  4.701792  4.816983  4.958197\n#> 2  4.350918  4.476999  4.595515  ...  4.933372  5.017778  5.147716\n#> 3  2.829240  2.936415  3.034001  ...  3.365471  3.446338  3.577815\n#> 4  3.456021  3.574613  3.658350  ...  3.966363  4.047775  4.166815\n#> 5  3.356988  3.505757  3.580486  ...  3.858433  3.961821  4.084200\n#> \n#> [6 rows x 9 columns]\n\n\nPintamos\n\n\nMostrar / ocultar código\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x_test, y_test)\n\nfor col in ['pred_0.1', 'pred_0.5', 'pred_0.9']:\n    ax.scatter(x_test.reshape(-1,1), preds[col], alpha=0.50, label=col)\n\nax.legend()\n\n\n\n\n\n\n\n\n\nY ya estaría, no parece mala alternativa si uno tiene que hacer este tipo de cosas.\n\n\n\n\n\n\nTip\n\n\n\nOjalá le sirva a mi amigo Kenet para una cosa que estaba bicheando.\n\n\nPues poco más. Feliz domingo\nCon R también se puede, como no.\n\n\nMostrar / ocultar código\nlibrary(reticulate) # para comunicar R y python y poder convertir datos y funciones de uno a otro bidireccionalmente\nlibrary(catboost)\n\nX_train <- as.matrix(py$x_train) # catboost en R espera  una matriz\nY_train <-  as.matrix(py$y_train)\n\n\nX_test <- as.matrix(py$x_test) \nY_test <-  as.matrix(py$y_test)\n\nhead(X_train) ; head(Y_train)\n#>           [,1]\n#> [1,] 0.6499924\n#> [2,] 0.9125034\n#> [3,] 0.4820761\n#> [4,] 0.2060426\n#> [5,] 0.5411845\n#> [6,] 0.1779657\n#>          [,1]\n#> [1,] 4.165367\n#> [2,] 5.018479\n#> [3,] 4.146041\n#> [4,] 2.942043\n#> [5,] 4.287877\n#> [6,] 3.589826\n\n(quantiles_str <-  py$quantile_str)\n#> [1] \"0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\"\n\n\n\n\nMostrar / ocultar código\ntrain_pool <- catboost.load_pool(data = X_train, label = Y_train)\ntest_pool <- catboost.load_pool(data = X_test)\nloss_function <-  paste0(\"MultiQuantile:alpha=\", quantiles_str)\n\nfit_params <-  list(\n    iterations = 100,\n    loss_function= loss_function\n    )\n\n\n\n\nMostrar / ocultar código\n\nmodel <- catboost.train(train_pool, params=fit_params)\n#> 0:   learn: 0.1977907    total: 50.6ms   remaining: 5s\n#> 1:   learn: 0.1934878    total: 56.9ms   remaining: 2.79s\n#> 2:   learn: 0.1891389    total: 61.3ms   remaining: 1.98s\n#> 3:   learn: 0.1849242    total: 66.1ms   remaining: 1.58s\n#> 4:   learn: 0.1808818    total: 71ms remaining: 1.35s\n#> 5:   learn: 0.1769520    total: 74.5ms   remaining: 1.17s\n#> 6:   learn: 0.1732251    total: 78.2ms   remaining: 1.04s\n#> 7:   learn: 0.1696774    total: 83ms remaining: 955ms\n#> 8:   learn: 0.1661482    total: 86.6ms   remaining: 876ms\n#> 9:   learn: 0.1628541    total: 89.7ms   remaining: 807ms\n#> 10:  learn: 0.1596378    total: 93.1ms   remaining: 753ms\n#> 11:  learn: 0.1565479    total: 97.3ms   remaining: 714ms\n#> 12:  learn: 0.1536016    total: 102ms    remaining: 683ms\n#> 13:  learn: 0.1507692    total: 105ms    remaining: 647ms\n#> 14:  learn: 0.1480845    total: 109ms    remaining: 615ms\n#> 15:  learn: 0.1454840    total: 113ms    remaining: 592ms\n#> 16:  learn: 0.1429863    total: 118ms    remaining: 576ms\n#> 17:  learn: 0.1407059    total: 122ms    remaining: 557ms\n#> 18:  learn: 0.1383697    total: 126ms    remaining: 536ms\n#> 19:  learn: 0.1361779    total: 129ms    remaining: 518ms\n#> 20:  learn: 0.1340707    total: 133ms    remaining: 500ms\n#> 21:  learn: 0.1320342    total: 137ms    remaining: 485ms\n#> 22:  learn: 0.1300775    total: 140ms    remaining: 468ms\n#> 23:  learn: 0.1282447    total: 144ms    remaining: 455ms\n#> 24:  learn: 0.1264588    total: 147ms    remaining: 441ms\n#> 25:  learn: 0.1247594    total: 151ms    remaining: 429ms\n#> 26:  learn: 0.1232137    total: 154ms    remaining: 416ms\n#> 27:  learn: 0.1216563    total: 157ms    remaining: 404ms\n#> 28:  learn: 0.1202034    total: 161ms    remaining: 394ms\n#> 29:  learn: 0.1187735    total: 164ms    remaining: 383ms\n#> 30:  learn: 0.1174263    total: 169ms    remaining: 375ms\n#> 31:  learn: 0.1161250    total: 172ms    remaining: 365ms\n#> 32:  learn: 0.1148583    total: 175ms    remaining: 356ms\n#> 33:  learn: 0.1136688    total: 180ms    remaining: 349ms\n#> 34:  learn: 0.1125292    total: 185ms    remaining: 343ms\n#> 35:  learn: 0.1114357    total: 188ms    remaining: 334ms\n#> 36:  learn: 0.1104088    total: 195ms    remaining: 332ms\n#> 37:  learn: 0.1094203    total: 199ms    remaining: 325ms\n#> 38:  learn: 0.1085302    total: 202ms    remaining: 316ms\n#> 39:  learn: 0.1076702    total: 205ms    remaining: 308ms\n#> 40:  learn: 0.1068348    total: 212ms    remaining: 305ms\n#> 41:  learn: 0.1060263    total: 216ms    remaining: 298ms\n#> 42:  learn: 0.1052530    total: 219ms    remaining: 290ms\n#> 43:  learn: 0.1045115    total: 223ms    remaining: 284ms\n#> 44:  learn: 0.1037861    total: 228ms    remaining: 278ms\n#> 45:  learn: 0.1031053    total: 232ms    remaining: 272ms\n#> 46:  learn: 0.1024645    total: 235ms    remaining: 265ms\n#> 47:  learn: 0.1018457    total: 242ms    remaining: 262ms\n#> 48:  learn: 0.1012497    total: 246ms    remaining: 256ms\n#> 49:  learn: 0.1006996    total: 250ms    remaining: 250ms\n#> 50:  learn: 0.1001835    total: 253ms    remaining: 243ms\n#> 51:  learn: 0.0996695    total: 257ms    remaining: 237ms\n#> 52:  learn: 0.0991990    total: 261ms    remaining: 232ms\n#> 53:  learn: 0.0987716    total: 265ms    remaining: 225ms\n#> 54:  learn: 0.0983443    total: 268ms    remaining: 219ms\n#> 55:  learn: 0.0979411    total: 272ms    remaining: 214ms\n#> 56:  learn: 0.0975621    total: 275ms    remaining: 208ms\n#> 57:  learn: 0.0972083    total: 279ms    remaining: 202ms\n#> 58:  learn: 0.0968639    total: 283ms    remaining: 197ms\n#> 59:  learn: 0.0965243    total: 287ms    remaining: 191ms\n#> 60:  learn: 0.0961923    total: 292ms    remaining: 187ms\n#> 61:  learn: 0.0958829    total: 298ms    remaining: 182ms\n#> 62:  learn: 0.0956101    total: 304ms    remaining: 178ms\n#> 63:  learn: 0.0953473    total: 308ms    remaining: 173ms\n#> 64:  learn: 0.0950908    total: 313ms    remaining: 168ms\n#> 65:  learn: 0.0948327    total: 317ms    remaining: 163ms\n#> 66:  learn: 0.0946033    total: 324ms    remaining: 160ms\n#> 67:  learn: 0.0943820    total: 329ms    remaining: 155ms\n#> 68:  learn: 0.0941723    total: 332ms    remaining: 149ms\n#> 69:  learn: 0.0939663    total: 338ms    remaining: 145ms\n#> 70:  learn: 0.0937677    total: 342ms    remaining: 140ms\n#> 71:  learn: 0.0935819    total: 346ms    remaining: 134ms\n#> 72:  learn: 0.0933969    total: 349ms    remaining: 129ms\n#> 73:  learn: 0.0932181    total: 353ms    remaining: 124ms\n#> 74:  learn: 0.0930514    total: 358ms    remaining: 119ms\n#> 75:  learn: 0.0928960    total: 361ms    remaining: 114ms\n#> 76:  learn: 0.0927433    total: 364ms    remaining: 109ms\n#> 77:  learn: 0.0925871    total: 369ms    remaining: 104ms\n#> 78:  learn: 0.0924635    total: 374ms    remaining: 99.3ms\n#> 79:  learn: 0.0923467    total: 377ms    remaining: 94.3ms\n#> 80:  learn: 0.0922292    total: 380ms    remaining: 89.2ms\n#> 81:  learn: 0.0921089    total: 386ms    remaining: 84.8ms\n#> 82:  learn: 0.0919860    total: 390ms    remaining: 79.9ms\n#> 83:  learn: 0.0918862    total: 394ms    remaining: 75ms\n#> 84:  learn: 0.0917925    total: 398ms    remaining: 70.2ms\n#> 85:  learn: 0.0916927    total: 404ms    remaining: 65.8ms\n#> 86:  learn: 0.0916089    total: 408ms    remaining: 61ms\n#> 87:  learn: 0.0915482    total: 409ms    remaining: 55.8ms\n#> 88:  learn: 0.0914624    total: 412ms    remaining: 50.9ms\n#> 89:  learn: 0.0913841    total: 416ms    remaining: 46.2ms\n#> 90:  learn: 0.0912981    total: 420ms    remaining: 41.6ms\n#> 91:  learn: 0.0912323    total: 424ms    remaining: 36.9ms\n#> 92:  learn: 0.0911620    total: 427ms    remaining: 32.2ms\n#> 93:  learn: 0.0910968    total: 431ms    remaining: 27.5ms\n#> 94:  learn: 0.0910390    total: 437ms    remaining: 23ms\n#> 95:  learn: 0.0909742    total: 441ms    remaining: 18.4ms\n#> 96:  learn: 0.0909243    total: 444ms    remaining: 13.7ms\n#> 97:  learn: 0.0908512    total: 449ms    remaining: 9.17ms\n#> 98:  learn: 0.0907883    total: 454ms    remaining: 4.58ms\n#> 99:  learn: 0.0907341    total: 457ms    remaining: 0us\n\n\n\n\nMostrar / ocultar código\npredicciones <- catboost.predict(model, pool = test_pool)\n\n\n\n\nMostrar / ocultar código\ncolnames(predicciones) <- paste0(\"quantile_\", 1:9) \n\nhead(predicciones)\n#>      quantile_1 quantile_2 quantile_3 quantile_4 quantile_5 quantile_6\n#> [1,]   3.401601   3.542894   3.619259   3.716297   3.794767   3.868639\n#> [2,]   4.150397   4.269908   4.355643   4.431774   4.502443   4.586961\n#> [3,]   4.350918   4.476999   4.595515   4.651673   4.713752   4.815297\n#> [4,]   2.829240   2.936415   3.034001   3.123039   3.208654   3.292293\n#> [5,]   3.456021   3.574613   3.658350   3.741498   3.824226   3.897069\n#> [6,]   3.356988   3.505757   3.580486   3.651319   3.720015   3.791630\n#>      quantile_7 quantile_8 quantile_9\n#> [1,]   3.938859   4.041568   4.165809\n#> [2,]   4.701792   4.816983   4.958197\n#> [3,]   4.933372   5.017778   5.147716\n#> [4,]   3.365471   3.446338   3.577815\n#> [5,]   3.966363   4.047775   4.166815\n#> [6,]   3.858433   3.961821   4.084200"
  },
  {
    "objectID": "test_quarto_things.html",
    "href": "test_quarto_things.html",
    "title": "to_test",
    "section": "",
    "text": "Note\n\n\n\ndfdf\n\n\n\ntab1tab2tab3\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nplot(mtcars$mpg, mtcars$cyl)\n\n\n\n\n\n\n\n\n\n\nparece que mola\n\n\na ver\n\n\n\n\n😨\n\n\n\n\n\n\nTip\n\n\n\nmi tip"
  }
]
{
  "hash": "8d45e572964703c9aba9c0bb7b294d3d",
  "result": {
    "markdown": "---\ntitle: Meta-análisis. Agregando encuestas\ndate: '2023-07-22'\ncategories:\n  - muestreo\n  - 2023\n  - encuestas electorales\n  - análisis bayesiano\n  \nimage: metanalisis.png\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\n## Introducción\n\nYa en\n[2022](https://muestrear-no-es-pecado.netlify.app/2022/01/01/cocinando/)\nos mostraba uno de los ingredientes principales de la cocina electoral,\nal menos de la tradicional, no de la postmoderna Alaminos-Tezanos.\n\nHoy os quiero contar como haría yo la agregación de encuestas, cuando no\nse tienen los datos brutos. En primer lugar aviso de lo que sigue a\ncontinuación sólo lo he hecho **por diversión** y faltaría mucho más\ntrabajo para considerarlo un intento serio.\n\nLa diversión viene por este\n[tweet](https://twitter.com/AnaBayes/status/1682035400507002881) de\n[Anabel Forte](https://twitter.com/AnaBayes) que puso como contestación\na un hilo dónde Kiko Llaneras explicaba su modelo de predicción\nagregando encuestas y haciendo simulaciones. Aquí el [hilo de\nkiko](https://twitter.com/kikollan/status/1681610401367326720) y en la\nimagen la respuesta de Ana.\n\n![Tweet de Ana](tweet_ana_bayes.png)\n\n\nTotal, que dado que conozco a Ana y a Virgilio y son bayesianos y yo sólo un aprendiz de la cosa, pues he intentado un metaanálisis bayesiano sencillo juntando varias encuestas. \n\n\n## Datos \n\nLo primero era intentar encontrar datos de las encuestas que se han hecho, importante que tengan tanto la estimación como el tamaño muestral. Si, ya sé que cada empresa tiene su cocina y sus cosas, que unas son telefónicas, que otras son tracking o paneles y tal, pero ya he dicho que lo estoy haciendo por diversión.. \n\nBueno, pues [aquí](https://www.epdata.es/datos/elecciones-generales-cortes-23j-resultados-analisis-encuestas-censo-comunidades-provincias-municipios-estadisticas-mapas-datos-graficos/690#encuestas) he encontrado la info que buscaba. El tema es que la tabla está en una tabla de datawrapper  [enlace_table](https://www.datawrapper.de/_/I2mqK/) y no he sido capaz de escrapear de forma programática, que se le va a hacer, no vale uno pa to. \n\nComo eran muchas encuestas pues he ido seleccionando algunas del mes de julio y al final me he quedado con unas 23. Para cada encuesta he puesto su tamaño muestral, la diferencia entre la fecha de las elecciones y la fecha de la realización de la encuesta, variable `time` , también he convertido a votos la estimación que dan para pp, psoe, sumar, vox y resto, simplemente multiplicando la estimación que dan por su tamaño muestral.  \n\n\nMejor vemos la tabla \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(DT)\ndf <-  read_csv(here::here(\"data/encuestas_agregadas.csv\")) |> \n    select(empresa, time, partido, everything())\n\ndatatable(df)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-ca88a647aee7a3666d45\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ca88a647aee7a3666d45\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\"],[\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"electo_mania\",\"electo_mania\",\"electo_mania\",\"electo_mania\",\"electo_mania\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"nc_report\",\"nc_report\",\"nc_report\",\"nc_report\",\"nc_report\",\"prisa\",\"prisa\",\"prisa\",\"prisa\",\"prisa\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"sigma2\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"celeste_tel\",\"celeste_tel\",\"celeste_tel\",\"celeste_tel\",\"celeste_tel\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"nc_report\",\"nc_report\",\"nc_report\",\"nc_report\",\"nc_report\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"ok_diario\",\"ok_diario\",\"ok_diario\",\"ok_diario\",\"ok_diario\",\"gesop\",\"gesop\",\"gesop\",\"gesop\",\"gesop\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"nc_report\",\"nc_report\",\"nc_report\",\"nc_report\",\"nc_report\",\"ok_diario\",\"ok_diario\",\"ok_diario\",\"ok_diario\",\"ok_diario\",\"prisa\",\"prisa\",\"prisa\",\"prisa\",\"prisa\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\"],[-21,-21,-21,-21,-21,-20,-20,-20,-20,-20,-20,-20,-20,-20,-20,-13,-13,-13,-13,-13,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-12,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-11,-10,-10,-10,-10,-10,-8,-8,-8,-8,-8,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7,-7],[\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\",\"pp\",\"psoe\",\"sumar\",\"vox\",\"resto\"],[0.326,0.27,0.129,0.144,0.131,0.368,0.282,0.128,0.115,0.107,0.347,0.285,0.128,0.132,0.1080000000000001,0.346,0.286,0.123,0.127,0.1180000000000001,0.34,0.291,0.123,0.134,0.112,0.367,0.279,0.12,0.12,0.114,0.36,0.278,0.122,0.124,0.116,0.312,0.295,0.133,0.147,0.113,0.344,0.286,0.127,0.13,0.113,0.311,0.288,0.15,0.145,0.1060000000000001,0.365,0.284,0.115,0.114,0.122,0.367,0.282,0.116,0.124,0.111,0.363,0.277,0.119,0.12,0.121,0.322,0.291,0.144,0.142,0.101,0.338,0.286,0.127,0.139,0.11,0.355,0.263,0.127,0.135,0.12,0.306,0.274,0.144,0.15,0.126,0.37,0.284,0.115,0.119,0.112,0.372,0.283,0.113,0.111,0.121,0.356,0.266,0.127,0.131,0.12,0.329,0.287,0.137,0.135,0.112,0.322,0.286,0.138,0.139,0.115,0.335,0.283,0.132,0.137,0.113],[1447,1447,1447,1447,1447,1503,1503,1503,1503,1503,2880,2880,2880,2880,2880,4482,4482,4482,4482,4482,2788,2788,2788,2788,2788,5000,5000,5000,5000,5000,1000,1000,1000,1000,1000,2000,2000,2000,2000,2000,4455,4455,4455,4455,4455,1300,1300,1300,1300,1300,1100,1100,1100,1100,1100,5502,5502,5502,5502,5502,1000,1000,1000,1000,1000,1300,1300,1300,1300,1300,800,800,800,800,800,1500,1500,1500,1500,1500,1200,1200,1200,1200,1200,7002,7002,7002,7002,7002,1000,1000,1000,1000,1000,1500,1500,1500,1500,1500,2000,2000,2000,2000,2000,1600,1600,1600,1600,1600,1500,1500,1500,1500,1500],[472,391,187,207,190,553,424,192,173,161,999,821,369,380,311,1551,1282,551,569,529,948,811,343,374,312,1835,1395,600,600,570,360,278,122,124,116,624,590,266,294,226,1533,1274,566,579,503,404,374,196,188,138,402,312,127,125,134,2019,1552,638,682,611,363,277,119,120,121,419,378,187,185,131,270,229,102,111,88,532,394,190,204,180,367,329,173,180,151,2591,1989,805,833,784,372,283,113,111,121,534,399,190,197,180,658,574,274,270,224,515,458,221,222,184,503,424,198,206,169]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>empresa<\\/th>\\n      <th>time<\\/th>\\n      <th>partido<\\/th>\\n      <th>estim<\\/th>\\n      <th>n<\\/th>\\n      <th>votos<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\nPintamos \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ncolores <-  c(\n    \"pp\" = \"#005999\",\n    \"psoe\" = \"#FF0126\", \n    \"sumar\" = \"#A00B85\", \n    \"vox\" = \"#51962A\", \n    \"resto\" = \"grey\"\n    )\n\ndf |> \n    ggplot(aes(x = time, y = estim,color = partido )) +\n    geom_point() +\n    scale_color_manual(values = colores) +\n    geom_smooth(se = FALSE)\n```\n\n::: {.cell-output-display}\n![](metanalisis_files/figure-html/unnamed-chunk-2-1.png){width=80%}\n:::\n:::\n\n\n\nLa selección de encuestas la he hecho sin mucho orden, son todas del mes de julio, algunas empresas repiten como sigma2 , gad3, simple_logica o sociométrica, otras veces he puesto como nombre el medio (okdiario o prisa). \n\nBueno, pues vamos a ver como hago el metaanálisis. \n\n### Preparación datos\n\nVoy a poner los datos en un formato que me conviene más para lo que quiero hacer.\n\n- n es tamaño de muestra\n- time : es días hasta elecciones, -7 quiere decir qeu la encuesta se publicó (o se hizo, no lo sé) 7 días antes del 23 de julio\n- Columnas resultantes de multiplicar la estimación en la encuesta para cada partido por el tamaño muestral\n\n\nComo vemos voy a considerar 23 encuestas. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_wider <- df |> \n    select(-estim) |> \n    pivot_wider( id_cols = c(empresa, n, time),\n                 names_from = partido, \n                 values_from = votos) |> \n    arrange(empresa)\n\nDT::datatable(df_wider)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-62cd3055503eeec9d8da\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-62cd3055503eeec9d8da\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"],[\"celeste_tel\",\"electo_mania\",\"gad3\",\"gad3\",\"gad3\",\"gad3\",\"gesop\",\"nc_report\",\"nc_report\",\"nc_report\",\"ok_diario\",\"ok_diario\",\"prisa\",\"prisa\",\"sigma2\",\"sigma2\",\"sigma2\",\"simple_logica\",\"simple_logica\",\"simple_logica\",\"socio_metrica\",\"socio_metrica\",\"socio_metrica\"],[1100,2788,1503,5000,5502,7002,1200,1000,1000,1000,1500,1500,2000,2000,2880,4482,4455,1300,1300,1600,1447,800,1500],[-11,-12,-20,-12,-11,-7,-8,-12,-11,-7,-10,-7,-12,-7,-20,-13,-12,-12,-11,-7,-21,-11,-7],[402,948,553,1835,2019,2591,367,360,363,372,532,534,624,658,999,1551,1533,404,419,515,472,270,503],[312,811,424,1395,1552,1989,329,278,277,283,394,399,590,574,821,1282,1274,374,378,458,391,229,424],[127,343,192,600,638,805,173,122,119,113,190,190,266,274,369,551,566,196,187,221,187,102,198],[125,374,173,600,682,833,180,124,120,111,204,197,294,270,380,569,579,188,185,222,207,111,206],[134,312,161,570,611,784,151,116,121,121,180,180,226,224,311,529,503,138,131,184,190,88,169]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>empresa<\\/th>\\n      <th>n<\\/th>\\n      <th>time<\\/th>\\n      <th>pp<\\/th>\\n      <th>psoe<\\/th>\\n      <th>sumar<\\/th>\\n      <th>vox<\\/th>\\n      <th>resto<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\nPues con esto ya puedo hacer mi intento de meta-análisis, que es probable que esté mal, que no soy un experto en estas cosas. \n\n\n## Meta-análisis\n\nPues lo voy a hacer de forma bayesiana. Los datos los tenemos a nivel de encuesta, por lo que puedo considerar que los votos estimados a cada partido en cada encuesta siguen una distribución multinomial , dónde `n` (tamaño muestral) es el número de intentos y tengo el vector de votos a cada partido que se obtendría. La suma de pp+psoe+sumar+vox+resto es igual a `n` para cada fila de los datos. \n\nTambién puedo considerar que las estimaciones de varias encuestas realizadas por la misma empresa no son independientes, no es descabellado ¿verdad?.  Y también podría considerar que las estimaciones varían conforme se acerca la fecha de las elecciones y que esta variación podría ser diferente para cada empresa encuestadora. Pues con estos ingredientes ya puedo hacer el \"meta-análisis\" \n\n\nUtilizo la librería `brms`  que me va a permitir hacerlo con una interfaz sencilla. Y en algún momento del futuro miraré como hacerlo con `numpyro` que me está picando con eso [Carlos](https://www.datanalytics.com/2023/07/04/3pl-numpyro/)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlibrary(cmdstanr)\nlibrary(brms)\nlibrary(tidybayes)\n\noptions(brms.backend=\"cmdstanr\")\n\n```\n:::\n\n\nCreamos una columna que una las columnas de los votos a partidos para que sea nuestro vector de respuesta _multinomial_\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndf_wider$cell_counts <- with(df_wider, cbind(pp, psoe,sumar, vox, resto))\n\nDT::datatable(head(df_wider))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-a7e9ea48a77946899f31\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-a7e9ea48a77946899f31\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[\"celeste_tel\",\"electo_mania\",\"gad3\",\"gad3\",\"gad3\",\"gad3\"],[1100,2788,1503,5000,5502,7002],[-11,-12,-20,-12,-11,-7],[402,948,553,1835,2019,2591],[312,811,424,1395,1552,1989],[127,343,192,600,638,805],[125,374,173,600,682,833],[134,312,161,570,611,784],[[402,312,127,125,134],[948,811,343,374,312],[553,424,192,173,161],[1835,1395,600,600,570],[2019,1552,638,682,611],[2591,1989,805,833,784]]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>empresa<\\/th>\\n      <th>n<\\/th>\\n      <th>time<\\/th>\\n      <th>pp<\\/th>\\n      <th>psoe<\\/th>\\n      <th>sumar<\\/th>\\n      <th>vox<\\/th>\\n      <th>resto<\\/th>\\n      <th>cell_counts<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7,8,9]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nY pasamos a ajustar el modelo, dónde vamos a considerar como efecto aleatorio la empresa y como efecto fijo el tiempo, aunque diferente para cada empresa. \n\n\nEn la fórmula de `brms` añadimos informacióna la variable respuesta, en este caso añadimos la info del tamaño muestral. Mirando cosas sobre meta-análisis con `brms` se puede añadir cosas como desviación estándar de la estimación del efecto y cosas así. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nformula <- brmsformula(\n    cell_counts | trials(n) ~  (time |empresa))\n\n# vemos las priors por defecto qeu h\n(priors <- get_prior(formula, df_wider, family = multinomial()))\n#>                 prior     class      coef   group resp    dpar nlpar lb ub\n#>                lkj(1)       cor                                           \n#>                lkj(1)       cor           empresa                         \n#>                (flat) Intercept                                           \n#>  student_t(3, 0, 2.5) Intercept                         mupsoe            \n#>  student_t(3, 0, 2.5)        sd                         mupsoe        0   \n#>  student_t(3, 0, 2.5)        sd           empresa       mupsoe        0   \n#>  student_t(3, 0, 2.5)        sd Intercept empresa       mupsoe        0   \n#>  student_t(3, 0, 2.5)        sd      time empresa       mupsoe        0   \n#>  student_t(3, 0, 2.5) Intercept                        muresto            \n#>  student_t(3, 0, 2.5)        sd                        muresto        0   \n#>  student_t(3, 0, 2.5)        sd           empresa      muresto        0   \n#>  student_t(3, 0, 2.5)        sd Intercept empresa      muresto        0   \n#>  student_t(3, 0, 2.5)        sd      time empresa      muresto        0   \n#>  student_t(3, 0, 2.5) Intercept                        musumar            \n#>  student_t(3, 0, 2.5)        sd                        musumar        0   \n#>  student_t(3, 0, 2.5)        sd           empresa      musumar        0   \n#>  student_t(3, 0, 2.5)        sd Intercept empresa      musumar        0   \n#>  student_t(3, 0, 2.5)        sd      time empresa      musumar        0   \n#>  student_t(3, 0, 2.5) Intercept                          muvox            \n#>  student_t(3, 0, 2.5)        sd                          muvox        0   \n#>  student_t(3, 0, 2.5)        sd           empresa        muvox        0   \n#>  student_t(3, 0, 2.5)        sd Intercept empresa        muvox        0   \n#>  student_t(3, 0, 2.5)        sd      time empresa        muvox        0   \n#>        source\n#>       default\n#>  (vectorized)\n#>       default\n#>       default\n#>       default\n#>  (vectorized)\n#>  (vectorized)\n#>  (vectorized)\n#>       default\n#>       default\n#>  (vectorized)\n#>  (vectorized)\n#>  (vectorized)\n#>       default\n#>       default\n#>  (vectorized)\n#>  (vectorized)\n#>  (vectorized)\n#>       default\n#>       default\n#>  (vectorized)\n#>  (vectorized)\n#>  (vectorized)\n```\n:::\n\n\n\nAjustamos el modelo \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_multinomial <-\n    brm(\n        formula,\n        df_wider,\n        multinomial(),\n        prior = priors,\n        iter = 4000,\n        warmup = 1000,\n        cores = 4,\n        chains = 4,\n        seed = 47,\n        backend = \"cmdstanr\",\n        control = list(adapt_delta = 0.95), \n        refresh = 0\n    )\n#> Running MCMC with 4 parallel chains...\n#> \n#> Chain 1 finished in 8.8 seconds.\n#> Chain 3 finished in 8.9 seconds.\n#> Chain 4 finished in 8.9 seconds.\n#> Chain 2 finished in 10.2 seconds.\n#> \n#> All 4 chains finished successfully.\n#> Mean chain execution time: 9.2 seconds.\n#> Total execution time: 10.3 seconds.\n\nsummary(model_multinomial)\n#>  Family: multinomial \n#>   Links: mupsoe = logit; musumar = logit; muvox = logit; muresto = logit \n#> Formula: cell_counts | trials(n) ~ (time | empresa) \n#>    Data: df_wider (Number of observations: 23) \n#>   Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 12000\n#> \n#> Group-Level Effects: \n#> ~empresa (Number of levels: 10) \n#>                                     Estimate Est.Error l-95% CI u-95% CI Rhat\n#> sd(mupsoe_Intercept)                    0.05      0.03     0.00     0.13 1.00\n#> sd(mupsoe_time)                         0.00      0.00     0.00     0.01 1.00\n#> sd(musumar_Intercept)                   0.14      0.06     0.05     0.28 1.00\n#> sd(musumar_time)                        0.00      0.00     0.00     0.02 1.00\n#> sd(muvox_Intercept)                     0.11      0.06     0.01     0.25 1.00\n#> sd(muvox_time)                          0.01      0.01     0.00     0.02 1.00\n#> sd(muresto_Intercept)                   0.05      0.04     0.00     0.16 1.00\n#> sd(muresto_time)                        0.01      0.00     0.00     0.01 1.00\n#> cor(mupsoe_Intercept,mupsoe_time)       0.06      0.58    -0.94     0.95 1.00\n#> cor(musumar_Intercept,musumar_time)     0.28      0.56    -0.88     0.98 1.00\n#> cor(muvox_Intercept,muvox_time)        -0.03      0.56    -0.95     0.93 1.00\n#> cor(muresto_Intercept,muresto_time)     0.21      0.58    -0.91     0.98 1.00\n#>                                     Bulk_ESS Tail_ESS\n#> sd(mupsoe_Intercept)                    3451     3599\n#> sd(mupsoe_time)                         2876     5041\n#> sd(musumar_Intercept)                   3697     3162\n#> sd(musumar_time)                        3236     4666\n#> sd(muvox_Intercept)                     2951     2225\n#> sd(muvox_time)                          1842     4433\n#> sd(muresto_Intercept)                   5233     4971\n#> sd(muresto_time)                        3078     3653\n#> cor(mupsoe_Intercept,mupsoe_time)       5869     6716\n#> cor(musumar_Intercept,musumar_time)     7015     7025\n#> cor(muvox_Intercept,muvox_time)         6490     6740\n#> cor(muresto_Intercept,muresto_time)     4284     6808\n#> \n#> Population-Level Effects: \n#>                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> mupsoe_Intercept     -0.20      0.03    -0.25    -0.15 1.00     6469     6941\n#> musumar_Intercept    -0.99      0.05    -1.09    -0.90 1.00     4232     5876\n#> muvox_Intercept      -0.97      0.05    -1.06    -0.87 1.00     3835     6313\n#> muresto_Intercept    -1.10      0.03    -1.15    -1.04 1.00     7182     7017\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nY ya tendríamos el modelo. \n\n\n## ¿Predicción / estimación?\n\nVuelvo a decir que esto es sólo por diversión, para hacer algo serio tendría que haber usado mayor número de encuestas y realizadas en diferentes momentos del tiempo, tener las estimaciones que daban en cada provincia y realizar la estimación de escaños. Todo eso y más ya lo hace, y muy bien, Kiko Llaneras para [_El País_ ](https://elpais.com/espana/elecciones-generales/2023-07-19/quien-va-a-ganar-las-elecciones-esto-dicen-las-encuestas.html)\n\n¿Cómo podríamos estimar lo que va a pasar el día de las elecciones con este modelo?\n\n\nPues podríamos considerar que las elecciones fueran una encuesta realizada por una _empresa_ que no tengo en los datos (un nuevo nivel de la variable empresa) , en este caso el gobierno, y ponemos la variable `time = 0` \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pongo n = 1 para que me devuelva las probabilidades , si ponenemos n = 15000000 nos devolvería una # estimación de cuántos votos van a cada partido\n\nnewdata <- tibble(\n    empresa = \"votaciones_dia_23\", \n    time = 0,\n    n= 1)\n\nnewdata\n#> # A tibble: 1 × 3\n#>   empresa            time     n\n#>   <chr>             <dbl> <dbl>\n#> 1 votaciones_dia_23     0     1\n```\n:::\n\n\n\nAhora utilizando una función de la librería `tidybayes` tenemos una forma fácil de añadir las _posterior predict_\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimaciones <-  newdata %>% \n    add_epred_draws(model_multinomial, allow_new_levels = TRUE) %>% \n    mutate(partido = as_factor(.category)) |> \n    select(-.category)\n\ndim(estimaciones )\n#> [1] 60000    10\n\nDT::datatable(head(estimaciones, 100))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-51153ea9fc9530a346ac\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-51153ea9fc9530a346ac\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\"],[\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\"],[\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\",\"votaciones_dia_23\"],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[0.3676332515309386,0.354985924684994,0.317969998995433,0.3526916575229171,0.3461580691347312,0.338140161612048,0.341559865315493,0.3388018811529116,0.3358168918391198,0.3525597339106822,0.3571459947352161,0.3433250384829394,0.3557304678817606,0.3346601279306838,0.3419273497594013,0.3291534651143471,0.3384162167579076,0.3428499005868273,0.3457536461371518,0.3369367471492516,0.3402791461731107,0.3449459952315391,0.3513179329444838,0.3325765299109378,0.3454497236504279,0.3364413914186942,0.3539783469389478,0.3426506331308407,0.3409705990452137,0.3501334558075403,0.3463288186591571,0.3515186676055376,0.3476359807708161,0.348590095349853,0.3438734165897767,0.3501254634142443,0.3490721090197551,0.3436847336742632,0.3464380975765564,0.3427446298506684,0.3422100773721929,0.3388645425331364,0.3397207474638965,0.3305863654109036,0.3449661422301379,0.3353526369369135,0.3338005294593941,0.3399972258894562,0.3585801203163321,0.3323061434474353,0.3468466578278699,0.3463815379196697,0.3509256326760209,0.338682585086096,0.3595748367895495,0.3459131509538066,0.3428146958580201,0.3289438488965427,0.3293911771434448,0.341082522666934,0.3527989085075651,0.3265642184613038,0.3381893817395422,0.3553909914955631,0.3399989147296247,0.3477679022460862,0.3513939716778612,0.3281014378868524,0.3339906468704446,0.3327667548878768,0.3389632877132301,0.3348754956468129,0.3357290259657251,0.331503819482879,0.3423790118074308,0.3367185782734087,0.3460965369660347,0.3420109230946067,0.3244249081943433,0.359210726429881,0.352017841659674,0.3472843998271745,0.3487573597188103,0.3338893178264886,0.338829823298666,0.3226612550839401,0.3296815106825186,0.3434361863143742,0.343270392481585,0.329524991199584,0.3450554253536346,0.3567258631365509,0.3443987647595893,0.3270367389296128,0.3575211835460132,0.3663755302856955,0.3408260385646744,0.3526721597101055,0.339723961961272,0.3382053487111304],[\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\",\"pp\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>.category<\\/th>\\n      <th>empresa<\\/th>\\n      <th>time<\\/th>\\n      <th>n<\\/th>\\n      <th>.row<\\/th>\\n      <th>.chain<\\/th>\\n      <th>.iteration<\\/th>\\n      <th>.draw<\\/th>\\n      <th>.epred<\\/th>\\n      <th>partido<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,5,6,7,8,9]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\",\"selectable\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nY tenemos 12000 estimaciones de la posteriori para cada partido. Esto se podría decir que es _equivalente_ a las 15000 simulaciones que hace Kiko con su modelo. Bueno, salvo que él en cada simulación calcula más cosas, como los escaños obtenidos etc.. \n\n\nPodemos hacer un summary de las posteriores y ver intervalo de _credibilidad_ al 80% por ejemplo \n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimaciones |> \n    group_by(partido) |> \n    summarise(\n        media = mean(.epred), \n        mediana= median(.epred), \n        low = quantile(.epred, 0.1), \n        high= quantile(.epred, 0.9)\n    )\n#> # A tibble: 5 × 5\n#>   partido media mediana   low  high\n#>   <fct>   <dbl>   <dbl> <dbl> <dbl>\n#> 1 pp      0.344   0.344 0.329 0.360\n#> 2 psoe    0.282   0.283 0.270 0.295\n#> 3 sumar   0.127   0.127 0.113 0.143\n#> 4 vox     0.131   0.131 0.118 0.144\n#> 5 resto   0.115   0.115 0.108 0.122\n```\n:::\n\n\nO pintar las distribuciones. .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nestimaciones %>% \n    ggplot(aes(x=.epred, fill = partido)) +\n    geom_density(alpha = 0.5 ) +\n    scale_x_continuous(labels = scales::percent) +\n    scale_fill_manual(values = colores) +\n    labs(title=\"Agregando encuestas por diversión. Resultado\",\n        x = \"Porcentaje estimado\", \n        y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](metanalisis_files/figure-html/unnamed-chunk-11-1.png){width=80%}\n:::\n:::\n\n\n¿Qué más cosas podemos hacer? Ya que tengo las posterioris puedo usarlas y calcular las posterioris del bloque PP+ Vox o de cualquier otra cosa. \n\n\nSupongamos que hubiera 15 millones de votos válidos. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvotantes <- 15e6\nposterioris <- estimaciones  |> \n    ungroup() |> \n    mutate(votos = votantes * .epred) |> \n    select(partido, votos) |> \n    pivot_wider(names_from = partido, values_from = votos) \n\n# tenemos columnas que son listas.  hay que hacer un unnest\nposterioris\n#> # A tibble: 1 × 5\n#>   pp             psoe           sumar          vox            resto         \n#>   <list>         <list>         <list>         <list>         <list>        \n#> 1 <dbl [12,000]> <dbl [12,000]> <dbl [12,000]> <dbl [12,000]> <dbl [12,000]>\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nposterioris <- posterioris  |> \n    unnest(c(pp, psoe, sumar, vox, resto)) \n\nhead(posterioris)\n#> # A tibble: 6 × 5\n#>         pp     psoe    sumar      vox    resto\n#>      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n#> 1 5514499. 4206887. 1712299. 1771281. 1795035.\n#> 2 5324789. 4471376. 1751240. 1737359. 1715236.\n#> 3 4769550. 4401150. 1911875. 2355184. 1562241.\n#> 4 5290375. 4255861. 1856355. 1862654. 1734756.\n#> 5 5192371. 4581113. 1769649. 1751220. 1705647.\n#> 6 5072102. 3877581. 1974599. 2322621. 1753096.\n```\n:::\n\n\n\nSumo votos de los bloques para cada una de las 12000 filas. Además. añado al bloque de la izquierda el 50% de los votos que están en resto. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterioris <- posterioris |> \n    mutate(\n        derecha = pp + vox, \n        izquierda = psoe + sumar + 0.5*resto) |> \n    mutate(derecha_posterior = derecha / votantes, \n           izquierda_posterior = izquierda/votantes)\n\nposterioris |> \n    head(20)\n#> # A tibble: 20 × 9\n#>          pp     psoe    sumar     vox  resto derecha izquierda derecha_posterior\n#>       <dbl>    <dbl>    <dbl>   <dbl>  <dbl>   <dbl>     <dbl>             <dbl>\n#>  1 5514499. 4206887. 1712299.  1.77e6 1.80e6  7.29e6  6816703.             0.486\n#>  2 5324789. 4471376. 1751240.  1.74e6 1.72e6  7.06e6  7080234.             0.471\n#>  3 4769550. 4401150. 1911875.  2.36e6 1.56e6  7.12e6  7094145.             0.475\n#>  4 5290375. 4255861. 1856355.  1.86e6 1.73e6  7.15e6  6979594.             0.477\n#>  5 5192371. 4581113. 1769649.  1.75e6 1.71e6  6.94e6  7203586.             0.463\n#>  6 5072102. 3877581. 1974599.  2.32e6 1.75e6  7.39e6  6728728.             0.493\n#>  7 5123398. 4158589. 1906458.  2.20e6 1.61e6  7.32e6  6872051.             0.488\n#>  8 5082028. 4094164. 2088813.  2.13e6 1.60e6  7.22e6  6983718.             0.481\n#>  9 5037253. 4327570. 2111705.  1.86e6 1.66e6  6.90e6  7268684.             0.460\n#> 10 5288396. 4174872. 1757828.  1.97e6 1.81e6  7.26e6  6836246.             0.484\n#> 11 5357190. 4194297. 1706649.  1.98e6 1.77e6  7.33e6  6783598.             0.489\n#> 12 5149876. 4219835. 2100181.  1.78e6 1.75e6  6.93e6  7193195.             0.462\n#> 13 5335957. 4264904. 1660458.  1.91e6 1.82e6  7.25e6  6837212.             0.483\n#> 14 5019902. 4167592. 2030026.  2.15e6 1.63e6  7.17e6  7014315.             0.478\n#> 15 5128910. 4336085. 1922000.  1.95e6 1.66e6  7.08e6  7088623.             0.472\n#> 16 4937302. 4463147. 2112536.  2.18e6 1.31e6  7.12e6  7229531.             0.474\n#> 17 5076243. 4368606. 1927403.  1.90e6 1.72e6  6.98e6  7158241.             0.465\n#> 18 5142749. 4191437. 1915944.  1.97e6 1.78e6  7.11e6  6996739.             0.474\n#> 19 5186305. 4053834. 1984494.  2.22e6 1.55e6  7.41e6  6815159.             0.494\n#> 20 5054051. 4176696. 2140116.  1.89e6 1.74e6  6.95e6  7185489.             0.463\n#> # ℹ 1 more variable: izquierda_posterior <dbl>\n```\n:::\n\n\n\nAhora nos podemos hacer preguntas como , ¿en cuántas de estas posterioris gana el bloque de la derecha así construido al de la izquierda? o ¿en cuántas la diferencia que le saca el bloque de la derecha es mayor a un punto porcentual? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterioris |> \n    mutate(gana_derecha = derecha_posterior>izquierda_posterior, \n           gana_derecha_mas1_pct = (derecha_posterior - izquierda_posterior) >= 0.01) |> \n    summarise(\n        mean(gana_derecha),\n        mean(gana_derecha_mas1_pct)\n        )\n#> # A tibble: 1 × 2\n#>   `mean(gana_derecha)` `mean(gana_derecha_mas1_pct)`\n#>                  <dbl>                         <dbl>\n#> 1                0.668                         0.474\n```\n:::\n\n\n\n## Coda\n\nBueno, pues así es como he pasado el sábado. ¿Cosas que le faltaría a esto para ser algo serio?\n\n* Que tuviera en cuenta más encuestas y analizara mejor qué tipo de encuestas son, sus cambios de estimación según el tiempo\n* Que añadiera estimación de escaños, lo cual no es trivial. \n* Añadir encuestas a nivel autónomico o datos de las municipales y poder hacer un modelo jerárquico en condiciones. \n\nY como decía al principio, seguramente esto del meta-análisis se pueda hacer de otra manera, mucho mejor, así que si alguien sabe, que lo ponga en los comentarios. Pues nada más, que voten ustedes lo que les de la gana mañana, que de eso se trata. \n\n\n## El código que genera en `Stan`\n\nCon la función `stancode` podemos ver las tripas de lo qeu se ha mandado a Stan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstancode(model_multinomial)\n#> // generated with brms 2.19.0\n#> functions {\n#>   /* compute correlated group-level effects\n#>    * Args:\n#>    *   z: matrix of unscaled group-level effects\n#>    *   SD: vector of standard deviation parameters\n#>    *   L: cholesky factor correlation matrix\n#>    * Returns:\n#>    *   matrix of scaled group-level effects\n#>    */\n#>   matrix scale_r_cor(matrix z, vector SD, matrix L) {\n#>     // r is stored in another dimension order than z\n#>     return transpose(diag_pre_multiply(SD, L) * z);\n#>   }\n#>   /* multinomial-logit log-PMF\n#>    * Args:\n#>    *   y: array of integer response values\n#>    *   mu: vector of category logit probabilities\n#>    * Returns:\n#>    *   a scalar to be added to the log posterior\n#>    */\n#>   real multinomial_logit2_lpmf(array[] int y, vector mu) {\n#>     return multinomial_lpmf(y | softmax(mu));\n#>   }\n#> }\n#> data {\n#>   int<lower=1> N; // total number of observations\n#>   int<lower=2> ncat; // number of categories\n#>   array[N, ncat] int Y; // response array\n#>   array[N] int trials; // number of trials\n#>   // data for group-level effects of ID 1\n#>   int<lower=1> N_1; // number of grouping levels\n#>   int<lower=1> M_1; // number of coefficients per level\n#>   array[N] int<lower=1> J_1; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_1_mupsoe_1;\n#>   vector[N] Z_1_mupsoe_2;\n#>   int<lower=1> NC_1; // number of group-level correlations\n#>   // data for group-level effects of ID 2\n#>   int<lower=1> N_2; // number of grouping levels\n#>   int<lower=1> M_2; // number of coefficients per level\n#>   array[N] int<lower=1> J_2; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_2_musumar_1;\n#>   vector[N] Z_2_musumar_2;\n#>   int<lower=1> NC_2; // number of group-level correlations\n#>   // data for group-level effects of ID 3\n#>   int<lower=1> N_3; // number of grouping levels\n#>   int<lower=1> M_3; // number of coefficients per level\n#>   array[N] int<lower=1> J_3; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_3_muvox_1;\n#>   vector[N] Z_3_muvox_2;\n#>   int<lower=1> NC_3; // number of group-level correlations\n#>   // data for group-level effects of ID 4\n#>   int<lower=1> N_4; // number of grouping levels\n#>   int<lower=1> M_4; // number of coefficients per level\n#>   array[N] int<lower=1> J_4; // grouping indicator per observation\n#>   // group-level predictor values\n#>   vector[N] Z_4_muresto_1;\n#>   vector[N] Z_4_muresto_2;\n#>   int<lower=1> NC_4; // number of group-level correlations\n#>   int prior_only; // should the likelihood be ignored?\n#> }\n#> transformed data {\n#>   \n#> }\n#> parameters {\n#>   real Intercept_mupsoe; // temporary intercept for centered predictors\n#>   real Intercept_musumar; // temporary intercept for centered predictors\n#>   real Intercept_muvox; // temporary intercept for centered predictors\n#>   real Intercept_muresto; // temporary intercept for centered predictors\n#>   vector<lower=0>[M_1] sd_1; // group-level standard deviations\n#>   matrix[M_1, N_1] z_1; // standardized group-level effects\n#>   cholesky_factor_corr[M_1] L_1; // cholesky factor of correlation matrix\n#>   vector<lower=0>[M_2] sd_2; // group-level standard deviations\n#>   matrix[M_2, N_2] z_2; // standardized group-level effects\n#>   cholesky_factor_corr[M_2] L_2; // cholesky factor of correlation matrix\n#>   vector<lower=0>[M_3] sd_3; // group-level standard deviations\n#>   matrix[M_3, N_3] z_3; // standardized group-level effects\n#>   cholesky_factor_corr[M_3] L_3; // cholesky factor of correlation matrix\n#>   vector<lower=0>[M_4] sd_4; // group-level standard deviations\n#>   matrix[M_4, N_4] z_4; // standardized group-level effects\n#>   cholesky_factor_corr[M_4] L_4; // cholesky factor of correlation matrix\n#> }\n#> transformed parameters {\n#>   matrix[N_1, M_1] r_1; // actual group-level effects\n#>   // using vectors speeds up indexing in loops\n#>   vector[N_1] r_1_mupsoe_1;\n#>   vector[N_1] r_1_mupsoe_2;\n#>   matrix[N_2, M_2] r_2; // actual group-level effects\n#>   // using vectors speeds up indexing in loops\n#>   vector[N_2] r_2_musumar_1;\n#>   vector[N_2] r_2_musumar_2;\n#>   matrix[N_3, M_3] r_3; // actual group-level effects\n#>   // using vectors speeds up indexing in loops\n#>   vector[N_3] r_3_muvox_1;\n#>   vector[N_3] r_3_muvox_2;\n#>   matrix[N_4, M_4] r_4; // actual group-level effects\n#>   // using vectors speeds up indexing in loops\n#>   vector[N_4] r_4_muresto_1;\n#>   vector[N_4] r_4_muresto_2;\n#>   real lprior = 0; // prior contributions to the log posterior\n#>   // compute actual group-level effects\n#>   r_1 = scale_r_cor(z_1, sd_1, L_1);\n#>   r_1_mupsoe_1 = r_1[ : , 1];\n#>   r_1_mupsoe_2 = r_1[ : , 2];\n#>   // compute actual group-level effects\n#>   r_2 = scale_r_cor(z_2, sd_2, L_2);\n#>   r_2_musumar_1 = r_2[ : , 1];\n#>   r_2_musumar_2 = r_2[ : , 2];\n#>   // compute actual group-level effects\n#>   r_3 = scale_r_cor(z_3, sd_3, L_3);\n#>   r_3_muvox_1 = r_3[ : , 1];\n#>   r_3_muvox_2 = r_3[ : , 2];\n#>   // compute actual group-level effects\n#>   r_4 = scale_r_cor(z_4, sd_4, L_4);\n#>   r_4_muresto_1 = r_4[ : , 1];\n#>   r_4_muresto_2 = r_4[ : , 2];\n#>   lprior += student_t_lpdf(Intercept_mupsoe | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_musumar | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muvox | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(Intercept_muresto | 3, 0, 2.5);\n#>   lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n#>             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n#>   lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n#>             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n#>   lprior += student_t_lpdf(sd_3 | 3, 0, 2.5)\n#>             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += lkj_corr_cholesky_lpdf(L_3 | 1);\n#>   lprior += student_t_lpdf(sd_4 | 3, 0, 2.5)\n#>             - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n#>   lprior += lkj_corr_cholesky_lpdf(L_4 | 1);\n#> }\n#> model {\n#>   // likelihood including constants\n#>   if (!prior_only) {\n#>     // initialize linear predictor term\n#>     vector[N] mupsoe = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] musumar = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muvox = rep_vector(0.0, N);\n#>     // initialize linear predictor term\n#>     vector[N] muresto = rep_vector(0.0, N);\n#>     // linear predictor matrix\n#>     array[N] vector[ncat] mu;\n#>     mupsoe += Intercept_mupsoe;\n#>     musumar += Intercept_musumar;\n#>     muvox += Intercept_muvox;\n#>     muresto += Intercept_muresto;\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       mupsoe[n] += r_1_mupsoe_1[J_1[n]] * Z_1_mupsoe_1[n]\n#>                    + r_1_mupsoe_2[J_1[n]] * Z_1_mupsoe_2[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       musumar[n] += r_2_musumar_1[J_2[n]] * Z_2_musumar_1[n]\n#>                     + r_2_musumar_2[J_2[n]] * Z_2_musumar_2[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muvox[n] += r_3_muvox_1[J_3[n]] * Z_3_muvox_1[n]\n#>                   + r_3_muvox_2[J_3[n]] * Z_3_muvox_2[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       // add more terms to the linear predictor\n#>       muresto[n] += r_4_muresto_1[J_4[n]] * Z_4_muresto_1[n]\n#>                     + r_4_muresto_2[J_4[n]] * Z_4_muresto_2[n];\n#>     }\n#>     for (n in 1 : N) {\n#>       mu[n] = transpose([0, mupsoe[n], musumar[n], muvox[n], muresto[n]]);\n#>     }\n#>     for (n in 1 : N) {\n#>       target += multinomial_logit2_lpmf(Y[n] | mu[n]);\n#>     }\n#>   }\n#>   // priors including constants\n#>   target += lprior;\n#>   target += std_normal_lpdf(to_vector(z_1));\n#>   target += std_normal_lpdf(to_vector(z_2));\n#>   target += std_normal_lpdf(to_vector(z_3));\n#>   target += std_normal_lpdf(to_vector(z_4));\n#> }\n#> generated quantities {\n#>   // actual population-level intercept\n#>   real b_mupsoe_Intercept = Intercept_mupsoe;\n#>   // actual population-level intercept\n#>   real b_musumar_Intercept = Intercept_musumar;\n#>   // actual population-level intercept\n#>   real b_muvox_Intercept = Intercept_muvox;\n#>   // actual population-level intercept\n#>   real b_muresto_Intercept = Intercept_muresto;\n#>   // compute group-level correlations\n#>   corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n#>   vector<lower=-1, upper=1>[NC_1] cor_1;\n#>   // compute group-level correlations\n#>   corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n#>   vector<lower=-1, upper=1>[NC_2] cor_2;\n#>   // compute group-level correlations\n#>   corr_matrix[M_3] Cor_3 = multiply_lower_tri_self_transpose(L_3);\n#>   vector<lower=-1, upper=1>[NC_3] cor_3;\n#>   // compute group-level correlations\n#>   corr_matrix[M_4] Cor_4 = multiply_lower_tri_self_transpose(L_4);\n#>   vector<lower=-1, upper=1>[NC_4] cor_4;\n#>   // extract upper diagonal of correlation matrix\n#>   for (k in 1 : M_1) {\n#>     for (j in 1 : (k - 1)) {\n#>       cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n#>     }\n#>   }\n#>   // extract upper diagonal of correlation matrix\n#>   for (k in 1 : M_2) {\n#>     for (j in 1 : (k - 1)) {\n#>       cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n#>     }\n#>   }\n#>   // extract upper diagonal of correlation matrix\n#>   for (k in 1 : M_3) {\n#>     for (j in 1 : (k - 1)) {\n#>       cor_3[choose(k - 1, 2) + j] = Cor_3[j, k];\n#>     }\n#>   }\n#>   // extract upper diagonal of correlation matrix\n#>   for (k in 1 : M_4) {\n#>     for (j in 1 : (k - 1)) {\n#>       cor_4[choose(k - 1, 2) + j] = Cor_4[j, k];\n#>     }\n#>   }\n#> }\n```\n:::\n",
    "supporting": [
      "metanalisis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.28/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.4/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.4/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.4/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
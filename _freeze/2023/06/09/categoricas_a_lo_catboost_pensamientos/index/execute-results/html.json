{
  "hash": "257ef246f57d3ddd5ff89c968bef17b5",
  "result": {
    "markdown": "---\ntitle: Categóricas a lo catboost. Pensamientos\ndate: '2023-06-09'\ncategories: \n  - Estadística\n  - categóricas\n  - R\n  - catboost\n  - 2023\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\n    code-fold: show\n    code-summary: \"Mostrar / ocultar código\"\n    html-math-method: mathml\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\neditor_options:\n  markdown:\n    wrap: none\nimage: \"\"\n---\n\n\nLa gente de Yandex es gente lista y son los que están detrás de [catboost](https://catboost.ai/). Ya el pasado mes de Abril conté como hacían la regresión cuantil y obtenían estimación de varios cuantiles a la vez [aquí](../../../04/23/quantile-catboost/)\n\n## Codificación de las categóricas.\n\nCatboost por defecto usa `one-hot-encoding` pero si por algo es conocido es por tener otro método de codificación, el cual viene descrito en la [docu](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic). Otro sitio dónde viene relativamente bien explicado es en este [post](https://towardsdatascience.com/how-catboost-encodes-categorical-variables-3866fb2ae640)\n\nVamos a ver el detalle, para cuándo hay variables categóricas y la variable a predecir es binaria.\n\nLa idea en la que se basan tiene que ver con los test de permutación. Lo que hacen son varias iteraciones desordenando los datos y en cada iteración\n\n1.  Desordenan las filas del data frame de forma aleatoria de forma que se crea un nuevo orden\n2.  La codificación del nivel de la variable categórica se calcula **para cada fila** como: $$\\text{avg_target} = \\dfrac{countInClass + prior}{totalCount +1}$$ pero usando sólo los datos previos a esa fila\n\nY luego para cada observación toman como codificación la media de las codificaciones obtenidas en las diferentes permutaciones.\n\nEn $\\text{avg_target}$ de la fila i se tiene que\n\n-   countInClass: Cuenta las veces que en todos los datos previos a la fila i, se tiene un target = 1 para cuando el nivel de la variable categórica es igual al de la fila i.\n\n-   prior: Constante que se define al principio del algoritmo. Puede ser la proporción de 1's en los datos por ejemplo.\n\n-   totalCount: El número de observaciones con el mismo nivel en la variable categórica que tiene la fila i, en los datos previos.\n\nEn el segundo [post](https://towardsdatascience.com/how-catboost-encodes-categorical-variables-3866fb2ae640) podemos ver la siguiente figura.\n\n![Figura](coches_cod_catboost.png){width=\"1224\"}\n\nEn este caso, si queremos calcular el valor de $\\text{avg_target}$ para la quinta observación es tan sencillo como\n\n-   prior : La fijamos a 0.05, por ejemplo\n-   countInClass: En los datos previos sólo había un caso en que el target fuera 1 y la marca Toyota, por lo tanto 1.\n-   totalCount: En los datos previos hay 2 observaciones con marca Toyota.\n\nAsí que $$\\text{avg_target} = \\dfrac{countInClass + prior}{totalCount +1} = \\dfrac{1+0.05}{2+1} = 0.35$$\n\nLos autores de catboost reconocen que de esta forma si sólo haces una permutación de los datos para los primeros valores no se tiene info suficiente para obtener una buena codificación, así que proponen hacer varias permutaciones y tomar como codificación la media de las codificaciones anteriores.\n\n## Pensamientos\n\nPero, pero...\n\n-   ¿No os recuerda un poco a como se hace un aprendizaje bayesiano? . Es decir parto de una priori (puede que poco informativa) y conforme voy obteniendo datos voy actualizando la distribución de mi parámetro, y de esa forma puedo obtener la posterior predictive distribution, que es la que aplicaría por ejemplo a un dato no visto.\n\n- De hecho al hacer varias permutaciones ¿ no está convergiendo de alguna manera la solución de catboost hacia la aproximación bayesiana? \n\n-   ¿No os parece un poco de sobreingeniería, para algo que quizá con una aproximación *estilo compadre bayesiana* se podría obtener algo muy similar y con menos esfuerzo?\n\n### Pruebecilla\n\nEl ejemplo que viene en la [docu](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic), dónde se ejemplifica con un pequeño dataset de 7 filas y muestran una de las permutaciones generadas.\n\n![permutation1](ejemplo_docu.png)\n\nQue tras aplicar el algoritmo quedaría para esta permutación queda como ![permutation_process](ejemplo_docu_processed.png)\n\nReplicamos en código\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # pa 4 tontás de hacer sample y de groups bys que hago luego\nlibrary(parallel) # para usar varios cores con mclapply\nlibrary(patchwork) # pa juntar ggplots\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\nmydf <- tribble(\n    ~id,~f2, ~cat, ~label, \n    1,53,\"rock\",  0,\n    2,55,\"indie\", 0, \n    3,40,\"rock\",  1, \n    4,42,\"rock\",  1,\n    5,34,\"pop\",   1,\n    6,48,\"indie\", 1,\n    7,45, \"rock\",  0\n)\n\nmydf\n#> # A tibble: 7 × 4\n#>      id    f2 cat   label\n#>   <dbl> <dbl> <chr> <dbl>\n#> 1     1    53 rock      0\n#> 2     2    55 indie     0\n#> 3     3    40 rock      1\n#> 4     4    42 rock      1\n#> 5     5    34 pop       1\n#> 6     6    48 indie     1\n#> 7     7    45 rock      0\n```\n:::\n\n\nFuncioncita para obtener la codificación a lo catboost\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_target <- function(prev_df, nivel, prior = 0.05){\n    countInClass <- sum(prev_df[['label']][prev_df[['cat']]== nivel])\n    totalCount <- sum(prev_df[['cat']]==nivel)\n    res <-  (countInClass + prior) /(totalCount + 1)\n    return(res)\n}\n```\n:::\n\n\nA la primer fila se le asigna siempre la prior\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# No estaba fino para ver como podría hacerlo sin iterar sobre todas las filas. \n\nfoo1 <-  function(df, prior = 0.05) {\n    \n    df$cat_code[1] <-  prior\n    \n    for (fila in 2:nrow(df)) {\n        prev_df <- df[1:(fila - 1),]\n        df$cat_code[fila] <-\n            avg_target(prev_df = prev_df, nivel = df$cat[fila], prior = prior)\n    }\n    return(df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres1 <- foo1(mydf, prior = 0.05)\nres1\n#> # A tibble: 7 × 5\n#>      id    f2 cat   label cat_code\n#>   <dbl> <dbl> <chr> <dbl>    <dbl>\n#> 1     1    53 rock      0    0.05 \n#> 2     2    55 indie     0    0.05 \n#> 3     3    40 rock      1    0.025\n#> 4     4    42 rock      1    0.35 \n#> 5     5    34 pop       1    0.05 \n#> 6     6    48 indie     1    0.025\n#> 7     7    45 rock      0    0.512\n```\n:::\n\n\nAhora lo repetimos varias veces. Dónde en cada iteración hacemos una permutación de las filas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo2 <-  function(df, prior = 0.05) {\n    require(tidyverse)\n    mynew_df <-  df |> slice_sample(prop = 1, replace = FALSE)\n    mynew_df$cat_code[1] <-  prior\n    \n    for (fila in 2:nrow(mynew_df)) {\n        prev_df <- mynew_df[1:(fila - 1),]\n        mynew_df$cat_code[fila] <-\n            avg_target(prev_df = prev_df, nivel = mynew_df$cat[fila], prior = prior)\n    }\n    return(mynew_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\niteraciones <-  1000\n\nres2 <-  bind_rows(mclapply(1:iteraciones, FUN = function(x) foo2(mydf,prior = 0.05), mc.cores = 10))\n\ndim(res2)\n#> [1] 7000    5\n\n(res2 <- res2 |>\n    group_by(id) |>\n    mutate(cat_code = mean(cat_code)) |>\n    distinct() |> \n    arrange(id) )\n#> # A tibble: 7 × 5\n#> # Groups:   id [7]\n#>      id    f2 cat   label cat_code\n#>   <dbl> <dbl> <chr> <dbl>    <dbl>\n#> 1     1    53 rock      0   0.338 \n#> 2     2    55 indie     0   0.298 \n#> 3     3    40 rock      1   0.185 \n#> 4     4    42 rock      1   0.186 \n#> 5     5    34 pop       1   0.05  \n#> 6     6    48 indie     1   0.0381\n#> 7     7    45 rock      0   0.343\n```\n:::\n\n\n### Aproximación compadre bayesiana\n\n¿Y si tomamos como priori una $\\mathcal{B}(2,2)$ y para cada categoría {rock, indie, pop} tomamos como su distribución a posteriori \n\n $\\mathcal{B}(2 + \\text{exitos en datos},2 + text{fracasos en los datos})$ y para obtener un valor de la codificación para cada observación simplemente extraemos un valor aleatorio de esa distribución a posteriori?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nres3 <-  mydf |> \n    group_by(cat) |> \n    mutate(\n        n = n(), \n        exitos = sum(label)\n    )  |> \n    ungroup() |> \n    mutate(\n     cat_code = map2_dbl(exitos, n, function(exitos, n) rbeta(1, exitos + 2 , n - exitos + 2))\n    ) \n\nres3\n#> # A tibble: 7 × 7\n#>      id    f2 cat   label     n exitos cat_code\n#>   <dbl> <dbl> <chr> <dbl> <int>  <dbl>    <dbl>\n#> 1     1    53 rock      0     4      2    0.271\n#> 2     2    55 indie     0     2      1    0.596\n#> 3     3    40 rock      1     4      2    0.679\n#> 4     4    42 rock      1     4      2    0.706\n#> 5     5    34 pop       1     1      1    0.417\n#> 6     6    48 indie     1     2      1    0.301\n#> 7     7    45 rock      0     4      2    0.694\n```\n:::\n\n\nPues tiene pinta de que esta aproximación podría ser tan válida como la que describen los de catboost y en principio es más sencilla. \n\n### Más datos\n\nCreemos un dataset artificial partiendo de estos mismos datos. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn= 200\nmydf_big= mydf[rep(seq_len(nrow(mydf)), n), ]\nmydf_big$id <- 1:nrow(mydf_big)\n\nnrow(mydf_big)\n#> [1] 1400\n\nhead(mydf_big, 10)\n#> # A tibble: 10 × 4\n#>       id    f2 cat   label\n#>    <int> <dbl> <chr> <dbl>\n#>  1     1    53 rock      0\n#>  2     2    55 indie     0\n#>  3     3    40 rock      1\n#>  4     4    42 rock      1\n#>  5     5    34 pop       1\n#>  6     6    48 indie     1\n#>  7     7    45 rock      0\n#>  8     8    53 rock      0\n#>  9     9    55 indie     0\n#> 10    10    40 rock      1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Cambiamos de forma aleatoria el valor de label para un % de las observaciones, para que no sea 200 veces exactamente el original\n\ntable(mydf_big$label)\n#> \n#>   0   1 \n#> 600 800\n\nmydf_big$label <- rbinom(n = nrow(mydf_big), size =1,  prob = ifelse(mydf_big$label==0, 0.3, 0.9))\ntable(mydf_big$label)\n#> \n#>   0   1 \n#> 490 910\n\n# vemos que hemos cambiado algunos valores en label\nhead(mydf_big, 10)\n#> # A tibble: 10 × 4\n#>       id    f2 cat   label\n#>    <int> <dbl> <chr> <int>\n#>  1     1    53 rock      0\n#>  2     2    55 indie     0\n#>  3     3    40 rock      1\n#>  4     4    42 rock      1\n#>  5     5    34 pop       1\n#>  6     6    48 indie     1\n#>  7     7    45 rock      0\n#>  8     8    53 rock      0\n#>  9     9    55 indie     0\n#> 10    10    40 rock      1\n```\n:::\n\n\n### Comparamos \n\n\nPara elegir las priori poco informativa para ambos métodos vemos una muestra de los datos de tamaño 10\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(muestra <- mydf_big |> \n    slice_sample(n = 10) |> \n    group_by(label) |>\n    count())\n#> # A tibble: 2 × 2\n#> # Groups:   label [2]\n#>   label     n\n#>   <int> <int>\n#> 1     0     2\n#> 2     1     8\n\n\n(prior_shape1 <-  muestra$n[muestra$label==1])\n#> [1] 8\n(prior_shape2 <-  muestra$n[muestra$label==0])\n#> [1] 2\n\n\n(prior_catboost <- prior_shape1 /(prior_shape1 + prior_shape2))\n#> [1] 0.8\n```\n:::\n\n\n#### Catboost codificación\n\n\n::: {.cell}\n\n```{.r .cell-code}\niteraciones = 50\ntictoc::tic(\"catbost_code\")\n\ncod_catboost <- bind_rows(mclapply(1:iteraciones, FUN = function(x) foo2(mydf_big, prior = prior_catboost), mc.cores = 10))\n\n\ntictoc::toc(log=TRUE)\n#> catbost_code: 3.503 sec elapsed\n\ndim(cod_catboost)\n#> [1] 70000     5\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncod_catboost <- cod_catboost |>\n    group_by(id) |>\n    mutate(cat_code = mean(cat_code)) |>\n    distinct() |> \n    arrange(id)\n\n\ndim(cod_catboost)\n#> [1] 1400    5\n\nhead(cod_catboost)\n#> # A tibble: 6 × 5\n#> # Groups:   id [6]\n#>      id    f2 cat   label cat_code\n#>   <int> <dbl> <chr> <int>    <dbl>\n#> 1     1    53 rock      0    0.629\n#> 2     2    55 indie     0    0.592\n#> 3     3    40 rock      1    0.622\n#> 4     4    42 rock      1    0.633\n#> 5     5    34 pop       1    0.895\n#> 6     6    48 indie     1    0.583\n```\n:::\n\n\n\n#### Estilo compadre codificación\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntictoc::tic(\"estilo compadre\")\nestilo_compadre <-  mydf_big |> \n    group_by(cat) |> \n    mutate(\n        n = n(), \n        exitos = sum(label)\n    )  |> \n    ungroup() |> \n    mutate(\n     cat_code = map2_dbl(exitos, n, function(exitos, n) rbeta(1, exitos + prior_shape1 , n - exitos + prior_shape2))\n    ) \n\ntictoc::toc(log = TRUE)\n#> estilo compadre: 0.025 sec elapsed\n\nhead(estilo_compadre)\n#> # A tibble: 6 × 7\n#>      id    f2 cat   label     n exitos cat_code\n#>   <int> <dbl> <chr> <int> <int>  <int>    <dbl>\n#> 1     1    53 rock      0   800    499    0.614\n#> 2     2    55 indie     0   400    231    0.583\n#> 3     3    40 rock      1   800    499    0.631\n#> 4     4    42 rock      1   800    499    0.643\n#> 5     5    34 pop       1   200    180    0.855\n#> 6     6    48 indie     1   400    231    0.559\n```\n:::\n\n\n\n\n¿cómo de parecidas son las dos codificaciones? \n\n\nPor el momento parece que bastante\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(cod_catboost$cat_code, estilo_compadre$cat_code)\n#> [1] 0.9808539\n```\n:::\n\n\n\nParece que la codificación estilo compadre es un poco más dispersa, lo cual no tiene por qué ser necesariamente malo. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ncod_catboost |> \n    group_by(cat) |> \n    summarise(media = mean(cat_code), \n              low = quantile(cat_code, 0.05), \n              high = quantile(cat_code, 0.95), \n              sd_value = sd(cat_code))\n#> # A tibble: 3 × 5\n#>   cat   media   low  high sd_value\n#>   <chr> <dbl> <dbl> <dbl>    <dbl>\n#> 1 indie 0.587 0.577 0.598  0.00611\n#> 2 pop   0.892 0.885 0.900  0.00487\n#> 3 rock  0.628 0.621 0.636  0.00464\n\n\nestilo_compadre |> \n    group_by(cat) |> \n    summarise(media = mean(cat_code), \n              low = quantile(cat_code, 0.05), \n              high = quantile(cat_code, 0.95), \n              sd_value = sd(cat_code))\n#> # A tibble: 3 × 5\n#>   cat   media   low  high sd_value\n#>   <chr> <dbl> <dbl> <dbl>    <dbl>\n#> 1 indie 0.584 0.546 0.621   0.0231\n#> 2 pop   0.895 0.855 0.926   0.0215\n#> 3 rock  0.625 0.597 0.651   0.0169\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- cod_catboost |> \n    ggplot(aes(x=cat_code, fill=cat))  +\n    geom_density(alpha = 0.4) +\n    labs(title = \"Gráfico densidad de las codificanes\", \n         subtitle = \"catboost\")\n\np2 <- estilo_compadre |> \n    ggplot(aes(x=cat_code, fill=cat))  +\n    geom_density(alpha = 0.4) +\n        labs(title = \"Gráfico densidad de las codificanes\", \n         subtitle = \"Estilo bayesian compadre\")\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=80%}\n:::\n:::\n\n\n\nY si hacemos un modelito tonto usando estas codificaciones. Si, ya sé que son datos fakes y que no tiene mucho sentido y tal, y que lo suyo sería con unos datos reales (mandadme algunos !! )\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set.seed(47)\n\nid_train <-  sample(1:nrow(mydf_big), size = 700)\n\n\ntrain_predict_simple <- function(df){\n    train <-  df[id_train, ]\n    test <- df[-id_train, ]\n    \n    fit_base <- glm(label ~ f2+ cat, data = train, family = binomial)\n    fit <-  glm(label ~  f2 +  cat_code, data = train, family = binomial)\n    \n    auc_base <- pROC::auc(test$label, predict(fit_base, test, type = \"response\"))\n    auc <- pROC::auc(test$label, predict(fit, test, type = \"response\"))\n    \n    return(list(auc_base = auc_base, auc = auc))\n    \n}\n\n\nmclapply(list(cod_catboost, estilo_compadre), train_predict_simple, mc.cores = 2)\n#> [[1]]\n#> [[1]]$auc_base\n#> Area under the curve: 0.8257\n#> \n#> [[1]]$auc\n#> Area under the curve: 0.8129\n#> \n#> \n#> [[2]]\n#> [[2]]$auc_base\n#> Area under the curve: 0.8257\n#> \n#> [[2]]$auc\n#> Area under the curve: 0.7958\n```\n:::\n\n\n## Más pensamientos\n\n* El ejemplo que he hecho no es del todo válido puesto que tanto para la codificación con catboost como la de estilo compadre han intervenido todos los datos. \n\n* La variable categórica que codifico sólo tiene 3 niveles, de hecho no haría falta hacer este tipo de codificación. Tengo pendiente probar con algo como códigos postales o similar. \n\n* La forma en que catboost hace esta codificación me parece que está en mitad entre la aproximación bayesiana y hacer un target_encoding al uso. De hecho si hay un nivel con muy pocos valores  el valor de la codificación de catboost para ese nivel va a parecerse más a la prior que elijas que a la proporción de éxitos en esa categoría, lo cual es muy parecido a la estimación bayesiana compadre. \n\n* Se podrían utilizar codificaciones basadas en modelos mixtos o algún tipo de combinación convexa entre la información particular que tiene una categoría y la general aportada por el conjunto de los datos. \n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
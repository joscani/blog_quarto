{
  "hash": "30ff38665eb9f3b34572a16883b4bf1a",
  "result": {
    "markdown": "---\ntitle: Vuelta a la facultad. SVD\ndate: '2023-12-03'\ndate-modified: last-modified\ncategories:\n  - 2023\n  - R\n  - Álgebra\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 9\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\nUnos compis del trabajo están haciendo cosas muy chulas utilizando SVD (Descomposición en valores singulares) y me ha recordado a los tiempos de la universidad.\n\nLa SVD es una factorización de una matriz rectangular tal que así.\n\n$$ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^T} $$\n\n\nVeamos un ejemplo, dónde tenemos una matriz de 5x4\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c( 10, -1, -2, 2,\n               4,  8, -1, 3,\n               -2,  5, -3, 4,\n               -7, -8,  6, -2,\n               -2 , 5, 4, -4\n               ),\n            byrow = T, nrow = 5, ncol = 4,\n            dimnames = list(c(\"P1\", \"P2\", \"P3\", \"P4\",\"P5\"),\n                            c(\"V1\", \"V2\", \"V3\",\"V4\")))\nX\n#>    V1 V2 V3 V4\n#> P1 10 -1 -2  2\n#> P2  4  8 -1  3\n#> P3 -2  5 -3  4\n#> P4 -7 -8  6 -2\n#> P5 -2  5  4 -4\n```\n:::\n\n\nHacemos la svd\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndvs <- svd(X)\ndvs\n#> $d\n#> [1] 17.033392 10.982941  6.873957  2.997880\n#> \n#> $u\n#>             [,1]        [,2]        [,3]        [,4]\n#> [1,] -0.41759401  0.66860794  0.28085132 -0.28960722\n#> [2,] -0.50584653 -0.30661800  0.13614448 -0.62790360\n#> [3,] -0.23820970 -0.40000698 -0.61431429 -0.21204097\n#> [4,]  0.71395758  0.08354597 -0.00214809 -0.68888308\n#> [5,]  0.05705485 -0.54033629  0.72470722  0.04840934\n#> \n#> $v\n#>            [,1]        [,2]       [,3]        [,4]\n#> [1,] -0.6360870  0.61508773  0.4578647 -0.08614145\n#> [2,] -0.6015603 -0.77316598  0.2003873 -0.01358129\n#> [3,]  0.3855737 -0.13572378  0.5864215 -0.69930092\n#> [4,] -0.2912926  0.07389525 -0.6374282 -0.70948815\n```\n:::\n\n\n\nY vemos que efectivamente \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndvs$u %*% diag(dvs$d) %*% t(dvs$v)\n#>      [,1] [,2] [,3] [,4]\n#> [1,]   10   -1   -2    2\n#> [2,]    4    8   -1    3\n#> [3,]   -2    5   -3    4\n#> [4,]   -7   -8    6   -2\n#> [5,]   -2    5    4   -4\n```\n:::\n\n\n\n\nAhora, la proyección de las filas en el espacio vectorial definido por la descomposión en valores singulares sería \n\n$$  Proj = \\mathbf{X} \\mathbf{V} $$\n\nDe forma que si tenemos unos nuevos datos y queremos proyectarlos sobre la estructura definida por la DVS ya calculada  se haría así. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnuevaX <- matrix(c( 20, -2, -3, 2.1,\n               5,  2, -1, 3\n               ),\n            byrow = T, nrow = 2, ncol = 4,\n            dimnames = list(c(\"P6\", \"P7\"),\n                            c(\"V1\", \"V2\", \"V3\",\"V4\")))\n\nnuevaX\n#>    V1 V2 V3  V4\n#> P6 20 -2 -3 2.1\n#> P7  5  2 -1 3.0\n\n(proyeccion = nuevaX %*% dvs$v)\n#>          [,1]      [,2]      [,3]      [,4]\n#> P6 -13.287055 14.410438 5.6586547 -1.087689\n#> P7  -5.643007  1.886516 0.1913919 -1.887033\n```\n:::\n\n\nY para proyectar los valores de X originales, podemos hacer lo mismo o como sabemos que \n\n$$ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^T} $$\n\nentonces \n\n$$ Proj = \\mathbf{X} \\mathbf{V} =  \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^T} \\mathbf{V} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{I} = \\mathbf{U} \\mathbf{\\Sigma} $$ \n\nEs decir, la proyección de las filas son los vectores singulares izquierdos por la matriz diagonal de los valores singulares. \n\n\n__¿Y para qué sirve esto?__  Pues una cosa interesante de la SVD es que nos sirve para reducir la dimensión de los datos, quedándonos con los k primeros valores singulares y los respectivos k primeros vectores singulares. \n\nSupongamos que queremos quedarnos con k = 3\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndvs_red <- svd(X, nu = 3, nv = 3)\ndvs_red\n#> $d\n#> [1] 17.033392 10.982941  6.873957  2.997880\n#> \n#> $u\n#>             [,1]        [,2]        [,3]\n#> [1,] -0.41759401  0.66860794  0.28085132\n#> [2,] -0.50584653 -0.30661800  0.13614448\n#> [3,] -0.23820970 -0.40000698 -0.61431429\n#> [4,]  0.71395758  0.08354597 -0.00214809\n#> [5,]  0.05705485 -0.54033629  0.72470722\n#> \n#> $v\n#>            [,1]        [,2]       [,3]\n#> [1,] -0.6360870  0.61508773  0.4578647\n#> [2,] -0.6015603 -0.77316598  0.2003873\n#> [3,]  0.3855737 -0.13572378  0.5864215\n#> [4,] -0.2912926  0.07389525 -0.6374282\n```\n:::\n\n\nPodemos reconstruir la matriz original  tal que así\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# matriz original\nX\n#>    V1 V2 V3 V4\n#> P1 10 -1 -2  2\n#> P2  4  8 -1  3\n#> P3 -2  5 -3  4\n#> P4 -7 -8  6 -2\n#> P5 -2  5  4 -4\n\n# reconstrucción \n\ndvs_red$u %*% diag(dvs_red$d[1:3]) %*% t(dvs_red$v)\n#>           [,1]      [,2]      [,3]      [,4]\n#> [1,]  9.925211 -1.011791 -2.607138  1.384017\n#> [2,]  3.837849  7.974435 -2.316350  1.664474\n#> [3,] -2.054758  4.991367 -3.444527  3.548997\n#> [4,] -7.177898 -8.028048  4.555812 -3.465227\n#> [5,] -1.987499  5.001971  4.101486 -3.897035\n```\n:::\n\n\n\nY así almacenando U, d y V podemos tener una aproximación de X ocupando menos espacio. \n\nUna utilidad de esto es que si pensamos en las filas como observaciones y las columnnas como variables, la SVD nos sirve para hacer una reducción de la dimensionalidad, de hecho los valores singulares al cuadrado son los autovalores de $\\mathbf{X^T} \\mathbf{X}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nZ = t(X) %*% X\n\neigen(Z)\n#> eigen() decomposition\n#> $values\n#> [1] 290.136446 120.624987  47.251282   8.987284\n#> \n#> $vectors\n#>            [,1]        [,2]       [,3]       [,4]\n#> [1,]  0.6360870  0.61508773 -0.4578647 0.08614145\n#> [2,]  0.6015603 -0.77316598 -0.2003873 0.01358129\n#> [3,] -0.3855737 -0.13572378 -0.5864215 0.69930092\n#> [4,]  0.2912926  0.07389525  0.6374282 0.70948815\n\ndvs_red$d^2\n#> [1] 290.136446 120.624987  47.251282   8.987284\n```\n:::\n\n\nSi nos quedamos con los k primeros singulares podríamos querer tener por ejemplo la proyección de las filas en ese subespacio, y usar esas coordenadas para ver distancias entre filas. \nUna propiedad deseable de ese subespacio es que las distancias entre los individuos sean parecidas o al menos ordenen de la misma forma que en el espacio original.  \n\n\nPor ejemplo las distancias euclídeas entre las filas de X \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist(X)\n#>           P1        P2        P3        P4\n#> P2 10.908712                              \n#> P3 13.601471  7.071068                    \n#> P4 20.445048 21.236761 17.635192          \n#> P5 15.874508 10.908712 10.630146 14.212670\n```\n:::\n\n\nVemos que P5 está más cerca de P3 que de P2. \n\n\nSi usamos sólo la matriz $\\mathbf{U_{n \\times k}}$ para ver distancias tenemos que no se mantiene esa relación entre las distancias\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist(dvs_red$u)\n#>           1         2         3         4\n#> 2 0.9898455                              \n#> 3 1.4055028 0.8022090                    \n#> 4 1.3049119 1.2881284 1.2309319          \n#> 5 1.3725326 0.8472829 1.3783512 1.1614943\n```\n:::\n\n\nEn cambio si utilizamos la proyección  tenemos que las distancias son del mismo orden que en los datos originales y que se mantiene la relación entre las diferentes filas. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nproyeccion <- (dvs_red$u %*% diag(dvs_red$d[1:3]))\n\ndist(proyeccion)\n#>           1         2         3         4\n#> 2 10.861467                              \n#> 3 13.599483  6.960296                    \n#> 4 20.409979 21.235974 17.577158          \n#> 5 15.842132 10.718639 10.601432 14.039748\n```\n:::\n\n\nPor eso, si queremos utilizar SVD para hacer una reducción de dimensionalidad y posteriormente utilizar técnicas que utilicen distancias puede tener más sentido utilizar la proyección $\\mathbf{U_{n\\times k}} \\mathbf{\\Sigma_{k \\times k}}$  que sólo $\\mathbf{U_{n\\times k}}$\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
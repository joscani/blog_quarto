{
  "hash": "92c54e871d2fe40802fa87392848a84e",
  "result": {
    "markdown": "---\ntitle: Arrow y S3\ndate: '2023-02-19'\ncategories: \n  - big data\n  - R\n  - C++\n  - S3\n  - AWS\n  - 2023\n\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\n    code-fold: show\n    code-summary: \"Mostrar / ocultar código\"\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\nimage: \"\"\n---\n\n\n\n## Intro \n\n[Apache Arrow](https://arrow.apache.org/) está de moda, permite trabajar de forma muy rápida con ficheros parquet por ejemplo, está escrito en C++, aunque también hay implementación en Rust, de hecho la librería [polars](https://www.pola.rs/) está escrita en Rust.\n\nLuego hay APis que permiten usar Arrow desde Java Julia, Go, Python o incluso R.  Vamos a ver un par de ejemplos de como usar arrow desde R. El primero de ellos es sólo copia de la excelente presentación de [Danielle Navarro](https://fosstodon.org/@djnavarro) que os dejo [aquí](https://djnavarro.net/slides-arrow-latinr-2022/#/title-slide) . Y el segundo ejemplo es como ver lo mismo pero con unos datos **fake** que he dejado en un bucket de S3 (del que no puedo poner la dirección)\n\n\n## Ejemplo 1 \n\nUna cosa maravillosa de Arrow es que puedes conectarte a un bucket remoto de S3 (o google cloud storage) y hacer consultas sobre varios millones de datos sin necesidad de que esos datos se traigan enteros a tu pc y sin necesidad de que te quepan en RAM. ¿Cómo lo hace? pues ni la más remota idea. Pero podéis leer las slides de Danielle para haceros una idea\n\n\nCargamos librerías, nos conectamos a un bucket de s3 y vemos que hay\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(tictoc) \nlibrary(duckdb) # compraberomos más tarde si usar duckdb mejora\n\nbucket <- s3_bucket(\"voltrondata-labs-datasets\", anonymous = TRUE)\nbucket$ls(\"nyc-taxi\")\n#>  [1] \"nyc-taxi/year=2009\" \"nyc-taxi/year=2010\" \"nyc-taxi/year=2011\"\n#>  [4] \"nyc-taxi/year=2012\" \"nyc-taxi/year=2013\" \"nyc-taxi/year=2014\"\n#>  [7] \"nyc-taxi/year=2015\" \"nyc-taxi/year=2016\" \"nyc-taxi/year=2017\"\n#> [10] \"nyc-taxi/year=2018\" \"nyc-taxi/year=2019\" \"nyc-taxi/year=2020\"\n#> [13] \"nyc-taxi/year=2021\" \"nyc-taxi/year=2022\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nbucket <- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi\", anonymous = TRUE)\nremote_taxi <- open_dataset(bucket) \nremote_taxi\n#> FileSystemDataset with 158 Parquet files\n#> vendor_name: string\n#> pickup_datetime: timestamp[ms]\n#> dropoff_datetime: timestamp[ms]\n#> passenger_count: int64\n#> trip_distance: double\n#> pickup_longitude: double\n#> pickup_latitude: double\n#> rate_code: string\n#> store_and_fwd: string\n#> dropoff_longitude: double\n#> dropoff_latitude: double\n#> payment_type: string\n#> fare_amount: double\n#> extra: double\n#> mta_tax: double\n#> tip_amount: double\n#> tolls_amount: double\n#> total_amount: double\n#> improvement_surcharge: double\n#> congestion_surcharge: double\n#> pickup_location_id: int64\n#> dropoff_location_id: int64\n#> year: int32\n#> month: int32\ntoc()\n#> 6.287 sec elapsed\n```\n:::\n\n\nCuánto tardaría en hacer el cálculo de cuántos viajes ha habido en Enero de 2019 y ver el número de viajes en los que ha habido más de un pasajero.  (Viendo el htop  de mi linuxmint se ve que no hay casi uso de mis cpus)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nresult <- remote_taxi |> \n    filter(year == 2019, month == 1) |>\n    summarize(\n        all_trips = n(),\n        shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n    ) |>\n    mutate(pct_shared = shared_trips / all_trips * 100) |>\n    collect()\n\nresult |> \n    print(n = 200)\n#> # A tibble: 1 × 3\n#>   all_trips shared_trips pct_shared\n#>       <int>        <int>      <dbl>\n#> 1   7667255      2094091       27.3\ntoc()\n#> 12.59 sec elapsed\n```\n:::\n\n\nNo está mal , ¿verdad? \n\n\n## Ejemplo 2 \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBUCKET   = Sys.getenv(\"BUCKET_COMUN\")\n\nROLE_ARN = Sys.getenv(\"ROLE_ARN\")\nS3_FOLDER = Sys.getenv(\"S3_FOLDER\")\n\n\n\n\nbucket_comun <- s3_bucket(bucket = BUCKET, \n                             role_arn = ROLE_ARN)\n\n\nds <- open_dataset(bucket_comun$path(S3_FOLDER),\n                   partitioning = c(\"year\", \"month\", \"day\"))\n```\n:::\n\n\n\nCuántos datos \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <int>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 24.383 sec elapsed\n```\n:::\n\nY usando `duckdb` como engine de consulta . \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntic()\nres <- ds %>% \n    filter(year == 2021 & month == 3)  |> \n    select(year, month, day ) |> \n    to_duckdb() %>%\n    group_by(day) |> \n    summarise(\n        n_filas = n()\n    ) |> \n    collect()\n\nres |> \n    arrange(day) |> \n    print(n  = 10)\n#> # A tibble: 30 × 2\n#>      day n_filas\n#>    <int>   <dbl>\n#>  1     1 6260454\n#>  2     2 6245312\n#>  3     3 6243455\n#>  4     4 6242304\n#>  5     5 6241816\n#>  6     6 6241089\n#>  7     7 6241633\n#>  8     8 6241651\n#>  9    10 6240299\n#> 10    11 6239817\n#> # … with 20 more rows\ntoc()\n#> 33.624 sec elapsed\n```\n:::\n\n\n\nPues a mi la verdad, arrow me parece impresionante. Poder hacer cosas como calcular medias, contar, filtrar etc, sobre conjuntos de datos que están en remoto sin tener una computadadora muy potente. \n\n\n## Nota \n\nPara instalar la última versión de Arrow en R, recomiendo que os leáis esta [documentación](https://arrow.apache.org/docs/r/articles/install.html)\n\n\nFeliz domingo\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
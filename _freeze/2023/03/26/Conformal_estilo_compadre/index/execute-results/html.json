{
  "hash": "3f0f27da5c8181c293ab4b71fbd45c80",
  "result": {
    "markdown": "---\ntitle: Conformal prediction. Estilo compadre\ndate: '2023-03-26'\ncategories: \n  - Estadística\n  - R\n  - 2023\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\n    code-fold: show\n    code-summary: \"Mostrar / ocultar código\"\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\nimage: \"\"\n---\n\n\n## Intro\n\nEl jueves pasado asistí al más que recomendable meetup de [PyData\nMadrid](https://www.meetup.com/es-ES/pydata-madrid/events/292228851/),\nque cuenta entre sus organizadores con el gran [Juan Luis Cano\nRodríguez](https://www.meetup.com/es-ES/pydata-madrid/?_locale=es-ES),\nantiguo compañero mío de curro y tocayo de iniciales.\n\nEl caso es que en una de las charlas, [Ignacio\nPeletier](https://www.linkedin.com/in/ignacio-peletier/), mencionó de\npasada lo del \"Conformal Prediction\". Y siendo que Ignacio es un gran\ncientífico de datos, y que hacía unos meses que había tenido varias\ncharlas con [Carlos](https://www.datanalytics.com/) sobre el\nparticular, pues he decidido ver un poco más en detalle de qué iba el\nasunto .\n\n## Recursos\n\nUn excelente sitio para empezar a bichear con este tema es el Readme de\neste [repo](https://github.com/valeman/awesome-conformal-prediction),\ndónde han ido recopilando enlaces a libros, posts, papers y código\nrelativo a lo de conformal prediction.\n\nEn particular, uno de los recursos que me ha gustado es este\n[minicurso](https://mindfulmodeler.substack.com/p/e-mail-course-on-conformal-prediction)\nde Christoph Molnar.\n\nOtro recurso útil es este\n[post](https://www.datanalytics.com/2023/03/02/prediccion-conforme/) de\nCarlos, dónde se esboza un poco en qué consiste esto de la predicción\nconforme y en por qué no es algo tan novedoso como se cree.\n\n## Experimentando\n\nLa predicción conforme se puede aplicar tanto a modelos de regresión de\nclasificación. Su objetivo es *simplemente* medir la incertidumbre de\nuna predicción dada.\n\nEn el caso de regresión no tiene mucho misterio:\n\n-   Se entrena un modelo usando un conjunto de datos de *entrenamiento*.\n\n-   Se mide el error en un conjunto de datos de *validación*,\n    *calibración*, utilizando la norma L1, es decir\n    $\\mid y - \\hat{y}\\mid$\n\n-   Se elige una medida de dispersión del error, por ejemplo el cuantil\n    $(1- \\alpha) = 0.95$ de los errores anteriores.\n\n-   Para una nueva predicción se da su intervalo como\n    $(\\hat{y} - q_{1-\\alpha}, \\hat{y} + q_{1-\\alpha})$\n\nEn el caso de clasificación la cosa es más divertida. Puesto que lo que\nse quiere obtener es un conjunto de etiquetas probables. Tipo {A} {A, B}\n{B, C}\n\nEn este caso según he leído\n[aquí](https://mindfulmodeler.substack.com/p/week-1-getting-started-with-conformal)\nel algoritmo sería\n\n-   Se entrena un modelo usando un conjunto de datos de *entrenamiento*.\n\n-   Se mide el error en un conjunto de datos de *validación*,\n    *calibración*, viendo para cada observación el valor que el modelo\n    le ha dado para la predicción de la clase verdadera. Es un conjunto\n    de validación , sabemos cuál es la verdad. Y se calcula el error\n    como $1- p_{i}$ siendo $p_i$, la probabilidad predicha para la clase\n    verdadera\n\n-   Se calcula el cuantil de orden $1-\\alpha$ de esos *errores* y se\n    guarda. Se entiende que el modelo está bien calibrado y que el\n    conjunto de validación y que los *scores* que da el modelo se pueden\n    asumir como probabilidades\n\n-   Para una nueva predicción se tendrá una $p_i$ para cada clase. Se\n    calcula $1-p_i$ para cada clase y se considera que esa clase forma\n    parte del *prediction set* si ese valor es menor o igual que el\n    valor del cuantil anterior.\n\nPues vamos a ver como se haría con R en estilo compadre, y puede que con\nalguna pequeña modificación por mi parte.\n\n### Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(MASS)\n```\n:::\n\n\nVamos a usar el conjunto de datos `housing` \n![Help housing](housing_help.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nskimr::skim(housing)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |        |\n|:------------------------|:-------|\n|Name                     |housing |\n|Number of rows           |72      |\n|Number of columns        |5       |\n|_______________________  |        |\n|Column type frequency:   |        |\n|factor                   |4       |\n|numeric                  |1       |\n|________________________ |        |\n|Group variables          |None    |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                         |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------------------|\n|Sat           |         0|             1|TRUE    |        3|Low: 24, Med: 24, Hig: 24          |\n|Infl          |         0|             1|FALSE   |        3|Low: 24, Med: 24, Hig: 24          |\n|Type          |         0|             1|FALSE   |        4|Tow: 18, Apa: 18, Atr: 18, Ter: 18 |\n|Cont          |         0|             1|FALSE   |        2|Low: 36, Hig: 36                   |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|    sd| p0| p25|  p50|   p75| p100|hist  |\n|:-------------|---------:|-------------:|-----:|-----:|--:|---:|----:|-----:|----:|:-----|\n|Freq          |         0|             1| 23.35| 17.67|  3|  10| 19.5| 31.75|   86|▇▅▂▁▁ |\n:::\n:::\n\n\n\nY vamos a justar un modelito tonto usando regresión logística ordinal, sobre los 40 primeros datos\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nhouse.plr <- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing[1:40,])\n\nhead(predict(house.plr, type = \"probs\"))\n#>         Low    Medium      High\n#> 1 0.3639420 0.2575531 0.3785049\n#> 2 0.3639420 0.2575531 0.3785049\n#> 3 0.3639420 0.2575531 0.3785049\n#> 4 0.3263764 0.2552793 0.4183443\n#> 5 0.3263764 0.2552793 0.4183443\n#> 6 0.3263764 0.2552793 0.4183443\n```\n:::\n\n\nGuardamos las predicciones para el conjunto de validación , que va a ser las filas de la 41 a la 55, junto con el valor de `Sat` verdadero\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npredictions <- as.data.frame(predict(house.plr, type = \"probs\", newdata = housing[41:55,]))\n\ntt <- cbind(predictions, True_class=housing$Sat[41:55])\n\ntt\n#>          Low    Medium      High True_class\n#> 41 0.3055662 0.2524868 0.4419471     Medium\n#> 42 0.3055662 0.2524868 0.4419471       High\n#> 43 0.1595073 0.1930758 0.6474169        Low\n#> 44 0.1595073 0.1930758 0.6474169     Medium\n#> 45 0.1595073 0.1930758 0.6474169       High\n#> 46 0.4671694 0.2484190 0.2844116        Low\n#> 47 0.4671694 0.2484190 0.2844116     Medium\n#> 48 0.4671694 0.2484190 0.2844116       High\n#> 49 0.4260865 0.2544760 0.3194375        Low\n#> 50 0.4260865 0.2544760 0.3194375     Medium\n#> 51 0.4260865 0.2544760 0.3194375       High\n#> 52 0.2425400 0.2363201 0.5211399        Low\n#> 53 0.2425400 0.2363201 0.5211399     Medium\n#> 54 0.2425400 0.2363201 0.5211399       High\n#> 55 0.4014928 0.2566312 0.3418760        Low\n```\n:::\n\n\nAhora, para la primera fila sería hacer (1-0.2524), puesto que la clase real es \"Medium\" y para la segunda sería (1-0.44), puesto que la clase real es \"High\". No estoy muy inspirado hoy y no he conseguido una forma elegante de hacerlo en R, y ChatGpt no me ha servido de mucha ayuda, seguramente porque aún no soy muy ducho preguntándole. \n\nAsí que he tirado iterando para cada fila con un map y quedándode con el valor predicho de la columna cuyo nombre coincida con el valor en `True_class`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntt$prob_true_class <- map_dbl(1:nrow(tt), .f = function(i) \n    tt[i, colnames(tt) == tt$True_class[i]])\n\ntt$resid <- 1-tt$prob_true_class\n\nhead(tt)\n#>          Low    Medium      High True_class prob_true_class     resid\n#> 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#> 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#> 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#> 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#> 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#> 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n```\n:::\n\n\nDefinimos un $\\alpha = 0.3$ y calculamos el cuantil 70 . \n\n\n::: {.cell}\n\n```{.r .cell-code}\n(qhat = quantile(tt$resid, 0.7))\n#>       70% \n#> 0.7507675\n```\n:::\n\n\n\nY ya estamos listos para hacer la *predicción conforme* para nuevos datos. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predecimos de la fila 51 a la 70 \npredicciones <- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\n\nhead(predicciones)\n#>          Low    Medium      High\n#> 51 0.4260865 0.2544760 0.3194375\n#> 52 0.2425400 0.2363201 0.5211399\n#> 53 0.2425400 0.2363201 0.5211399\n#> 54 0.2425400 0.2363201 0.5211399\n#> 55 0.4014928 0.2566312 0.3418760\n#> 56 0.4014928 0.2566312 0.3418760\n```\n:::\n\n\nNos creamos un data.frame que indique si el valor de 1 - predicciones es menor o igual que el cuantil elegido\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset <- as.data.frame(1 - predicciones <= qhat)\n\nhead(set)\n#>      Low Medium High\n#> 51  TRUE   TRUE TRUE\n#> 52 FALSE  FALSE TRUE\n#> 53 FALSE  FALSE TRUE\n#> 54 FALSE  FALSE TRUE\n#> 55  TRUE   TRUE TRUE\n#> 56  TRUE   TRUE TRUE\n```\n:::\n\n\n\nAl igual que antes, utilizo un map para obtener el conjunto de etiquetas, para la primera fila serían todas, para la segunda sería {\"Medium\", \"High\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nset$conformal <-  map_chr(1:nrow(set), .f= function(i) {\n     set_list = colnames(set)[unlist(set[i,])]\n     paste0(set_list, collapse = \",\")\n     })\n\nhead(set)\n#>      Low Medium High       conformal\n#> 51  TRUE   TRUE TRUE Low,Medium,High\n#> 52 FALSE  FALSE TRUE            High\n#> 53 FALSE  FALSE TRUE            High\n#> 54 FALSE  FALSE TRUE            High\n#> 55  TRUE   TRUE TRUE Low,Medium,High\n#> 56  TRUE   TRUE TRUE Low,Medium,High\n```\n:::\n\n\n\nSe lo pego al dataset original de test (filas 51  a 70), junto con las predicciones y la clase verdadera.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nset_fin <-  cbind( True_class = housing$Sat[51:70], as.data.frame(predicciones),\n                  set_conformal =set$conformal)\n\nhead(set_fin)\n#>    True_class       Low    Medium      High   set_conformal\n#> 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High\n#> 52        Low 0.2425400 0.2363201 0.5211399            High\n#> 53     Medium 0.2425400 0.2363201 0.5211399            High\n#> 54       High 0.2425400 0.2363201 0.5211399            High\n#> 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High\n#> 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High\n```\n:::\n\n\nY ya estaría. \n\nUna cosa que se suele calcular es la cobertura de cada clase, es decir, la proporción de veces que cada clase  está dentro del conjunto. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_fin <- set_fin |> \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                               .y = set_conformal , \n                               ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nhead(set_fin)\n#>    True_class       Low    Medium      High   set_conformal class_in_set\n#> 51       High 0.4260865 0.2544760 0.3194375 Low,Medium,High         TRUE\n#> 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#> 55        Low 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n#> 56     Medium 0.4014928 0.2566312 0.3418760 Low,Medium,High         TRUE\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset_fin |> \n    group_by(True_class) |> \n    summarise(cov = mean(class_in_set))\n#> # A tibble: 3 × 2\n#>   True_class   cov\n#>   <ord>      <dbl>\n#> 1 Low        0.571\n#> 2 Medium     0.5  \n#> 3 High       1\n```\n:::\n\n\n\n\n### Modificación 1. \n\nNo me convence lo de tener un sólo cuantil, común a todas las clases, ¿no sería mejor tener una medida de cómo se distribuyen los errores para cada una de las clases?\n\n\nUsamos el conjunto de *validación* dónde tenemos el $1-p_i$ que nos dice en cuánto se ha equivocado el modelo en predecir la clase real\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(tt)\n#>          Low    Medium      High True_class prob_true_class     resid\n#> 41 0.3055662 0.2524868 0.4419471     Medium       0.2524868 0.7475132\n#> 42 0.3055662 0.2524868 0.4419471       High       0.4419471 0.5580529\n#> 43 0.1595073 0.1930758 0.6474169        Low       0.1595073 0.8404927\n#> 44 0.1595073 0.1930758 0.6474169     Medium       0.1930758 0.8069242\n#> 45 0.1595073 0.1930758 0.6474169       High       0.6474169 0.3525831\n#> 46 0.4671694 0.2484190 0.2844116        Low       0.4671694 0.5328306\n```\n:::\n\n\nCalculamos el quantil 70 para cada clase, y así vemos que varía por clase\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(qhat_by_class <- tt |> \n    group_by(True_class) |> \n    summarise(qhat = quantile(resid, 0.7)) |> \n        pivot_wider(names_from = True_class, values_from = qhat))\n#> # A tibble: 1 × 3\n#>     Low Medium  High\n#>   <dbl>  <dbl> <dbl>\n#> 1 0.726  0.761 0.656\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredicciones <- predict(house.plr, newdata = housing[51:70,], type  = \"probs\")\ncomplementarios <- 1-predicciones\nhead(complementarios)\n#>          Low    Medium      High\n#> 51 0.5739135 0.7455240 0.6805625\n#> 52 0.7574600 0.7636799 0.4788601\n#> 53 0.7574600 0.7636799 0.4788601\n#> 54 0.7574600 0.7636799 0.4788601\n#> 55 0.5985072 0.7433688 0.6581240\n#> 56 0.5985072 0.7433688 0.6581240\n```\n:::\n\n\nY vemos si cada $1-p_i$ es menor o igual que el cuantil correspondiente de cada clase\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_adjust <- data.frame(Low = complementarios[,1] <= qhat_by_class$Low,\n                        Medium = complementarios[,2] <= qhat_by_class$Medium,\n                         High = complementarios[,3] <= qhat_by_class$High )\n\n\nhead(set_adjust)\n#>      Low Medium  High\n#> 51  TRUE   TRUE FALSE\n#> 52 FALSE  FALSE  TRUE\n#> 53 FALSE  FALSE  TRUE\n#> 54 FALSE  FALSE  TRUE\n#> 55  TRUE   TRUE FALSE\n#> 56  TRUE   TRUE FALSE\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset_adjust$conformal <-  map_chr(1:nrow(set_adjust), .f= function(i) {\n    set_list = colnames(set_adjust)[unlist(set_adjust[i,])]\n    paste0(set_list, collapse = \",\")\n})\n\nhead(set_adjust)\n#>      Low Medium  High  conformal\n#> 51  TRUE   TRUE FALSE Low,Medium\n#> 52 FALSE  FALSE  TRUE       High\n#> 53 FALSE  FALSE  TRUE       High\n#> 54 FALSE  FALSE  TRUE       High\n#> 55  TRUE   TRUE FALSE Low,Medium\n#> 56  TRUE   TRUE FALSE Low,Medium\n```\n:::\n\n\nComo antes, nos quedamos con la clase de verdad, la predicción en probabilidad de cada clase y la predicción conforme\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_adjust_fin <-  cbind( True_class = housing$Sat[51:70], as.data.frame(predict(house.plr, newdata = housing[51:70,],type=\"probs\")),\n                   set_conformal =set_adjust$conformal)\n\nhead(set_adjust_fin)\n#>    True_class       Low    Medium      High set_conformal\n#> 51       High 0.4260865 0.2544760 0.3194375    Low,Medium\n#> 52        Low 0.2425400 0.2363201 0.5211399          High\n#> 53     Medium 0.2425400 0.2363201 0.5211399          High\n#> 54       High 0.2425400 0.2363201 0.5211399          High\n#> 55        Low 0.4014928 0.2566312 0.3418760    Low,Medium\n#> 56     Medium 0.4014928 0.2566312 0.3418760    Low,Medium\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset_adjust_fin <- set_adjust_fin |> \n    mutate(\n        class_in_set = map2_lgl(.x = True_class,\n                                .y = set_conformal , \n                                ~ .x %in%  unlist(str_split(.y,\",\")))\n    )\n\nset_adjust_fin\n#>    True_class       Low    Medium      High   set_conformal class_in_set\n#> 51       High 0.4260865 0.2544760 0.3194375      Low,Medium        FALSE\n#> 52        Low 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 53     Medium 0.2425400 0.2363201 0.5211399            High        FALSE\n#> 54       High 0.2425400 0.2363201 0.5211399            High         TRUE\n#> 55        Low 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#> 56     Medium 0.4014928 0.2566312 0.3418760      Low,Medium         TRUE\n#> 57       High 0.4014928 0.2566312 0.3418760      Low,Medium        FALSE\n#> 58        Low 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#> 59     Medium 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#> 60       High 0.3622588 0.2575226 0.3802186 Low,Medium,High         TRUE\n#> 61        Low 0.1967801 0.2160332 0.5871868            High        FALSE\n#> 62     Medium 0.1967801 0.2160332 0.5871868            High        FALSE\n#> 63       High 0.1967801 0.2160332 0.5871868            High         TRUE\n#> 64        Low 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#> 65     Medium 0.4732214 0.2472855 0.2794931      Low,Medium         TRUE\n#> 66       High 0.4732214 0.2472855 0.2794931      Low,Medium        FALSE\n#> 67        Low 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#> 68     Medium 0.4320378 0.2537829 0.3141793      Low,Medium         TRUE\n#> 69       High 0.4320378 0.2537829 0.3141793      Low,Medium        FALSE\n#> 70        Low 0.2470311 0.2378946 0.5150743            High        FALSE\n```\n:::\n\n\nY aquí ya vemos que la cobertura es distinta y que la clase \"High\" ya no está en el 100% de los *prediction sets*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_adjust_fin |> \n    group_by(True_class) |> \n    summarise(cov = mean(class_in_set))\n#> # A tibble: 3 × 2\n#>   True_class   cov\n#>   <ord>      <dbl>\n#> 1 Low        0.571\n#> 2 Medium     0.667\n#> 3 High       0.429\n```\n:::\n\n\nDe hecho si tabulamos ambas predicciones conformes , vemos que de las 10 predicciones  que el primer método ponía como {Low, Medium, High} , el segundo  pone 7 como {Low, Medium }  y 3 como {Low, Medium, High}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntable(set_fin$set_conformal, set_adjust_fin$set_conformal)\n#>                  \n#>                   High Low,Medium Low,Medium,High\n#>   High               7          0               0\n#>   Low,High           0          3               0\n#>   Low,Medium,High    0          7               3\n```\n:::\n\n\n\n\n\n### Modificación 2. \n\nVale, todo esto está muy bien, pero ¿y si simplemente para cada observación ordeno de forma decreciente su probabilidad predicha y me quedo con las clases que lleguen al 60% de probabilidad, por ejemplo? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n(predicciones_df <-  as.data.frame(predicciones ))\n#>          Low    Medium      High\n#> 51 0.4260865 0.2544760 0.3194375\n#> 52 0.2425400 0.2363201 0.5211399\n#> 53 0.2425400 0.2363201 0.5211399\n#> 54 0.2425400 0.2363201 0.5211399\n#> 55 0.4014928 0.2566312 0.3418760\n#> 56 0.4014928 0.2566312 0.3418760\n#> 57 0.4014928 0.2566312 0.3418760\n#> 58 0.3622588 0.2575226 0.3802186\n#> 59 0.3622588 0.2575226 0.3802186\n#> 60 0.3622588 0.2575226 0.3802186\n#> 61 0.1967801 0.2160332 0.5871868\n#> 62 0.1967801 0.2160332 0.5871868\n#> 63 0.1967801 0.2160332 0.5871868\n#> 64 0.4732214 0.2472855 0.2794931\n#> 65 0.4732214 0.2472855 0.2794931\n#> 66 0.4732214 0.2472855 0.2794931\n#> 67 0.4320378 0.2537829 0.3141793\n#> 68 0.4320378 0.2537829 0.3141793\n#> 69 0.4320378 0.2537829 0.3141793\n#> 70 0.2470311 0.2378946 0.5150743\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificacion_2 <- predicciones_df |> \n     rownames_to_column(var = \"individuo\") |> \n    pivot_longer(cols = Low:High) |> \n    group_by(individuo) |> \n    arrange( desc(value)) |> \n    mutate(suma_acumulada = cumsum(value)) |> \n    arrange(individuo)\n\nhead(modificacion_2, 10)\n#> # A tibble: 10 × 4\n#> # Groups:   individuo [4]\n#>    individuo name   value suma_acumulada\n#>    <chr>     <chr>  <dbl>          <dbl>\n#>  1 51        Low    0.426          0.426\n#>  2 51        High   0.319          0.746\n#>  3 51        Medium 0.254          1    \n#>  4 52        High   0.521          0.521\n#>  5 52        Low    0.243          0.764\n#>  6 52        Medium 0.236          1    \n#>  7 53        High   0.521          0.521\n#>  8 53        Low    0.243          0.764\n#>  9 53        Medium 0.236          1    \n#> 10 54        High   0.521          0.521\n```\n:::\n\n\nUhmm, pero no me acaba de convencer ordenar de forma descendente por la probabilidad predicha de cada clase. Por ejemplo para el individuo 51, si tomo Low +High llegaría a 0.74, pero si tomo Low + Medium llego al 67% . Si quisiera el menor conjunto de etiquetas que lleguen como mínimo al 60% la opción buena sería Low + Medium para ese individuo. \n\nNo me veo con ganas de implementar todas las posibles sumas de probabilidades estimadas y elegir el conjunto que cumpla la restricción de llegar al menos al 60% y si hay varios para mismo individuos que se quede con el conjunto más pequeño.\n\n\n\n## Conclusión. \n\n* Lo de la predicción conforme para el caso de regresión me parece bastante sencillo, no es más que sumar y restar una medida de dispersión de los residuos a la predicción para nuevos datos.  \n\n * Para clasificación es un poco más interesante, sobre todo para casos en los que el usuario quiere una etiqueta o etiquetas y no se conforma con las probabilidades predichas de cada clase. \n \n \n* Subyace la hipótesis de que los scores del modelo están bien calibrados y reflejan la verdadera probabilidad. \n\n\nPues nada más, tengan un feliz día. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
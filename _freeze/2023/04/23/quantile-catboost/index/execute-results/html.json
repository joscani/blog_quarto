{
  "hash": "46fc8f3827983f79771559b1e1fe50e1",
  "result": {
    "markdown": "---\ntitle: Regresión cuantil a lo machín lenin con catboost\ndate: '2023-04-23'\ncategories: \n  - Estadística\n  - machine learning\n  - R\n  - 2023\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\n    code-fold: show\n    code-summary: \"Mostrar / ocultar código\"\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\nimage: \"\"\n---\n\n\nHay veces, más de las que se cree, en que nos interesa estimar un cuantil en vez de la media. Si tenemos una variable dependinte $y$ y una o varias independientes $X$, lo que se suele hacer es una regresión cuantil.\n\nSi visitamos uno de los papers originales de dicha técnica [Computing Regression Quantiles](https://www.jstor.org/stable/2347802) vemos que trata de minimizar la siguiente expresión.\n\n$$ \n\\arg\\min_b R_{\\theta}(b)  = \\sum_{i = 1}^{n}\\rho_\\theta \\left( y_i - x_ib\\right)\n$$\nCon $\\theta \\in (0,1)$  y\n\n$$\n\\begin{equation}\n    \\rho_\\theta(u) = \n        \\begin{cases}\n        \\theta u  &  u \\geq 0\\\\\n        (\\theta -1) & u  < 0 \\\\\n        \\end{cases}\n\\end{equation}\n$$\n\nLo cual es simplemente \"penalizar\" por $\\theta$ cuando el residuo sea mayor o igual que 0, es decir, cuando nos equivocamos por arriba y por $(\\theta -1)$ si nos equivocamos por abajo.\n\nEjemplo, si $y_i = 40$ y $f(x) = 50$ y queremos estimar el cuantil 0.95. Entonces como el residuo es menor que 0, se pondera por 0.05\n\n$$\\rho_(40 - 50) = (0.95 -1) (40 - 50) = 0.5 $$ Si en cambio\n$f(x) = 30$, es decir, nos equivocamos por abajo, pero a la misma distancia del valor real entonces\n\n$$\\rho(40-30) = 0.95 (40-30) = 9.5 $$\n\nY por tanto la función a minimizar $\\arg\\min_b R_{\\theta}(b)$ cuando $\\theta > 0.5$ va a tener un valor mucho mayor cuando nos\n\"equivocamos\" por abajo que por arriba. Y debido a cómo está definido $\\rho_\\theta(u)$ se consigue la regresión cuantil con cuantil igual a $\\theta$. En el paper (de 1987) viene mejor explicado y el algoritmo para resolverlo en el caso de que $f(x)$ sea lineal.\n\n::: callout-warning\nPero, ¿qué me estás contando??\n\n¡¡Estamos en la segunda década del siglo XXI y ahora todo es IA y Machín Lenin!!\n:::\n\nFuera coñas, el caso es que la gente de yandex en su librería [catboost](https://catboost.ai/) han utilizado esto para hacer la regresión cuantil, simplemente utilizando la expresión anterior como función de pérdida. [Aquí](https://catboost.ai/en/docs/concepts/loss-functions-regression) se puede ver las diferentes funciones de pérdida que usan según el caso.\n\nPara la regresión cuantil usan\n\n$$L(t, a, \\alpha) = \\dfrac{\\sum_{i}^{N} \\omega_i(\\alpha - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}$$\nDónde \n\n* $t_i$ es el valor real de la variable\n* $a_i$ el valor de la predicción \n* $\\alpha$ es el cuantil que se quiere predecir \n* $\\omega_i$ son los pesos de cada observación \n* I es la función indicadora\n\nComo vemos, es lo mismo que se cuenta en el paper de 1987. Pero al meterlo como función de pérdida en el algoritmo sirve para el algoritmo de boosting que se utiliza en la librería.\n\n::: callout-important\nOjo, que los de yandex han ido un poquito más allá\n:::\n\nLa gente de catboost, atinadamente ha dicho, y ¿por qué no construimos un función de pérdida que minimice globalmente varios cuantiles? Lo cual es algo así como \"encuéntrame la distribución de los parámetros que mejor se ajusta a estos datos en vez de un sólo parámetro\". \n\nPero esto son arbolitos y boosting, no hay lo que se dice un parámetro de la función propiamente dicho, por lo que al final lo que se \"aprende\" debe ser la configuración de árboles que minimiza globalmente los cuantiles indicados.  \n\nBueno, la función de pérdida \"multi-quantile\" es una modificación simple de la anterior. \n\n$$L(t, a, \\alpha_q) = \\dfrac{\\sum_{i}^{N} \\omega_i \\sum_{q=1}^{Q}(\\alpha_q - I(t_i \\leq a_i))(t_i-a_i)}{\\sum_{i}^{N} \\omega_i}$$\n\n## Ejemplo\n\n\n\nEl ejemplo  no es mío, lo he visto por algún sitio que no me acuerdo.\n\n::: callout-tip\n`catboost` se puede utilizar en R y python.\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\nsns.set()\n\nn = 800\n\n# X aleatorias\nx_train = np.random.rand(n)\nx_test = np.random.rand(n)\n\n# un poquito de ruido gaussiano\n\nnoise_train = np.random.normal(0, 0.3, n)\nnoise_test = np.random.normal(0, 0.3, n)\n\n# Simulamos y_train e y _x como y = 2 + 3 * x + ruido\na, b = 2, 3\n\n# al lio\ny_train = a * x_train + b + noise_train\ny_test = a * x_test + b + noise_test\n```\n:::\n\n\nPintamos \n\n\n::: {.cell}\n\n```{.python .cell-code}\nsns.scatterplot(x = x_train, y = y_train).set(title = \"Ejemplillo\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=80%}\n:::\n:::\n\n\nVaos a predecir 10 cuantiles\n\n\n::: {.cell}\n\n```{.python .cell-code}\nquantiles = [q/10 for q in range(1, 10)]\n\n# se ponen en string separados por commas\nquantile_str = str(quantiles).replace('[','').replace(']','')\n\nprint(quantile_str)\n#> 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n```\n:::\n\n\n\nModelito\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = CatBoostRegressor(iterations=100,\n                          loss_function=f'MultiQuantile:alpha={quantile_str}')\n\nmodel.fit(x_train.reshape(-1,1), y_train)\n#> 0:\tlearn: 0.1977907\ttotal: 49.8ms\tremaining: 4.93s\n#> 1:\tlearn: 0.1934878\ttotal: 54.5ms\tremaining: 2.67s\n#> 2:\tlearn: 0.1891389\ttotal: 61.2ms\tremaining: 1.98s\n#> 3:\tlearn: 0.1849242\ttotal: 64.9ms\tremaining: 1.56s\n#> 4:\tlearn: 0.1808818\ttotal: 68.4ms\tremaining: 1.3s\n#> 5:\tlearn: 0.1769520\ttotal: 71.7ms\tremaining: 1.12s\n#> 6:\tlearn: 0.1732251\ttotal: 74.7ms\tremaining: 993ms\n#> 7:\tlearn: 0.1696774\ttotal: 77.9ms\tremaining: 896ms\n#> 8:\tlearn: 0.1661482\ttotal: 80.9ms\tremaining: 818ms\n#> 9:\tlearn: 0.1628541\ttotal: 84.5ms\tremaining: 760ms\n#> 10:\tlearn: 0.1596378\ttotal: 87.6ms\tremaining: 709ms\n#> 11:\tlearn: 0.1565479\ttotal: 90.7ms\tremaining: 665ms\n#> 12:\tlearn: 0.1536016\ttotal: 93.8ms\tremaining: 628ms\n#> 13:\tlearn: 0.1507692\ttotal: 96.9ms\tremaining: 595ms\n#> 14:\tlearn: 0.1480845\ttotal: 100ms\tremaining: 567ms\n#> 15:\tlearn: 0.1454840\ttotal: 103ms\tremaining: 541ms\n#> 16:\tlearn: 0.1429863\ttotal: 106ms\tremaining: 518ms\n#> 17:\tlearn: 0.1407059\ttotal: 109ms\tremaining: 497ms\n#> 18:\tlearn: 0.1383697\ttotal: 112ms\tremaining: 477ms\n#> 19:\tlearn: 0.1361779\ttotal: 115ms\tremaining: 460ms\n#> 20:\tlearn: 0.1340707\ttotal: 118ms\tremaining: 444ms\n#> 21:\tlearn: 0.1320342\ttotal: 122ms\tremaining: 432ms\n#> 22:\tlearn: 0.1300775\ttotal: 125ms\tremaining: 418ms\n#> 23:\tlearn: 0.1282447\ttotal: 129ms\tremaining: 408ms\n#> 24:\tlearn: 0.1264588\ttotal: 133ms\tremaining: 399ms\n#> 25:\tlearn: 0.1247594\ttotal: 137ms\tremaining: 390ms\n#> 26:\tlearn: 0.1232137\ttotal: 141ms\tremaining: 381ms\n#> 27:\tlearn: 0.1216563\ttotal: 145ms\tremaining: 373ms\n#> 28:\tlearn: 0.1202034\ttotal: 149ms\tremaining: 365ms\n#> 29:\tlearn: 0.1187735\ttotal: 153ms\tremaining: 357ms\n#> 30:\tlearn: 0.1174263\ttotal: 157ms\tremaining: 350ms\n#> 31:\tlearn: 0.1161250\ttotal: 162ms\tremaining: 344ms\n#> 32:\tlearn: 0.1148583\ttotal: 172ms\tremaining: 350ms\n#> 33:\tlearn: 0.1136688\ttotal: 176ms\tremaining: 342ms\n#> 34:\tlearn: 0.1125292\ttotal: 180ms\tremaining: 335ms\n#> 35:\tlearn: 0.1114357\ttotal: 184ms\tremaining: 327ms\n#> 36:\tlearn: 0.1104088\ttotal: 187ms\tremaining: 318ms\n#> 37:\tlearn: 0.1094203\ttotal: 190ms\tremaining: 309ms\n#> 38:\tlearn: 0.1085302\ttotal: 193ms\tremaining: 301ms\n#> 39:\tlearn: 0.1076702\ttotal: 196ms\tremaining: 294ms\n#> 40:\tlearn: 0.1068348\ttotal: 200ms\tremaining: 288ms\n#> 41:\tlearn: 0.1060263\ttotal: 204ms\tremaining: 281ms\n#> 42:\tlearn: 0.1052530\ttotal: 207ms\tremaining: 274ms\n#> 43:\tlearn: 0.1045115\ttotal: 212ms\tremaining: 269ms\n#> 44:\tlearn: 0.1037861\ttotal: 216ms\tremaining: 264ms\n#> 45:\tlearn: 0.1031053\ttotal: 220ms\tremaining: 259ms\n#> 46:\tlearn: 0.1024645\ttotal: 225ms\tremaining: 254ms\n#> 47:\tlearn: 0.1018457\ttotal: 229ms\tremaining: 248ms\n#> 48:\tlearn: 0.1012497\ttotal: 232ms\tremaining: 242ms\n#> 49:\tlearn: 0.1006996\ttotal: 235ms\tremaining: 235ms\n#> 50:\tlearn: 0.1001835\ttotal: 239ms\tremaining: 229ms\n#> 51:\tlearn: 0.0996695\ttotal: 242ms\tremaining: 223ms\n#> 52:\tlearn: 0.0991990\ttotal: 246ms\tremaining: 218ms\n#> 53:\tlearn: 0.0987716\ttotal: 249ms\tremaining: 212ms\n#> 54:\tlearn: 0.0983443\ttotal: 252ms\tremaining: 206ms\n#> 55:\tlearn: 0.0979411\ttotal: 255ms\tremaining: 200ms\n#> 56:\tlearn: 0.0975621\ttotal: 258ms\tremaining: 195ms\n#> 57:\tlearn: 0.0972083\ttotal: 261ms\tremaining: 189ms\n#> 58:\tlearn: 0.0968639\ttotal: 265ms\tremaining: 184ms\n#> 59:\tlearn: 0.0965243\ttotal: 269ms\tremaining: 179ms\n#> 60:\tlearn: 0.0961923\ttotal: 272ms\tremaining: 174ms\n#> 61:\tlearn: 0.0958829\ttotal: 275ms\tremaining: 168ms\n#> 62:\tlearn: 0.0956101\ttotal: 278ms\tremaining: 163ms\n#> 63:\tlearn: 0.0953473\ttotal: 281ms\tremaining: 158ms\n#> 64:\tlearn: 0.0950908\ttotal: 284ms\tremaining: 153ms\n#> 65:\tlearn: 0.0948327\ttotal: 287ms\tremaining: 148ms\n#> 66:\tlearn: 0.0946033\ttotal: 290ms\tremaining: 143ms\n#> 67:\tlearn: 0.0943820\ttotal: 293ms\tremaining: 138ms\n#> 68:\tlearn: 0.0941723\ttotal: 297ms\tremaining: 133ms\n#> 69:\tlearn: 0.0939663\ttotal: 300ms\tremaining: 129ms\n#> 70:\tlearn: 0.0937677\ttotal: 304ms\tremaining: 124ms\n#> 71:\tlearn: 0.0935819\ttotal: 307ms\tremaining: 119ms\n#> 72:\tlearn: 0.0933969\ttotal: 310ms\tremaining: 115ms\n#> 73:\tlearn: 0.0932181\ttotal: 313ms\tremaining: 110ms\n#> 74:\tlearn: 0.0930514\ttotal: 316ms\tremaining: 105ms\n#> 75:\tlearn: 0.0928960\ttotal: 320ms\tremaining: 101ms\n#> 76:\tlearn: 0.0927433\ttotal: 323ms\tremaining: 96.4ms\n#> 77:\tlearn: 0.0925871\ttotal: 326ms\tremaining: 91.9ms\n#> 78:\tlearn: 0.0924635\ttotal: 329ms\tremaining: 87.6ms\n#> 79:\tlearn: 0.0923467\ttotal: 333ms\tremaining: 83.1ms\n#> 80:\tlearn: 0.0922292\ttotal: 336ms\tremaining: 78.7ms\n#> 81:\tlearn: 0.0921089\ttotal: 339ms\tremaining: 74.3ms\n#> 82:\tlearn: 0.0919860\ttotal: 342ms\tremaining: 70.1ms\n#> 83:\tlearn: 0.0918862\ttotal: 346ms\tremaining: 65.9ms\n#> 84:\tlearn: 0.0917925\ttotal: 349ms\tremaining: 61.6ms\n#> 85:\tlearn: 0.0916927\ttotal: 353ms\tremaining: 57.4ms\n#> 86:\tlearn: 0.0916089\ttotal: 356ms\tremaining: 53.1ms\n#> 87:\tlearn: 0.0915482\ttotal: 356ms\tremaining: 48.6ms\n#> 88:\tlearn: 0.0914624\ttotal: 362ms\tremaining: 44.7ms\n#> 89:\tlearn: 0.0913841\ttotal: 365ms\tremaining: 40.5ms\n#> 90:\tlearn: 0.0912981\ttotal: 368ms\tremaining: 36.4ms\n#> 91:\tlearn: 0.0912323\ttotal: 371ms\tremaining: 32.3ms\n#> 92:\tlearn: 0.0911620\ttotal: 374ms\tremaining: 28.2ms\n#> 93:\tlearn: 0.0910968\ttotal: 378ms\tremaining: 24.1ms\n#> 94:\tlearn: 0.0910390\ttotal: 381ms\tremaining: 20ms\n#> 95:\tlearn: 0.0909742\ttotal: 384ms\tremaining: 16ms\n#> 96:\tlearn: 0.0909243\ttotal: 387ms\tremaining: 12ms\n#> 97:\tlearn: 0.0908512\ttotal: 390ms\tremaining: 7.95ms\n#> 98:\tlearn: 0.0907883\ttotal: 393ms\tremaining: 3.97ms\n#> 99:\tlearn: 0.0907341\ttotal: 396ms\tremaining: 0us\n#> <catboost.core.CatBoostRegressor object at 0x7f75bdded3c0>\n```\n:::\n\n\nPredecimos\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n# Make predictions on the test set\npreds = model.predict(x_test.reshape(-1, 1))\npreds = pd.DataFrame(preds, columns=[f'pred_{q}' for q in quantiles])\n\npreds.head(6)\n#>    pred_0.1  pred_0.2  pred_0.3  ...  pred_0.7  pred_0.8  pred_0.9\n#> 0  3.401601  3.542894  3.619259  ...  3.938859  4.041568  4.165809\n#> 1  4.150397  4.269908  4.355643  ...  4.701792  4.816983  4.958197\n#> 2  4.350918  4.476999  4.595515  ...  4.933372  5.017778  5.147716\n#> 3  2.829240  2.936415  3.034001  ...  3.365471  3.446338  3.577815\n#> 4  3.456021  3.574613  3.658350  ...  3.966363  4.047775  4.166815\n#> 5  3.356988  3.505757  3.580486  ...  3.858433  3.961821  4.084200\n#> \n#> [6 rows x 9 columns]\n```\n:::\n\n\nPintamos \n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x_test, y_test)\n\nfor col in ['pred_0.1', 'pred_0.5', 'pred_0.9']:\n    ax.scatter(x_test.reshape(-1,1), preds[col], alpha=0.50, label=col)\n\nax.legend()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=80%}\n:::\n:::\n\n\nY ya estaría, no parece mala alternativa si uno tiene que hacer este tipo de cosas. \n\n:::callout-tip\nOjalá le sirva a mi amigo Kenet para una cosa que estaba bicheando. \n::: \n\nPues poco más. __Feliz domingo__\n\nCon R también se puede, como no. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate) # para comunicar R y python y poder convertir datos y funciones de uno a otro bidireccionalmente\nlibrary(catboost)\n\nX_train <- as.matrix(py$x_train) # catboost en R espera  una matriz\nY_train <-  as.matrix(py$y_train)\n\n\nX_test <- as.matrix(py$x_test) \nY_test <-  as.matrix(py$y_test)\n\nhead(X_train) ; head(Y_train)\n#>           [,1]\n#> [1,] 0.6499924\n#> [2,] 0.9125034\n#> [3,] 0.4820761\n#> [4,] 0.2060426\n#> [5,] 0.5411845\n#> [6,] 0.1779657\n#>          [,1]\n#> [1,] 4.165367\n#> [2,] 5.018479\n#> [3,] 4.146041\n#> [4,] 2.942043\n#> [5,] 4.287877\n#> [6,] 3.589826\n\n(quantiles_str <-  py$quantile_str)\n#> [1] \"0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_pool <- catboost.load_pool(data = X_train, label = Y_train)\ntest_pool <- catboost.load_pool(data = X_test)\nloss_function <-  paste0(\"MultiQuantile:alpha=\", quantiles_str)\n\nfit_params <-  list(\n    iterations = 100,\n    loss_function= loss_function\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\nmodel <- catboost.train(train_pool, params=fit_params)\n#> 0:\tlearn: 0.1977907\ttotal: 50.6ms\tremaining: 5s\n#> 1:\tlearn: 0.1934878\ttotal: 56.9ms\tremaining: 2.79s\n#> 2:\tlearn: 0.1891389\ttotal: 61.3ms\tremaining: 1.98s\n#> 3:\tlearn: 0.1849242\ttotal: 66.1ms\tremaining: 1.58s\n#> 4:\tlearn: 0.1808818\ttotal: 71ms\tremaining: 1.35s\n#> 5:\tlearn: 0.1769520\ttotal: 74.5ms\tremaining: 1.17s\n#> 6:\tlearn: 0.1732251\ttotal: 78.2ms\tremaining: 1.04s\n#> 7:\tlearn: 0.1696774\ttotal: 83ms\tremaining: 955ms\n#> 8:\tlearn: 0.1661482\ttotal: 86.6ms\tremaining: 876ms\n#> 9:\tlearn: 0.1628541\ttotal: 89.7ms\tremaining: 807ms\n#> 10:\tlearn: 0.1596378\ttotal: 93.1ms\tremaining: 753ms\n#> 11:\tlearn: 0.1565479\ttotal: 97.3ms\tremaining: 714ms\n#> 12:\tlearn: 0.1536016\ttotal: 102ms\tremaining: 683ms\n#> 13:\tlearn: 0.1507692\ttotal: 105ms\tremaining: 647ms\n#> 14:\tlearn: 0.1480845\ttotal: 109ms\tremaining: 615ms\n#> 15:\tlearn: 0.1454840\ttotal: 113ms\tremaining: 592ms\n#> 16:\tlearn: 0.1429863\ttotal: 118ms\tremaining: 576ms\n#> 17:\tlearn: 0.1407059\ttotal: 122ms\tremaining: 557ms\n#> 18:\tlearn: 0.1383697\ttotal: 126ms\tremaining: 536ms\n#> 19:\tlearn: 0.1361779\ttotal: 129ms\tremaining: 518ms\n#> 20:\tlearn: 0.1340707\ttotal: 133ms\tremaining: 500ms\n#> 21:\tlearn: 0.1320342\ttotal: 137ms\tremaining: 485ms\n#> 22:\tlearn: 0.1300775\ttotal: 140ms\tremaining: 468ms\n#> 23:\tlearn: 0.1282447\ttotal: 144ms\tremaining: 455ms\n#> 24:\tlearn: 0.1264588\ttotal: 147ms\tremaining: 441ms\n#> 25:\tlearn: 0.1247594\ttotal: 151ms\tremaining: 429ms\n#> 26:\tlearn: 0.1232137\ttotal: 154ms\tremaining: 416ms\n#> 27:\tlearn: 0.1216563\ttotal: 157ms\tremaining: 404ms\n#> 28:\tlearn: 0.1202034\ttotal: 161ms\tremaining: 394ms\n#> 29:\tlearn: 0.1187735\ttotal: 164ms\tremaining: 383ms\n#> 30:\tlearn: 0.1174263\ttotal: 169ms\tremaining: 375ms\n#> 31:\tlearn: 0.1161250\ttotal: 172ms\tremaining: 365ms\n#> 32:\tlearn: 0.1148583\ttotal: 175ms\tremaining: 356ms\n#> 33:\tlearn: 0.1136688\ttotal: 180ms\tremaining: 349ms\n#> 34:\tlearn: 0.1125292\ttotal: 185ms\tremaining: 343ms\n#> 35:\tlearn: 0.1114357\ttotal: 188ms\tremaining: 334ms\n#> 36:\tlearn: 0.1104088\ttotal: 195ms\tremaining: 332ms\n#> 37:\tlearn: 0.1094203\ttotal: 199ms\tremaining: 325ms\n#> 38:\tlearn: 0.1085302\ttotal: 202ms\tremaining: 316ms\n#> 39:\tlearn: 0.1076702\ttotal: 205ms\tremaining: 308ms\n#> 40:\tlearn: 0.1068348\ttotal: 212ms\tremaining: 305ms\n#> 41:\tlearn: 0.1060263\ttotal: 216ms\tremaining: 298ms\n#> 42:\tlearn: 0.1052530\ttotal: 219ms\tremaining: 290ms\n#> 43:\tlearn: 0.1045115\ttotal: 223ms\tremaining: 284ms\n#> 44:\tlearn: 0.1037861\ttotal: 228ms\tremaining: 278ms\n#> 45:\tlearn: 0.1031053\ttotal: 232ms\tremaining: 272ms\n#> 46:\tlearn: 0.1024645\ttotal: 235ms\tremaining: 265ms\n#> 47:\tlearn: 0.1018457\ttotal: 242ms\tremaining: 262ms\n#> 48:\tlearn: 0.1012497\ttotal: 246ms\tremaining: 256ms\n#> 49:\tlearn: 0.1006996\ttotal: 250ms\tremaining: 250ms\n#> 50:\tlearn: 0.1001835\ttotal: 253ms\tremaining: 243ms\n#> 51:\tlearn: 0.0996695\ttotal: 257ms\tremaining: 237ms\n#> 52:\tlearn: 0.0991990\ttotal: 261ms\tremaining: 232ms\n#> 53:\tlearn: 0.0987716\ttotal: 265ms\tremaining: 225ms\n#> 54:\tlearn: 0.0983443\ttotal: 268ms\tremaining: 219ms\n#> 55:\tlearn: 0.0979411\ttotal: 272ms\tremaining: 214ms\n#> 56:\tlearn: 0.0975621\ttotal: 275ms\tremaining: 208ms\n#> 57:\tlearn: 0.0972083\ttotal: 279ms\tremaining: 202ms\n#> 58:\tlearn: 0.0968639\ttotal: 283ms\tremaining: 197ms\n#> 59:\tlearn: 0.0965243\ttotal: 287ms\tremaining: 191ms\n#> 60:\tlearn: 0.0961923\ttotal: 292ms\tremaining: 187ms\n#> 61:\tlearn: 0.0958829\ttotal: 298ms\tremaining: 182ms\n#> 62:\tlearn: 0.0956101\ttotal: 304ms\tremaining: 178ms\n#> 63:\tlearn: 0.0953473\ttotal: 308ms\tremaining: 173ms\n#> 64:\tlearn: 0.0950908\ttotal: 313ms\tremaining: 168ms\n#> 65:\tlearn: 0.0948327\ttotal: 317ms\tremaining: 163ms\n#> 66:\tlearn: 0.0946033\ttotal: 324ms\tremaining: 160ms\n#> 67:\tlearn: 0.0943820\ttotal: 329ms\tremaining: 155ms\n#> 68:\tlearn: 0.0941723\ttotal: 332ms\tremaining: 149ms\n#> 69:\tlearn: 0.0939663\ttotal: 338ms\tremaining: 145ms\n#> 70:\tlearn: 0.0937677\ttotal: 342ms\tremaining: 140ms\n#> 71:\tlearn: 0.0935819\ttotal: 346ms\tremaining: 134ms\n#> 72:\tlearn: 0.0933969\ttotal: 349ms\tremaining: 129ms\n#> 73:\tlearn: 0.0932181\ttotal: 353ms\tremaining: 124ms\n#> 74:\tlearn: 0.0930514\ttotal: 358ms\tremaining: 119ms\n#> 75:\tlearn: 0.0928960\ttotal: 361ms\tremaining: 114ms\n#> 76:\tlearn: 0.0927433\ttotal: 364ms\tremaining: 109ms\n#> 77:\tlearn: 0.0925871\ttotal: 369ms\tremaining: 104ms\n#> 78:\tlearn: 0.0924635\ttotal: 374ms\tremaining: 99.3ms\n#> 79:\tlearn: 0.0923467\ttotal: 377ms\tremaining: 94.3ms\n#> 80:\tlearn: 0.0922292\ttotal: 380ms\tremaining: 89.2ms\n#> 81:\tlearn: 0.0921089\ttotal: 386ms\tremaining: 84.8ms\n#> 82:\tlearn: 0.0919860\ttotal: 390ms\tremaining: 79.9ms\n#> 83:\tlearn: 0.0918862\ttotal: 394ms\tremaining: 75ms\n#> 84:\tlearn: 0.0917925\ttotal: 398ms\tremaining: 70.2ms\n#> 85:\tlearn: 0.0916927\ttotal: 404ms\tremaining: 65.8ms\n#> 86:\tlearn: 0.0916089\ttotal: 408ms\tremaining: 61ms\n#> 87:\tlearn: 0.0915482\ttotal: 409ms\tremaining: 55.8ms\n#> 88:\tlearn: 0.0914624\ttotal: 412ms\tremaining: 50.9ms\n#> 89:\tlearn: 0.0913841\ttotal: 416ms\tremaining: 46.2ms\n#> 90:\tlearn: 0.0912981\ttotal: 420ms\tremaining: 41.6ms\n#> 91:\tlearn: 0.0912323\ttotal: 424ms\tremaining: 36.9ms\n#> 92:\tlearn: 0.0911620\ttotal: 427ms\tremaining: 32.2ms\n#> 93:\tlearn: 0.0910968\ttotal: 431ms\tremaining: 27.5ms\n#> 94:\tlearn: 0.0910390\ttotal: 437ms\tremaining: 23ms\n#> 95:\tlearn: 0.0909742\ttotal: 441ms\tremaining: 18.4ms\n#> 96:\tlearn: 0.0909243\ttotal: 444ms\tremaining: 13.7ms\n#> 97:\tlearn: 0.0908512\ttotal: 449ms\tremaining: 9.17ms\n#> 98:\tlearn: 0.0907883\ttotal: 454ms\tremaining: 4.58ms\n#> 99:\tlearn: 0.0907341\ttotal: 457ms\tremaining: 0us\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredicciones <- catboost.predict(model, pool = test_pool)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(predicciones) <- paste0(\"quantile_\", 1:9) \n\nhead(predicciones)\n#>      quantile_1 quantile_2 quantile_3 quantile_4 quantile_5 quantile_6\n#> [1,]   3.401601   3.542894   3.619259   3.716297   3.794767   3.868639\n#> [2,]   4.150397   4.269908   4.355643   4.431774   4.502443   4.586961\n#> [3,]   4.350918   4.476999   4.595515   4.651673   4.713752   4.815297\n#> [4,]   2.829240   2.936415   3.034001   3.123039   3.208654   3.292293\n#> [5,]   3.456021   3.574613   3.658350   3.741498   3.824226   3.897069\n#> [6,]   3.356988   3.505757   3.580486   3.651319   3.720015   3.791630\n#>      quantile_7 quantile_8 quantile_9\n#> [1,]   3.938859   4.041568   4.165809\n#> [2,]   4.701792   4.816983   4.958197\n#> [3,]   4.933372   5.017778   5.147716\n#> [4,]   3.365471   3.446338   3.577815\n#> [5,]   3.966363   4.047775   4.166815\n#> [6,]   3.858433   3.961821   4.084200\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "fbc6121b1b31dc9fb1b989c93daeebb2",
  "result": {
    "markdown": "---\ntitle: Ejemplillo con NMF\ndate: '2020-10-21'\ncategories:\n  - estadística\n  - correspondencias\n  - factorización\n  - nmf \n  - 2020\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\n    code-fold: show\n    code-summary: \"Mostrar / ocultar código\"\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"   \n---\n\n\nAndo falto de ideas, no sé si es la pandemia, el teletrabajo ( o la esclavitud en tiempos modernos como me gusta llamarlo) u otra cosa. Total, que me he puesto a bichear un post antiguo de mi amigo Carlos Gil sobre [NMF](https://www.datanalytics.com/2014/06/19/factorizaciones-positivas-de-matrices-igualmente-positivas/) (factorización no negativa de matrices). Cómo siempre el lo cuenta mucho mejor que yo.\n\nTotal, que puede que en breve me toque tener algo a lo que quizá se pueda aplicar este tipo de técnicas, a saber, tener clientes y productos.\n\nDe hecho voy a usar su mismo ejemplo.\n\nNota: La librería es mejor si se instala desde BioConductor con BiocManager.\n\nLa librería NMF está bastante bien, utiliza paralelización, por debajo está escrita en C, pero tiene el incoveniente de que aún no está implementado un método `predict` para nuevos datos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(NMF) # BiocManager::install(\"NMF\")\n\na <- as.matrix(caith)\nres <- nmf(a, rank = 2)\n \na\n#>        fair red medium dark black\n#> blue    326  38    241  110     3\n#> light   688 116    584  188     4\n#> medium  343  84    909  412    26\n#> dark     98  48    403  681    85\n```\n:::\n\n\nEl caso es factorizar esta matriz en dos matrices no negativas. Otras formas de factorizar esta matriz podría ser con análisis de correspondencias simples. Pero vamos a la descomposición por nmf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw <-  res@fit@W\nh <-  res@fit@H\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nw \n#>             [,1]      [,2]\n#> blue    34.41271 386.48565\n#> light   32.29480 896.90510\n#> medium 438.78664 576.80290\n#> dark   720.24703   5.38822\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nh\n#>           fair        red    medium      dark        black\n#> [1,] 0.1141435 0.06226416 0.6667132 0.8736829 9.626828e-02\n#> [2,] 0.7049219 0.11239402 0.7074373 0.1715769 1.971208e-13\n```\n:::\n\n\nReconstruimos a\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw %*% h\n#>             fair       red   medium      dark     black\n#> blue   276.37021  45.58136 296.3578  96.37782  3.312852\n#> light  635.93433 102.81758 656.0355 182.10365  3.108965\n#> medium 456.68567  92.14988 700.5967 482.32648 42.241237\n#> dark    86.00979  45.45118 484.0100 630.19204 69.336946\n```\n:::\n\n\nY si comparamos con a\n\n\n::: {.cell}\n\n```{.r .cell-code}\na - (w %*% h)\n#>              fair       red    medium       dark       black\n#> blue     49.62979 -7.581356 -55.35776  13.622177  -0.3128523\n#> light    52.06567 13.182418 -72.03547   5.896346   0.8910349\n#> medium -113.68567 -8.149881 208.40327 -70.326485 -16.2412371\n#> dark     11.99021  2.548819 -81.01004  50.807961  15.6630545\n```\n:::\n\n\nBueno, la reconstrucción no es perfecta, pero bueno, no está tan mal.\n\nBien, tal y como cuenta Carlos en su entrada ahora podemos normalizar las filas de W y de H, de forma que tengamos probabilidades. Dónde entonces H sería funciones de probabilidad sobre las filas de la matriz original y W serán ponderaciones. O como dice él, *H* es un menú de preferencias (imaginemos que tenemos usuarios en filas y productos en columnas), en este caso hemos hecho una reducción de dimensión para quedarnos en 2 preferencias, (sería el equivalente conceptual al número de componentes en un PCA o en un CA), y *W* serían las ponderaciones que cada usuario da a cada una de las preferencias (sus coordenadas en un correspondencias siguiendo el símil)\n\nNormalicemos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw_hat <- w / rowSums(w)\nw_hat\n#>              [,1]        [,2]\n#> blue   0.08176014 0.918239864\n#> light  0.03475549 0.965244506\n#> medium 0.43205116 0.567948839\n#> dark   0.99257448 0.007425522\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nh_hat <-  h / rowSums(h)\nh_hat\n#>            fair        red    medium      dark        black\n#> [1,] 0.06295585 0.03434180 0.3677257 0.4818799 5.309678e-02\n#> [2,] 0.41555704 0.06625716 0.4170398 0.1011460 1.162043e-13\n```\n:::\n\n\nAsí, el primer \"menú\" está compuesto por los \"productos\" fair, red, etc, en proporción a como indica la primera fila de h_hat. Y el individuo \"blue\" prefiere el primer menú en casi un 0.9 de probabilidad vs un  alrededor de 0.1 de preferencia del menú 2. En un PCA diríamos que esos son los \"loadings\". \n\nLas filas de *W* a veces se asocian con arquetipos o individuos promedio. Los individuos \"blue\" tienen esos pesos los dos factores latentes.\n\nEn este caso dónde tenemos color de ojos (fila) y color del pelo (columnas), vemos que en las dos distribuciones multinomiales  que hay en sendas filas de `h_hat` (si, eso es lo que son, dos distribuciones multinomiales), la probabilidad de tener el pelo negro es bastante pequeña (tiene que ver con la tabla de contingencia original, hay muy pocos con el pelo negro). \nPero vemos que hay un arquetipo, (el de os ojos oscuros) para el cual el peso que da al menú de preferencias dónde la probabilidad de tener el pelo negro sea mayor. Es decir, al final es una mixtura de distribuciones multinomiales.\n\nEn realidad lo que hace NMF es descubrir la estructura subyacente de los datos en un espacio de menor dimensión que el original. Bueno, pues con *W* y *H* normalizadas podemos construir una matriz diagonal *D* que simplemente nos genere muestras de individuos y en qué columnas caen.\n\nPodemos utilizar como matriz diagonal la suma de las filas de a, y así obtener\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(d <- diag(rowSums(a)))\n#>      [,1] [,2] [,3] [,4]\n#> [1,]  718    0    0    0\n#> [2,]    0 1580    0    0\n#> [3,]    0    0 1774    0\n#> [4,]    0    0    0 1315\n```\n:::\n\n\nY podemos hacer $A \\approx D \\cdot W \\cdot H$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %*% w_hat %*% h_hat\n#>           fair       red   medium      dark     black\n#> [1,] 277.67093  45.69909 296.5397  94.97332  3.116981\n#> [2,] 637.21749 102.93373 656.2149 180.71812  2.915739\n#> [3,] 466.94392  93.07840 702.0314 471.24977 40.696489\n#> [4,]  86.22994  45.47111 484.0408 629.95432 69.303794\n```\n:::\n\n\nQue se parece bastante a `w %*% h` , o podríamos usar otra *D*, en este caso para obtener qué matriz se obtendría para 10 casos de cada fila. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-  diag(c(10,10, 10, 10))\nd %*% w_hat %*% h_hat\n#>          fair       red   medium     dark      black\n#> [1,] 3.867283 0.6364776 4.130079 1.322748 0.04341200\n#> [2,] 4.033022 0.6514793 4.153259 1.143786 0.01845405\n#> [3,] 2.632153 0.5246809 3.957336 2.656425 0.22940524\n#> [4,] 0.655741 0.3457879 3.680919 4.790527 0.52702505\n```\n:::\n\n\ny bueno, la verdad es que me pregunto si esto se parece o no a un análisis de correspondencias. Veamos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(FactoMineR)\nres_ca <-  CA (a, ncp = 2, graph = FALSE)\nfactoextra::fviz_ca(res_ca)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=80%}\n:::\n:::\n\nLo primero que hay que darse cuenta es que ambas técnicas no son del todo comparables, el correspondencias busca encontrar dimensiones que expliquen la mayor cantidad de inercia (distancia Chi-cuadrado) y es parecido al PCA en el sentido de que la primera dimensión es la que más explica, etc.. De hecho el CA, diagonaliza 2 matrices derivadas de la tabla de contingencia, una la de los perfiles filas y otra la de los perfiles columna. Y las pinta junta de acuerdo a algún teorema baricéntrico que tuve que demostrar en algún examen allá por los lejanos 90's. \n\nPero en realidad si nos fijamos en las coordenadas de las filas en el CA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_ca$row$coord\n#>              Dim 1       Dim 2\n#> blue   -0.40029985  0.16541100\n#> light  -0.44070764  0.08846303\n#> medium  0.03361434 -0.24500190\n#> dark    0.70273880  0.13391383\n```\n:::\n\n\nNo es más que ver las filas en un subespacio (el calculado por el CA) del espacio definido por las columnas  y de forma análoga pasa con las columnas. Estas coordenadas podrían ser una forma de codificar la variable categórica. Cabe preguntarse si tienen relación con la estructura obtenida por el NMF. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoordenadas_filas <-  cbind(res_ca$row$coord, w_hat)\ncolnames(coordenadas_filas)[3:4] <-  paste0(\"nmf_\", 1:2)\ncoordenadas_filas\n#>              Dim 1       Dim 2      nmf_1       nmf_2\n#> blue   -0.40029985  0.16541100 0.08176014 0.918239864\n#> light  -0.44070764  0.08846303 0.03475549 0.965244506\n#> medium  0.03361434 -0.24500190 0.43205116 0.567948839\n#> dark    0.70273880  0.13391383 0.99257448 0.007425522\n```\n:::\n\n\ny \n\n::: {.cell}\n\n```{.r .cell-code}\ncor(coordenadas_filas)\n#>             Dim 1       Dim 2       nmf_1       nmf_2\n#> Dim 1  1.00000000 -0.05155554  0.99991352 -0.99991352\n#> Dim 2 -0.05155554  1.00000000 -0.04510151  0.04510151\n#> nmf_1  0.99991352 -0.04510151  1.00000000 -1.00000000\n#> nmf_2 -0.99991352  0.04510151 -1.00000000  1.00000000\n```\n:::\n\n\nResultado coherente, ¿no? . En este ejemplo de juguete una única dimensión del correspondencias explica el 86,5% de la inercia. \n\nCosas buenas del nmf.\n\n* Nos da una interpretación más natural de ciertas cosas\n* Las dimensiones encontradas al no estar ordenadas por importancia, no sufren del efecto tamaño de otras técnicas que buscan el mejor primer resumen y luego el segundo mejor resumen de los datos. La verdad que no estoy seguro de esto que acabo de escribir. \n* Es equivalente a un LDA cuando se utiliza como función objetivo la divergencia de Kullback-Leibler. \n\n\nAh, se me olvidaba. ¿qué pasa si tengo una nueva fila/usario?, la librería NMF no permite predecir, y aunque se podría implementar, buscando un poco se encuentra la forma\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(NNLM)\n\nres_nmf2 <-  nnmf(a, k = 2, loss = \"mkl\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres_nmf2$W\n#>              [,1]       [,2]\n#> blue    0.6928050 13.3441587\n#> light   0.1273311 30.9726568\n#> medium 13.0923170 19.8721597\n#> dark   22.4117740  0.1060281\nres_nmf2$H\n#>           fair      red   medium      dark      black\n#> [1,]  3.740643 2.012339 21.49645 28.091524 3.09335848\n#> [2,] 20.516743 3.311351 21.09274  5.764011 0.08766068\n```\n:::\n\n\nNuevas filas\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nb <- matrix( data = c(20, 30, 40, 0,20, 10, 10, 30,10, 90), nrow=2, byrow = TRUE)\ncolnames(b) <-  colnames(a)\nrownames(b) <-  c(\"tipo_n1\", \"tipo_n2\")\nb\n#>         fair red medium dark black\n#> tipo_n1   20  30     40    0    20\n#> tipo_n2   10  10     30   10    90\n```\n:::\n\n\nY tiene un método predict\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(res_nmf2, newdata = b, which = \"W\")\n#> $coefficients\n#>             [,1]    [,2]\n#> tipo_n1 0.734150 1.32159\n#> tipo_n2 2.566985 0.00000\n#> \n#> $n.iteration\n#> [1] 30\n#> \n#> $error\n#>          MSE          MKL target.error \n#>   1305.05201     26.74997     26.74997 \n#> \n#> $options\n#> $options$method\n#> [1] \"scd\"\n#> \n#> $options$loss\n#> [1] \"mkl\"\n#> \n#> $options$max.iter\n#> [1] 10000\n#> \n#> $options$rel.tol\n#> [1] 1e-12\n#> \n#> \n#> $call\n#> nnlm(x = t(object$H), y = t(newdata), method = method, loss = loss)\n#> \n#> attr(,\"class\")\n#> [1] \"nnlm\"\n```\n:::\n\n\nLo dicho, una técnica muy interesante y útil. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
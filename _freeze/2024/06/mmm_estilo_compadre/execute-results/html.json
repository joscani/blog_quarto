{
  "hash": "510e67925416b14baecacc8549f5db32",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MMM. Estilo compadre\"  \ndate: '2024-06-01'\ncategories: \n  - 2024\n  - Marketing \n  - análisis bayesiano\n  - R\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8\n    fig-align: center\n    code-fold: show\n    code-summary: \"Show the code\"\nimage: curva_saturacion.png\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\n::: callout-note\n## Listening\n\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/4bO6DljpuAeQh6HS20i0I5?utm_source=generator\" width=\"100%\" height=\"250\" frameBorder=\"0\" allowfullscreen allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\">\n\n</iframe>\n\n\n:::\n\n\n\n## Confesión\n\nHe de reconocer que mi conocimiento sobre lo que se ha dado en llamar el Marketing/Media Mix Modelling\nes más bien escaso por no decir que inexistente. No obstante, una reunión el otro día en el trabajo \nhizo despertar mi  curiosidad. Así que voy a contar un poco lo que he aprendido. \n\n\n## Qué es esto del marketing  mix modelling. \n\nPues en teoría es un intento de inferencia causal, que intenta responder a la pregunta de ¿qué pasa \ncon las ventas si aumento mi inversión publicitaria en un canal _a_ ? ¿ Y si cambio por completo el \nmix de gasto en publicidad?  ¿Si tengo un presupuesto _Z_, cuál es la forma óptima de asignar gasto a cada medio? \n\n\nBien, pues para poder contestar a estas preguntas lo primero de todo es asumir que las ventas están \nrelacionadas e incluso son causa de la inversión en publicidad. Suena a palabras mayores, desde luego,\npero que le vamos a hacer, esto es lo que hay. \n\nUno podría (y debería) pintar un DAG que expresase lo que creemos saber sobre la relación entre las \nvariables. Quizá el gasto en _radio_ influya en que el gasto en _google_ sea más eficaz, y cosas así. \nPero como yo estoy empezando a aprender sobre este tema, voy a partir del supuesto, de que una regresión\nmúltiple me permite expresar las relaciones _causales_ entre las variables, y además que se dan todo \nel resto de supuestos para la _inferencia causal_ y por tanto, podría interpretar los coeficientes como \nefectos causales. Si, ya lo sé, esto es un triple salto mortal, pero es lo que se hace todos los días \nen las empresas. Ya habrá tiempo de cuestionar estas cosas. \n\n\n\n## Unos datillos de ejemplo\n\nEn la librería `datarium` tenemos unos datos de marketing que nos pueden servir\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlibrary(datarium)\n\ndatos  <- marketing\n```\n:::\n\n\nEstos datos son muy sencillos, son 200 filas y  4 columnas que se corresponden con la inversión \npublicitaria en 3 canales (youtube, facebook, newspaper) y una última columna con el monto de las\nventas. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(datos)\n#> [1] 200   4\nhead(datos)\n#>   youtube facebook newspaper sales\n#> 1  276.12    45.36     83.04 26.52\n#> 2   53.40    47.16     54.12 12.48\n#> 3   20.64    55.08     83.16 11.16\n#> 4  181.80    49.56     70.20 22.20\n#> 5  216.96    12.96     70.08 15.48\n#> 6   10.44    58.68     90.00  8.64\n```\n:::\n\n\nLeyendo la documentación sobre este conjunto de datos se podría asumir que los datos están ordenados \npor fechas, voy a suponer que cada fila corresponde a una semana, así que voy a pegar una variable de semana \ny otra que sea simplemente del 1 al 200. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatos$fecha_arbitraria <- seq.Date(as.Date(\"2020-01-02\"), length.out =200, by = \"week\")\ndatos$semana <- lubridate::week(datos$fecha_arbitraria)\ndatos$time <- 1:200\n\ndatos$mes <- lubridate::month(datos$fecha_arbitraria)\ndatos$anyo <- lubridate::year(datos$fecha_arbitraria)\n\nhead(datos)\n#>   youtube facebook newspaper sales fecha_arbitraria semana time mes anyo\n#> 1  276.12    45.36     83.04 26.52       2020-01-02      1    1   1 2020\n#> 2   53.40    47.16     54.12 12.48       2020-01-09      2    2   1 2020\n#> 3   20.64    55.08     83.16 11.16       2020-01-16      3    3   1 2020\n#> 4  181.80    49.56     70.20 22.20       2020-01-23      4    4   1 2020\n#> 5  216.96    12.96     70.08 15.48       2020-01-30      5    5   1 2020\n#> 6   10.44    58.68     90.00  8.64       2020-02-06      6    6   2 2020\n```\n:::\n\n\nPintamos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndatos  %>%\n  ggplot(aes(fecha_arbitraria, sales)) +\n  geom_point() + \n  geom_line()\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-4-1.png){width=80%}\n:::\n:::\n\n\n\n## Teoría marketiniana\n\nEn estas cosas de marketing mix modelling hay algunas cosillas peculiares. La primera es que se asume, y \nno me parece mal, que en un instante _t_ la inversión publicitaria que \"causa\" las ventas no es sólo la\ndel instante _t_ sino que también hay cierto efecto \"remanente\" de la inversión pasada. Y por tanto en \nvez de considerar la variable tal cual en un instante _t_ se considera algo como \n\n$$ X_{Adstock} = X_t + \\lambda_{1} \\times X_{t-1} + ...\\lambda_{n} \\times X_{t-n} $$\n\npero además vamos a poner un período de _olvido_  de 2 . \n\nEn  R podemos hacerlo de la siguiente forma. \nPor ejemplo para facebook\n\nSe supone que los datos están ordenados de fecha más antigua a más reciente. El criterio de \nponer set_rate_fb = 0.1 ha sido arbitrario, pero se podría estimar.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set adstock fb rate\nset_rate_fb <- 0.1\nset_memory <- 2\nget_adstock_fb <- rep(set_rate_fb, set_memory+1) ^ c(0:set_memory)\n\nads_fb <- stats::filter(c(rep(0, set_memory), datos$facebook), get_adstock_fb, method=\"convolution\")\nads_fb <- ads_fb[!is.na(ads_fb)]\n\nhead(datos$facebook)\n#> [1] 45.36 47.16 55.08 49.56 12.96 58.68\nhead(ads_fb)\n#> [1] 45.3600 51.6960 60.2496 55.5396 18.4668 60.4716\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\nplot(seq(1,length(datos$facebook)), datos$facebook, type=\"h\", \n     main = \"Adstocked Facebook\",\n     xlab=\"Time (Weeks)\", ylab=\"Facebook\", \n     ylim=c(0, max(c(datos$facebook, ads_fb))), \n     frame.plot=FALSE)\nlines(ads_fb, col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-6-1.png){width=80%}\n:::\n:::\n\n\nHacemos lo mismo para resto de canales, con diferente valor\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n#set adstock youtube rate\nset_rate_yt <- 0.15\nset_memory <- 2\nget_adstock_youtube <- rep(set_rate_yt, set_memory+1) ^ c(0:set_memory)\n\n#set adstock news rate\nset_rate_news <- 0.25\nset_memory <- 2\nget_adstock_news <- rep(set_rate_news, set_memory+1) ^ c(0:set_memory)\n\n\nads_youtube <- stats::filter(c(rep(0, set_memory), datos$youtube), get_adstock_youtube, method=\"convolution\")\nads_youtube <- ads_youtube[!is.na(ads_youtube)]\n\n#adstocked newpaper\nads_news <- stats::filter(c(rep(0, set_memory), datos$newspaper), get_adstock_news, method=\"convolution\")\nads_news <- ads_news[!is.na(ads_news)]\n#plot\n\n\ndatos <- cbind(datos, ads_fb, ads_youtube, ads_news)\nhead(datos)\n#>   youtube facebook newspaper sales fecha_arbitraria semana time mes anyo\n#> 1  276.12    45.36     83.04 26.52       2020-01-02      1    1   1 2020\n#> 2   53.40    47.16     54.12 12.48       2020-01-09      2    2   1 2020\n#> 3   20.64    55.08     83.16 11.16       2020-01-16      3    3   1 2020\n#> 4  181.80    49.56     70.20 22.20       2020-01-23      4    4   1 2020\n#> 5  216.96    12.96     70.08 15.48       2020-01-30      5    5   1 2020\n#> 6   10.44    58.68     90.00  8.64       2020-02-06      6    6   2 2020\n#>    ads_fb ads_youtube ads_news\n#> 1 45.3600    276.1200  83.0400\n#> 2 51.6960     94.8180  74.8800\n#> 3 60.2496     34.8627 101.8800\n#> 4 55.5396    186.0975  94.3725\n#> 5 18.4668    244.6944  92.8275\n#> 6 60.4716     47.0745 111.9075\n```\n:::\n\n\n\n## Modelando\n\nPues en esta primera parte de MMM estilo compadre se trata simplemente de estimar las ventas en base al tiempo y al \nadstock, es decir, un ejercicio de estimación de series temporales. De hecho, las consultoras que se dedican \na estos temas hacen una serie de modelos lineales, a veces encadenados (y sin ajustar conjuntamente, lo cual \nes un error importante), y algunas asumen modelos lineales sin _Intercept_ , es decir, tienen la asunción \nfuerte (y errónea) que todas las ventas se deben exclusivamente a variables modeladas y que a inversión 0, las ventas\nno existen. Yo no voy a asumir semejante cosa, dios me libre.\n\n\n### Primer modelo sencillito\n\nPues ya que el software lo permite, vamos a hacer un modelito, pero bayesiano.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlibrary(cmdstanr)\nlibrary(brms)\n```\n:::\n\n\nEn vez de modelar las ventas , modelo su logaritmo neperiano, por qué? porque me da la gana, y porque a veces\nasí se consiguen mejores resultados. En realidad habría que probar ambas cosas. De hecho, quizá\nsea mejor modelar las ventas en vez del log(ventas) en aras a la interpretabilidad de los parámetros. Podéis probar, \nyo ya lo voy a dejar con logaritmo. \n \nTambién meto un término de splines, y entreno con los primeros 150 datos y dejo de test los últimos 50\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntrain <- datos[1:150, ]\ntest <-  datos[151:200, ]\n\n\nmod_splines <- brm(log(sales) ~ ads_fb + ads_youtube + ads_news  + s(time, k = 10),\n              data = train,\n              family = gaussian(),\n              backend = \"cmdstanr\",\n              cores = 4,\n              file = here::here(\"2024/06/mod_splines\") )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsummary(mod_splines)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: log(sales) ~ ads_fb + ads_youtube + ads_news + s(time, k = 10) \n#>    Data: train (Number of observations: 150) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Smoothing Spline Hyperparameters:\n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sds(stime_1)     0.24      0.25     0.01     0.92 1.00      873     1522\n#> \n#> Regression Coefficients:\n#>             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept       1.82      0.05     1.72     1.92 1.00     4112     3263\n#> ads_fb          0.01      0.00     0.01     0.01 1.00     4291     2309\n#> ads_youtube     0.00      0.00     0.00     0.00 1.00     3771     2568\n#> ads_news        0.00      0.00    -0.00     0.00 1.00     4136     2779\n#> stime_1         0.07      0.53    -0.84     1.43 1.00     1088      808\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.20      0.01     0.18     0.23 1.00     3608     3041\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\n### Algún chequeo del modelo\n\nUno de los chequeos básico es comparar la variable respuesta en los datos con la que \ngenera el modelo. \n\nSe pinta la variable real y las generadas mediante la _posterior predictive_ . Vamos a pintar 100 posteriors\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npp_check(mod_splines, ndraws  = 100)\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-11-1.png){width=80%}\n:::\n:::\n\n\ny no tiene mala pinta. \n\nVeamos como predice el modelo para los datos de test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npredicciones <- posterior_predict(mod_splines, newdata = test, ndraws = 1000)\npredicciones_medias <- apply(predicciones, 2, mean)\n\ntest$ventas_predichas <- exp(predicciones_medias)\n```\n:::\n\n\nVemos alguna métrica como el *RMSE* o el *MAE*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nyardstick::rmse(test, sales, ventas_predichas)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        2.45\nyardstick::mae(test, sales, ventas_predichas)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard        1.91\n```\n:::\n\n\nPintamos las ventas predicas vs las reales en test, para ver si caen cerca de la recta $y = x$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntest  %>%\n  ggplot(aes(x = ventas_predichas, y = sales)) +\n  geom_point()  +\n  geom_smooth(method =\"lm\", linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 1)\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-14-1.png){width=80%}\n:::\n:::\n\n\n\n\nY bueno, ni tan mal. \n\n## Optimizando\n\nPues el tema en esto del MMM consiste ahora en encontrar cuánto hay que invertir en cada\ncanal para optimizar las ventas , pero sujeto a unas restricciones, por ejemplo presupuestarias.\n\nTodo esto con la asunción de que el modelo ajustado representa la verdadera relación causal\ny no sólo asociación. \n\nEl principal problema que yo veo a estas cosas es que si yo he entrenado con un rango de gasto\nhistórico en un canal A, el modelo puede haber estimado que a más gasto más ventas y extrapolar\nmucho más allá de lo aconsejable.\n\n\nLo primero es crear una función que simule las ventas obtenidas. \n\nSuponemos que nuestros nuevos datos irán temporalmente detrás de los datos de train, y que por tanto \nel adstock se ve influido por datos antiguos\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsimular_ventas_siguiente_periodo <- function(facebook_new, youtube_new, news_new,  modelo) {\n \n  nuevo_time = max(train$time) + 1\n  \n  # obtener add_stock \n  ads_fb <- stats::filter(c(rep(0, set_memory), train$facebook, facebook_new), get_adstock_fb, method=\"convolution\")\n  ads_fb <- ads_fb[!is.na(ads_fb)]\n  ads_fb <-  tail(ads_fb, 1)\n  \n  \n  ads_youtube <- stats::filter(c(rep(0, set_memory), train$youtube, youtube_new), get_adstock_youtube, method=\"convolution\")\n  ads_youtube <- ads_youtube[!is.na(ads_youtube)]\n  ads_youtube <-  tail(ads_youtube, 1)\n  \n  \n  ads_news <- stats::filter(c(rep(0, set_memory), train$newspaper, news_new), get_adstock_news, method=\"convolution\")\n  ads_news <- ads_news[!is.na(ads_news)]\n  ads_news <-  tail(ads_news, 1)\n  \n  newdata = data.frame(ads_fb = ads_fb, ads_youtube = ads_youtube, ads_news = ads_news, time = nuevo_time)\n  \n # estimamos con el modelo  \n  ventas_simuladas <- posterior_predict(modelo, newdata = newdata, ndraws = 500)\n \n  return(as.numeric(ventas_simuladas))\n}\n```\n:::\n\n\nLa probamos, incrementando en 1% la media de gasto  en cada canal que se ha visto en los datos de entrenamiento\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfacebook_new <-  mean(train$facebook) * 1.01\nyoutube_new <-  mean(train$youtube) * 1.01\nnews_new <-  mean(train$newspaper) * 1.01\n\n# Obtenemos la posteriori, teniendo en cuenta el adstock.\n\nsimulaciones <- simular_ventas_siguiente_periodo(facebook_new, youtube_new, news_new,  mod_splines)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsimulaciones %>% \n  as_tibble() %>% \n  ggplot(aes(x = exp(value))) +\n  geom_density() +\n  labs(title = \"Distribución de ventas esperadas\",\n       x = \"Ventas esperadas\",\n       y = \"Densidad\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-17-1.png){width=80%}\n:::\n:::\n\n\n### Una observación\n\nAl haber modelado el log de las ventas, la interpretación de los parámetros ya no es tan directa, pero podemos utilizar\nla función de simular ventas para hacer análisis de sensibilidad y contestar a qué pasa si varío las cosas de \ntal o cual manera. Esto es relativamente simple y puede aportar más valor que incluso la optimización, puesto que permite\nal usuario jugar con cuánto invertir en cada medio. Y en cierta forma sentirse realizado, aún cuando , como he advertido\nla relación causal esté sólo en su imaginación.\n\n\nDejamos los valores de inversión en facebook y news como la media vista en train y variamos la inversión en youtube.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nfacebook_mean = mean(train$facebook)\nnews_mean = mean(train$newspaper)\nyoutube_grid = seq(0, 400, length.out = 100)\n\nsimular_ventas_grid_youtube  <-  partial(simular_ventas_siguiente_periodo, facebook_new = facebook_mean, \nnews_new = news_mean, modelo = mod_splines)\nsim_grid_youtube <- youtube_grid  %>% map(simular_ventas_grid_youtube)\n\n# sumarizo las 100 posteriors por su mediana, por simpliciddad\n\nsim_grid_youtube_medians <- sim_grid_youtube  %>% map_dbl(median)\nsim_grid_youtube_means <- sim_grid_youtube  %>% map_dbl(mean)\nsim_grid_youtube_low <- sim_grid_youtube  %>% map_dbl(quantile, probs = 0.1)\nsim_grid_youtube_high <- sim_grid_youtube  %>% map_dbl(quantile, probs = 0.9)\n\nto_plot <- data.frame(youtube_inv = youtube_grid,\n                      ventas_median = exp(sim_grid_youtube_medians),\n                      ventas_low = exp(sim_grid_youtube_low),\n                      ventas_high = exp(sim_grid_youtube_high)\n                      )\n\nto_plot %>%\n  ggplot(aes(x = youtube_inv, y = ventas_median)) +\n  geom_point() +\n  geom_line() +\n  geom_ribbon(aes(ymin = ventas_low, ymax = ventas_high), alpha = 0.3) +\n  labs(\n       title = \"Cambio en ventas estimadas variando inversión en youtube\",\n       subtitle = \"Intervalos de credibilidad al 80%\"\n  )\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-18-1.png){width=80%}\n:::\n:::\n\n\n\n__Volvamos a la optimización__\n\n\nBien, pues ahora tenemos que construir la función a optimizar. Vamos a poner un presupuesto máximo de 450. \nY en la función a optimizar añadimos una penalización alta si la suma de los valores a optimizar supera este\npresupuesto. Aparte de este valor máximo también vamos a poner umbrales al valor mínimo y máximo del gasto en cada \ncanal. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nfuncion_objetivo <- function(facebook_new, youtube_new, news_new, modelo, presupuesto = 450) {\n  \n  penalizacion <- 0\n  \n  # Calcular la suma de los parámetros\n  suma_par <- sum(facebook_new, youtube_new, news_new)\n  \n  # Si la suma de los parámetros supera el presupuesto, añadir una penalización\n  if (suma_par > presupuesto) {\n    penalizacion <- 1e6 * (suma_par - presupuesto)^2\n  }\n  \n  ventas_simuladas <- simular_ventas_siguiente_periodo(facebook_new, youtube_new, news_new, modelo)\n  \n  # para optimizar necesitamos un solo valor, no las posteriores, sumarizamos con la media, pero podría \n  # ser otra cosa\n\n  ventas_esperadas <- mean(ventas_simuladas)\n  return(list(Score = -ventas_esperadas + penalizacion)) # Negativo para maximizar\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\n(prueba <- funcion_objetivo(facebook_new, youtube_new, news_new, modelo= mod_splines))\n#> $Score\n#> [1] -2.700242\n\n# las ventas serán el exp de menos el resultado\nexp(-prueba$Score)\n#> [1] 14.88333\n```\n:::\n\n\n\n\n\nAhora sería optimizar esa función con algún solver, yo voy a usar una optimización bayesiana y en \nprocesamiento paralelo\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlibrary(ParBayesianOptimization)\nlibrary(doParallel)\nlibrary(foreach)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Configurar el clúster para usar 5 núcleos\n\nnum_cores <- 5\ncl <- makeCluster(num_cores)\nregisterDoParallel(cl)\n```\n:::\n\n\n\nLímites de gasto. ¿Por qué pongo estos? Porque quiero, y he añadido que el mínimo en news sea 13 \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbounds <- list(\n    facebook_new = c(0, 224)\n  , youtube_new = c(0, 400)\n  , news_new = c(13, 34)\n)\n```\n:::\n\n\nMe creo una función parcial para rellenar por defecto algunos parámetros de la función como el modelo a usar\ny el presupuesto. Esto lo hago porque muchas veces los _solvers_ requieren quela función objetivo tenga sólo\nun argumento.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Aquí pongo un presupuesto máximo de 500\nf_objetivo_partial  <- partial(funcion_objetivo, modelo = mod_splines, presupuesto = 500)\nf_objetivo_partial\n#> <partialised>\n#> function (...) \n#> funcion_objetivo(modelo = mod_splines, presupuesto = 500, ...)\n```\n:::\n\n \n\nPara hacer la computación en paralelo  hay que copiar las librerías y los datos y funciones en cada \"conexión\", al menos\npara usar esta librería\n\n\n\nLas librerías hay que enviarlas a cada proceso de R con `clusterEvalQ` y las funciones y datos usados \ncon `clusterExport`.  Seguro que hay una forma de no mandar una copia de los datos a cada proceso, \ny lo he hecho alguna vez, pero no recuerdo como. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nclusterEvalQ(cl, {\n  library(ParBayesianOptimization)\n  library(tidyverse)\n  library(brms)\n  })\n#> [[1]]\n#>  [1] \"brms\"                    \"Rcpp\"                   \n#>  [3] \"lubridate\"               \"forcats\"                \n#>  [5] \"stringr\"                 \"dplyr\"                  \n#>  [7] \"purrr\"                   \"readr\"                  \n#>  [9] \"tidyr\"                   \"tibble\"                 \n#> [11] \"ggplot2\"                 \"tidyverse\"              \n#> [13] \"ParBayesianOptimization\" \"stats\"                  \n#> [15] \"graphics\"                \"grDevices\"              \n#> [17] \"utils\"                   \"datasets\"               \n#> [19] \"methods\"                 \"base\"                   \n#> \n#> [[2]]\n#>  [1] \"brms\"                    \"Rcpp\"                   \n#>  [3] \"lubridate\"               \"forcats\"                \n#>  [5] \"stringr\"                 \"dplyr\"                  \n#>  [7] \"purrr\"                   \"readr\"                  \n#>  [9] \"tidyr\"                   \"tibble\"                 \n#> [11] \"ggplot2\"                 \"tidyverse\"              \n#> [13] \"ParBayesianOptimization\" \"stats\"                  \n#> [15] \"graphics\"                \"grDevices\"              \n#> [17] \"utils\"                   \"datasets\"               \n#> [19] \"methods\"                 \"base\"                   \n#> \n#> [[3]]\n#>  [1] \"brms\"                    \"Rcpp\"                   \n#>  [3] \"lubridate\"               \"forcats\"                \n#>  [5] \"stringr\"                 \"dplyr\"                  \n#>  [7] \"purrr\"                   \"readr\"                  \n#>  [9] \"tidyr\"                   \"tibble\"                 \n#> [11] \"ggplot2\"                 \"tidyverse\"              \n#> [13] \"ParBayesianOptimization\" \"stats\"                  \n#> [15] \"graphics\"                \"grDevices\"              \n#> [17] \"utils\"                   \"datasets\"               \n#> [19] \"methods\"                 \"base\"                   \n#> \n#> [[4]]\n#>  [1] \"brms\"                    \"Rcpp\"                   \n#>  [3] \"lubridate\"               \"forcats\"                \n#>  [5] \"stringr\"                 \"dplyr\"                  \n#>  [7] \"purrr\"                   \"readr\"                  \n#>  [9] \"tidyr\"                   \"tibble\"                 \n#> [11] \"ggplot2\"                 \"tidyverse\"              \n#> [13] \"ParBayesianOptimization\" \"stats\"                  \n#> [15] \"graphics\"                \"grDevices\"              \n#> [17] \"utils\"                   \"datasets\"               \n#> [19] \"methods\"                 \"base\"                   \n#> \n#> [[5]]\n#>  [1] \"brms\"                    \"Rcpp\"                   \n#>  [3] \"lubridate\"               \"forcats\"                \n#>  [5] \"stringr\"                 \"dplyr\"                  \n#>  [7] \"purrr\"                   \"readr\"                  \n#>  [9] \"tidyr\"                   \"tibble\"                 \n#> [11] \"ggplot2\"                 \"tidyverse\"              \n#> [13] \"ParBayesianOptimization\" \"stats\"                  \n#> [15] \"graphics\"                \"grDevices\"              \n#> [17] \"utils\"                   \"datasets\"               \n#> [19] \"methods\"                 \"base\"\n  \n  \n  \nclusterExport(cl,c('simular_ventas_siguiente_periodo', 'train','bounds', 'funcion_objetivo','f_objetivo_partial', 'mod_splines', \n                   'set_memory', 'get_adstock_fb', 'get_adstock_youtube', \n                   'get_adstock_news'))\n```\n:::\n\n\n\nY ya podríamos optimizar. He puesto un máximo de 200 iteraciones y un tiempo máximo de 300 segundos. Para \nver más opciones consultar la ayuda de la función, que hace optimización usando procesos gaussianos\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbayes_opt <- bayesOpt(\n  FUN = f_objetivo_partial,\n  bounds = bounds,\n  initPoints = 10,\n  acq = \"ei\",\n  iters.n = 200,\n  iters.k = 10,\n  kappa = 2,\n  eps = 0.0, \n  parallel = TRUE,\n  otherHalting = list(timeLimit = 300)\n)\n```\n:::\n\n\n\n\nY los resultados los podemos ver ordenando por `Score`,  el mejor resultado será el que tenga menor `Scores` \n(estamos minimizando )\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mostrar los resultados\nprint(bayes_opt)\n#> Class: bayesOpt\n#> \n#>                     Epochs: 15\n#>                 Iterations: 168\n#>        Average FUN Seconds: 0.51\n#>        Highest FUN Seconds: 0.73\n#> Final Expected Improvement: 0.5197976\n#>                 GP Updated: FALSE\n#>                Stop Status: Stopped Early. See $stopStatus\nbayes_opt$scoreSummary %>% \n  arrange(Score) \n#>      Epoch Iteration facebook_new youtube_new news_new gpUtility acqOptimum\n#>      <num>     <int>        <num>       <num>    <num>     <num>     <lgcl>\n#>   1:     0         7     202.9549    176.0488 15.10575        NA      FALSE\n#>   2:     0         8     168.9526    243.1588 21.22847        NA      FALSE\n#>   3:     0         6     132.1047    346.3188 18.72091        NA      FALSE\n#>   4:     0         9     101.5759    363.3885 24.58786        NA      FALSE\n#>   5:     0         3     198.6377     53.1898 14.67486        NA      FALSE\n#>  ---                                                                       \n#> 164:    15       158     217.7454    400.0000 32.30452 0.4272278      FALSE\n#> 165:    15       154     224.0000    392.5891 34.00000 0.3830230      FALSE\n#> 166:    15       157     224.0000    400.0000 32.03328 0.4884866      FALSE\n#> 167:    15       152     224.0000    400.0000 32.30285 0.4930121      FALSE\n#> 168:    15       151     224.0000    400.0000 34.00000 0.5197976       TRUE\n#>      inBounds Elapsed         Score errorMessage\n#>        <lgcl>   <num>         <num>       <lgcl>\n#>   1:     TRUE   0.450 -4.285181e+00           NA\n#>   2:     TRUE   0.578 -4.171461e+00           NA\n#>   3:     TRUE   0.405 -4.157552e+00           NA\n#>   4:     TRUE   0.647 -3.955043e+00           NA\n#>   5:     TRUE   0.675 -3.877969e+00           NA\n#>  ---                                            \n#> 164:     TRUE   0.576  2.251499e+10           NA\n#> 165:     TRUE   0.410  2.267709e+10           NA\n#> 166:     TRUE   0.395  2.434638e+10           NA\n#> 167:     TRUE   0.529  2.443058e+10           NA\n#> 168:     TRUE   0.545  2.496400e+10           NA\n```\n:::\n\n\nLos valores óptimos que ha encontrado de gasto en cada canal, cumpliendo las restricciones\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(result_bayes <- bayes_opt$scoreSummary %>% \n arrange(Score) %>% \n head(1) %>% \n select(facebook_new, youtube_new, news_new, Score) )\n#>    facebook_new youtube_new news_new     Score\n#>           <num>       <num>    <num>     <num>\n#> 1:     202.9549    176.0488 15.10575 -4.285181\n```\n:::\n\nComprobamos que no se ha pasdo del presupuesto\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(result_bayes, sum(facebook_new, youtube_new, news_new))\n#> [1] 394.1095\n```\n:::\n\n\n\nSimulamos las ventas que se esperan con esta optimización.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulaciones_optim <- simular_ventas_siguiente_periodo(result_bayes$facebook_new, result_bayes$youtube_new, result_bayes$news_new,\n                                                       modelo = mod_splines)\nexp(quantile(simulaciones_optim, c(0.2, 0.5, 0.8)))\n#>      20%      50%      80% \n#> 57.90940 73.23327 90.95111\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\n(p_optimizacion_mod_spline <- simulaciones_optim %>% \n  as_tibble() %>% \n  ggplot(aes(x = exp(value))) +\n  geom_density() +\n  labs(title = \"Distribución de ventas esperadas tras optimización\",\n       subtitle = \"Modelo simple con splines\",\n       x = \"Ventas esperadas\",\n       y = \"Densidad\") +\n  theme_minimal())\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-31-1.png){width=80%}\n:::\n:::\n\n\n\nY básicamente esta sería la idea principal de hacer un MMM.\n\n\n## Curvas de saturación y como modelarlas\n\nSin embargo también se considera el concepto de curva de saturación, \nque no es más que la asunción de que llega un momento en que por más que te gastes en publicidad, no vas a tener más ventas. \n\n\nPor ejemplo, para eso veamos si eso se ve en los datos \n\n\nCon los datos reales y gastos en youtube se intuye que  podría existir más allá del \ndominio observado de la variable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n(p_youtube_real <- datos %>% \n  group_by(ads_youtube) %>%\n  summarise(ventas_mean = mean(log(sales))) %>%\n  ggplot(aes(x = ads_youtube, y = ventas_mean)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_continuous(limits = c(0, 450)) +\n  scale_y_continuous(limits = c(0, 4)) +\n  labs(title = \"Curva real en los datos\",\n       x = \"Gasto en youtube\",\n       y = \"Ventas, en log\"))\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-32-1.png){width=80%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(p_facebook_real <- datos %>% \n  group_by(ads_fb) %>%\n  summarise(ventas_mean = mean(log(sales))) %>%\n  ggplot(aes(x = ads_fb, y = ventas_mean)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_continuous(limits = c(0, 100)) +\n  scale_y_continuous(limits = c(0, 4)) +\n  labs(title = \"Curva real en los datos\",\n       x = \"Gasto en facebook\",\n       y = \"Ventas\"))\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-33-1.png){width=80%}\n:::\n:::\n\n\n\n\n### Mas teoría marketiniana\n\nUna forma  de modelar la curva de saturación que tienen los de marketing es utilizar la saturación de \nHill, que viene de la bioquímica y ámbitos similares. Y bueno, tal y como dice Richard McElreath, \n\"ciencia antes que estadística\" y quién soy yo para refutar que se use lo de las curvas de saturación \nde Hill. \n\nPodemos considerar algo como  esto pero para cada canal\n$$ \n\\text{log(ventas)} = \\dfrac{\\beta \\cdot \\text{adstock}^{h}}{S^h + \\text{adstock}^h}\n$$\n\n\nPor lo que tendríamos para cada canal un parámetro $\\beta$ un parámetro $h$ y un parámetro $S$ . \n\nEsta función no es lineal en los parámetros, pero podemos definir el modelo generativo y ajustar a los\ndatos usando `MCMC` \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# como no me deja poner splines meto un coeficiente al tiempo. \n\nhill_adstock_formula <- bf(\n  log(sales) ~ bintercept + bfb * ads_fb ^ hfb  / (Sfb^hfb + ads_fb ^ hfb)  +\n    byoutube * ads_youtube ^ hyou / (Syoutube^hyou + ads_youtube ^ hyou) +\n    bnews * ads_news ^ hnews / (Snews^hnews + ads_news ^ hnews) + btime * time, \n  bfb + Sfb + byoutube + Syoutube + bnews + Snews + btime + hfb + hyou + hnews ~ 1,\n  nl = TRUE\n)\n# Define los priors para los parámetros del modelo\n\npriors <- c(\n  prior(normal(0, 10), nlpar = \"bintercept\"),\n  prior(normal(0, 10), nlpar = \"bfb\", lb = 0),\n  prior(normal(0, 10), nlpar = \"Sfb\"),\n  prior(normal(0, 10), nlpar = \"byoutube\", lb = 0 ),\n  prior(normal(0, 50), nlpar = \"Syoutube\"),\n  prior(normal(0, 50), nlpar = \"bnews\", lb = -0.1),\n  prior(normal(0, 50), nlpar = \"Snews\"),\n  prior(normal(0, 50), nlpar = \"btime\", lb = 0),\n  prior(lognormal(log(1), 0.5), nlpar = \"hfb\", lb = 0),\n  prior(lognormal(log(1), 0.5), nlpar = \"hyou\", lb = 0),\n  prior(lognormal(log(1), 0.5), nlpar = \"hnews\", lb = 0)\n)\n\n\n\n# Ajusta el modelo con brms pero con más iteraciones,\n\nmodelo_hill_adstock <- brm(\n  formula = hill_adstock_formula,\n  data = datos,\n  family = gaussian(),\n  prior = priors,\n  chains = 4, iter = 6000, warmup = 1000, \n  control = list( adapt_delta = 0.97),\n  backend = \"cmdstanr\",cores = 4, file = here::here(\"2024/06/mod_hill\") \n)\n```\n:::\n\n\n### Algunos chequeos del modelo \n\nComo antes, comparamos la distribucion de las ventas con las generadas por el modelo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Muestra un resumen del modelo\nsummary(modelo_hill_adstock)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: log(sales) ~ bfb * ads_fb^hfb/(Sfb^hfb + ads_fb^hfb) + byoutube * ads_youtube^hyou/(Syoutube^hyou + ads_youtube^hyou) + bnews * ads_news^hnews/(Snews^hnews + ads_news^hnews) + btime * time \n#>          bfb ~ 1\n#>          Sfb ~ 1\n#>          byoutube ~ 1\n#>          Syoutube ~ 1\n#>          bnews ~ 1\n#>          Snews ~ 1\n#>          btime ~ 1\n#>          hfb ~ 1\n#>          hyou ~ 1\n#>          hnews ~ 1\n#>    Data: datos (Number of observations: 200) \n#>   Draws: 4 chains, each with iter = 6000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 20000\n#> \n#> Regression Coefficients:\n#>                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> bfb_Intercept          0.69      0.09     0.54     0.88 1.00     2575     2820\n#> Sfb_Intercept         34.66      3.59    28.77    42.91 1.00     2990     2447\n#> byoutube_Intercept     3.20      0.13     2.99     3.49 1.00     1669     1572\n#> Syoutube_Intercept    37.05      3.28    32.19    45.05 1.00     1913     1567\n#> bnews_Intercept       -0.01      0.06    -0.10     0.14 1.00     2447     2365\n#> Snews_Intercept       38.65     31.64     1.38   114.80 1.00     2133     3496\n#> btime_Intercept        0.00      0.00     0.00     0.00 1.00     3306     2238\n#> hfb_Intercept          2.62      0.44     1.89     3.61 1.00     3007     4464\n#> hyou_Intercept         0.84      0.08     0.67     1.01 1.00     1719     1662\n#> hnews_Intercept        1.06      0.57     0.34     2.52 1.00     4290     4024\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.13      0.01     0.12     0.14 1.00     4955     6336\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\npp_check(modelo_hill_adstock, ndraws = 100)\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-35-1.png){width=80%}\n:::\n:::\n\n\nAunque tengo que leer a Carlos sobre el tema de Leave One Out, es una cosa que se utiliza bastante\nen validación de modelos bayesianos, debido al trabajo de Aki Vehtari, Gelman y Gabry en este \ncampo , ver [aquí](https://link.springer.com/article/10.1007/s11222-016-9696-4)\n\n\n\nEl Pareto k diagnostic nos indica que sólo hay problemas con un dato, así que tampoco nos vamos a preocupar mucho.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo(modelo_hill_adstock)\n#> \n#> Computed from 20000 by 200 log-likelihood matrix.\n#> \n#>          Estimate   SE\n#> elpd_loo    114.0 41.6\n#> p_loo        25.6 17.7\n#> looic      -228.0 83.3\n#> ------\n#> MCSE of elpd_loo is NA.\n#> MCSE and ESS estimates assume MCMC draws (r_eff in [0.1, 0.6]).\n#> \n#> Pareto k diagnostic values:\n#>                          Count Pct.    Min. ESS\n#> (-Inf, 0.7]   (good)     199   99.5%   1122    \n#>    (0.7, 1]   (bad)        0    0.0%   <NA>    \n#>    (1, Inf)   (very bad)   1    0.5%   <NA>    \n#> See help('pareto-k-diagnostic') for details.\n```\n:::\n\n\nVeamos que tal predice en test\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_test <-  posterior_predict(modelo_hill_adstock, newdata = test)\n\ntest$ventas_predichas_mod_hill  <- exp(apply(pred_test, 2, mean))\n\nyardstick::rmse(test,sales, ventas_predichas_mod_hill)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        1.42\nyardstick::mae(test,sales, ventas_predichas_mod_hill)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard        1.08\n```\n:::\n\n\nQue es mejor predicción que el modelo simple original\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rmse y mae con modelo simple\nyardstick::rmse(test,sales, ventas_predichas)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard        2.45\nyardstick::mae(test,sales, ventas_predichas)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 mae     standard        1.91\n```\n:::\n\n\n\nUsamos el modelo para obtener predicciones al global de los datos, train y test\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicciones <- posterior_predict(modelo_hill_adstock, newdata = datos, ndraws = 1000) \n\n# Calcular las predicciones medias\npredicciones_medias <- apply(predicciones, 2, mean)\n\n# Agregar las predicciones al dataframe de nuevos datos\ndatos$ventas_predichas <- exp(predicciones_medias)\n\n\n# Curva de saturación para publicidad\n\np_youtube_estimada <- datos %>% \n  group_by(ads_youtube) %>%\n  summarise(ventas_predichas_mean = median(ventas_predichas),\n            low = quantile(ventas_predichas, 0.05), \n            high = quantile(ventas_predichas, 0.95)) %>%\n  ggplot(aes(x = ads_youtube, y = ventas_predichas_mean)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = low, ymax = high), alpha = 0.3) +\n  labs(title = \"Curva de Saturación para Youtube\",\n       x = \"Gasto en Youtube\",\n       y = \"Ventas Predichas\")\np_youtube_estimada\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-39-1.png){width=80%}\n:::\n:::\n\n\nY esta curva tiene muchos picos, porque en el conjunto de datos original no están todas las posibles combinaciones\nentre las diferentes inversiones en los canales. \n\n### Curva de saturación usando un conjunto de datos sintético\n\nPodríamos obtener las curvas de saturación marginalizando la inversión en un canal, teniendo en cuenta muchas\nposibles combinaciones de inversiones en los otros canales. Esta sería una forma de aproximarse a la curva de \nsaturación, aunque el volumen de ventas que sale estará sobreestimado, puesto que es un dataset sintético, dónde \nestán sobre-representadas combinaciones de inversiones con valores mucho más altos que los observados en los datos. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Crear un rango de valores para publicidad y promociones\nads_fb <- seq(0, 400, length.out = 30)\nads_youtube <- seq(0, 400, length.out = 30)\nads_news <- seq(0, 400, length.out = 30)\ntime <-  rep(201, 30)\n\n# Crear un nuevo dataframe para predicciones\nnuevos_datos <- expand.grid(\n  ads_fb = ads_fb,\n  ads_youtube = ads_youtube,\n  ads_news = ads_news,\n  time = time\n) \nhead(nuevos_datos)\n#>     ads_fb ads_youtube ads_news time\n#> 1  0.00000           0        0  201\n#> 2 13.79310           0        0  201\n#> 3 27.58621           0        0  201\n#> 4 41.37931           0        0  201\n#> 5 55.17241           0        0  201\n#> 6 68.96552           0        0  201\n# son demasiados datos , tomo una muestra, que no es pecado\ndim(nuevos_datos)\n#> [1] 810000      4\n```\n:::\n\n\nCombinar todo con todo nos da más de 800 mil filas, lo cual es excesivo. Así que, tal y como se llama \neste blog, muestrear no es pecado, tomamos una muestra de estos datos sintéticos que también nos vale.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nnuevos_datos <-  nuevos_datos %>% \n  sample_n(4000)\n\npredicciones <- posterior_predict(modelo_hill_adstock, newdata = nuevos_datos, ndraws = 1000) \n\n# Calcular las predicciones medias\n\n# Agregar las predicciones, en log de ventas,  al dataframe de nuevos datos\nnuevos_datos$ventas_predichas <- apply(predicciones, 2, mean)\n\n# Curva de saturación para publicidad\n\np_youtube_estimada_bis <- nuevos_datos %>% \n  group_by(ads_youtube) %>%\n  summarise(ventas_predichas_mean = median(ventas_predichas),\n            low = quantile(ventas_predichas, 0.05), \n            high = quantile(ventas_predichas, 0.95)) %>%\n  ggplot(aes(x = ads_youtube, y = ventas_predichas_mean)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = low, ymax = high), alpha = 0.2)+ \n  labs(title = \"Curva de Saturación para Youtube\",\n       x = \"Gasto en Youtube\",\n       y = \"Ventas Predichas\")\nlibrary(patchwork)\np_youtube_real + p_youtube_estimada_bis\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-41-1.png){width=80%}\n:::\n:::\n\n\n## Última optimización\n\nNo faltaría más que optimizar la inversión publicitaria que nos saldría con este modelo, y manteniendo mismo presupuesto\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nf_objetivo_partial_nueva  <- partial(funcion_objetivo, modelo = modelo_hill_adstock, presupuesto = 500)\nf_objetivo_partial_nueva\n#> <partialised>\n#> function (...) \n#> funcion_objetivo(modelo = modelo_hill_adstock, presupuesto = 500, \n#>     ...)\n\n# exporto el modelo y la funcion objetivo a los procesos en paralelo\n\nclusterExport(cl,c('f_objetivo_partial_nueva', 'modelo_hill_adstock'))\n```\n:::\n\n\nOptimizamos\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nbayes_opt_nueva <- bayesOpt(\n  FUN = f_objetivo_partial_nueva,\n  bounds = bounds,\n  initPoints = 10,\n  acq = \"ei\",\n  iters.n = 200,\n  iters.k = 10,\n  kappa = 2,\n  eps = 0.0, \n  parallel = TRUE,\n  otherHalting = list(timeLimit = 300)\n)\n```\n:::\n\n\nY al igual que antes obtenemos los resultados de la optimización\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(bayes_opt_nueva)\n#> Class: bayesOpt\n#> \n#>                     Epochs: 14\n#>                 Iterations: 166\n#>        Average FUN Seconds: 1\n#>        Highest FUN Seconds: 1.18\n#> Final Expected Improvement: 1.661111e-218\n#>                 GP Updated: TRUE\n#>                Stop Status: Stopped Early. See $stopStatus\nbayes_opt_nueva$scoreSummary %>% \n  arrange(Score) \n#>      Epoch Iteration facebook_new youtube_new news_new     gpUtility acqOptimum\n#>      <num>     <int>        <num>       <num>    <num>         <num>     <lgcl>\n#>   1:     0        10     99.65962    363.3601 22.38239            NA      FALSE\n#>   2:    13       155    186.81997    276.0443 30.76712  0.000000e+00      FALSE\n#>   3:    13       154    199.64561    263.3377 31.71919 3.200023e-316      FALSE\n#>   4:    13       156    210.76776    251.6358 34.00000 4.741560e-231      FALSE\n#>   5:    13       147    202.50964    249.9072 32.76780 2.121759e-281       TRUE\n#>  ---                                                                           \n#> 162:     3        38    218.94138    395.4012 32.60496  3.226181e-01      FALSE\n#> 163:     3        39    215.57359    400.0000 31.55908  3.330606e-01      FALSE\n#> 164:     3        36    215.06796    398.3064 34.00000  3.369026e-01      FALSE\n#> 165:     3        34    224.00000    400.0000 33.15132  4.906921e-01      FALSE\n#> 166:     3        31    224.00000    400.0000 34.00000  5.049163e-01       TRUE\n#>      inBounds Elapsed         Score errorMessage\n#>        <lgcl>   <num>         <num>       <lgcl>\n#>   1:     TRUE   1.011 -3.446360e+00           NA\n#>   2:     TRUE   0.928 -3.393987e+00           NA\n#>   3:     TRUE   0.952 -3.383473e+00           NA\n#>   4:     TRUE   0.956 -3.373160e+00           NA\n#>   5:     TRUE   1.036 -3.360140e+00           NA\n#>  ---                                            \n#> 162:     TRUE   0.930  2.159359e+10           NA\n#> 163:     TRUE   0.932  2.164802e+10           NA\n#> 164:     TRUE   0.984  2.171920e+10           NA\n#> 165:     TRUE   1.006  2.469654e+10           NA\n#> 166:     TRUE   0.948  2.496400e+10           NA\n```\n:::\n\n\nComprobamos que se cumple la restricción de presupuesto\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(result_bayes_nueva <- bayes_opt_nueva$scoreSummary %>% \n arrange(Score) %>% \n head(1) %>% \n select(facebook_new, youtube_new, news_new) )\n#>    facebook_new youtube_new news_new\n#>           <num>       <num>    <num>\n#> 1:     99.65962    363.3601 22.38239\n```\n:::\n\n\nVemos que la optimización es diferente, el modelo simple no tenía modelada la saturación\nda mucho más inversión en facebook que el modelo con saturación.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult_bayes %>% select(facebook_new, youtube_new, news_new)\n#>    facebook_new youtube_new news_new\n#>           <num>       <num>    <num>\n#> 1:     202.9549    176.0488 15.10575\n```\n:::\n\n\nComprobamos que no se ha pasado del presupuesto\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nwith(result_bayes_nueva, sum(facebook_new, youtube_new, news_new))\n#> [1] 485.4021\n```\n:::\n\n\n\nSimulamos las ventas que se esperan con esta optimización.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nsimulaciones_optim_nueva <- simular_ventas_siguiente_periodo(result_bayes_nueva$facebook_new, result_bayes_nueva$youtube_new, result_bayes$news_new,\n                                                       modelo = modelo_hill_adstock)\nexp(quantile(simulaciones_optim_nueva, c(0.2, 0.5, 0.8)))\n#>      20%      50%      80% \n#> 27.91540 31.31265 34.85574\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n\n(p_optim_hill <- simulaciones_optim_nueva %>% \n  as_tibble() %>% \n  ggplot(aes(x = exp(value))) +\n  geom_density() +\n  labs(title = \"Distribución de ventas esperadas tras optimización\",\n       subtitle = \"Modelo con saturación de Hill\",\n       x = \"Ventas esperadas\",\n       y = \"Densidad\") +\n  theme_minimal())\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-49-1.png){width=80%}\n:::\n:::\n\n\nA nivel de ventas estimadas, me resulta más creíble la que devuelve el modelo con saturación. El modelo más \nsimple extrapola demasiado. Al fin y al cabo modelar la saturación es \"equivalente\" a restringir los coeficientes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\np_optimizacion_mod_spline  + p_optim_hill\n```\n\n::: {.cell-output-display}\n![](mmm_estilo_compadre_files/figure-html/unnamed-chunk-50-1.png){width=80%}\n:::\n:::\n",
    "supporting": [
      "mmm_estilo_compadre_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "f1878a1ae26ac352b7a8e0bda0107750",
  "result": {
    "markdown": "---\ntitle: Codificación parcial y python\ndate: '2019-07-15'\ncategories:\n  - 2019    \n  - ciencia de datos\n  - estadística\n  - R\n  - python\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\nO como se conoce en estos tiempos modernos __one hot encoding__. En realidad se trata simplemente de cómo codificar una variable categórica en un conjunto de números que un algoritmo pueda utilizar. \n\nYa hablé de esto mismo en el post **[codificación de variables categóricas I](https://muestrear-no-es-pecado.netlify.com/2019/02/27/codificacion-de-variables-categoricas-i/)**\n\nBásicamente, la codificación parcial lo que hace es crearse tantas variables indicadoras como niveles tengo en mi variable menos 1. \n\nEjemplo. \nConstruimos un conjunto de datos simple, con 3 variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(155)\n\nx1 <- rnorm(n = 100, mean = 4, sd = 1.5 )\nx2_cat <- factor(rep(c(\"a\",\"b\",\"c\",\"d\"),  25 ))\ny <- numeric(length = 100)\n\n# Construimos artificialmente relación entre x e y\ny <- 2 +  4 * x1 + rnorm(25, 0, 1)\n\n# cambiamos \"el intercept\" según la variable categórica\n\ny[x2_cat == \"a\"] <- y[x2_cat == \"a\"] + 8\ny[x2_cat == \"b\"] <- y[x2_cat == \"b\"] - 5\ny[x2_cat == \"c\"] <- y[x2_cat == \"c\"] + 3\ny[x2_cat == \"d\"] <- y[x2_cat == \"d\"] - 3\n\ndat <- data.frame(y, x1, x2_cat)\nhead(dat)\n#>          y       x1 x2_cat\n#> 1 31.00555 5.200100      a\n#> 2 19.13571 5.061407      b\n#> 3 20.49049 3.888438      c\n#> 4 17.93157 4.978832      d\n#> 5 25.23501 3.989047      a\n#> 6 21.86234 6.272138      b\n```\n:::\n\n\n\nEn R al definir x2_cat como un factor él ya sabe que para ciertos métodos (por ejemplo una regresión) hay que codificar esa variable y por defecto utiliza la codificación parcial. Con la función *contrasts* vemos como lo hace.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(dat$x2_cat)\n#>   b c d\n#> a 0 0 0\n#> b 1 0 0\n#> c 0 1 0\n#> d 0 0 1\n```\n:::\n\n\nY en las columnas tenemos las 3 variables indicadoras que ha construido. ¿Por qué 3? pues muy fácil, puesto que la \"a\" se puede codificar con el valor 0 en las tres variables indicadoras.\nEsto que parece una obviedad evita problemas de colinealidad en los algoritmos de regresión por ejemplo.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <-  lm(y ~ x1 + x2_cat, data = dat)\nsummary(fit1)\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2_cat, data = dat)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.55549 -0.67355 -0.00943  0.59814  1.99480 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.12924    0.28393   35.67   <2e-16 ***\n#> x1            3.94536    0.06034   65.39   <2e-16 ***\n#> x2_catb     -12.95740    0.26148  -49.55   <2e-16 ***\n#> x2_catc      -4.97712    0.25845  -19.26   <2e-16 ***\n#> x2_catd     -10.98359    0.25785  -42.60   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9094 on 95 degrees of freedom\n#> Multiple R-squared:  0.9855,\tAdjusted R-squared:  0.9849 \n#> F-statistic:  1616 on 4 and 95 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n¿Qué hubiera pasado si hubiéramos tratado con 4 variables indicadoras?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat2 <- dat\ndat2$ind1 <- ifelse(dat$x2_cat == \"a\", 1, 0)\ndat2$ind2 <- ifelse(dat$x2_cat == \"b\", 1, 0)\ndat2$ind3 <- ifelse(dat$x2_cat == \"c\", 1, 0)\ndat2$ind4 <- ifelse(dat$x2_cat == \"d\", 1, 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dat2[dat2$x2_cat==\"d\", ],3)\n#>           y       x1 x2_cat ind1 ind2 ind3 ind4\n#> 4  17.93157 4.978832      d    0    0    0    1\n#> 8   9.30838 2.736958      d    0    0    0    1\n#> 12 12.31765 2.943479      d    0    0    0    1\n```\n:::\n\n\n\nSi metemos ahora esas variables en el modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <-  lm(y ~ x1 +  ind2 + ind3 + ind4 + ind1, data = dat2)\nsummary(fit2)\n#> \n#> Call:\n#> lm(formula = y ~ x1 + ind2 + ind3 + ind4 + ind1, data = dat2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.55549 -0.67355 -0.00943  0.59814  1.99480 \n#> \n#> Coefficients: (1 not defined because of singularities)\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  10.12924    0.28393   35.67   <2e-16 ***\n#> x1            3.94536    0.06034   65.39   <2e-16 ***\n#> ind2        -12.95740    0.26148  -49.55   <2e-16 ***\n#> ind3         -4.97712    0.25845  -19.26   <2e-16 ***\n#> ind4        -10.98359    0.25785  -42.60   <2e-16 ***\n#> ind1               NA         NA      NA       NA    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9094 on 95 degrees of freedom\n#> Multiple R-squared:  0.9855,\tAdjusted R-squared:  0.9849 \n#> F-statistic:  1616 on 4 and 95 DF,  p-value: < 2.2e-16\n```\n:::\n\n\nY vemos que  como hay colinealidad R no estima el coeficiente de una de las variables indicadoras y hasta nos avisa con el mensaje ` Coefficients: (1 not defined because of singularities)`\n\nPues la verdad es que mola que R sepa como tratar las categóricas si las has definido como `factor` pero también hace que la gente se olvide de que lo que en realidad hace es la codificación parcial. \n\nHablando de esto con un colega salió a colación  que en python hay que explicitar la codificación y que quizá eso sea bueno porque así se sabe lo que se está haciendo y no hay lugar a dudas. Hasta aquí todo correcto, salvo que leyendo la documentación de [pandas get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) resulta que por defecto construye tantas variables indicadoras como categorías y sólo tiene como opcional lo de quitar la primera con el parámetro `drop_first`, total me dije, no pasa nada, veamos como lo hace `scikit learn` y nada, resulta que por defecto también deja todas [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). \n\nReflexionando me dije, bueno, pues entonces cuando haga una regresión lineal con sklearn si uso las opciones por defecto de codificar las categóricas pues me debe saltar lo mismo que en R, es decir que hay un coeficiente que no puede estimar, pero resulta que sklearn hace un pelín de trampa y no salta el error, y no salta porque en sklearn la regresión lineal no ajusta una regresión lineal clásica, sino que por defecto y sin que tú lo pidas te hace una regresión regularizada y entonces no salta ese problema. \n\nPues la verdad , ¿qué puedo decir? no me hace gracia que por defecto no me quite la variable indicadora que sobra ni que haga regresión con regularización sin yo decirle nada.\n\n\nEn fin, veamos el ejemplo con python, aprovecho que escribo en un rmarkdown y puedo **[pasar objetos de R a python](https://rstudio.github.io/reticulate/articles/r_markdown.html#python-chunks)** entre chunks sin muchos problemas.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndat_py = r.dat\ndat_py.describe()\n#>                 y          x1\n#> count  100.000000  100.000000\n#> mean    18.633855    3.988010\n#> std      7.402175    1.540500\n#> min     -1.163152    0.315987\n#> 25%     14.283915    2.815231\n#> 50%     19.325282    3.885684\n#> 75%     22.438970    5.062777\n#> max     43.075931    8.304381\ndat_py.x2_cat.value_counts()\n#> a    25\n#> b    25\n#> c    25\n#> d    25\n#> Name: x2_cat, dtype: int64\n```\n:::\n\n\nconvertimos a dummies con pandas por ejemplo\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndat_py = pd.get_dummies(data=dat_py)\nprint(dat_py.head())\n#>            y        x1  x2_cat_a  x2_cat_b  x2_cat_c  x2_cat_d\n#> 0  31.005546  5.200100         1         0         0         0\n#> 1  19.135715  5.061407         0         1         0         0\n#> 2  20.490494  3.888438         0         0         1         0\n#> 3  17.931571  4.978832         0         0         0         1\n#> 4  25.235006  3.989047         1         0         0         0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nx_variables = ['x1', 'x2_cat_a', 'x2_cat_b','x2_cat_c','x2_cat_d']\n# Selecciono y convierto a numpy array\nX = dat_py[x_variables].values  \ny = dat_py['y'].values\nX[0:3]\n#> array([[5.20010037, 1.        , 0.        , 0.        , 0.        ],\n#>        [5.06140731, 0.        , 1.        , 0.        , 0.        ],\n#>        [3.88843797, 0.        , 0.        , 1.        , 0.        ]])\ny[0:3]\n#> array([31.00554596, 19.1357146 , 20.49049385])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\nlm = LinearRegression()\nfit_python = lm.fit(X,y)\nprint('Intercept: ',fit_python.intercept_)\n#> Intercept:  2.899716060541241\nprint('Coef: ',fit_python.coef_)\n#> Coef:  [ 3.94536095  7.22952708 -5.72787734  2.25241099 -3.75406074]\n```\n:::\n\n\nY vemos que si estima todos los coeficientes cuando no debería haber podido, esto tiene que ver como he dicho antes con que `LinearRegression` de sklearn no es la regresión lineal al uso sino que mete regularización.\n\nOtro día veremos la librería `statmodels` de python cuya salida nos da una información más rica de los modelos y bastante parecida a lo que estamos acostumbrados con R.\n\n**Nota:** Leyendo la docu de `LinearRegression` en ningún sitio dice que use regularización así que no alcanzo a entender por qué ha podido estimar todos los coeficientes. A ver si alguno de mis amigos *pythonisos* me lo aclara. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
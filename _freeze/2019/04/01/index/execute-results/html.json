{
  "hash": "50c8731c80010b8eb13ba77087bc2b94",
  "result": {
    "markdown": "---\ntitle: Burbuja o no burbuja, esa es la cuestión\ndate: '2019-04-01'\ncategories:\n  - 2019    \n  - estadística\n  - tiempos modernos\n  - R\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\nLlevo un tiempo que me llegan noticias tales como \"Con el big data predecimos cuál va a ser tu próxima tienda  y cuánto te vas a gastar\" o \"predecimos los rebotes por partido de un jugador con un margen de error de un rebote cada 6 partidos\" y cosas aún más peregrinas.\n\nLa verdad es que entre la gente más o menos seria que nos dedicamos a esto creo que está bastante claro que nuestra labor es reducir (y medir) la incertidumbre de algunos procesos de toma de decisiones, lean [esto](https://www.datanalytics.com/2019/03/18/las-decisiones-son-lo-primario-la-estadistica-es-subsidiaria/) si quieren saber más en qué debería consistir nuestro trabajo. \n\n¡No hacemos magia, no tenemos ingestado todo Internet, no sabemos si te vas a tomar esta tarde una caña con bocata de calamares o 2 vinos !\n\nOtro amigo mío dice que el \"big data son los padres\",  y no le falta razón, sobre todo por el problema de gestión de expectativas, entre todos estamos vendiendo que con la Inteligencia Artificial o el deep learning vamos a hacer maravillas. En realidad todos suspiramos por unos datos limpios y por unos modelos sencillos, pero útiles.\n\nY otro tema es el de la cantidad de gente que quiere entrar en el sector de la analítica pidiendo 50k con 1 año de experiencia, cuándo no saben ni interpretar un coeficiente de una regresión lineal. Mejor me callo que me enervo. \n\nPero bueno, veamos qué tal ha ido la evolución de búsquedas en Google de ciertos términos relacionados con estos temas.\n\nVamos a usar la librería gtrendsR que ataca a la API de googleTrends\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gtrendsR)\nlibrary(tidyverse)\nlibrary(patchwork)\n```\n:::\n\n\n\nA petición de lector del blog, compañero de trabajo y sin embargo amigo pongo las búsquedas desde más atrás, pero sólo para las globales \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfecha_inicio <- '2009-01-01'\nintervalo <- paste(fecha_inicio, '2019-04-01', sep = \" \")\n\nhype_words_all <- gtrends(c('big data','machine learning','artificial intelligence','deep learning'), time = intervalo)\n\n\n\n# hype_words_spain <- gtrends(c(\"big data\",\"machine learning\",\"artificial intelligence\",\"deep learning\"), geo=\"ES\", time = intervalo)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninterest_over_time_all <-  hype_words_all$interest_over_time \ninterest_over_time_all <- interest_over_time_all %>% filter(hits >0 & !is.na(hits))\n\n# \n# interest_over_time_spain <-  hype_words_spain$interest_over_time \n# interest_over_time_spain <- interest_over_time_spain %>% filter(hits >0 & !is.na(hits))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(interest_over_time_all, aes(x = date, y = hits, color=keyword)) +\n  geom_point(size = rel(0.5)) +\n  geom_smooth(span = 0.8) +\n  labs(title=\"World: Interest last ten years\", y = \"relative interest\")\n\np1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=80%}\n:::\n\n```{.r .cell-code}\n# p2 <- ggplot(interest_over_time_spain, aes(x = date, y = hits, color=keyword)) +\n#   geom_point(size = rel(0.5)) +\n#   geom_smooth() +\n#   labs(title=\"Spain: Interest last five years\", y = \"relative interest\")\n\n# p1 + p2  + plot_layout(ncol = 1)\n```\n:::\n\n\nPues ¿qué les parece? ¿Vamos camino de un burbuja o ya estamos en ella?\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
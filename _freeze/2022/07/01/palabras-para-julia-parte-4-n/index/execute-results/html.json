{
  "hash": "18da4f7196b04c5c19d307845c5747e8",
  "result": {
    "markdown": "---\ntitle: Palabras para Julia (Parte 4 /n). Predicción con Turing\nauthor: jlcr\ndate: '2022-07-01'\nslug: palabras-para-julia-parte-4-n\ncategories:\n  - Julia\n  - análisis bayesiano\n  - 2022\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\nEn [Palabras para Julia parte 3](https://muestrear-no-es-pecado.netlify.app/2022/03/20/palabras-para-julia-parte-3-n/) hablaba de modelos bayesianos con [Turing.jl](https://turing.ml/stable/), y me quedé con una espinita clavada, que era la de poder predecir de forma relativamente fácil con Turing, o incluso guardar de alguna forma la \"posterior samples\" y poder usar mi modelo en otra sesión de Julia. \n\nEmpiezo una serie de entradas cuyo objetivo es ver si puedo llegar a la lógica para poner \"en producción\" un modelo bayesiando con Turing, pero llegando incluso a crear un binario en linux que me permita predecir con un modelo y desplegarlo incluso en entornos dónde no está instalado Julia. La verdad, que no sé si lo conseguiré, pero al menos aprendo algo por el camino. \n\nSi, ya sé que existen los dockers y todo eso, pero no está de más saber que existen alternativas que quizá sean mejores.  Ya en el pasado he tratado temas de cómo productivizar modelos  de h2o sobre spark [aquí](https://muestrear-no-es-pecado.netlify.app/2019/03/12/productivizando-modelos-binarios-con-h20/) o  con Julia [aquí](https://muestrear-no-es-pecado.netlify.app/2021/08/16/palabras-para-julia-parte-2-n/).  El objetivo final será llegar a tener un binario en linux que tome como argumento la ruta dónde se haya guardado las posterior samples de un modelo bayesiano y la ruta con especificación de dicho modelo en texto (para que Turing sepa como usar esas posterior samples) y que nos genere la posterior predictive para nuevos datos. \n\n\nAsí que vamos al lío. Empezamos por ver como entrenamos un modelo bayesiano con Turing y como se puede guardar y utilizar posteriormente.\n\n## Entrenamiento con Julia\n\nVamos a hacer un ejemplo sencillo, entrenando una regresión lineal múltiple de forma bayesiana. \nEl dataset forma parte del material del libro Introduction to Statistical Learning. [Advertising](https://www.statlearning.com/s/Advertising.csv)\n\n\n```julia\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\n\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\nmm = DataFrame(CSV.File(\"data/Advertising.csv\"))\ndescribe(mm)\n\n\n```\n\n```bash\n\n200×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │       1    230.1     37.8       69.2     22.1\n   2 │       2     44.5     39.3       45.1     10.4\n   3 │       3     17.2     45.9       69.3      9.3\n  ⋮  │    ⋮        ⋮        ⋮         ⋮         ⋮\n 198 │     198    177.0      9.3        6.4     12.8\n 199 │     199    283.6     42.0       66.2     25.5\n 200 │     200    232.1      8.6        8.7     13.4\n                                     194 rows omitted\n\njulia> describe(mm)\n5×7 DataFrame\n Row │ variable   mean      min   median   max    nmissing  eltype   \n     │ Symbol     Float64   Real  Float64  Real   Int64     DataType \n─────┼───────────────────────────────────────────────────────────────\n   1 │ Column1    100.5      1     100.5   200           0  Int64\n   2 │ TV         147.043    0.7   149.75  296.4         0  Float64\n   3 │ radio       23.264    0.0    22.9    49.6         0  Float64\n   4 │ newspaper   30.554    0.3    25.75  114.0         0  Float64\n   5 │ sales       14.0225   1.6    12.9    27.0         0  Float64\n\n```\n\nEspecificamos el modelo,  y aquí tengo que comentar un par de cosas. Una que julia gracias a que implementa eficazmente el  [Multiple dispatch](https://en.wikipedia.org/wiki/Multiple_dispatch), podemos tener una misma función que devuelva cosas diferentes dependiendo de que le pasemos, así una función puede tener diferentes métodos.  El otro aspecto es el uso del condition en Turing (alias `|`) se puede especificar el modelo sin pasar como argumento la variable dependiente y usarla solo para obtener la posterior, lo cual nos va a permitir hacer algo como `predict( modelo(Xs), cadena_mcmc)`, y no tener que pasar la `y` como un valor perdido. \n\n\n```julia\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n    \n    radio::AbstractVector{<:Real},\n    newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\n\n``` \n\nAhora tenemos el mismo modelo, que me a servir tanto para pasarle  como argumentos escalares como vectores, nótese que la función Normal tomo como argumento la desviación típica, mientrar que MvNormal toma una matriz de varianzas/covarianzas. Se aconseja el uso de MvNormal en Turing pues mejora el tiempo de cálculo de la posteriori.\n\n\nObtenemos la posteriori de los parámetros, pasándole como datos el dataset de Advertising. Es importante que la columna de la variable dependiente se pase como `NamedTuple`, esto se puede hacer en julia usando `(; vector_y)` . \n\n\n```julia\n\n# utilizamos 4 cadenas con n_samples = 2000  para cada una\n\n# usamos | para pasarle los datos de Y que no habiamos pasado en la especificacion del modelo\n\nchain = sample(mm_model_sin_sales(mm.TV, mm.radio, mm.newspaper) | (; mm.sales),\n    NUTS(0.65),MCMCThreads(),\n    2_000, 4)\n    \n```\n\nY en unos 18 segundos tenemos nuestra MCMC Chain. \n\n```bash\nChains MCMC chain (2000×17×4 Array{Float64, 3}):\n\nIterations        = 1001:1:3000\nNumber of chains  = 4\nSamples per chain = 2000\nWall duration     = 18.06 seconds\nCompute duration  = 71.54 seconds\nparameters        = a, tv_coef, radio_coef, newspaper_coef, σ₁\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n      parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n          Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n               a    2.0952    0.2712     0.0030    0.0038   5123.1176    0.9999       71.6139\n         tv_coef    0.0481    0.0013     0.0000    0.0000   7529.0954    0.9998      105.2461\n      radio_coef    0.1983    0.0087     0.0001    0.0001   5230.9995    1.0000       73.1220\n  newspaper_coef    0.0040    0.0059     0.0001    0.0001   6203.9490    1.0002       86.7224\n              σ₁    1.7205    0.0874     0.0010    0.0011   5441.9631    1.0000       76.0709\n\nQuantiles\n      parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n          Symbol   Float64   Float64   Float64   Float64   Float64 \n\n               a    1.5635    1.9104    2.0982    2.2788    2.6182\n         tv_coef    0.0455    0.0472    0.0481    0.0489    0.0508\n      radio_coef    0.1814    0.1923    0.1983    0.2042    0.2155\n  newspaper_coef   -0.0077    0.0001    0.0040    0.0078    0.0157\n              σ₁    1.5585    1.6607    1.7169    1.7781    1.8997\n```\n\nVale, estupendo,en `chain` tenemos las 8000 samples para cada uno  de los 5 parámetros , y también las de temas del ajuste interno por HMC, de ahí lo de (2000×17×4 Array{Float64, 3}).  \nPero ¿cómo podemos predecir para nuevos datos? \n\nPues podemos pasarle simplemente  3 escalares correspondientes a las variables TV, radio y newspaper. \n\nEs necesario pasarle a la función predict  la llamada al modelo con los nuevos datos  `mm_model_sin_sales(tv_valor, radio_valor,newspaper_valor) `  y las posterioris (la cadena MCMC) de los parámetros.\n\n\n```bash\n\njulia> predict(mm_model_sin_sales(2, 5, 7), chain)\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales    3.2203    1.7435     0.0195    0.0176   8053.9924    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales   -0.1863    2.0441    3.2553    4.4030    6.6547\n```\n\nTambién podemos pasarle más valores\n\n\n```bash\n\njulia> mm_last = last(mm, 3)\n3×5 DataFrame\n Row │ Column1  TV       radio    newspaper  sales   \n     │ Int64    Float64  Float64  Float64    Float64 \n─────┼───────────────────────────────────────────────\n   1 │     198    177.0      9.3        6.4     12.8\n   2 │     199    283.6     42.0       66.2     25.5\n   3 │     200    232.1      8.6        8.7     13.4\n\n\njulia> predicciones = predict(mm_model_sin_sales(mm_last.TV, mm_last.radio, mm_last.newspaper), chain)\nChains MCMC chain (2000×3×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales[1], sales[2], sales[3]\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n    sales[1]   12.5192    1.7427     0.0195    0.0170   8270.6268    1.0000\n    sales[2]   24.3266    1.7560     0.0196    0.0222   7720.4172    1.0001\n    sales[3]   14.9901    1.7327     0.0194    0.0188   8039.4940    0.9999\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n    sales[1]    9.0888   11.3344   12.5241   13.6990   15.9571\n    sales[2]   20.8369   23.1519   24.3414   25.4967   27.7429\n    sales[3]   11.6549   13.8304   14.9617   16.1471   18.3733\n\n``` \n\nPodría quedarme con las predicciones para sales[1] y calcular el intervalo de credibilidad el 80%\n\n```bash\njulia> quantile(reshape(Array(predicciones[\"sales[1]\"]), 8000), [0.1, 0.5, 0.9])\n3-element Vector{Float64}:\n 10.28185973755853\n 12.524091380928425\n 14.74877121738519\n\n```\n\n## Guardar cadena y predecir\n\nAhora viene la parte que nos interesa a los que nos dedicamos a esto y queremos usar un modelo entrenado hace 6 meses sobre datos de hoy. Guardar lo que hicimos y predecir sin necesidad de reentrenar.\n\n\n\nGuardamos la posteriori \n\n```julia\n\n\nwrite( \"cadena.jls\", chain)\n``` \n\nY ahora, cerramos julia y abrimos de nuevo. \n\n\n```julia\n\nusing LinearAlgebra, Plots\nusing Turing\nusing ReverseDiff, Memoization \nusing DataFrames\nusing CSV\nusing Random\nusing StatsPlots\nusing Distributions\nusing StatsBase\n\nimport Logging\nLogging.disable_logging(Logging.Warn)\n\n# posteriori guardada\nchain = read(\"cadena.jls\", Chains)\n\n# Especificación del modelo (esto puede ir en otro fichero .jl)\n\n# Si tengo en un fichero jl el código de @model, lo puedo incluir ahí. \n\n\n# ruta = \"especificacion_modelo.jl\"\n# include(ruta)\n\n\n@model function mm_model_sin_sales(TV::Real, radio::Real, newspaper::Real)\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n    \n    # antes \n    mu = a + tv_coef * TV + radio_coef * radio + newspaper_coef * newspaper \n    sales ~ Normal(mu, σ₁)\nend\n\n@model function mm_model_sin_sales(TV::AbstractVector{<:Real},\n     radio::AbstractVector{<:Real},\n      newspaper::AbstractVector{<:Real})\n    # Prior coeficientes\n    a ~ Normal(0, 0.5)\n    tv_coef  ~ Normal(0, 0.5)\n    radio_coef  ~ Normal(0, 0.5)\n    newspaper_coef  ~ Normal(0, 0.5)\n    σ₁ ~ Gamma(1, 1)\n           \n    mu = a .+ tv_coef .* TV .+ radio_coef .* radio .+ newspaper_coef .* newspaper \n    sales ~ MvNormal(mu, σ₁^2 * I)\nend\n\n\n\n\n\n```\n\nY aqui viene la parte importante. En la que utilizamos el modelo guardado, que no es más que las  posterioris de los parámetros que hemos salvado en disco previamente.\n\n\n```julia\n\n## predecimos la misma observación , fila 198 del dataset\n\npredict(mm_model_sin_sales(177, 9.3, 6.4 ), chain)\n\n```\n\n\n```bash\nChains MCMC chain (2000×1×4 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 4\nSamples per chain = 2000\nparameters        = sales\ninternals         = \n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64 \n\n       sales   12.4723    1.7285     0.0193    0.0186   8326.7650    0.9998\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n       sales    9.0844   11.3106   12.4727   13.6334   15.7902\n       \n```\n\nY voilá. Sabiendo que se puede guardar la posteriori y  usarla luego , veo bastante factible poder llegar al objetivo de crear un \"motor de predicción \" de modelos bayesianos con Turing, que sea un ejecutable y que tome como argumentos la posteriori guardada de un modelo ajustado y en texto (con extensión jl ) y escriba el resultado en disco.  Y lo dicho, que pueda desplegar este ejecutable en cualquier sistema linux, sin tener que instalar docker ni nada, solo hacer un `unzip `\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "3720e80311cb9cd98111cc7d818d3551",
  "result": {
    "markdown": "---\ntitle: Mediator. Full luxury bayes\nauthor: jlcr\ndate: '2022-02-12'\nslug: mediator-full-luxury-bayes\ncategories:\n  - bayesian\n  - estadística\ntags:\n  - análisis bayesiano\n  - causal inference\n  - R\ndescription: ''\ntopics: []\n---\n\n\n\n\nContinuando con la serie sobre cosas de inferencia causal y full luxury bayes, antes de que empiece mi amigo [Carlos Gil](https://www.datanalytics.com/), y dónde seguramente se aprenderá más. \n\n\nEste ejemplo viene motivado precisamente por una charla que tuve el otro día con él. \n\nSea el siguiente diagrama causal \n\n\n::: {.cell}\n\n```{.r .cell-code}\n  library(tidyverse)\n  library(dagitty)\n  library(ggdag)\n  \n  g <- dagitty(\"dag{ \n  x -> y ;\n  z -> y ;\n  x -> z\n }\")\n  \n  \n  ggdag(g) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nSe tiene que `z` es un mediador entre x e y, y la teoría nos dice que si quiero obtener el efecto directo de x sobre y he de condicionar por `z` , y efectivamente, así nos lo dice el backdoor criterio. Mientras que si quiero el efecto total de x sobre y no he de condicionar por `z`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  adjustmentSets(g, exposure = \"x\", outcome = \"y\", effect = \"total\"  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n {}\n```\n:::\n\n```{.r .cell-code}\n  adjustmentSets(g, exposure = \"x\", outcome = \"y\", effect = \"direct\"  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ z }\n```\n:::\n:::\n\n\n\n\n## Datos simulados\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(155)\nN <- 1000 \n\nx <- rnorm(N, 2, 1) \nz <- rnorm(N, 4+ 4*x , 2 ) # a z le ponemos más variabilidad, pero daría igual\ny <- rnorm(N, 2+ 3*x + z, 1)\n```\n:::\n\n\n\n**Efecto total de x sobre y **\n\nTal y como hemos simulado los datos, se esperaría que el efecto *total* de x sobre y fuera \n\n$$ \\dfrac{cov(x,y)} {var(x)} \\approx 7 $$\n\nY qué el efecto *directo* fuera de 3 \n\nEfectivamente \n\nEfecto total\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# efecto total\nlm(y~x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      5.881        7.112  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# efecto directo\nlm(y~x+z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + z)\n\nCoefficients:\n(Intercept)            x            z  \n     2.0318       3.0339       0.9945  \n```\n:::\n:::\n\n\n\n## Full luxury bayes\n\nHagamos lo mismo pero estimando el dag completo \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\nlibrary(rethinking)\nset_cmdstan_path(\"~/Descargas/cmdstan/\")\n\ndat <- list(\n  N = N,\n  x = x,\n  y = y,\n  z = z\n)\nset.seed(1908)\n\nflbi <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a0,\n    z ~ normal( mu , sigma ),\n    \n    mu <- a1 + bx * x,\n   \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx2 * x + bz * z,\n\n    # priors\n    \n    c(a0,a1,a2,bx,bx2, bz) ~ normal( 0 , 0.5 ),\n    c(sigma,tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 10 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 finished in 7.9 seconds.\nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 10 finished in 10.1 seconds.\nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 8 finished in 11.0 seconds.\nChain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 5 finished in 11.2 seconds.\nChain 9 finished in 11.2 seconds.\nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 finished in 12.2 seconds.\nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 finished in 12.5 seconds.\nChain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 12.8 seconds.\nChain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 13.1 seconds.\nChain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 finished in 13.2 seconds.\n\nAll 10 chains finished successfully.\nMean chain execution time: 11.5 seconds.\nTotal execution time: 13.3 seconds.\n```\n:::\n:::\n\n\n\nY recuperamos los coeficientes y varianzas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(flbi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           mean         sd      5.5%    94.5%     n_eff     Rhat4\nbz    1.0131931 0.01557219 0.9883466 1.038241  8226.042 1.0005919\nbx2   2.9610334 0.07098661 2.8447278 3.073501  9640.122 1.0004040\nbx    4.1535508 0.06180592 4.0552767 4.252771  8753.661 1.0001456\na2    1.9440850 0.09315785 1.7952495 2.093027 11266.267 1.0001373\na1    3.7089914 0.13578637 3.4916007 3.926010  8709.448 1.0002893\na0    1.9857103 0.03108199 1.9357773 2.035712 16252.310 0.9997523\nk     0.9719412 0.02166791 0.9378896 1.007410 15276.158 0.9999036\ntau   0.9859183 0.02216068 0.9512944 1.021932 15735.140 1.0001540\nsigma 1.9636543 0.04382554 1.8950489 2.034130 15855.098 1.0000898\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# extraemos 10000 muestras de la posteriori \npost <- extract.samples(flbi, n = 10000) \n```\n:::\n\n\nY el efecto directo de x sobre y sería\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(post$bx2, probs = c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%      50%    97.5% \n2.823659 2.962330 3.099855 \n```\n:::\n:::\n\n\n\nEn este ejemplo sencillo, podríamos estimar el efecto causal de x sobre y simplemente sumando las posterioris \n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(post$bx + post$bx2, c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%      50%    97.5% \n6.928046 7.114180 7.299252 \n```\n:::\n:::\n\n\n\nTambién podríamos obtener el efecto causal  total de x sobre y  simulando una intervención. En este caso, al ser la variable continua, lo que queremos obtener como de diferente es y cuando $X = x_i$ versus cuando $X = x_i+1$. \n\nSiempre podríamos ajustar otro modelo bayesiano dónde no tuviéramos en cuenta a z y obtendríamos la estimación de ese efecto total de x sobre y, pero siguiendo a Rubin y Gelman, la idea es incluir en nuestro modelo toda la información disponible. Y tal y como dice Richard McElreath , [Statistical Rethinking 2022](https://github.com/rmcelreath/stat_rethinking_2022), el efecto causal se puede estimar simulando la intervención. \n\nAsí que el objetivo es dado nuestro modelo que incluye la variable z, simular la intervención cuando $X = x_i$  y  cuando $X = x_i+1$ y una estimación del efecto directo es la resta de ambas distribuciones a posteriori de y. \n\n\n\n\nCreamos función para calcular el efecto de la intervención `y_do_x`\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_total_effect <- function(x_value = 0, incremento = 0.5) {\n  n <- length(post$bx)\n  with(post, {\n    # simulate z, y  for x= x_value\n    z_x0 <- rnorm(n, a1 + bx * x_value  , sigma)\n    y_x0 <- rnorm(n, a2  + bz * z_x0 + bx * x_value , tau)\n    \n    # simulate z,y for x= x_value +1 \n    z_x1 <- rnorm(n, a1 + bx * (x_value + incremento)  , sigma)\n    y_x1 <- rnorm(n, a2  + bz * z_x1 + bx2 * (x_value + incremento) , tau)\n    \n    \n    # compute contrast\n    y_do_x <- (y_x1 - y_x0) / incremento\n    return(y_do_x)\n  })\n  \n}\n```\n:::\n\n\nDado un valor de x, podemos calcular el efecto total\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_do_x_0_2 <- get_total_effect(x_value = 0.2) \n\nquantile(y_do_x_0_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        0%        25%        50%        75%       100% \n-15.324628   2.395551   6.702379  10.987520  28.909002 \n```\n:::\n:::\n\n\nPodríamos hacer lo mismo para varios valores de x\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_seq <- seq(-0.5, 0.5, length = 1000)\ny_do_x <-\n  mclapply(x_seq,  function(lambda)\n    get_total_effect(x_value = lambda))\n```\n:::\n\n\nY para cada uno de estos 1000 valores tendría 10000 valores de su efecto total de x sobre y. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(y_do_x[[500]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10000\n```\n:::\n\n```{.r .cell-code}\nhead(y_do_x[[500]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   9.839792  14.319618   4.533996  16.899879  15.140380 -12.237945\n```\n:::\n:::\n\n\n\nCalculamos los intervalos de credibilidad del efecto total de x sobre y para cada valor de x. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lo hacemos simplemente por cuantiles, aunque podríamos calcular el hdi también, \n\nintervalos_credibilidad <-  mclapply( y_do_x, function(x) quantile(x, probs = c(0.025, 0.5, 0.975)))\n\n# Media e intervalor de credibilidad para el valor de x_seq en la posición 500 \nmean(y_do_x[[500]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.157054\n```\n:::\n\n```{.r .cell-code}\nintervalos_credibilidad[[500]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     2.5%       50%     97.5% \n-5.108428  7.108147 19.521261 \n```\n:::\n:::\n\n\n**intervalo de predicción clásico con el lm**\n\nHabría que calcular la predicción de y para un valor de x y para el de x + 1, podemos calcular los intervalos de predicción clásicos parea cada valor de x, pero no para la diferencia ( al menos con la función predict)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt_lm <- lm(y~x)\npredict(mt_lm, newdata = list(x= x_seq[[500]]), interval  = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr     upr\n1 5.877439 1.578777 10.1761\n```\n:::\n\n```{.r .cell-code}\npredict(mt_lm, newdata = list(x= x_seq[[500]] +1), interval  = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr     upr\n1 12.98993 8.698051 17.2818\n```\n:::\n:::\n\n\n\n**Pero como sería obtener el intervalo de credibilidad para la  media de los efectos totales?**\n\nCalculando para cada valor de x la media de la posteriori del efecto global y juntando todas las medias. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes_mean <- lapply(y_do_x, mean)\n\nquantile(unlist(slopes_mean), c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%      50%    97.5% \n6.020193 7.160083 8.307472 \n```\n:::\n:::\n\n\nQue tiene mucha menos variabilidad que el efecto global  en un valor concreto de x, o si juntamos todas las estimaciones\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(unlist(y_do_x),  c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     2.5%       50%     97.5% \n-5.226904  7.174074 19.556935 \n```\n:::\n:::\n\n\nEvidentemente, podríamos simplemente no haber tenido en cuenta la variable z en nuestro modelo bayesiano. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nflbi_2 <- ulam(\n  alist(\n    # x model, si quiero estimar la media de x sino, no me hace falta\n    x ~ normal(mux, k),\n    mux <- a1,\n    \n    y ~ normal( nu , tau ),\n    nu <- a2 + bx * x ,\n    \n    # priors\n    \n    c(a1,a2,bx) ~ normal( 0 , 0.5 ),\n    c(tau, k) ~ exponential( 1 )\n  ), data=dat , chains=10 , cores=10 , warmup = 500, iter=2000 , cmdstan=TRUE )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 10 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 5 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 5 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 5 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 5 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 5 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 6 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 6 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 7 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 7 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 7 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 8 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 8 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 8 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 9 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 9 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 9 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 10 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 10 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 5 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 5 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 6 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 6 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 6 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 7 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 7 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 8 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 9 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 10 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 5 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 5 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 6 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 6 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 6 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 7 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 7 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 8 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 8 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 8 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 9 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 9 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 9 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 10 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 5 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 6 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 6 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 7 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 8 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 9 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 10 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 5 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 6 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 6 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 7 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 8 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 9 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 9 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 10 Iteration:  501 / 2000 [ 25%]  (Sampling) \nChain 10 Iteration:  600 / 2000 [ 30%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 5 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 6 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 7 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 8 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 8 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 9 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 10 Iteration:  700 / 2000 [ 35%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 5 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 6 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 6 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 7 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 8 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 9 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 10 Iteration:  800 / 2000 [ 40%]  (Sampling) \nChain 10 Iteration:  900 / 2000 [ 45%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 5 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 6 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 6 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 7 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 8 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 9 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 9 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 10 Iteration: 1000 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 5 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 6 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 6 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 6 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 8 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 9 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 10 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 10 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 finished in 1.1 seconds.\nChain 6 finished in 1.1 seconds.\nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 5 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 5 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 7 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 7 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 8 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 8 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 9 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 9 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 10 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 5 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 7 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 8 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 9 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 10 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 5 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 5 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 7 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 7 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 8 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 8 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 9 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 9 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 10 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 10 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 finished in 1.5 seconds.\nChain 2 finished in 1.5 seconds.\nChain 3 finished in 1.4 seconds.\nChain 5 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 7 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 7 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 8 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 8 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 9 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 9 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 10 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 10 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 10 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 5 finished in 1.5 seconds.\nChain 7 finished in 1.5 seconds.\nChain 8 finished in 1.5 seconds.\nChain 9 finished in 1.5 seconds.\nChain 10 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 10 finished in 1.5 seconds.\n\nAll 10 chains finished successfully.\nMean chain execution time: 1.4 seconds.\nTotal execution time: 1.8 seconds.\n```\n:::\n:::\n\n\n\nY obtenemos directamente el efecto total de x sobre y.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(flbi_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         mean         sd      5.5%    94.5%    n_eff     Rhat4\nbx  7.1948083 0.06787458 7.0863389 7.302933 10446.14 1.0001211\na2  5.6085615 0.14948341 5.3685467 5.848723 10598.69 1.0001680\na1  1.9860307 0.03058033 1.9368689 2.034631 14116.13 1.0004122\nk   0.9721054 0.02159773 0.9382407 1.006761 15044.78 0.9995902\ntau 2.1898100 0.04953736 2.1120600 2.269831 14995.33 0.9999084\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npost2 <- extract.samples(flbi_2,  n = 10000)\n\nquantile(post2$bx, probs = c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%      50%    97.5% \n7.061389 7.194525 7.327361 \n```\n:::\n:::\n\n\n\n## Pensamientos finales\n\n* Parece que no es tan mala idea incluir en tu modelo bayesiano toda la información disponible. \n* Ser pluralista es una buena idea, usando el backdoor criterio dado que nuestro dag sea correcto, nos puede llevar a un modelo más simple y fácil de estimar. \n* Como dije en el último [post](https://muestrear-no-es-pecado.netlify.app/2022/02/09/2022-02-09/), estimar todo el dag de forma conjunta puede ser útil en varias situaciones. \n\n* Ya en 2009 se hablaba de esto [aquí](https://statmodeling.stat.columbia.edu/2009/07/10/rubinism_separa/)\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
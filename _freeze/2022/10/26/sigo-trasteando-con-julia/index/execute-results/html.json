{
  "hash": "e75d963768f666ce094ea92a985bbc3b",
  "result": {
    "markdown": "---\ntitle: Sigo trasteando con julia\nauthor: jlcr\ndate: '2022-10-26'\ncategories:\n  - julia\n  - produccion\n  - linux\n  - 2022\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\nslug: sigo-trasteando-con-julia\n---\n\n\n\nSiguiendo con lo que contaba [aquí](https://muestrear-no-es-pecado.netlify.app/2021/08/16/palabras-para-julia-parte-2-n/) me he construido un binario para predecir usando un modelo de xgboost con Julia. La ventaja es que tengo un tar.gz que puedo descomprimir en cualquier linux (por ejemplo un entorno de producción sin acceso a internet y que no tenga ni vaya a tener julia instalado, ni docker ni nada de nada), descomprimir y poder hacer un `miapp_para_predecir mi_modelo_entrenado.jls csv_to_predict.csv resultado.csv` y que funcione y vaya como un tiro. \n\n\nPongo aquí los ficheros relevantes. \n\nPor ejemplo mi fichero para entrenar un modelo y salvarlo . \n\nFichero `train_ boston.jl`\n\n\n```julia \n# Training model julia\nusing  CSV,CategoricalArrays, DataFrames, MLJ, MLJXGBoostInterface\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\ndf1[:, :target] .= ifelse.(df1[!, :medv_20].== \"NG20\", 1, 0)\nconst target = CategoricalArray(df1[:, :target])\n\nconst X = df1[:, Not([:medv_20, :target])]\n\nTree = @load XGBoostClassifier pkg=XGBoost\ntree_model = Tree(objective=\"binary:logistic\", max_depth = 6, num_round = 800)\nmach = machine(tree_model, X, target)\n\nThreads.nthreads()\nevaluate(tree_model, X, target, resampling=CV(shuffle=true),measure=log_loss, verbosity=0)\nevaluate(tree_model, X, target,\n                resampling=CV(shuffle=true), measure=bac, operation=predict_mode, verbosity=0)\n\n\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\nyhat = predict(mach, X[test,:])\n\nevaluate(tree_model, X[test,:], target[test], measure=auc, operation=predict_mode, verbosity=0)\n\nniveles = levels.(yhat)[1]\nniveles[1]\n\nlog_loss(yhat, target[test]) |> mean\n\nres = pdf(yhat, niveles)\nres_df = DataFrame(res,:auto)\n\nMLJ.save(\"models/boston_xg.jls\", mach)\n\n```\n\n\nY luego los ficheros que uso para construirme la app binaria .. Recordemos del [post que mencionaba](https://muestrear-no-es-pecado.netlify.app/2021/08/16/palabras-para-julia-parte-2-n/) que lo que necesito es el código del programa principal (el main) y un fichero de precompilación que sirve para que al crear la app se compilen las funciones que voy a usar. \n\n\nfichero `precomp.jl`, \n\n```julia\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"models/mimodelo_xg_binario.jls\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, [\"target_0\", \"target_1\"])\n\nCSV.write(\"data/predicciones.csv\", res_df)\n\n```\n\n\nfichero `xgboost_predict_binomial.jl` , aquí es dónde está el main \n\n```julia\nmodule xgboost_predict_binomial\n\nusing CSV, DataFrames, MLJ, MLJBase, MLJXGBoostInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, ntasks= Sys.CPU_THREADS)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, [\"target_0\", \"target_1\"])          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\n\n```\n\ny si todo está correcto y siguiendo las instrucciones del post anterior, se compilaría haciendo por ejemplo esto\n\n```julia\nusing PackageCompiler\ncreate_app(\"../xgboost_predict_binomial\", \"../xg_binomial_inference\",\n precompile_execution_file=\"../xgboost_predict_binomial/src/precomp_file.jl\", force=true, filter_stdlibs = true, cpu_target = \"x86_64\")\n```\n\n\nY esto me crea una estructura de directorios dónde está mi app y todo lo necesario para ejecutar julia en cualqueir linux. \n\n```bash\n\n╰─ $ ▶ tree -L 2 xg_binomial_inference\nxg_binomial_inference\n├── bin\n│   ├── julia\n│   └── xgboost_predict_binomial\n├── lib\n│   ├── julia\n│   ├── libjulia.so -> libjulia.so.1.8\n│   ├── libjulia.so.1 -> libjulia.so.1.8\n│   └── libjulia.so.1.8\n└── share\n    └── julia\n```\n\ny poner por ejemplo en el `.bashrc` el siguiente alias. \n\n```bash\nalias motor_xgboost=/home/jose/Julia_projects/xgboost_model/xg_binomial_inference/bin/xgboost_predict_binomial\n```\n\ny ya está listo. \n\nAhora tengo un dataset a predecir de 5 millones de filas \n\n```bash\n\n╰─ $ ▶ wc -l data/test.csv \n5060001 data/test.csv\n\n```\n\n```bash\n\n head -n4 data/test.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\n\n```\n\n\ny bueno, tardo unos 11 segundos en obtener las predicciones y escribir el resultado\n \n```bash\n╰─ $ ▶ time motor_xgboost models/boston_xg.jls data/test.csv pred.csv\n\nreal\t0m11,091s\nuser\t0m53,293s\nsys\t0m2,321s\n\n\n```\n\ny comprobamos que lo ha hecho bien\n\n```bash\n\n╰─ $ ▶ wc -l  pred.csv \n5060001 pred.csv\n\n\n╰─ $ ▶ head -n 5 pred.csv \ntarget_0,target_1\n0.9999237,7.63197e-5\n0.99120975,0.008790266\n0.99989164,0.00010834133\n0.99970543,0.00029458306\n\n```\n\n\nY nada, pues esto puede servir para subir modelos a producción en entornos poco amigables (sin python3, sin R, sin julia, sin spark, sin docker, sin internet). Es un poco `old style` que me diría mi arquenazi favorito Rubén, pero \n\n\nOs dejo el tar.gz para que probéis, también os dejo el `Project.toml`y el `Manifest.toml` y el fichero con el que he entrenado los datos.  para que uséis el mismo entorno de julia que he usado yo. \n\n\n[enlace_drive](https://drive.google.com/drive/folders/1jQW-QQNoABlMdUHhlwHvY9MQnZh1x_Yi?usp=sharing)\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
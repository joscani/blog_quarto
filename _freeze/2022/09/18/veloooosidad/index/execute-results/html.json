{
  "hash": "c7381c7315e8fc5545d8cd87b1208199",
  "result": {
    "markdown": "---\ntitle: Veeelooosidad\nauthor: jlcr\ndate: '2022-09-18'\ncategories:\n  - R\n  - python\n  - C++\n  - Rust\n  - 2022\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\nslug: veloooosidad\n---\n\n\n\nNo, este post no va sobre la canción de [Medina Azahara](https://www.youtube.com/watch?v=hEj1z_ihVX8) sino  de comparar un par de librerías para lectura y procesamiento de datos. A saber, [polars](https://pola-rs.github.io/polars-book/user-guide/index.html) escrita en Rust y con api en __python__ versus [vroom](https://vroom.r-lib.org/) en combinación con librerías como [data.table](https://github.com/Rdatatable/data.table) o [collapse](https://sebkrantz.github.io/collapse/) en R. Estas últimas usan por debajo C++, así que tanto por el lado de python como por el de R el principal mérito se debe a usar Rust y C++. \n\n\n\n## Datos, hardware y entornos\n\nPara hacer la comparación vamos a usar un dataset de 100 millones de filas y 9 columnas, el mismo que se usa en [h2o.ai db-benchmark](https://github.com/h2oai/db-benchmark). \n\nLo voy a probar en mi pc, que es un [slimbook](https://slimbook.es/) de justo antes de la pandemia, con 1gb de ssd, 32Gb de RAM y procesador Intel i7-9750H (12) @ 4.500GHz  con 6 núcleos (12 hilos) y corriendo Linux Mint 20.\n\n### R\n\nPara R voy a chequear vroom y data.table para leer los datos y data.table, tidytable y collapse para el procesamiento\n\nR:  Uso R version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"\nvroom: 1.5.7\ndata.table: 1.14.2\ntidytable: 0.8.1.9\ncollapse: 1.8.8\n\n### Python\nUso un entorno de conda con python 3.6.12\npolars: '0.12.5'\n\n\n\n## Scripts \n\n### R \n\n\nEn R voy a usar microbenchmark para realizar varias ejecuciones\n\nFichero: tests.R\n\n\n```r\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vroom)\nlibrary(tidytable)\n\nlibrary(microbenchmark)\n\n# Check lectu\n\nsetDTthreads(0L)\n\nlectura <- microbenchmark(\n    vroom  = vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE), \n    data.table = data.table::fread(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\"),\n    times = 3L\n)\n\nprint(lectura)\n\n# group by sum\n\nx <- vroom::vroom(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\", show_col_types = FALSE)\n\n# x= sample_frac(x, size = 0.1)\nx_dt <- qDT(x)\n\ngroup_by_performance <- microbenchmark(\n    data.table = x_dt[, lapply(.SD, mean, na.rm = TRUE), keyby = id1, .SDcols = 7:9],\n    # dplyr      = x %>%\n    #     group_by(id1, id2) %>%\n    #     summarise(v1 = sum(v1, na.rm = TRUE)) %>% \n    #     ungroup(),\n    tidytable = x_dt %>%\n        summarize.(v1 = sum(v1),\n                   v2 = sum(v2),\n                   v3 = sum(v3),\n                   .by = c(id1, id2)),\n    # base_R = tapply(x$v1, list(x$id1, x$id2), sum, na.rm = TRUE),\n\n    collapse= x_dt %>%\n        fgroup_by(id1, id2) %>%\n        fsummarise(v1 = fsum(v1),\n                   v2 = fsum(v2),\n                   v3 = fsum(v3)),\n\n    collapse_pure = {\n        g <- GRP(x, ~ id1 +id2)\n        fsum(x$v1, g)\n        fsum(x$v2, g)\n    },\n    times = 5L\n)\n\nprint(group_by_performance)\n```\n\n\n### Python\n\nFichero: tests.py\n\n\n```python\nimport polars as pl\nimport time\n\nstart = time.time()\ndf = pl.read_csv(\"/home/jose/Rstudio_projects/db-benchmark/data/G1_1e8_1e1_5_1.csv\")\nend = time.time()\n\nprint(end -start)\n\nstart = time.time()\n\n(\n    df\n.lazy()\n    .groupby(['id1','id2'])\n    .agg(\n        [\n            pl.col(\"v1\").sum().alias('v1_sum'),\n            pl.col(\"v2\").sum().alias('v2_sum'),\n            pl.col(\"v3\").sum().alias('v3_sum')\n        ]\n    )\n.collect()\n)\nend = time.time()\nprint(end - start)\n```\n\n## Resultados \n\nPara comparar, ejecuto los scripts desde consola y teniendo cerrado navegadores, ides y demás. \n\n### R \n\n```bash\nRscript tests.R\n```\n\n__Lectura en R__\n\n```\nUnit: seconds\n       expr       min        lq      mean    median        uq       max neval\n      vroom  7.783958  7.953598  8.185716  8.123239  8.386596  8.649953     3\n data.table 41.914928 42.809751 45.213309 43.704575 46.862499 50.020424     3\n\n```\n\nGroup by y sum en R. \n\n```\nUnit: seconds\n          expr      min       lq     mean   median       uq       max neval cld\n    data.table 1.469617 1.476545 1.550360 1.486647 1.633409  1.685581     5   a\n     tidytable 1.182273 1.189111 1.291734 1.279313 1.314287  1.493686     5   a\n      collapse 1.799175 1.813744 6.255215 1.891603 2.076616 23.694936     5   a\n collapse_pure 1.553002 1.555598 1.570758 1.566454 1.571605  1.607132     5   a\n\n```\n\nPor lo que más o menos, usar vroom para leer y tidytable, data.table o collapse para hacer el cálculo sale por unos 10 segundos o un poco menos. \n\n\n\n### Python\n\n```bash\npython tests.py \n```\n```\n7.755492448806763\n1.8228027820587158\n\n```\nY vemos que con polars tenemos más o menos los mismos tiempos. \n\n\n## Conclusiones\n\nTanto en R como en Python tenemos librerías muy rápidas que , si tenemos suficiente RAM podemos trabajar con conjunto de datos bastante tochos  y hacer cosas en tiempos más que razonables.  \n\nPolars es una librería muy nueva y muy bien hecha, ojalá hagan api para R. No obstante, data.table lleva tiempo en R y su desempeño es consistente en múltiples situaciones. Mi consejo es echarle un ojo al [fastverse](https://github.com/fastverse/fastverse).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
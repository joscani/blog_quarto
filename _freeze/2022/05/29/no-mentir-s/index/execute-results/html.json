{
  "hash": "759dc5469f0e3439ee7d204cd5e4f60f",
  "result": {
    "markdown": "---\ntitle: No mentirás\nauthor: jlcr\ndate: '2022-05-29'\nslug: no-mentir-s\ncategories:\n  - estadística\n  - machine learning\n  - python\n  - R\n  - 2022\ndescription: ''\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\nHay veces que uno se deja llevar por la emoción cuando hace algo y a veces se exagera un poco con lo que hace tu criatura. \n\nTal es el caso de la librería [Nanyml](https://nannyml.readthedocs.io/en/stable/), la cual tiene buena pinta pero exagera en al menos dos partes. La primera y más evidente es cuándo dice que puede estimar el desempeño futuro de un modelo  sin comparar con lo que realmente pase, así promete el [Estimating Performance without Targets](https://nannyml.readthedocs.io/en/stable/quick.html#estimating-performance-without-targets) \n\n\nOs juro que me he leído la documentación varias veces e incluso he visto el código y en ningún lado he visto que haga eso que promete. \n\nEn realidad lo que hace no es más que basarse en dos asunciones que, si se leen en primer lugar, hace que la afirmación presuntuosa de estimar el desempeño de un modelo sin ver el target se caiga por su propio peso.  A saber, las dos asunciones son. \n\n* El modelo retorna probabilidades bien calibradas siempre. \n* La relación de $P[y | X]$ no cambia .\n\nEstas dos asunciones por si solas lo que nos dicen es que vas a medir el desempeño de un modelo (sin ver el verdadero valor del target) asumiendo de partida que el modelo es tan bueno como lo era cuando lo entrenaste. \n\nLa segunda parte es en lo que denomina [CBPE algorithm](https://nannyml.readthedocs.io/en/stable/how_it_works/performance_estimation.html#cbpe-algorithm) que si se lee con atención no es otra cosa que simplemente utilizar el modelo para obtener predicciones sobre un nuevo conjunto de datos. \n\nAsí, para calcular el AUC estimado, lo que hace es asumir que el modelo es bueno, y obtener las diferentes matrices de confusión que se derivan de escoger los posibles puntos de corte y, aquí viene el tema, considerar que el valor predicho por el modelo, es el verdadero valor. \n\nCon estas asunciones , cualquier cambio en la métrica del AUC se debería sólo y exclusivamente a cambios en la estructura de la población y no a que el modelo haya dejado de ser bueno (lo cual es imposible puesto que es una de las asunciones).. \n\nEjemplo. Si tenemos 3 grupos distintos dónde tenemos un evento binario. Supongamos que el primero de ellos viene de una población con proporción igual a 0.25, el segundo grupo viene de una población con proporción de 0.8 y el tercero de una población con proporción de 0.032.  Si tomamos 1000, 300 y 600 observaciones de cada población respectivamente podemos simular tener un score que cumpla la condición de estar bien calibrado \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps1 <- rbeta(1000, 1, 3)\nps2 <- rbeta(300, 4, 1)\nps3 <- rbeta(600, 2, 60)\n\nps <- c(ps1, ps2, ps3)\n\nmean(ps1) ;  mean(ps2); mean(ps3)\n#> [1] 0.2516992\n#> [1] 0.8020855\n#> [1] 0.03233513\n```\n:::\n\n\nLa distribución de los \"scores\" sería\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=80%}\n:::\n:::\n\n\n\nPues el CBPE no sería otra cosa que calcular el auc del modelo ¡¡asumiendo que las probabilidades estimadas son correctas!! . Es como intentar demostrar algo teniendo como asunción que es cierto. Pero vayamos al cálculo. \n\nSiguiendo lo descrito por la documentación y comprobando con el código de la librería se tendría que\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ntpr_fpr <- function(threshold, ps) {\n  yj <- ifelse(ps >= threshold, 1, 0) \n  p_false = abs(yj - ps)\n  p_true = 1- p_false\n  n <- length(yj)\n  tp <- sum(p_true[yj == 1])\n  fp <- sum(p_false[yj==1])\n  tn <- sum(p_true[yj==0] )\n  fn <- sum(p_false[yj==0] )\n  tpr <- tp / (tp + fn)\n  fpr <- fp /(fp + tn)\n  return(data.frame(tpr = tpr, fpr = fpr))\n}\n\n\npscortes = sort(unique(ps), decreasing = TRUE)\n\ndfs <-  lapply(pscortes, function(x) tpr_fpr(x, ps))\n\nvalores <- do.call(rbind, dfs)\n\n\nplot(valores$fpr, valores$tpr, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=80%}\n:::\n\n```{.r .cell-code}\n\nsimple_auc <- function(TPR, FPR){\n  # inputs already sorted, best scores first \n  dFPR <- c(diff(FPR), 0)\n  dTPR <- c(diff(TPR), 0)\n  sum(TPR * dFPR) + sum(dTPR * dFPR)/2\n}\n\nwith(valores, simple_auc(tpr, fpr))\n#> [1] 0.8914538\n```\n:::\n\n\nCómo se ve, para calcular el auc sólo se tiene en cuenta las probabilidades estimadas, por lo que pierde todo el sentido para obtener un desempeño de cómo de bien lo hace el modelo. \n\nDe hecho, si hubiera simulado para cada observación una bernoulli tomando como probabilidad de éxito el score tendría lo siguiente, y tomo esa simulación como el valor real , obtengo el mismo auc que con CBPE. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlabels <- rbinom(length(ps), 1, ps)\n(res <- pROC::auc(labels, ps))\n#> Area under the curve: 0.8962\n```\n:::\n\n\nEs decir, en la misma definición de lo que es una matriz de confusión y las métricas asociadas va implícita la idea de comparar la realidad con la estimación, __si sustituyes la realidad por la estimación , entonces pierde el sentido__. \n\n\nPero veamos para qué si puede servir esta cosa. Pues nos puede servir para detectar cambios de distribuciones conjuntas entre dos conjuntos de datos. Me explico, supongamos que quiero predecir sobre un conjunto de datos que en vez de tener 1000 observaciones de la primera población hay 200, y que de la segunda hay 100 y 10000 de la tercera. Pues en este caso, el cambio en el auc se debe solo a eso, al cambio de la estructura de la población global. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nps1_new <- rbeta(200, 1, 3)\nps2_new <- rbeta(100, 4, 1)\nps3_new <- rbeta(10000, 2, 60)\n\nps_new <- c(ps1_new, ps2_new, ps3_new)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlabels <- rbinom(length(ps_new), 1, ps_new)\n(res <- pROC::auc(labels, ps_new))\n#> Area under the curve: 0.7616\n```\n:::\n\n\nLa bajada del \"auc estimado\" solo se debe a cambios en la estructura de la nueva población que tiene muchas más observaciones de la población 3. \n\nPor lo tanto, lo que `nannyml` hace  y no está mal, ojo, es simplemente ver cuál serían métricas agregadas (como el auc) cuando cambia la estructura pero no la probabilidad condicionada de y con respecto a las variables independientes.  \n\nLo que no me parece bien es poner en la documentación que calcula el desempeño de un modelo sin ver el target, puesto que confunde y ya ha dado lugar a algún post en \"towards data science\" (gente, formaros primero con libros antes de leer post de estos sitios) con más humo que madera.\n\nY como se suele decir \"No mentirás\". \n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "41072753ca4d5b0aabfb093a7a22ad5b",
  "result": {
    "markdown": "---\ntitle: Palabras para Julia ( Parte 2/n)\nauthor: jlcr\ndate: '2021-08-16'\nslug: palabras-para-julia-parte-2-n\ncategories:\n  - Julia\n  - ciencia de datos\n  - 2021\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n\n---\n\n\n\n\n## Introducción \n\n¿Qué os parecería tener un modelo guardado y un binario en linux que tomando como parámetros el modelo y el dataset a predecir guardara las predicciones en un csv? \n\nY todo eso que funcione en cualquier Linux, de forma que puedas copiar esa aplicación de un Ubuntu a un EC2 con amazon linux (un centos) y que funcione igual sin tener que tener Julia instalado en el EC2.\n\nY no estoy hablando de tener un docker o tener un entorno de conda dónde lo despliegas y tu script dónde se predice necesita ser interpretado, sino de una aplicación compilada dónde tiene el runtime de Julia y todo lo necesario para correr. De hecho la estructura de esa aplicación sería algo así.\n\n```bash\n $ ▶ tree -L 1 ../bin_blog\n../bin_blog\n├── artifacts\n├── bin\n└── lib\n\n```\n\nBueno, pues vamos a ver cómo se consigue eso utilizando el paquete [`PackageCompiler`](https://github.com/JuliaLang/PackageCompiler.jl) En primer lugar algunas consideraciones sobre latencia y precompilados  que podéis ver en el video de [Kristoffer Carlson](https://www.youtube.com/watch?v=7gHYrZ4QWZs). \n\n* Julia tiene los paquetes precompilados, por lo que cuando arrancas el pc y haces `using paquete` tarda un rato. \n* Una vez compilado, la primera vez que lo invocas tarda un tiempo.\n* Aunque ya hayas hecho `using paquete` la primera vez que usas una función también tiene que compilarla y tarda otro rato. \n\npor ejemplo, vemos que la primera vez que hago `using Plots` tarda unos 3 segundos, o que la primera vez que uso la función `plot` también tarda, pero la siguiente vez es muy rápido. \n\n```bash\njulia> @time using Plots\n  3.267061 seconds (7.93 M allocations: 551.989 MiB, 3.89% gc time, 0.17% compilation time)\n\njulia> @time using Plots\n  0.666359 seconds (665.66 k allocations: 37.314 MiB, 6.76% gc time, 99.99% compilation time)\n\n\njulia> @time p = plot(rand(2,2))\n  2.436100 seconds (3.11 M allocations: 186.676 MiB, 7.61% gc time, 57.23% compilation time)\n[ Info: Precompiling GR_jll [d2c73de3-f751-5644-a686-071e5b155ba9]\n\njulia> @time p = plot(rand(2,2))\n  0.000883 seconds (3.93 k allocations: 228.672 KiB)\n```\n\nPero, ¿no era una de las características de Julia su velocidad, cómo podemos apañar esto?. Pues hay varias formas. \n\n* Crear un archivo `startup.jl` en `\\.julia\\config\\` dónde escribamos lo que queremos que se cargue al iniciar julia.\n* Crear una sysimage de julia con [`PackageCompiler`](https://julialang.github.io/PackageCompiler.jl/dev/sysimages.html) de forma que podamos hacer algo como `julia --sysimage mi_sysimage.so` y se inicie Julia con los paquetes que queramos ya compilados y cargados.\n\nEl paquete `PackageCompiler` permite también crear una aplicación de forma que crea una sysimage de julia junto con un script con una función main que es la que se ejecuta al llamar al binario que crea. La ventaja de esta aproximación es que podemos crear un binario que funcione en cualquier linux aunque no tengamos Julia instalado. \n\n## Objetivo\n\nEl objetivo es construir un \"Motor de Modelos\" (recuerdos a mi amigo Roberto Sancho) que funciones en cualquier linux, y que dado un modelo previamente entrenado y la ruta de un fichero con los datos, haga la predicción del modelo y escriba un csv con el resultado. \n\nAl final se podría usar de la siguiente forma\n\n```bash\nmotor_modelos modelo_entrenado.jlso datos_to_predict.csv resultado.csv\n```\n\nEn las pruebas que he hecho, para predecir un fichero de 5 millones de filas y escribir el csv con el resultado de la predicción de un modelo `randomForest` ha tardado unos 15 segundos en todo el proceso. \n\n\n## Creando el entorno necesario\n\nLo primero que tenemos que hacer es seguir el proceso como para crear un paquete en julia. \nIniciamos julia en el REPL y entrando en el modo paquete con `]`  utilizamos `generate`\n\n```bash\n(@v1.6) pkg> generate decision_tree_app\n  Generating  project decision_tree_app:\n    decision_tree_app/Project.toml\n    decision_tree_app/src/decision_tree_app.jl\n```\n\nEsto crea el directorio decision_tree_app así como un Project.toml dónde se va a ir guardando la referencia y las versiones de las librerías que usemos, y también crea el fichero `src/decision_tree_app.jl` con la estructura mínima.\n\n``` bash\n╭─ jose @ jose-PROX15 ~\n│\n╰─ $ ▶ cat decision_tree_app/src/decision_tree_app.jl \nmodule decision_tree_app\n\ngreet() = print(\"Hello World!\")\n\nend # module\n\n```\n\nPues sobre esta base es la que vamos a trabajar. Ahora tenemos que activar el entorno con \n\n```bash\n(@v1.6) pkg> activate .\n  Activating environment at `~/decision_tree_app/Project.toml`\n \n```\n\nDe esta forma cada vez que añadamos un paquete con `add nombrepquete` se queda guardado la referencia en el el `Project.toml` y se creará un `Manifest.toml`, estos dos archivos son los que nos servirán para reproducir el mismo entorno en otro sitio, equivalente a un requirements en python. \n\n## Crear la aplicación \n\nEn el paquete `PackageCompiler` existe la función `create_app` que tomando como argumentos, el directorio de la aplicación, directorio dónde compilar y uno o varios ficheros de ejemplo del flujo que se va a realizar, creará la apliación compilada.\n\n### Fichero de precompilación\n\nEs importante tener un fichero de precompilación que sea ejemplo simple de lo que tiene que hacer la aplicación. A saber, leer un modelo, leer unos datos, predecir y escribir el resultado. \n\nPara eso, entrenamos un modelo sobre iris, y guardamos el modelo\n\n**Fichero train.jl** \n\n```julia\n\n# Training model julia\nusing CSV,CategoricalArrays, DataFrames, MLJ\n\n\ndf1 = CSV.read(\"data/iris.csv\", DataFrame)\n\nconst target =  CategoricalArray(df1[:, :Species])\nconst X = df1[:, Not(:Species)]\n\n\nTree = @load RandomForestClassifier pkg=DecisionTree\ntree = Tree(n_trees = 20)\ntree.max_depth = 3\n\nmach = machine(tree, X, target)\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\n\nMLJ.save(\"mimodelo.jlso\", mach, compression=:gzip)\n\n\n```\n\nY una vez que tenemos el modelo guardado creamos el fichero de precompilación que pasaremos como argumento a `create_app`\n\n**Fichero precomp_file.jl en directorio src**\n\nEn este fichero al llamar a las funciones `CSV.read`, `predict`, o `CSV.write` se consigue que al crear la aplicación compilada esas funciones se compilen y la latencia sea mínima. \n\n```julia\nusing MLJ, CSV, DataFrames, MLJDecisionTreeInterface\n\n# uso rutas absolutas\ndf1 = CSV.read(\"/home/jose/Julia_projects/decision_tree_app/data/iris.csv\", DataFrame)\nX = df1[:, Not(:Species)]\n\npredict_only_mach = machine(\"/home/jose/Julia_projects/decision_tree_app/mimodelo.jlso\")\n\nŷ = predict(predict_only_mach, X) \n\n\npredict_mode(predict_only_mach, X)\n\nniveles = levels.(ŷ)[1]\n\nres = pdf(ŷ, niveles) # con pdf nos da la probabilidad de cada nivel\nres_df = DataFrame(res,:auto)\nrename!(res_df, niveles)\n\nCSV.write(\"/home/jose/Julia_projects/decision_tree_app/data/predicciones.csv\", res_df)\n\n```\n\nCuando compilamos una aplicación con PackageCompiler lo que se ejecuta es la función julia_main  que se encuentre en el módulo que creamos con el mismo nombre que el nombre de la aplicación.\n\n**Fichero decision_tree_app.jl en src**\n\n```julia\nmodule decision_tree_app\n\nusing MLJ, CSV, DataFrames, MLJDecisionTreeInterface\n\nfunction julia_main()::Cint\n    try\n        real_main()\n    catch\n        Base.invokelatest(Base.display_error, Base.catch_stack())\n        return 1\n    end\n    return 0\nend\n\n# ARGS son los argumentos pasados por consola \n\nfunction real_main()\n    if length(ARGS) == 0\n        error(\"pass arguments\")\n    end\n\n# Read model\n    modelo = machine(ARGS[1])\n# read data. El fichero qeu pasemos tiene que tener solo las X.(con su nombre)\n    X = CSV.read(ARGS[2], DataFrame, tasks=10)\n# Predict    \n    ŷ = predict(modelo, X)            # predict\n    niveles = levels.(ŷ)[1]           # get levels of target\n    res = pdf(ŷ, niveles)             # predict probabilities for each level\n    \n    res_df = DataFrame(res,:auto)     # convert to DataFrame\n    rename!(res_df, niveles)          # Column rename\n    CSV.write(ARGS[3], res_df)        # Write in csv\nend\n\n\nend # module\n\n```\n\nAsí nos queda la estructura\n\n``` bash\n tree -L 1 src/\nsrc/\n├── decision_tree_app.jl\n├── precomp_file.jl\n└── train.jl\n```\n\nAhora ya podemos compilar la aplicación\n\n```julia \n\nusing PackageCompiler\n\ncreate_app(\"../decision_tree_app\", \"../bin_blog\", precompile_execution_file=\"../decision_tree_app/src/precomp_file.jl\", force=true, filter_stdlibs = true)\n\n\n```\n\nY después de unos 30 minutos ya tenemos en el directorio bin_blog todo lo necesario, el runtime de julia embebido, las librerías compiladas , etcétera, de forma que copiando esa estructura en otro ordenador (con linux) ya funcionaría nuestra app sin tener Julia instalado. \n\n```\n\ntree -L 1 bin_blog/\nbin_blog/\n├── artifacts\n├── bin\n└── lib\n```\nPor ejemplo en lib tenemos\n\n```\ntotal 272\ndrwxrwxr-x 3 jose jose   4096 ago 14 10:37 ./\ndrwxrwxr-x 5 jose jose   4096 ago 14 10:37 ../\ndrwxrwxr-x 2 jose jose   4096 ago 14 10:37 julia/\nlrwxrwxrwx 1 jose jose     15 ago 14 10:37 libjulia.so -> libjulia.so.1.6*\nlrwxrwxrwx 1 jose jose     15 ago 14 10:37 libjulia.so.1 -> libjulia.so.1.6*\n-rwxr-xr-x 1 jose jose 266232 ago 14 10:37 libjulia.so.1.6*\n```\ny en lib/julia  tiene este aspecto\n\n\n```\ndrwxrwxr-x 2 jose jose     4096 ago 14 10:37 ./\ndrwxrwxr-x 3 jose jose     4096 ago 14 10:37 ../\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libamd.so -> libamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libamd.so.2 -> libamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    39059 ago 14 10:37 libamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libatomic.so.1 -> libatomic.so.1.2.0*\n-rwxr-xr-x 1 jose jose   147600 ago 14 10:37 libatomic.so.1.2.0*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libbtf.so -> libbtf.so.1.2.6*\nlrwxrwxrwx 1 jose jose       15 ago 14 10:37 libbtf.so.1 -> libbtf.so.1.2.6*\n-rwxr-xr-x 1 jose jose    13108 ago 14 10:37 libbtf.so.1.2.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcamd.so -> libcamd.so.2.4.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcamd.so.2 -> libcamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    43470 ago 14 10:37 libcamd.so.2.4.6*\n-rwxr-xr-x 1 jose jose    28704 ago 14 10:37 libccalltest.so*\n-rwxr-xr-x 1 jose jose    39128 ago 14 10:37 libccalltest.so.debug*\nlrwxrwxrwx 1 jose jose       19 ago 14 10:37 libccolamd.so -> libccolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       19 ago 14 10:37 libccolamd.so.2 -> libccolamd.so.2.9.6*\n-rwxr-xr-x 1 jose jose    47652 ago 14 10:37 libccolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libcholmod.so -> libcholmod.so.3.0.13*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libcholmod.so.3 -> libcholmod.so.3.0.13*\n-rwxr-xr-x 1 jose jose  1005880 ago 14 10:37 libcholmod.so.3.0.13*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libcolamd.so -> libcolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       18 ago 14 10:37 libcolamd.so.2 -> libcolamd.so.2.9.6*\n-rwxr-xr-x 1 jose jose    31250 ago 14 10:37 libcolamd.so.2.9.6*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcurl.so -> libcurl.so.4.7.0*\nlrwxrwxrwx 1 jose jose       16 ago 14 10:37 libcurl.so.4 -> libcurl.so.4.7.0*\n-rwxr-xr-x 1 jose jose   654080 ago 14 10:37 libcurl.so.4.7.0*\n-rwxr-xr-x 1 jose jose    22120 ago 14 10:37 libdSFMT.so*\n-rwxr-xr-x 1 jose jose   758680 ago 14 10:37 libgcc_s.so.1*\nlrwxrwxrwx 1 jose jose       20 ago 14 10:37 libgfortran.so.4 -> libgfortran.so.4.0.0*\n\n```\n\ny en bin\n\n```bash\ntotal 245180\ndrwxrwxr-x 2 jose jose      4096 ago 14 10:50 ./\ndrwxrwxr-x 5 jose jose      4096 ago 14 10:37 ../\n-rwxrwxr-x 1 jose jose     17928 ago 14 10:50 decision_tree_app*\n-rwxrwxr-x 1 jose jose 251030272 ago 14 10:50 decision_tree_app.so*\n```\n\n\n## Utilizando la aplicación\n\nLo primero que comprobamos es si la aplicación funciona con el modelo que tenemos sobre iris. \n\nIntentamos predecir un fichero tal que así\n\n```bash\nhead test_to_predict.csv \nSepal.Length,Sepal.Width,Petal.Length,Petal.Width\n5.1,3.5,1.4,0.2\n4.9,3,1.4,0.2\n4.7,3.2,1.3,0.2\n4.6,3.1,1.5,0.2\n5,3.6,1.4,0.2\n5.4,3.9,1.7,0.4\n4.6,3.4,1.4,0.3\n5,3.4,1.5,0.2\n4.4,2.9,1.4,0.2\n\n``` \n\nAhora ejecutamos la app pasándole el modelo guardado en `train.jl` y el csv\n\n\n```bash\n╰─ $ ▶ time ../bin_blog/bin/decision_tree_app mimodelo.jlso test_to_predict.csv predicho.csv\n\nreal\t0m2,046s\nuser\t0m2,344s\nsys\t0m0,581s\n```\n\n```bash\n╰─ $ ▶ head -20 predicho.csv \nsetosa,versicolor,virginica\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n0.9,0.1,0.0\n0.95,0.05,0.0\n1.0,0.0,0.0\n1.0,0.0,0.0\n0.95,0.05,0.0\n```\n\n## Uso como motor para predecir.\n\nUna vez tenemos la app queremos utilizarla con otros modelos y otros datos sin necesidad de tener que compilar de nuevo. \n\nLo primero es usar las mismas versiones de las librerías que hemos usado en la app. Para eso copiamos los archivos Project.toml y Manifest.toml en otro directorio y activamos el entorno  con `activate .` e instalamos los paquetes con `instanstiate`\n\n```bash\n\n─ $ ▶ cd entorno_modelos/\n╭─ jose @ jose-PROX15 ~/Julia_projects/entorno_modelos\n│\n╰─ $ ▶ julia \n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.6.2 (2021-07-14)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\n(@v1.6) pkg> activate .\n  Activating environment at `~/Julia_projects/entorno_modelos/Project.toml`\n\n(decision_tree_app) pkg> instantiate\n\n\n```\n\nY ya podemos entrenar un nuevo modelo. En este caso voy a entrenar un modelo sobre los datos del precio de las viviendas en Boston, pero dónde he creado variables categórica que diferencie entre si el precio es mayor que 20 o menor, variable medv_20 con niveles (G20, NG20)\n\n```bash\n─ $ ▶ head data/boston.csv \n\"crim\",\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\",\"ptratio\",\"lstat\",\"medv_20\"\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98,\"G20\"\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14,\"G20\"\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03,\"G20\"\n0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,2.94,\"G20\"\n0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,5.33,\"G20\"\n0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,5.21,\"G20\"\n0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,12.43,\"G20\"\n0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,19.15,\"G20\"\n0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,29.93,\"NG20\"\n\n```\n\nY nuestro fichero de entrenamiento es el siguiente. Dejo solo lo de entrenar y guardar el modelo, otro día vemos la validación cruzada etc.. \n\n```julia\n# Training model julia\nusing CSV,CategoricalArrays, DataFrames, MLJ\n\n\ndf1 = CSV.read(\"data/boston.csv\", DataFrame)\n\nconst target =  CategoricalArray(df1[:, :medv_20])\nconst X = df1[:, Not(:medv_20)]\n\nTree = @load RandomForestClassifier pkg=DecisionTree\ntree = Tree(n_trees = 20)\ntree.max_depth = 5\n\n\nmach = machine(tree, X, target)\n\ntrain, test = partition(eachindex(target), 0.7, shuffle=true)\n\nfit!(mach, rows=train)\n\n\nMLJ.save(\"boston_rf.jlso\", mach, compression=:gzip)\n```\n\nY ya podemos usar nuestra aplicación compilada para predecir con el modelo que acabamos de entrenar. Para simular más un proceso real, usamos el conjunto de datos de boston pero con 5 millones de filas. \n\nFichero sin la variable a predecir\n```bash\n╰─ $ ▶ head -10  boston_to_predict.csv \ncrim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,lstat\n0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,4.98\n0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,9.14\n0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,4.03\n0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,2.94\n0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,5.33\n0.02985,0,2.18,0,0.458,6.43,58.7,6.0622,3,222,18.7,5.21\n0.08829,12.5,7.87,0,0.524,6.012,66.6,5.5605,5,311,15.2,12.43\n0.14455,12.5,7.87,0,0.524,6.172,96.1,5.9505,5,311,15.2,19.15\n0.21124,12.5,7.87,0,0.524,5.631,100,6.0821,5,311,15.2,29.93\n\n\n\n```\n\n```bash\n╰─ $ ▶ wc -l  boston_to_predict.csv \n5060001 boston_to_predict.csv\n\n```\n\nY ahora utilizamos nuestreo \"Motor de Modelos\" . Y en mi pc, tarda en predecir y escribir el resultado en torno a los 15 segundos. \n\n```bash\n╰─ $ ▶ time ../bin_blog/bin/decision_tree_app boston_rf.jlso boston_to_predict.csv res.csv\n\nreal\t0m14,080s\nuser\t0m28,949s\nsys\t0m3,442s\n\n```\n\n\n```bash\n╰─ $ ▶ wc -l res.csv \n5060001 res.csv\n╭─ jose @ jose-PROX15 ~/Julia_projects/entorno_modelos\n│\n╰─ $ ▶ head -10 res.csv \nG20,NG20\n0.95,0.05\n0.95,0.05\n1.0,0.0\n1.0,0.0\n1.0,0.0\n1.0,0.0\n0.85,0.15\n0.4,0.6\n0.25,0.75\n\n\n```\n\n\n## Conclusión \n\nEsto es sólo un ejemplo de como crear una aplicación compilada con Julia, en este caso para temas de \"machín lenin\", pero podría ser para otra cosa. \n\nComo ventaja, que podemos crear un tar.gz con toda la estructura creada en directorio `bin_blog` y ponerlo en otro linux y qué funcione sin necesidad de que sea la misma distribución de linux ni de que esté Julia instalado. Esto podría ser útil en entornos productivos en los que haya restricciones a la hora de instalar software. \n\nAún tengo que explorar como leer y escribir ficheros de s3 con [AWSS3.jl](https://github.com/JuliaCloud/AWSS3.jl) y más cosas relacionadas. \n\n\nHasta otra. \n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
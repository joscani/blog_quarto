{
  "hash": "802187ad544a8b11d104b4123d0ef779",
  "result": {
    "markdown": "---\ntitle: Análisis de correspondencias \"old_style\"\nauthor: jlcr\ndate: '2021-10-21'\nslug: análisis-de-correspondencias-old-style\ncategories:\n  - ciencia de datos\n  - estadística\n  - R\n  - correspondencias\n  - factorización\n  - factorial\n  - 2021\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n::: {.cell}\n\n:::\n\n\n\nQuién me conoce sabe que siento debilidad por el análisis de datos categóricos, en particular por técnicas como el análisis de correspondencias simple o múltiple o por las cosas más modernas que hay. \nNo en vano se me dió especialmente bien en la universidad, en parte debido a que por fin me centré después de unos años locos,  y en parte debido a algún buen profesor. El caso es que en el curro utilizamos este tipo de técnicas para encontrar relaciones entre variables categóricas que quizá hayan pasado desapercibidas en un primer análisis. \n\nAntes de nada voy a dar un par de referencias en castellano, bastante útiles. \n\n* [La práctica del análisis de correspondencias. Michael Greenacre](https://www.fbbva.es/wp-content/uploads/2017/05/dat/DE_2008_practica_analisis_correspondencias.pdf)\n* [Análisis de datos multivariantes. Daniel Peña](https://www.researchgate.net/publication/40944325_Analisis_de_Datos_Multivariantes)\n\n\nDe hecho el ejemplo que voy a contar y la notación que voy a usar viene en el libro de Daniel. Es un ejemplo de Fisher (si, ese al que todo el mundo odia hoy en día) de 1940, sobre la relación entre el color de los ojos (en filas) y el color del pelo (en columnas). Se trata de una simple tabla de contingencia. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(\"hair_eyes_color.csv\")\n\nrownames(df) <- df$color_ojos\n\ndf_tabla <- as.matrix(df[,-1])\n\ndf_tabla\n#>          rubio pelirrojo castaño oscuro negro\n#> claros     688       116     584    188     4\n#> azules     326        38     241    110     3\n#> castaños   343        84     909    412    26\n#> oscuros     98        48     403    618    85\n```\n:::\n\n\nCuyos totales por filas y columnas son \n\n\n::: {.cell}\n\n```{.r .cell-code}\naddmargins(df_tabla)\n#>          rubio pelirrojo castaño oscuro negro  Sum\n#> claros     688       116     584    188     4 1580\n#> azules     326        38     241    110     3  718\n#> castaños   343        84     909    412    26 1774\n#> oscuros     98        48     403    618    85 1252\n#> Sum       1455       286    2137   1328   118 5324\n```\n:::\n\n\n## Proyección de las filas.\n\nPodríamos plantearnos la relación entre las filas (color de ojos) , ¿cómo de similares son los que tienen los ojos claros con los que tienen los ojos azules, respecto a su color del pelo?\n\nParece claro que deberíamos centrarnos en los porcentajes por filas (perfiles fila)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.table(df_tabla, 1)\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646\n#> azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273\n#> castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144\n#> oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374\n```\n:::\n\n\nA partir de ahora vamos a usar la tabla de frecuencias relativas, cuyos elementos llamaremos $f_{ij}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(tabla_frecuencias <-  prop.table(df_tabla))\n#>               rubio   pelirrojo    castaño     oscuro        negro\n#> claros   0.12922615 0.021788129 0.10969196 0.03531180 0.0007513148\n#> azules   0.06123216 0.007137491 0.04526672 0.02066116 0.0005634861\n#> castaños 0.06442524 0.015777611 0.17073629 0.07738542 0.0048835462\n#> oscuros  0.01840721 0.009015778 0.07569497 0.11607814 0.0159654395\n```\n:::\n\n\nQue tiene los mismos porcentajes por filas. \n\n::: {.cell}\n\n```{.r .cell-code}\n(perfiles_fila <- prop.table(tabla_frecuencias, 1))\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.43544304 0.07341772 0.3696203 0.1189873 0.002531646\n#> azules   0.45403900 0.05292479 0.3356546 0.1532033 0.004178273\n#> castaños 0.19334837 0.04735062 0.5124014 0.2322435 0.014656144\n#> oscuros  0.07827476 0.03833866 0.3218850 0.4936102 0.067891374\n```\n:::\n\n\nSe podría considerar utilizar la distancia euclídea para ver como de parecidas son las filas ojos claros y ojos azules, pero eso presenta un problema, que es la distribución del color del pelo en la tabla, dónde por ejemplo el porcentaje de rubios es mayor que el de pelirrojos. Así que usar esa distancia no sería justo. Podríamos definir una distancia ponderada que fuera $d(i, i') = \\sum_j ((f_{ij} - f_{i'j})^2/f_{.j})$ dónde $f_{.j}$ es la distribución de las columnas en la población, vamos, qué % de rubios, pelirrojos, etc hay en mis datos. \n\nEn forma matricial, en notación de Daniel sería ![](eq2.png) \n\nEsta distancia es equivalente a la euclídea en la matriz Y definida como dividir cada elemento de los perfiles fila por la raíz cuadrada del peso de la columna. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# peso de loas columnas\n(f.j <- colSums(tabla_frecuencias))\n#>      rubio  pelirrojo    castaño     oscuro      negro \n#> 0.27329076 0.05371901 0.40138993 0.24943651 0.02216379\n\n# peso de las filas\n(fi. <- rowSums(tabla_frecuencias))\n#>    claros    azules  castaños   oscuros \n#> 0.2967693 0.1348610 0.3332081 0.2351615\n```\n:::\n\n\nPara hacerlo lo hacemos usando matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# matriz diagonal con la raíz de los porcentjes totales de las columnas( col masses)\n(DC_12 <- diag(1/sqrt(f.j)))\n#>          [,1]     [,2]     [,3]     [,4]     [,5]\n#> [1,] 1.912879 0.000000 0.000000 0.000000 0.000000\n#> [2,] 0.000000 4.314555 0.000000 0.000000 0.000000\n#> [3,] 0.000000 0.000000 1.578399 0.000000 0.000000\n#> [4,] 0.000000 0.000000 0.000000 2.002258 0.000000\n#> [5,] 0.000000 0.000000 0.000000 0.000000 6.717041\n\n# Matriz diagonal con 1/fi. \n(DF_1 <- diag(1/fi.))\n#>         [,1]     [,2]     [,3]     [,4]\n#> [1,] 3.36962 0.000000 0.000000 0.000000\n#> [2,] 0.00000 7.415042 0.000000 0.000000\n#> [3,] 0.00000 0.000000 3.001127 0.000000\n#> [4,] 0.00000 0.000000 0.000000 4.252396\n```\n:::\n\n\nMatriz Y \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# se puede hacer usando los perfiles fila o directamente utilizando DF_1 y DC_12\n# Y <- perfiles_fila %*% DC_12\n\nY <- DF_1 %*% as.matrix(tabla_frecuencias) %*% DC_12\nrownames(Y) <- rownames(tabla_frecuencias)\ncolnames(Y) <- colnames(tabla_frecuencias)\nY\n#>              rubio pelirrojo   castaño    oscuro      negro\n#> claros   0.8329499 0.3167648 0.5834082 0.2382433 0.01700517\n#> azules   0.8685217 0.2283469 0.5297968 0.3067526 0.02806563\n#> castaños 0.3698521 0.2042969 0.8087737 0.4650114 0.09844593\n#> oscuros  0.1497302 0.1654142 0.5080629 0.9883349 0.45602916\n```\n:::\n\n\nEn esta tabla Y, la distancia euclídea entre filas coincide con la distancia ponderada que habíamos definido dónde la distancia entre dos filas venía ponderada por el peso de cada columna. \n\nPodríamos ahora plantearnos utilizar una descomposición en valores y vectores propios sobre esta tabla, pero tendríamos el problema de que el peso de cada fila sería el mismo, por eso se hace necesario tener en cuenta el peso de cada fila. \n\nPodemos construir ahora una matriz __Z__ dónde se pondere por el peso de las filas y el de las columnas. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cremaos matriz diagonal con 1/sqrt(fi.)\n(DF_12 <- diag(1 / sqrt(fi.)))\n#>          [,1]     [,2]     [,3]     [,4]\n#> [1,] 1.835653 0.000000 0.000000 0.000000\n#> [2,] 0.000000 2.723057 0.000000 0.000000\n#> [3,] 0.000000 0.000000 1.732376 0.000000\n#> [4,] 0.000000 0.000000 0.000000 2.062134\n```\n:::\n\n\nCreamos matriz Z como \n\n\n::: {.cell}\n\n```{.r .cell-code}\nZ <- DF_12 %*% as.matrix(tabla_frecuencias) %*% DC_12\n\nrownames(Z) <- rownames(tabla_frecuencias)\ncolnames(Z) <- colnames(tabla_frecuencias)\nZ\n#>               rubio  pelirrojo   castaño    oscuro       negro\n#> claros   0.45376229 0.17256250 0.3178206 0.1297867 0.009263827\n#> azules   0.31895094 0.08385681 0.1945596 0.1126501 0.010306662\n#> castaños 0.21349407 0.11792869 0.4668580 0.2684240 0.056827106\n#> oscuros  0.07260933 0.08021509 0.2463773 0.4792778 0.221144304\n```\n:::\n\n\nSobre esta matriz Z que no es más que la tabla de frecuencias relativas estandarizada por el peso de las filas y el de las columnas podemos diagonalizar la matriz __Z'Z__.\nEsta matriz tiene un valor propio igual a 1, pero los importantes son los siguientes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_diag <- eigen(t(Z) %*% Z)\nres_diag$values\n#> [1]  1.000000e+00  1.873957e-01  2.847581e-02  9.026139e-04 -5.424744e-18\nres_diag$vectors\n#>            [,1]        [,2]        [,3]        [,4]        [,5]\n#> [1,] -0.5227722 -0.64488569  0.50331668 -0.21984084  0.09578107\n#> [2,] -0.2317736 -0.11671207  0.06710395  0.91248080 -0.30908759\n#> [3,] -0.6335534 -0.02776729 -0.74923836 -0.04663792  0.18521831\n#> [4,] -0.4994362  0.65005451  0.30376531 -0.21371886 -0.43593979\n#> [5,] -0.1488751  0.38361288  0.29755318  0.26682945  0.81911020\n```\n:::\n\n\nTeniendo la matriz Y, que son los perfiles (porcentajes) fila ponderados  por el peso de las columnas, podemos proyectar esas filas sobre las dimensiones obtenidas por los vectores propios obtenidos asociados a los autovalores menores que 1. Esa será la mejor representación de las filas en un subespacio de las columnas. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# proyeccion 1. con valor menor que 1 \nvector_propio1 <- res_diag$vectors[,2]\nvector_propio2 <- res_diag$vectors[,3]\n\n(coord1 <- Y %*% vector_propio1 )\n#>                 [,1]\n#> claros   -0.42893286\n#> azules   -0.39128686\n#> castaños  0.05523421\n#> oscuros   0.68743802\n(coord2 <- Y %*% vector_propio2 )\n#>                 [,1]\n#> claros    0.08081194\n#> azules    0.15705214\n#> castaños -0.23555524\n#> oscuros   0.14171621\n```\n:::\n\n\nY ya lo puedo pintar \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n(to_plot <-  data.frame(Dim1 = coord1[,1], Dim2 = coord2[,1], color_ojos= rownames(Y)))\n#>                 Dim1        Dim2 color_ojos\n#> claros   -0.42893286  0.08081194     claros\n#> azules   -0.39128686  0.15705214     azules\n#> castaños  0.05523421 -0.23555524   castaños\n#> oscuros   0.68743802  0.14171621    oscuros\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nto_plot %>% \n  ggplot(aes(x=Dim1, y=Dim2)) +\n  geom_label(aes(label= color_ojos)) +\n  scale_x_continuous(limits = c(-0.8,0.8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=80%}\n:::\n:::\n\n\n## Proyección de las columnas\n\nLa proyección de las columnas en un subespacio de las filas se hace de manera análoga , solo que en vez de diagonalizar __Z'Z__ se hace con __Z Z'__, que tiene los mismos valores propios que los obtenidos. \n\nDe aquí viene la relación baricéntrica entre las filas y las columnas. \n\n## Relación con la distancia Chi-cuadrado\n\nSi se desarrolla la expresión de la distancia Chi-cuadrado , tal como se hace en el libro de Daniel Peña se llega a que se corresponde con la distancia euclídea en la matriz __Y__ que son los perfiles filas  ponderados por el peso de cada columna. \n\nEsta implicación es importante, puesto que al descomponer el estadístico Chi-cuadrado que nos mide la asociación entre variables categóricas (filas y columnas), estamos descubriendo qué filas están asociados con determinadas columnas. \n\n## Uso con FactoMineR \n\nSólo trataba de dar una pequeña explicación de la relación entre el análisis de correspondencias y la diagonalización de matrices. Hay mucha más explicación en los libros que he enlazado al principio. En el día a día, podemos usar librerías específicas para calcular este análisis, como `FactoMineR` en R o `prince` en python. \n\nVeamos como se usa con `FactoMiner`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(FactoMineR)\nlibrary(factoextra) # pa los dibujitos\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres_ca <- CA(df_tabla, graph = FALSE)\n```\n:::\n\n\nY podemos pintar las filas en el espacio de las columnas \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_ca_row(res_ca) +\n   scale_x_continuous(limits = c(-0.8,0.8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=80%}\n:::\n:::\n\n\n\nY vemos que las coordenadas son las mismas que hemos obtenido nosotros antes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_ca$row$coord\n#>                Dim 1       Dim 2        Dim 3\n#> claros   -0.42893286  0.08081194  0.032336831\n#> azules   -0.39128686  0.15705214 -0.065353061\n#> castaños  0.05523421 -0.23555524 -0.005724586\n#> oscuros   0.68743802  0.14171621  0.004781725\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nto_plot\n#>                 Dim1        Dim2 color_ojos\n#> claros   -0.42893286  0.08081194     claros\n#> azules   -0.39128686  0.15705214     azules\n#> castaños  0.05523421 -0.23555524   castaños\n#> oscuros   0.68743802  0.14171621    oscuros\n```\n:::\n\n\n\nLa representación conjunta. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_ca(res_ca) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=80%}\n:::\n:::\n\n\nLa librería `FactoMineR` junto con `factoextra` devuelven también múltiples ayudas a la interpretación como la contribución de cada fila o columna a la estructura factorial etc. \nPor otro lado, la librería `FactoInvestigate`  que toma como input un análisis factorial (pca, ca o mca), devuelve un informe en inglés (en Rmd) describiendo lo que significa cada dimensión obtenida. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
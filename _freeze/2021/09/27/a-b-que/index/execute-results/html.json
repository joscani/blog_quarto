{
  "hash": "6b7145e1d7b7fcf027969b06ba6ef876",
  "result": {
    "markdown": "---\ntitle: ¿A/B qué?\nauthor: jlcr\ndate: '2021-09-27'\ncategories:\n  - análisis bayesiano\n  - R\n  - 2021\nexecute: \n  message: false\n  warning: false\n  echo: true\nformat: \n  html: \n    fig-height: 5\n    fig-dpi: 300\n    fig-width: 8.88\n    fig-align: center\nknitr:\n  opts_chunk:\n    out.width: 80%\n    fig.showtext: TRUE\n    collapse: true\n    comment: \"#>\"\n---\n\n::: {.cell}\n\n:::\n\n\n\nRecuerdo siendo yo más bisoño cuando escuché a los marketinianos hablar del A/B testing para acá , A/B testing para allá. En mi ingenuidad pensaba que era alguna clase de rito que sólo ellos conocían, y encima lo veía como requisito en las ofertas de empleo que miraba. \n\nMi decepción fue mayúscula cuando me enteré que esto del A/B testing no es más que un nombre marketiniano para hacer un contraste de proporciones o contrastes de medias, vamos, un prop.test o un t.test, ya que ni siquera trataban el caso de tener varios grupos o la existencia de covariables. Ains, esas dos asignaturas en la carrera de diseño de experimentos y de ver fórmulas y sumas de cuadrados a diestro y siniestro, esos anovas, y ancovas. \n\nTotal que hoy vengo a contar alguna forma diferente a la de la fórmula para hacer este tipo de contrastes. \n\nSupongamos que tenemos los siguientes datos, inventados\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <-  data.frame( \n  exitos         = c(2, 200,  10, 20,  4,  200, 300,  20,  90,  90),\n  fracasos       = c(8, 1000, 35, 80, 20,  400, 900, 400, 230, 150) ,\n  gcontrol       = factor(c(1,0,1,1,1,0,0,0,0,0)))\n\ndf$n = df$exitos + df$fracasos\ndf\n#>    exitos fracasos gcontrol    n\n#> 1       2        8        1   10\n#> 2     200     1000        0 1200\n#> 3      10       35        1   45\n#> 4      20       80        1  100\n#> 5       4       20        1   24\n#> 6     200      400        0  600\n#> 7     300      900        0 1200\n#> 8      20      400        0  420\n#> 9      90      230        0  320\n#> 10     90      150        0  240\n```\n:::\n\n\n\n\nTenemos 10 experimentos binomiales y hemos obtenido esos resultados, (podría ser por ejemplo la proporción de clientes que han contratado un producto A en 10 meses)\n\nPodriamos ver la proporción en cada fila \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf$prop <- df$exitos/df$n\ndf\n#>    exitos fracasos gcontrol    n       prop\n#> 1       2        8        1   10 0.20000000\n#> 2     200     1000        0 1200 0.16666667\n#> 3      10       35        1   45 0.22222222\n#> 4      20       80        1  100 0.20000000\n#> 5       4       20        1   24 0.16666667\n#> 6     200      400        0  600 0.33333333\n#> 7     300      900        0 1200 0.25000000\n#> 8      20      400        0  420 0.04761905\n#> 9      90      230        0  320 0.28125000\n#> 10     90      150        0  240 0.37500000\n\nggplot(df, aes(prop, fill = gcontrol)) +\n  geom_density(alpha = 0.3) +\n  labs(fill = \"Gcontrol\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=80%}\n:::\n:::\n\n\nPues como ahora me estoy volviendo bayesiano, ¿por qué no ajustar un modelo bayesiano a estos datos y obtener la posteriori de cada uno de las proporciones y de su diferencia. Vamos a ajustar lo que a veces se denomina una regresión binomial, dónde tenemos éxitos y ensayos. Normalmente la gente está acostumbrada a ajustar regresiones logísticas dónde la variable dependiente es 1 o 0, en este caso, la información está agregada, pero es equivalente. \n\nAl tener el número de \"ensayos\" de cada experimento, se va a tener en cuenta, de forma que no va a ser lo mismo un experimento con 20 ensayos que uno con 200, aun cuando tengan la misma proporción.\n\nUsando la librería brms y stan sería así de sencillo\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nlibrary(cmdstanr)\nset_cmdstan_path(\"~/cmdstan/\")\n\nprior <-  get_prior(exitos | trials(n) ~  0 + gcontrol , \n                    data = df, \n                    family = binomial)\n\nmod_brm <-\n    brm(data = df, family = binomial,\n        exitos | trials(n) ~ 0 + gcontrol,\n        prior = prior,\n        iter = 2500, warmup = 500, cores = 6, chains = 6, \n        seed = 10, \n        backend = \"cmdstanr\")\n#> Running MCMC with 6 parallel chains...\n#> \n#> Chain 1 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 1 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 1 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 1 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 1 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 1 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 1 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 1 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 1 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 1 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 1 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 1 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 1 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 1 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 1 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 1 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 1 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 1 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 1 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 1 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 1 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 1 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 1 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 1 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 1 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 1 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 1 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 2 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 2 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 2 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 2 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 2 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 2 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 2 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 2 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 2 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 2 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 2 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 2 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 2 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 2 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 2 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 2 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 2 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 2 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 2 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 2 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 2 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 2 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 2 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 2 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 2 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 2 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 2 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 3 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 3 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 3 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 3 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 3 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 3 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 3 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 3 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 3 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 3 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 3 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 3 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 3 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 3 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 3 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 3 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 3 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 3 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 3 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 3 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 3 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 3 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 3 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 3 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 3 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 3 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 3 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 4 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 4 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 4 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 4 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 4 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 4 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 4 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 4 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 4 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 4 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 4 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 4 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 4 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 4 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 4 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 4 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 4 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 4 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 4 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 4 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 4 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 4 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 4 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 4 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 4 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 4 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 4 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 5 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 5 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 5 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 5 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 5 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 5 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 5 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 5 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 5 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 5 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 5 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 5 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 5 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 5 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 5 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 5 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 5 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 5 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 5 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 5 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 5 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 5 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 5 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 5 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 5 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 5 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 5 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 6 Iteration:    1 / 2500 [  0%]  (Warmup) \n#> Chain 6 Iteration:  100 / 2500 [  4%]  (Warmup) \n#> Chain 6 Iteration:  200 / 2500 [  8%]  (Warmup) \n#> Chain 6 Iteration:  300 / 2500 [ 12%]  (Warmup) \n#> Chain 6 Iteration:  400 / 2500 [ 16%]  (Warmup) \n#> Chain 6 Iteration:  500 / 2500 [ 20%]  (Warmup) \n#> Chain 6 Iteration:  501 / 2500 [ 20%]  (Sampling) \n#> Chain 6 Iteration:  600 / 2500 [ 24%]  (Sampling) \n#> Chain 6 Iteration:  700 / 2500 [ 28%]  (Sampling) \n#> Chain 6 Iteration:  800 / 2500 [ 32%]  (Sampling) \n#> Chain 6 Iteration:  900 / 2500 [ 36%]  (Sampling) \n#> Chain 6 Iteration: 1000 / 2500 [ 40%]  (Sampling) \n#> Chain 6 Iteration: 1100 / 2500 [ 44%]  (Sampling) \n#> Chain 6 Iteration: 1200 / 2500 [ 48%]  (Sampling) \n#> Chain 6 Iteration: 1300 / 2500 [ 52%]  (Sampling) \n#> Chain 6 Iteration: 1400 / 2500 [ 56%]  (Sampling) \n#> Chain 6 Iteration: 1500 / 2500 [ 60%]  (Sampling) \n#> Chain 6 Iteration: 1600 / 2500 [ 64%]  (Sampling) \n#> Chain 6 Iteration: 1700 / 2500 [ 68%]  (Sampling) \n#> Chain 6 Iteration: 1800 / 2500 [ 72%]  (Sampling) \n#> Chain 6 Iteration: 1900 / 2500 [ 76%]  (Sampling) \n#> Chain 6 Iteration: 2000 / 2500 [ 80%]  (Sampling) \n#> Chain 6 Iteration: 2100 / 2500 [ 84%]  (Sampling) \n#> Chain 6 Iteration: 2200 / 2500 [ 88%]  (Sampling) \n#> Chain 6 Iteration: 2300 / 2500 [ 92%]  (Sampling) \n#> Chain 6 Iteration: 2400 / 2500 [ 96%]  (Sampling) \n#> Chain 6 Iteration: 2500 / 2500 [100%]  (Sampling) \n#> Chain 1 finished in 0.1 seconds.\n#> Chain 2 finished in 0.1 seconds.\n#> Chain 3 finished in 0.1 seconds.\n#> Chain 4 finished in 0.1 seconds.\n#> Chain 5 finished in 0.1 seconds.\n#> Chain 6 finished in 0.1 seconds.\n#> \n#> All 6 chains finished successfully.\n#> Mean chain execution time: 0.1 seconds.\n#> Total execution time: 0.8 seconds.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(mod_brm) %>% \n    round(digits = 2)\n#>           Estimate Est.Error  Q2.5 Q97.5\n#> gcontrol0    -1.23      0.04 -1.31 -1.16\n#> gcontrol1    -1.39      0.19 -1.77 -1.03\n```\n:::\n\n\nY ya tengo la estimación de cada proporción sin más que hacer el invlogit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(mod_brm) %>% \n    round(digits = 2) %>% \n  inv_logit_scaled()\n#>            Estimate Est.Error      Q2.5     Q97.5\n#> gcontrol0 0.2261814 0.5099987 0.2124868 0.2386673\n#> gcontrol1 0.1994078 0.5473576 0.1455423 0.2630841\n```\n:::\n\nUna cosa buena de la estimación bayesiana es que tengo la posteriori completa de ambas proporciones\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- as_tibble(mod_brm)\npost\n#> # A tibble: 12,000 × 4\n#>    b_gcontrol0 b_gcontrol1 lprior  lp__\n#>          <dbl>       <dbl>  <dbl> <dbl>\n#>  1       -1.17       -1.39      0 -128.\n#>  2       -1.18       -1.38      0 -128.\n#>  3       -1.20       -1.10      0 -128.\n#>  4       -1.24       -1.45      0 -127.\n#>  5       -1.23       -1.79      0 -129.\n#>  6       -1.21       -1.60      0 -128.\n#>  7       -1.21       -1.62      0 -128.\n#>  8       -1.19       -1.68      0 -129.\n#>  9       -1.19       -1.59      0 -128.\n#> 10       -1.30       -1.54      0 -129.\n#> # … with 11,990 more rows\n```\n:::\n\n\nY tenemos 2000 muestras por 6 cadenas, 12000 muestras aleatorias de cada proporción. \n\n\nAhora puedo hacer cosas como ver la distribución a posteriori de la diferencia\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost$diferencia = inv_logit_scaled(post$b_gcontrol1) - inv_logit_scaled(post$b_gcontrol0)\n\nggplot(post, aes(diferencia)) +\n  geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=80%}\n:::\n:::\n\n\nIntervalo de credibilidad al 80%\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(post$diferencia, probs = c(0.1,0.9))\n#>         10%         90% \n#> -0.06377131  0.01499445\n```\n:::\n\n\nY si sospechamos que hay más estructura en nuestros datos podemos modelarla igulmente, por ejemplo las proporciones podrían tener relación con el mes del año o con cualquier otra cosa. \n\nEn fin, un método alternativo para hacer A/B testing o como se llame ahora. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}